Under review as a conference paper at ICLR 2019

SELF-TUNING NETWORKS: BILEVEL OPTIMIZATION OF HYPERPARAMETERS USING STRUCTURED BEST-RESPONSE FUNCTIONS
Anonymous authors Paper under double-blind review

ABSTRACT
Hyperparameter optimization is a bi-level optimization problem, where the optimal parameters on the training set depend on the current hyperparameters. The best-response function which maps hyperparameters to these optimal parameters allows gradient-based hyperparameter optimization but is difficult to represent and compute when the parameters are high dimensional, as in neural networks. We develop efficient best-response approximations for neural networks by applying insights from the structure of the optimal response in a Jacobian-regularized two-layer linear network to deep, nonlinear networks. The approximation works by scaling and shifting the hidden units by amounts which depend on the current hyperparameters. We use our approximation for a gradient-based hyperparameter optimization algorithm which alternates between approximating the bestresponse in a neighborhood around the current hyperparameters and optimizing the hyperparameters using the approximate best-response. We show this method outperforms competing hyperparameter optimization methods on large-scale deep learning problems. We call our networks, which update their own hyperparameters online during training, Self-Tuning Networks.

1 INTRODUCTION

Regularization hyperparameters such as weight decay, data augmentation, and dropout (Srivastava et al., 2014) are crucial to the generalization of neural networks (NNs), but are difficult to tune. Popular approaches to hyperparameter optimization (HO) include grid search, random search (Bergstra & Bengio, 2012), and Bayesian optimization (Snoek et al., 2012). These approaches work well with low-dimensional hyperparameters and ample computational resources. However, they pose HO as a black-box optimization problem, ignoring structure which could be exploited for faster convergence.

We can formulate HO as a bilevel optimization problem. Let  and  denote parameters and hyperparameters, and let LT and LV be functions mapping parameters and hyperparameters to the loss on the training and validation sets. We aim to solve1 :

 = arg min LV (, ) subject to  = arg min LT (, )


(1)

Substituting the best-response function () = arg min LT (, ) gives a single-level problem:

 = arg min LV (, ())


(2)

Gradient-based HO methods attempt to solve Problem 1 through approximation of the best-response or its Jacobian. It is possible to do so by differentiating through the training procedure (Maclaurin et al., 2015) or by implicitly deriving the best-response Jacobian (Larsen et al., 1996), but these methods struggle with discrete hyperparameters, and are computationally expensive. Recently, Lorraine & Duvenaud (2018) proposed directly approximating the best-response by a parametric function ^. They jointly optimized  and , optimizing  so ^   in a neighborhood around the

1Uniqueness of the arg min is assumed.

1

Under review as a conference paper at ICLR 2019

current hyperparameters, and optimizing  by using ^ as a proxy for  in Eq. 2:

  arg min LV (, ^())


(3)

This worked well for small-scale problems on MNIST using L2 regularization, but it is difficult to represent and compute ^ when  is high-dimensional.

In this paper, we construct scalable and compact best-response approximations for NNs. We analyze the best-response structure in a Jacobian-regularized two-layer linear network, and show it can be modelled as scaling network's hidden units by amounts which depend on the current hyperparameters. Exploiting this structure, we extend the approach to deep, nonlinear networks, giving a memory efficient best-response approximation which scales to modern architectures.

Using this approximation for HO gives a gradient-based HO algorithm that is (1) computationally inexpensive, (2) can optimize all regularization hyperparameters, including discrete hyperparameters, and (3) scales to large NNs. We call the resulting networks Self-Tuning Networks (STNs), since they update their own hyperparameters online during training. We empirically evaluate the performance of STNs for HO on large-scale deep-learning problems on the PTB and CIFAR-10 datasets. We compare their performance to other popular HO approaches and find that STNs achieve lower validation loss in less time.

2 BILEVEL OPTIMIZATION

A bilevel optimization problem consists of two sub-problems called the upper-level and lower-level problems, where the upper level problem must be solved subject to optimality of the lower-level problem. Minimax problems are a subclass of bilevel programs. Bilevel problems were first studied in economics to model leader/follower firm dynamics (Von Stackelberg, 2010) and have since found uses in many different fields (see Colson et al. (2007) for an overview). In machine learning, many problems can be formulated as bilevel programs, including HO, GAN training (Goodfellow et al., 2014), meta-learning, and neural architecture search (Zoph & Le, 2016).

Solving bilevel problems exactly is difficult. Even if all objectives and constraints are linear, bilevel
problems are strongly NP-hard (Hansen et al., 1992; Vicente et al., 1994). Due to the difficulty of
obtaining exact solutions, most work has focused on restricted settings, considering linear, quadratic,
and convex functions. In contrast, we focus on obtaining local solutions in the nonconvex, differentiable, and unconstrained setting. We let F, f : Rn × Rm  R denote the upper- and lower-level objectives and   Rn,   Rm denote the upper- and lower-level variables. We aim to solve:

min F (, )
Rn

(4a)

subject to   arg min f (, )
Rm

(4b)

2.1 GRADIENT DESCENT VIA THE BEST-RESPONSE FUNCTION

Using gradient information provides drastic speed-ups over black-box optimization methods (Nes-

terov, 2013). Thus, it is desirable to design a gradient-based algorithm for solving Problem 4. The

simplest

method

is

simultaneous

gradient

descent,

updating



using

F 

and



using

f 

.

However,

simultaneous gradient descent fails if  does not affect the upper-level objective explicitly, i.e. if

F 

 0. In

lower-level

this case, one solution is Problem 4b has a unique

to use the optimum

best-response function (Gibbons, 1992). Assume the () for each . Substituting , the best-response

function, converts Problem 4 into a single-level problem:

min F () := F (, ())
Rn

(5)

If  is differentiable, we can minimize Eq. 5 using gradient descent on F  w.r.t. . This method requires a unique optimum for Problem 4b for each  and differentiability of . In general, this is difficult to verify. We give sufficient conditions, which almost certainly do not hold in practice, for this to be the case in a neighborhood of a point (0, 0) where 0 solves Problem 4b given 0.

2

Under review as a conference paper at ICLR 2019

Lemma 1. (Fiacco & Ishizuka, 1990) Let 0 solve Problem equation 4b for 0. Suppose f is

C2 in a neighborhood of (0, 0) and the Hessian neighborhood U of 0, there exists a continuously

di2fff2e(ren0t,iab0)leisfupnocstiitoinvedefi: nUite. TRhemn

for some such that

() is the unique solution to Problem equation 4b given  for all   U and (0) = 0.

Proof. See Appendix A.1.

2.2 APPROXIMATING THE BEST-RESPONSE FUNCTION

Although the solution to Problem 4b is, in general, a set, assuming uniqueness of a solution and

differentiability of , as is often done in the HO literature, can yield fruitful algorithms in practice.

Gradient-based HO methods can be interpreted as approximating either the best-response  or

its

Jacobian

 

.

The most popular approaches, described in Section 5, either approximate 

by

differentiating

gradient

descent

or

implicitly

derive

 

.

However, these approaches can be

computationally expensive. When applied to HO, they struggle with discrete hyperparameters and

some continuous hyperparameters like dropout probabilities. Alternative approaches which directly approximate  were proposed by Lorraine & Duvenaud (2018). We detail their algorithms below.

1. Global Approximation. The first algorithm proposed by Lorraine & Duvenaud (2018) approximated  as a differentiable function ^ with parameters . If  represents NN weights, then the mapping ^ is a hypernetwork (Lorraine & Duvenaud, 2018; Schmidhuber, 1992; Ha et al., 2016). A distribution p() is fixed then gradient descent w.r.t.  minimizes:

Ep() f (, ^())

(6)

If support(p) is broad and ^ is sufficiently flexible, then Eq. 6 is minimized exactly and ^ is used as a proxy for  in Problem 5, resulting in the following objective F^:

min F^(, ) := F (, ^())
Rn

(7)

2. Local Approximation. In practice, ^ is usually insufficiently flexible to model  on support(p). The second algorithm of Lorraine & Duvenaud (2018) locally approximated  in a
neighborhood around the current upper-level parameter . A noise distribution p( |) parametrized by a scale parameter   R+n is fixed, then  is found by minimizing the objective:

f^(, , ) = E p( |) f ( + , ^( + ))

(8)

Intuitively, the upper-level parameter  is perturbed by a small amount, so the lower-level parameter learns how to respond. An alternating gradient descent scheme is then used, where  is updated to minimize equation 8 and  is updated to minimize equation 7. This approach worked well for problems using L2 weight decay on MNIST (LeCun et al., 1998). However, it is unclear if the approach works with different regularizers and whether it scales to larger problems. It requires ^, which is a priori unwieldy for high dimensional . It is also unclear how to set the scale , which defines the size of the neighborhood on which  is trained, and whether the approach can be adapted
to discrete hyperparameters.

3 SELF-TUNING NETWORKS
We first construct a compact best-response approximation ^ that is memory efficient and scales to large NNs. We then describe a method to automatically adjust the size of the neighborhood on which  is trained during training. We formally describe our algorithm and discuss how it easily handles discrete hyperparameters. We call the resulting networks, which update their own hyperparameters online during training, self-tuning networks (STNs).

3

Under review as a conference paper at ICLR 2019
Mask the hidden state Change to principal
component basis Matmul

Project to 1-D Space Matmul

Matmul

Add

Construct -dependent mask

Figure 1: Best-response architecture for an L2-Jacobian regularized two-layer linear network.

3.1 COMPACT BEST-RESPONSE APPROXIMATIONS FOR NEURAL NETWORKS

When  represents the weights of a NN, ^ must be flexible enough to model how the weights should respond, while remaining memory-efficient and tractable for larger networks. We begin by investigating the structure of the best-response in a simpler setting to glean insights which can be applied to more complicated problems. Consider a regression problem where a 2-layer linear network with weights  = (Q, s)  RD×D × RD predicts targets t from inputs x  RD:

a(x; ) = Qx, y(x; ) = s, a(x; )

(9)

Suppose

we

use

a

squared-error

loss

regularized

with

an

L2

penalty

on

the

Jacobian

y x

,

where

the

penalty weight hyperparameter is reparametrized using exp to lie in R rather than R+:

LT (, ) =

(y(x;

)

-

t)2

+

1 |D|

exp()

y 2 (x; )
x

(x,t)D

(10)

Lemma 2. Let 0 = (Q0, s0), where Q0 is the change-of-basis matrix to the principal components of the data matrix and s0 solves the unregularized version of Problem 10 given Q0. Then there exist w, b  RD such that the best-response function2 () = (Q(), s()) for Problem 10 is:

Q() = (w + b) Q0, s() = s0

(11)

Here, indicates row-wise rescaling.

Proof. See Appendix A.2.

y(x; ()) implements y(x; 0) with an additional sigmoidal gating of the hidden units: a(x; ()) = Q()x = (w + b) (Q0x) = (w + b) a(x; 0)

(12)

We show this architecture as Figure 1. Inspired by this example, we use a linearized version as
a best-response architecture for deep nonlinear networks. Our architecture scales and shifts the
rows of the weights and biases (equivalently, the hidden units of the network) by amounts which depend on the hyperparameters. For a layer's weight matrix W and a bias b, we approximate the best-response as3:

W () = Wshift + , u Wscale, b() = bshift + , v bscale,

(13)

This best-response architecture is tractable to compute and memory-efficient: if a normal layer uses p weights, it uses 2p + n weights. There is no guarantee this architecture can represent the best-response function in general, but intuitively it makes sense that some hidden units should be (de-)emphasized for stronger regularization. NN generalization is strongly related to the network's Jacobian norm (Novak et al., 2018), suggesting that the structure of the best-response from Problem 10 may carry over to other types of regularization. Louizos & Welling (2017) showed probability distributions induced by noisily scaling activations could be complex and multimodal.

This architecture allows the use of different perturbed hyperparameters for each minibatch element since the activations of each batch element can be scaled and shifted separately, enabling ^ to quickly learn how to respond in a neighborhood around the current hyperparameter values.

2This is an abuse of notation since there is not a unique solution to Problem 10 for a given . 3We describe modifications for convolutional filters in Appendix B.

4

Under review as a conference paper at ICLR 2019

3.2 ADAPTING THE HYPERPARAMETER DISTRIBUTION

The entries of , which define the size of the neighborhood  is trained on, are crucial to the STN's performance. If the entries are too large, then ^ will not be flexible enough to capture the bestresponse over the sampled neighborhood. However, its entries must remain sufficiently large so that ^ captures the local shape around the current hyperparameter values. As the smoothness of the loss landscape changes during training, it may be beneficial to enlarge or shrink the entries of .
To address these issues we propose adjusting  during training based on the sensitivity of the upperlevel objective to sampled hyperparameters. We include an entropy term weighted by   [0, 1] which enlarges entries of . The resulting objective is:

F^V (, , ) = E p( |)[F ( + , ^( + ))] -  H[p( |)]

(14)

The resulting objective is similar to a variational inference (VI) objective, where the first term is analogous to the negative-log likelihood, but  = 1. As  ranges from 0 to 1, our objective interpolates between variational optimization (Staines & Barber, 2012) and VI, as noted by Khan et al. (2018). Similar objectives have been used in the VI literature for better training (Blundell et al., 2015) and representation learning (Higgins et al., 2016).
On its own, minimizing the first term eventually moves all probability mass towards an optimum , resulting in  = 0. This compels  to achieve a balance between shifting towards 0 to achieve better performance on the first term while remaining sufficiently high to avoid a heavy entropy penalty.  must be set carefully to ensure the  is not pushed too high. When benchmarking our algorithm's performance, we evaluate F^ at the current hyperparameter 0 and do not evaluate over a range as required by F^V . This is common practice when using stochastic operations during training, e.g., batch-norm or dropout.

3.3 TRAINING ALGORITHM

We now describe the complete STN training algorithm and discuss how it can tune hyperparam-
eters which other gradient-based algorithms cannot, such as discrete hyperparameters. We use an unconstrained parametrization   Rn of the hyperparameters. We let r denote the element-wise function which maps  to the appropriate constrained space. These element-wise transformations
will involve a non-differentiable discretization for discrete hyperparameters.

We let LT and LV denote training and validation functions which are (possibly stochastic, e.g., if
using dropout) functions of the hyperparameters and model parameters, and define functions f, F
by f (, ) = LT (r(), ) and F (, ) = LV (r(), ). STNs are trained by a gradient descent scheme which alternates between updating  for Ttrain steps to minimize f^(, , ) and updating  and  for Tvalid steps to minimize F^V (, , ). We give our complete algorithm as Algorithm 1.

The possible non-differentiability of r due to discrete hyperparameters poses no problem. To dif-

ferentiate f^ w.r.t.

, we only require

f 

and

^ 

.

To differentiate F^V

w.r.t.

i

corresponding to a

discrete hyperparameter, there are two cases we must consider:

Case 1: For most regularization schemes, LV does not depend on i directly and hence the only gradient is through ^. Thus, the reparametrization gradient can be used when differentiating F^V .

Case 2: If LV relies explicitly on i, then we can use the REINFORCE gradient estimator

(Williams,

1992)

to

estimate

F^V i

.

An

example

hyperparameter

requiring

this

approach

is

the

num-

ber of hidden units, which directly affects the validation loss.

3.4 SUFFICIENCY OF LINEAR BEST-RESPONSE APPROXIMATIONS
To assess how flexible the approximation must be to model the best-response, we provide theoretical results for the behavior of linear approximations in the quadratic setting. We show that minimizing equation 8 yields the correct optimal-response gradient, hence linear approximations are sufficient in this restricted setting.

5

Under review as a conference paper at ICLR 2019

Algorithm 1 STNN Training Algorithm

Initialize: Model parameters , hyperparameters 

while not converged do
for t = 1, . . . , Ttrain do  p( |)







-



 

f ( +

, ^( +

))

for t = 1, . . . , Tvalid do

 p( |)







-



 

F ( +

, ^( +

)) -  H[p( |)]







-



 

F ( +

, ^( +

)) -  H[p( |)]

Theorem 3.

Suppose f

is quadratic with

2f 2

0 and p( |) is Gaussian with mean 0 and variance

2I.

Suppose

^

is

linear

and



=

arg min

f^(, , ).

Then

we

have

^ 

(0

)

=

 

().

Proof. See Appendix A.3.

4 EXPERIMENTS
We empirically investigated the behavior of STNs when tuning a single hyperparameter, and found STNs greedily adapt hyperparameters and discover hyperparameter schedules which outperform any single, fixed hyperparameter. We provide experiments showing that better generalization is a consequence of the schedule itself, and not due to increased regularization from stochasticity of sampling hyperparameters or limited capacity of the approximation ^. Next, we compared the performance of STNs to commonly used HO methods on the CIFAR-10 and PTB datasets. We show that STNs reach a lower validation loss more quickly than other popular approaches. We do not compare with other gradient-based approaches since they cannot tune dropout probabilities.
4.1 HYPERPARAMETER SCHEDULES
By tuning just a single hyperparameter, we found that STNs do not converge to a single fixed hyperparameter, but rather discover schedules for adapting the hyperparameters online. We examined this in detail on the PTB corpus (Marcus et al., 1993) using a Self-Tuning LSTM (Hochreiter & Schmidhuber, 1997) (ST-LSTM) and tuning the output dropout rate applied to the hidden units.
First, we found the schedule discovered by an ST-LSTM for output dropout, shown in Figure 2, outperforms the best, fixed output dropout rate (0.68) found by a fine-grained grid search, achieving 83.25 vs 85.33 validation perplexity. We argue this is a consequence of the schedule itself, and not due to regularizing effects from sampling hyperparameters or the limited capacity of ^.
To determine whether hyperparameter stochasticity is responsible for improved generalization, we trained a standard LSTM while perturbing its dropout rate around the best value found by grid search. We perturbed using (1) random Gaussian perturbations, and (2) sinusoid perturbations, which implement a cyclic regularization schedule. STNs outperform both perturbation methods.
To determine whether limited capacity of ^ acts as an additional regularizer, we trained a standard LSTM from scratch using the schedule for output dropout discovered by the ST-LSTM, and again find that it outperforms the best fixed value, proving that the schedule itself provides some benefit. The final validation and test perplexities of each variant are shown in Table 1.
Next, we show that the schedules discovered by STNs are greedy, i.e., the hyperparameters are adapted to be well-suited for the current training phase. As shown empirically in Appendix E, lower regularization performs best early in training, while higher regularization performs better later. We found the STN schedule implements a type of curriculum by using a low dropout rate early in training, which aids optimization, and gradually increases the dropout rate as training proceeds,

6

Under review as a conference paper at ICLR 2019

PTB CIFAR-10

Method

Val Perplexity Test Perplexity Val Loss Test Loss

Grid Search Random Search Bayesian Optimization
STN

97.32 84.81 72.13 70.93

94.58 81.46 69.29 68.12

.1686 .1518 .1366 .1339

.1701 .1517 .1403 .1408

Table 2: Final validation and test performance of each method on the PTB word-level language modeling task, and the CIFAR-10 image-classification task.

leading to better generalization. We show the schedules the STN discovers for different initial dropout rates in Figure 2. The same schedule is followed no matter what initial rate is used.
1.0

Method

Val Test

0.8

Output Dropout Rate

p = 0.68 (Fixed) p  N (0.68,  = 0.05) p = 0.68 + 0.1 sin(k) p = 0.78 (Converged STN)
STN (Ours)

85.33 84.39 85.05 88.12 83.25

82.82 81.58 82.42 84.94 80.54

Following STN Schedule 83.62 80.58

Table 1: Comparing fixed and time-varying methods for setting the output dropout hyperparameter for an LSTM.

0.6

0.4 0.2 0.0 0

Init=0.05 Init=0.3 Init=0.5 Init=0.7 Init=0.9
5000 10000 15I0t0e0ra2t0io0n00 25000 30000 35000

Figure 2: Output dropout schedules found by the ST-LSTM for different initial dropout rates

4.2 LANGUAGE MODELLING

We evaluated a ST-LSTM on the PTB corpus (Marcus et al., 1993), which is widely used as a benchmark for RNN regularization due to its small size (Gal & Ghahramani, 2016; Merity et al., 2018; Wen et al., 2018). We used a 2-layer LSTM with 650 hidden units per layer, and 650-dimensional word embeddings. We tuned 7 hyperparameters: variational dropout rates for the input, hidden state, and output; embedding dropout (i.e., setting rows of the embedding matrix to 0); DropConnect on the hidden-to-hidden weight matrix; and coefficients  and  that control the strength of activation regularization and temporal activation regularization, respectively. For LSTM tuning, we obtained the best results when using a fixed perturbation scale of 1 for the hyperparameters. Additional details about the experimental setup and the role of these hyperparameters can be found in Appendix C.

We compare STNs to grid search, random search, and Bayesian optimization. 4. Figure 3a shows the

best validation perplexity achieved by each method over time. STNs outperform the other methods,

achieving lower validation perplexity more quickly. We show the schedules the STN finds for each

hyperparameter in Figures 3b and 3c.

140 130 120

Grid Random BayesOpt STN

1.0 0.8

110 0.6

Output Input Hidden Embedding Weight

1.4 1.2 1.0 0.8

Alpha Beta

100 90

0.4 0.6 0.4

80 0.2 0.2

70
0 20000 400T0im0 e (6s0)000 80000 100000

0.0 0 20000 I4te00ra00tion 60000 80000

0.0
0 20000 I4te00ra00tion 60000 80000

(a) Time comparison

(b) STN schedule for dropout

(c) STN schedule for  and 

Figure 3: A comparison of the best validation perplexity achieved on PTB over time, by grid search, random search, Bayesian optimization, and STNs. STNs achieve better (lower) validation perplexity in less time.

Best Val Perplexity Dropout Rate Value

4For grid search and random search, we used the Ray Tune libraries (https://github.com/ ray-project/ray/tree/master/python/ray/tune). For Bayesian optimization, we used Spearmint (https://github.com/HIPS/Spearmint)
7

Dropout

Under review as a conference paper at ICLR 2019

Dropout0

0.6

Dropout1 Dropout2

Dropout3

0.4

Dropout4 Input Dropout

FC Dropout 0

0.2 FC Dropout 1

0.0 0 2000 4000 6000 8000

# Filters

400

350

300

250 200

Layer 0 Layer 1

150 100

Layer 2 Layer 3 Layer 4

0 2000 4000 6000 8000

Continuous Data Augmentation

Discrete Data Augmentation

6

Cutholes Cutlength

0.8

Hue Saturation

4

0.6

Contrast Bright

0.4

2 0.2

00

2000 4T0im00e (s) 6000 8000

0.0 0

2000 4T0im00e (s) 6000 8000

Figure 5: The hyperparameter schedule prescribed by the STN while training for image classification. The dropouts are indexed by the convolutional layer their applied to. FC dropout is for the fully-connected layers.

Best Validation Loss

4.3 IMAGE CLASSIFICATION
We evaluated Self-Tuning CNNs (ST-CNNs) on the CIFAR-10 (Krizhevsky, 2009) dataset, where it is easy to overfit with high-capacity networks. We used the AlexNet architecture (Krizhevsky et al., 2012). We optimized:

0.19 0.18 0.17 0.16

Grid search Random search Bayesian optimization STN

(1) discrete architectural hyperparameters of the number of filters in each convolutional layer

0.15

of AlexNet; (2) continuous hyperparameters of per-layer activation dropout and input dropout. (3) discrete data augmentation hyperparameters of the length and number of cut-out holes (DeVries & Taylor, 2017), and (4) continuous data augmentation hyperparameters controlling the amount of noise to apply to the hue, saturation,

0.14
0.13 0 10000 20T0im00e (s)30000 40000
Figure 4: A comparison of the best validation loss achieved on CIFAR-10 over time, by grid search, random search, Bayesian optimization, and STNs. STNs

brightness, and contrast of an image.

outperform other methods for many computational bud-

gets by finding the best validation loss. We compared STNs to grid search, random

search, and Bayesian optimization. Figure 4 shows the lowest validation loss achieved by each

method over time. Details of the optimization procedures are in Appendix D. The baselines meth-

ods decay the learning rate at most 3 times by a factor of 10 every time we complete an epoch

without the validation loss improving, after which we halt optimization early. Again, we find STNs

find better hyperparameter configurations in less time than other methods.

5 RELATED WORK
Bi-level Optimization Colson et al. (2007) is an overview of bilevel problems and Bard (2013) is a comprehensive textbook. When the objectives/constraints are restricted to be linear, quadratic or convex, a common approach replaces the lower-level problem with its KKT conditions added as constraints for the upper-level problem (Hansen et al., 1992; Vicente et al., 1994). In the unrestricted setting, our work loosely resembles trust-region methods (Colson et al., 2005), which repeatedly approximate the problem locally using a simpler bilevel program. In closely related work, Sinha et al. (2013) used evolutionary techniques to estimate the best-response function iteratively.
8

Under review as a conference paper at ICLR 2019

Hypernetworks Hypernetworks, first considered by Schmidhuber (1993; 1992), are functions mapping to the weights of a NN . Predicting certain weights in CNNs has been developed in various forms (Denil et al., 2013; Yang et al., 2015). Ha et al. (2016) used hypernetworks to generate weights for modern CNNs and RNNs. Brock et al. (2017) used hypernetworks to globally approximate an best-response for architecture search. Because the architecture is not optimized during training, this required a large hypernetwork, unlike ours which locally approximates the best-response.

Gradient-Based HO There are two main approaches. The first approximates (0) using

T (0, 0), the value of  after T steps of gradient descent on f w.r.t.  starting at (0, 0).

The

descent

steps

are

differentiated

through

to

approximate

 

(0

)



T 

(0,

0).

This ap-

proach was proposed by Domke (2012) and used by Maclaurin et al. (2015), Luketina et al. (2016)

and Franceschi et al. (2018). The second approach uses the Implicit Function Theorem to derive

 

(0

)

under

certain

conditions

(see

Appendix

A.4).

This

was

first

used

for

HO

in

NNs

(Larsen

et al., 1996) and developed further by Pedregosa (2016). Similar approaches have been used for

HO in log-linear models (Foo et al., 2008), kernel selection (Chapelle et al., 2002; Seeger, 2007),

and image reconstruction (Kunisch & Pock, 2013; Calatroni et al., 2016). Both approaches struggle

with certain hyperparameters, since they differentiate gradient descent or the training loss w.r.t. the

hyperparameters. In addition, differentiating gradient descent becomes prohibitively expensive as

the

number

of

descent

steps

increases,

while

implicitly

deriving

 

requires

using

Hessian-vector

products with conjugate gradient solvers to avoid directly computing the Hessian.

Model-Based HO A common model-based approach is Bayesian optimization, which models p(r|, D), the conditional probability of the performance on some metric r given hyperparameters  and a dataset D = {(i, ri)}. p(r|h, D) can be modelled using various methods (Hutter et al., 2011; Bergstra et al., 2011; Snoek et al., 2012; 2015). D is constructed iteratively, where the next  to train on is chosen by maximizing an acquisition function C(h; p(r|, D)) which balances
exploration and exploitation. Training each model to completion can be avoided if assumptions
are made on learning curve behavior (Swersky et al., 2014; Klein et al., 2016). These approaches require building inductive biases into p(r|h, D) which may not hold in practice, do not take advan-
tage of NN structure when used for HO, and do not scale well with the number of hyperparameters.
However, these approaches have consistency guarantees in the limit, unlike ours.

Model-Free HO Model-free approaches include grid-search and random search. Bergstra & Bengio (2012) advocated using random search over grid search. Successive Halving (Jamieson & Talwalkar, 2016) and Hyperband (Li et al., 2016) extend random search by adaptively allocating resources to promising configurations using multi-armed bandit techniques. These methods ignore any structure in the problem, unlike ours which makes use of rich gradient information. However, unlike our method, it is possible to parallelize model-free methods over computing resources.

Hyperparameter Scheduling To our knowledge, the only other work that has considered schedules for hyperparameters is Population Based Training (PBT) (Jaderberg et al., 2017). In PBT, a population of NNs is trained in parallel, where the hyperparameters and weights of under-performing NNs are periodically replaced by their better-performing counterparts. The hyperparameters are randomly perturbed before copying. Thus, a single model can experience different hyperparameters over the course of training, implementing a schedule. STNs replace the population of NNs by a single hypernetwork and use gradients to tune hyperparameters during a single training run. The schedule discovered by the STN can be seen as an instance of curriculum learning (Bengio et al., 2009), a strategy for training machine learning models, where the model is first trained on simple examples and then on gradually more difficult ones.

6 CONCLUSION
We proposed Self-Tuning Networks (STNs), which efficiently approximate the best-response to hyperparameters by scaling and shifting their hidden units. This allowed us to use gradient-based optimization to tune all kinds of regularization hyperparameters, including discrete hyperparameters. We validated this approach on large-scale problems by showing STNs can achieve better generalization performance than competing approaches in less time. We believe STNs offer a compelling path towards large-scale, automated hyperparameter tuning for neural networks.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Eugene L Allgower and Kurt Georg. Numerical continuation methods: An introduction, volume 13. Springer Science & Business Media, 2012.
Jonathan F Bard. Practical bilevel optimization: Algorithms and applications, volume 30. Springer Science & Business Media, 2013.
Yoshua Bengio, Je´ro^me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 41­48. ACM, 2009.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281­305, 2012.
James S. Bergstra, Re´mi Bardenet, Yoshua Bengio, and Bala´zs Ke´gl. Algorithms for hyperparameter optimization. In Advances in Neural Information Processing Systems, pp. 2546­2554. 2011.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. arXiv preprint arXiv:1505.05424, 2015.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. SMASH: One-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.
Luca Calatroni, Chung Cao, Juan Carlos De Los Reyes, Carola-Bibiane Scho¨nlieb, and Tuomo Valkonen. Bilevel approaches for learning of variational imaging models. 2016.
Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and Sayan Mukherjee. Choosing multiple parameters for Support Vector Machines. Machine Learning, 46(1-3):131­159, 2002.
Beno^it Colson, Patrice Marcotte, and Gilles Savard. A trust-region method for nonlinear bilevel programming: Algorithm and computational experience. Computational Optimization and Applications, 30(3):211­227, 2005.
Beno^it Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of Operations Research, 153(1):235­256, 2007.
Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pp. 2148­2156, 2013.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.
Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pp. 318­326, 2012.
John Duchi. Properties of the trace and matrix derivatives. 2007.
Anthony V Fiacco and Yo Ishizuka. Sensitivity and stability analysis for nonlinear programming. Annals of Operations Research, 27(1):215­235, 1990.
Chuan-sheng Foo, Chuong B Do, and Andrew Y Ng. Efficient multiple hyperparameter learning for log-linear models. In Advances in Neural Information Processing Systems, pp. 377­384, 2008.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, and Massimilano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. arXiv preprint arXiv:1806.04910, 2018.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pp. 1019­1027, 2016.
Robert Gibbons. A primer in game theory. Harvester Wheatsheaf, 1992.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
10

Under review as a conference paper at ICLR 2019
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.
Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel programming. SIAM Journal on Scientific and Statistical Computing, 13(5):1194­1217, 1992.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer Series in Statistics. Springer New York Inc., New York, NY, USA, 2001.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a constrained variational framework. 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In International Conference on Learning and Intelligent Optimization, pp. 507­523. Springer, 2011.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population-based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.
Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics, pp. 240­248, 2016.
Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable Bayesian deep learning by weight-perturbation in Adam. arXiv preprint arXiv:1806.04854, 2018.
Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with Bayesian neural networks. 2016.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097­1105, 2012.
Karl Kunisch and Thomas Pock. A bilevel optimization approach for parameter learning in variational models. SIAM Journal on Imaging Sciences, 6(2):938­983, 2013.
Jan Larsen, Lars Kai Hansen, Claus Svarer, and M Ohlsson. Design and regularization of neural networks: The optimal use of a validation set. In Neural Networks for Signal Processing [1996] VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, pp. 62­71. IEEE, 1996.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Bandit-based configuration evaluation for hyperparameter optimization. 2016.
Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hypernetworks. arXiv preprint arXiv:1802.09419, 2018.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural networks. arXiv preprint arXiv:1703.01961, 2017.
Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning of continuous regularization hyperparameters. In International Conference on Machine Learning, pp. 2952­2960, 2016.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pp. 2113­ 2122, 2015.
11

Under review as a conference paper at ICLR 2019
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313­330, 1993.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. International Conference on Learning Representations, 2018.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: An empirical study. arXiv preprint arXiv:1802.08760, 2018.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Conference on Machine Learning, pp. 737­746, 2016.
Ju¨rgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131­139, 1992.
Ju¨rgen Schmidhuber. A `self-referential' weight matrix. In ICANN'93, pp. 446­450. Springer, 1993.
Matthias Seeger. Cross-validation optimization for large scale hierarchical classification kernel methods. In Advances in Neural Information Processing Systems, pp. 1233­1240, 2007.
Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. Efficient evolutionary algorithm for singleobjective bilevel optimization. arXiv preprint arXiv:1303.3901, 2013.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, pp. 2951­2959, 2012.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, M Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural networks. In International Conference on Machine Learning, pp. 2171­2180, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Joe Staines and David Barber. Variational optimization. arXiv preprint arXiv:1212.4507, 2012.
Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014.
Luis Vicente, Gilles Savard, and Joaquim Ju´dice. Descent approaches for quadratic bilevel programming. Journal of Optimization Theory and Applications, 81(2):379­399, 1994.
Heinrich Von Stackelberg. Market Structure and Equilibrium. Springer Science & Business Media, 2010.
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudoindependent weight perturbations on mini-batches. International Conference on Learning Representations, 2018.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1476­1483, 2015.
Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural Turing machines-revised. arXiv preprint arXiv:1505.00521, 2015.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.
12

Under review as a conference paper at ICLR 2019

A PROOFS

A.1 LEMMA 1

Because 0 solves Problem 4b given 0, by the first-order optimality condition, we must have:

f  (0, 0) = 0

(15)

The

Jacobian

of

f 

decomposes

as

a

block

matrix

with

subblocks

given

by:

2f 

2f 2

(16)

We

know

f

is

C2

in

some

neighborhood

of

(0, 0),

so

f 

is

continuously

differentiable

in

this

neighborhood.

By

assumption,

the

Hessian

2f 2

is

positive

definite

and

hence

invertible

at

(0, 0).

By the Implicit Function Theorem, there exists a neighborhood V of 0 and a unique continuously

differentiable function 

:V

 Rm

such that

f 

(,

())

=

0

for





V

and (0) = 0.

Furthermore,

by

continuity

we

know

there

is

a

neighborhood

W1

× W2

of

(0, 0)

such

that

2f 2

is

positive definite on this neighborhood. Setting U = V  W1  ()-1(W2), we can conclude that

 

2f 2

(,



())

0

sufficient optimality

for all   conditions,

U. we

Combining this with conclude that ()

f 

(,

())

=

0

and

using

second-order

is the unique solution to Problem 4b for all

  U.

A.2 LEMMA 2

This discussion mostly follows from Hastie et al. (2001). We let X  RN×D denote the data matrix where N is the number of training examples, and D is the dimensionality of the data. We let t  RN
denote the associated targets. We can write the SVD decomposition of X as:

X = UDV T

(17)

where U and V are N × D and D × D orthogonal matrices and D is a diagonal matrix with entries

d1  d2  · · ·  dD  0. We next simplify the function y(x; ) by setting v = sT Q, so that

y(x; )

=

sT Qx

=

vT x.

We

see

that

the

Jacobian

y x



v

is

constant,

and

Problem

10

simplifies

to standard L2-regularized least-squares linear regression with the following loss function:

(vT x - t)2 + 1 exp() v 2 |D|
(x,t)D

(18)

It is well-known (see Hastie et al. (2001), Chapter 3) that the optimal solution v() minimizing Equation 18 is given by:

v() = (XT X + exp()I)-1XT t = V (D2 + exp()I)-1DU T t

(19)

Furthermore, the optimal solution v to the unregularized version of Problem 18 is given by:

v = V D-1U T t

(20)

Recall that we defined Q0 = V T , i.e. the change-of-basis matrix from the standard basis to the principal components of the data matrix, and we defined s0 to solve the unregularized regression problem given Q0. Thus, we require that Q0T s0 = v which implies s0 = D-1U T t.
There are not unique solutions to Problem 10, so we take any functions Q(), s() which satisfy Q()T s() = v() as "best-response functions". We will show that our chosen functions Q() = (w + b) Q0 and s() = s0, where w = 0 and bi = 2 log(di) for i = 1, . . . , D, meet this criteria. We start by noticing that for any d  R+, we have:

1 1 d2 ( + 2 log(d)) = 1 + exp(- - 2 log(d)) = 1 + d-2 exp() = d2 + exp()

(21)

13

Under review as a conference paper at ICLR 2019

It follows that:

Q()T s() = [(w + b) Q0]T s0

T
  ( + 2 log(d1))  

=

diag

 

...

 

Q0

 

s0





( + 2 log(dm))

  d21 

d12 +exp()



=

QT0

diag 

 

...



 

s0

  d2D 
dD2 +exp()

  d21 

d21 +exp()

 = V diag 


...

  D-1U T t 

  dD2 
dD2 +exp()

  d1 

d21 +exp()

 = V diag 


...

  U T t 





dD

d2D +exp()

= V (D2 + exp()I)-1D U T t

= v()

(22) (23)
(24)
(25)
(26) (27) (28)

A.3 THEOREM 3

By assumption, f is quadratic so there exist A  Rn×n, B  Rn×m, C  Rm×m and d  Rn, e  Rm such that:

1 f (, ) =
2

T

T

AB BT C

 + dT  + eT  

(29)

Furthermore,

since

we

assume

2f 2

0, we must have C

0. One can easily compute that:

f (, ) = BT  + C + e 

2f 2 (, ) = C Setting the derivative equal to 0 and using second-order sufficient conditions, we have:

(30) (31)

() = -C-1(e - BT )

(32)

Hence, we find:

 () = -C-1BT 

We can substitute ^() = U  + b and the definition of f into equation 8 to obtain:

(33)

e(U , b, ) := E p( |)[(0 + )T A(0 + ) + 2(0 + )T B(U (0 + ) + b) + (U (0 + ) + b)T C(U (0 + ) + b) + dT (0 + ) + eT (U (0 + ) + b)]

(34)

Expanding above, we can find that equation 34 is equal to:

E p( |) 1 + 2 + 3 + 4

(35)

14

Under review as a conference paper at ICLR 2019

where we have:

1

1 =
2

T0 A0 + 2 T A0 +

TA

2 = T0 BU 0 + 0T BU + T0 Bb + T BU 0 + T BU + T Bb

(36) (37)

3

=

1 2

(T0

U

T

C

U

0

+

0U T CU

+ 0U T Cb +

T U T CU 0

+ T U T CU + T U T Cb + bT CU 0 + bT CU

+ bT C)

(38)

4 = dT 0 + dT + eT U 0 + eT U + eT b

(39)

We can simplify these expressions considerably by using linearity of expectation and that  p( |)

has mean 0. Doing so, then differentiating and making use of various matrix-derivative equalities

(Duchi, 2007), we find:

e (U , b, )
b

=

1 2

C

T

U

0

+

1 2 CU 0

+

BT 0

+

e

+

Cb

(40)

e U

(U , b, )

=

BT 00T

+

2BT

+

C bT0

+

eT0

+

CU 00T

+

2CU

Setting

the

derivative

e b

(U

,

b,

)

equal

to

0,

we

have:

(41)

b = -C-1(CT U 0 + BT 0 + e)

(42)

Setting

the

derivative

for

e U

(U ,

b,

)

equal

to

0,

we

have:

CU (0T0 + 2C) = -BT 00T - 2BT - Cb0T - eT0

(43)

Substituting the expression for b given by equation 42 into equation 43 and simplifying gives:

CU (00T + 2I) = -2BT + CU 00T = 2CU = -2BT = U = C-1B

(44) (45) (46)

This

is

exactly

the

best-response

Jacobian

 

()

as

given

by

equation

33.

Substituting U

=

C-1B into the equation 42 gives:

b = C-1BT 0 - C-1BT 0 - C-1e

This

is

()

-

 

(),

as

claimed.

(47)

A.4 BEST-RESPONSE GRADIENT LEMMA

Lemma 4. Under the same conditions as Lemma 1 and using the same notation, for all   U , we

have that:

 () = - 

2f 2

(,

())

-1

2f (, ()) 

(48)

Proof. Define  : U  Rn × Rm by () = (, ()). By first-order optimality conditions, we

know:

f   () = 0   U 

(49)

Hence, for all   U :

 0=

f   ()

 

=

2f 2

(

())

 

()

+

2f (()) 

=

2f 2

(,

())

 

()

+

2f (, ()) 

Rearranging gives Equation 48.

(50) (51) (52)

15

Under review as a conference paper at ICLR 2019

B BEST-RESPONSE APPROXIMATIONS FOR CONVOLUTIONAL FILTERS

We let L denote the number of layers, Cl the number of channels in layer l's feature map, and Kl the size of the kernel in layer l. We let W l,c  RCl-1×Kl×Kl and bl,c  R denote the weight and bias respectively of the cth convolution kernel in layer l (so c  {1, . . . , Cl}). For ul,c, al,c  Rn,
we define best-response approximations W^ l,c and b^l,c by:

W^ l,c() = , ul,c Wslc,acle + Wslh,icft

(53)

b^l,c() = , al,c bls,ccale + bls,hcift

(54)

Thus, the best-response parameters used for modeling W l,c, bl are l,c = {ul,c, al,c, Wslc,acle, Wslh,icft, bls,ccale, bls,hcift}. We can compute the number of parameters used as |l,c| = 2n + 2(|W l,c| + |bl,c|). Summing over channels c, we find the total number of parameters
is 2nCl + 2p, where p is the total number of parameters in the normal CNN layer. Hence, we use twice the number of parameters in a normal CNN, plus an additional overhead which depends on
the number of hyperparameters.

C LANGUAGE MODELLING EXPERIMENT DETAILS

Here we present additional details on the setup of our LSTM experiments on PTB, and on the role of each hyperparameter we tune.

We tune variational dropout (re-using the same dropout mask for each step in a sequence) on the input to the LSTM, the hidden state between the LSTM layers, and the output of the LSTM. We also tune embedding dropout, which sets entire rows of the word embedding matrix to 0, effectively removing certain words from all sequences. We regularize the hidden-to-hidden weight matrix using DropConnect (zeroing out weights rather than activations). Finally, we use activation regularization (AR) and temporal activation regularization (TAR). AR penalizes large activations, and is defined as:

||m ht||2

(55)

where m is a dropout mask and ht is the output of the LSTM at time t. TAR is a slowness regularizer, defined as:

||ht - tt+1||2

(56)

We tune the scaling coefficients  and .

D IMAGE CLASSIFICATION EXPERIMENT DETAILS
Here we present additional details on the CNN experiments. For all results we hold out 20% of the data for validation.
Our optimization algorithm for the elementary model in the baselines is as follows: We use SGD with a learning rate of 0.01, momentum of 0.9, mini-batches of size 128, and run for at most 200 epochs.
Our optimization algorithm for the ST-CNN is as follows: The ST-CNN is optimized with the same algorithm as the elementary model in the baselines. In other words, we use SGD with learning rate 0.01, momentum of 0.9, mini-batches of size 128. We use the same optimization algorithm as the ST-LSTM for the hyperparameters. This means the hyperparameters are optimized with Adam using a learning rate of 0.003. We alternate between training the hypernet and hyperparameters with the same schedule as the ST-LSTM - i.e. Ttrain = 15 steps on the training step and Tvalid = 10 steps on the validation set.
Additionally, we use an entropy weight of 0 during training when optimizing the scale of the hyperparameter distributions.

16

Output Dropout Output Dropout Output Dropout

Under review as a conference paper at ICLR 2019

E ADDITIONAL DETAILS ON HYPERPARAMETER SCHEDULES

Here, we draw connections between the hyperparameter schedules learned by STNs and curriculum learning. Curriculum learning is an instance of a family of continuation methods (Allgower & Georg, 2012), which optimize non-convex functions by solving a sequence of functions that are ordered by increasing difficulty. In a continuation method, one considers a family of training criteria C() with parameter , where C1() is the true/final objective we wish to minimize, and C0() represents the training criterion for a simpler version of the problem; one starts by optimizing C0() and then gradually increases  from 0 to 1, while keeping  at a local minimum of C(). When training an RL-NTM (Zaremba & Sutskever, 2015), the authors maintain a distribution over the complexity c of a problem instance (defined as the maximum length of the target output for a given input), which is shifted towards harder problems as the performance of the model improves.

Figure 6 shows the validation perplexity for different sets of input and output dropout, at various points during training. The x- and y-axes represent the values of input and output dropout, respectively. We see that at the start of training, the best validation loss is achieved with small values of both input and output dropout. As we train for more epochs, the best validation performance is achieved with larger dropout rates.

0.9 Epoch 1
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0.00.0 0.1 0.2 I0n.p3u0t.4Dr0o.5po0u.6t 0.7 0.8 0.9

0.9 Epoch 10
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0.00.0 0.1 0.2 I0n.p3u0t.4Dr0o.5po0u.6t 0.7 0.8 0.9

0.9 Epoch 25
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0.00.0 0.1 0.2 I0n.p3u0t.4Dr0o.5po0u.6t 0.7 0.8 0.9

(a) (b) (c)

Figure 6: Validation performance of a baseline LSTM given different settings of input and output dropout over time.

Figure 7 shows the perturbed values for output dropout we use to investigate whether the improved performance yielded by STNs is due to the regularization effect, and not the schedule itself.

Output Dropout Prob Output Dropout Prob Output Dropout Prob
Output Dropout Rate

1.0 0.8 0.6 0.4 0.2
0.0 0 2000 4000 I6t0e0r0at8io0n00 100001200014000

1.0 0.8 0.6 0.4 0.2
0.0 0 2000 4000 I6t0e0r0at8io0n00 100001200014000

1.0 0.8 0.6 0.4 0.2 0.0 0

10000 Ite2r0a0t0i0on 30000

40000

1.0

0.8

0.6

0.4 0.2 0.0 0

Init=0.05 Init=0.3 Init=0.5 Init=0.7 Init=0.9 5000 10000 15I0t0e0ra2t0io0n00 25000 30000 35000

(a) p  N (0.68, 0.05) (b) p = 0.68 with sin noise

(c) ST-LSTM

(d) ST-LSTM Initial Hyperparameters

Figure 7: Output dropout rates

We can observe the benefits of greedy hyperparameter schedules by forming a schedule based on the performance in time of models from a grid search over output dropout. As shown in Figure 8, the schedule formed by taking the best output dropout value in each epoch yields better generalization than any of the fixed hyperparameter values from the initial grid search.

17

Under review as a conference paper at ICLR 2019

Output Dropout
Val Perplexity

1.0 0.8 0.6 0.4 0.2
0.0 0 10 20Epoch30 40 50
(a) Greedy schedule for output dropout, derived from grid search

140 0.0

130

0.1 0.2

0.3

120 0.4

0.5

110

0.6 0.7

0.8

100 schedule

90

800 5 10 15 2E0poc2h5 30 35 40 45

(b) Comparison of fixed output dropout values and the dropout schedule derived from grid searches

Figure 8: Grid-search derived output dropout schedules. All experiments here use learning rate decay.

18

