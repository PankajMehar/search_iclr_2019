Under review as a conference paper at ICLR 2019

IMPROVED LEARNING OF ONE-HIDDEN-LAYER CONVOLUTIONAL NEURAL NETWORKS WITH OVERLAPS
Anonymous authors Paper under double-blind review

ABSTRACT
We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned. Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks. Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems. We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.

1 INTRODUCTION

Giving provably efficient algorithms for learning neural networks is a core challenge in machine learning theory. The case of convolutional architectures has recently attracted much interest due to their many practical applications. Recently Brutzkus & Globerson (2017) showed that distributionfree learning of one simple non-overlapping convolutional filter is NP-hard. A natural open question is whether we can design provably efficient algorithms to learn convolutional neural networks under mild assumptions.

We consider a convolutional neural network of the form

k
f px, w, aq " ÿ aj `wJPjx
j"1

(1)

where w P Rr is a shared convolutional filter, a P Rk is the second linear layer and

Pj

"

rlom0 o

n

lo

I mo n

0 lo mo n

s P Rr^d

pj´1qs r d´pj´1qs`r

selects the ppj ´ 1qs ` 1q-th to ppj ´ 1qs ` rq-th coordinates of x with stride s and  p¨q is the activation function. Note here that both w and a are unknown vectors to be learned and there may be overlapping patches because the stride size s may be smaller than the filter size r.

Our Contributions We give the first efficient algorithm that can provably learn a convolutional neural network with two unknown layers with commonly used overlapping patches. Our main result is the following theorem.

Theorem 1.1 (Main Theorem (Informal)).

Suppose s



t

r 2

u

`

1

and

the

marginal

distribution

is

symmetric and isotropic. Then the convolutional neural network defined in equation 1 with piecewise

linear activation functions is learnable in polynomial time.

We refer readers to Theorem 3.1 for the precise statement.

Technical Insights Our algorithm is a novel combination of the algorithm for isotonic regression and the landscape analysis of non-convex problems. First, inspired by recent work on isotonic regression, we extend the idea in Goel et al. (2018) to reduce learning a CNN with piecewise linear activation to learning a convolutional neural network with linear activation (c.f. Section 4). Second,

1

Under review as a conference paper at ICLR 2019

we show learning a linear convolutional filter can be reduced to a non-convex matrix factorization problem which admits a provably efficient algorithm based on non-convex geometry (Ge et al., 2017a). Third, in analyzing our algorithm, we present a robust analysis of Convotron algorithm proposed by Goel et al. (2018), in which we draw connections to the spectral properties of Toeplitz matrices. We believe these ideas may inspire further development in designing provable learning algorithms for neural networks and other complex models.
Related Work From the point of view of learning theory, it is well known that training is computational infeasible in the worst case (Goel et al., 2016; Brutzkus & Globerson, 2017). Thus distributional assumptions are needed for efficient learning. A line of research has focused on analyzing the dynamics of gradient descent conditioned on the input distribution being standard Gaussian (Tian, 2017; Soltanolkotabi, 2017; Li & Yuan, 2017; Zhong et al., 2017b; Brutzkus & Globerson, 2017; Zhong et al., 2017a; Du et al., 2017). Specifically for convolutional nets, existing analyses heavily relied on the analytical formulas which can only be derived if the input is Gaussian and patches are non-overlapping.
Recent work has tried to relax the Gaussian input assumption and the non-overlapping structure for learning convolutional filters. Du et al. (2017) showed if the patches are sufficiently close to each other then stochastic gradient descent can recover the true filter. Goel et al. (2018) proposed a modified iterative algorithm inspired from isotonic regression that gives the first recovery guarantees for learning a filter for commonly used overlapping patches under much weaker assumptions on the distribution. However, these two analyses only work for learning one unknown convoutional filter.
Moving away from gradient descent, various works have shown positive results for learning general simple fully connected neural networks in polynomial time and sample complexity under certain assumptions using techniques such as kernel methods (Goel et al., 2016; Zhang et al., 2015; Goel & Klivans, 2017a;b) and tensor decomposition (Sedghi & Anandkumar, 2014; Janzamin et al., 2015). The main drawbacks include the shift to improper learning for kernel methods and the knowledge of the probability density function for tensor methods. In contrast to this, our algorithm is proper and does not assume that the input distribution is known.
Learning a neural network is often formulated as a non-convex problem. If the objective function satisfies (1) all saddle points and local maxima are strict (i.e., there exists a direction with negative curvature), and (2) all local minima are global (no spurious local minmum), then noise-injected (stochastic) gradient descent (Ge et al., 2015; Jin et al., 2017) finds a global minimum in polynomial time. Recent work has studied these properties for the landscape of neural networks (Kawaguchi, 2016; Choromanska et al., 2015; Hardt & Ma, 2016; Haeffele & Vidal, 2015; Mei et al., 2016; Freeman & Bruna, 2016; Safran & Shamir, 2016; Zhou & Feng, 2017; Nguyen & Hein, 2017a;b; Ge et al., 2017b; Zhou & Feng, 2017; Safran & Shamir, 2017; Du & Lee, 2018). A crucial step in our algorithm is reducing the convolutional neural network learning problem to matrix factorization and using the geometric properties of matrix factorization.

2 PRELIMINARIES

We use bold-faced letters for vectors and matrices. We use }¨}2 to denote the Euclidean norm of a finite-dimensional vector. For a matrix A, we use max pAq to denote its eigenvalue and min pAq its smallest singular value. Let Op¨q and  p¨q denote standard Big-O and Big-Omega notations, only hiding absolute constants.
In our setting, we have n data points txi, yiuin"1 where xi P Rd and y P R. We assume the label is generated by a two-layer convolutional neural network with filter size r, stride s and k hidden neurons. Compactly we can write the formula in the following form: yi " f pxi, w°, a°q, xi ,, Z where the prediction function f is defined in equation 1. To obtain a proper scaling, we let }w°}2 }a°}2 " 1. We also define the induced patch matrix as
P pxq " rP1x . . . Pkxs P Rr^k

which will be play an important role in our algorithm design. Our goal is to properly learn this

convolutional neural network, i.e., design a polynomial time algorithm which outputs a pair pw, aq

that

satisfies

Ex,,Z

" pf

pw,

a,

xq

´

f

pw°,

a°,

xqq2i



.

2

Under review as a conference paper at ICLR 2019

Algorithm 1 Learning One-hidden-Layer Convolutional Network
Input: Input distribution Z. Number of iterations: T1, T2. Number of samples: T3Step sizes: 1  0, 2  0. Output: Parameters of the one-hidden-layer CNN: w and a. 1: Stage 1: Run Double Convotron (Algorithm 2) for T1 iterations with step size 1 to obtain apT1q. 2: Stage 2: Run Convotron (Algorithm 3) using apT1q and ´apT1q for T2 iterations and step size 2 to obtain wp`q and wp´q. 3: Stage 3: Choose parameters with lower empirical loss on T3 samples drawn from Z from `wp`q, apT q and `wp´q, ´apT q.

3 MAIN RESULT
In this section we describe our main result. We first list our main assumptions, followed by the detailed description of our algorithm. Lastly we state the main theorem which gives the convergence guarantees of our algorithm.

3.1 ASSUMPTIONS

Our first assumption is on the input distribution Z. We assume the input distribution is symmetric, bounded and has identity covariance. The symmetry assumption is used in Goel et al. (2018) and many learning theory papers Baum (1990). The identity covariance assumption is true if the data whitened. Further, in many architectures, the input of certain layers is assumed having these properties because of the use of batch normalization (Ioffe & Szegedy, 2015) or other techniques. Lastly, the boundedness is a standard regularity assumption to exclude pathological input distributions. We remark that this assumption considerably weaker than the standard Gaussian input distribution assumption used in Tian (2017); Zhong et al. (2017a); Du et al. (2017), which has the rotational invariant property.
Assumption 3.1 (Input Distribution Assumptions). We assume the input distribution satisfies the following conditions.

· Symmetry: P pxq " P p´xq . · Identity covariance: Ex,,Z "xxT  " I.

· Boundedness: @x ,, Z, }x}2  B almost surely for some B  0.

Our second assumption is on the patch structure. In this paper we assume the stride is larger than half of the filter size. This is indeed true for a wide range of convolutional neural network used in computer vision. For example some architecture has convolutional filter of size 3 and stride 2 and some use non-overlapping architectures (He et al., 2016).

Assumption 3.2 (Large Stride).

s



t

r 2

u

`

1.

Next we assume the activation function is piecewise linear. Commonly used activation functions like rectified linear unit (ReLU), Leaky ReLU and linear activation all belong to this class.
Assumption 3.3 (Piece-wise Linear Activation).

"x pxq " x

if x  0 . if x  0

3.2 ALGORITHM
Now we are ready to describe our algorithm (see Algorithm 1). The algorithm has three stages, first we learn the outer layer weights upto sign, second we use these fixed outer weights to recover the filter weight and last we choose the best weight combination thus recovered.

3

Under review as a conference paper at ICLR 2019

Stage 1: Learning the Non-overlapping Part of the Convolutional Filter and Linear Weights

Our first observation is even if there may be overlapping patches, as long as there exists some non-

overlapping part, we can learn this part and the second layer jointly. To be specific, with filter size

being

r

and

stride

being

s,

if

s



t

r 2

u

`

1,

for

j

"

1. . . . , k

we

define

the

selection

matrix

for

the

non-overlapping part of each patch

Pnj on " r

0 lo mo n

I lo mo n

0 lo mo n

s P Rp2s´rq^d.

pj´1qs`r 2s´r d´pj`1qs

Note that for any j1  j2, there is no overlapping between the selected coordinates by Pnj1on and Pnj2on. Therefore, for a filter w, there is a segment rwr´s`1, . . . , wss with length p2s ´ rq which acts on the non-overlapping part of each patches. We denote wnon " rwr´s`1, . . . , wss and our goal in this stage is to learn wn°on and a° jointly.
In this stage, our algorithm proceeds as follows. Given wnon, a and a sample px, yq, we define

g

pwnon,

a,

x,

yq

"1

2 `



´f^pwnon,

a, xq

´

¯ y

k
ÿ
j"1

aiPnj onx

`

1 4

´ }wnon

}22

´

}a}22¯

wnon

h pwnon,

a,

x,

yq

"1

2 `



´f^pwnon,

a, xq

´

¯ y

¨wnJonPn1 onx  . . . ,`
wnJon Pnk on x

1 4

´}a}22

´

}wnon}22¯

a

(2) (3)

where

f^pwnon,

a,

xq

"

k
j"1

aj



`wnJonPjnonx

is

the

prediction

function

only

using

wnon.

As will be apparent in Section 4, g and h are unbiased estimates of the gradient for the loss func-

tion

of

learning

a

linear

CNN.

The

term

1 4

´}wnon}22

´

}a}22¯ wnon

and

1 4

´}a}22

´

}wnon}22¯ a

are

is

the

gradient

induced

by

the

regularization

1 4

´}wnon}22

´

}a}22

¯2 ,

which

is

used

to

balance

the

magnitude between wnon and a and make the algorithm more stable.

With some initialization wnp0oqn and ap0q, we use the following iterative updates inspired by isotonic regression (Goel et al., 2018), for t " 0, . . . , T1 ´ 1

´¯

wnpto`n1q Ðwnptoqn ´ 1g

wnptoqn, aptq, xptq, yptq

`

1

ptq
wnon

,

´¯ apt`1q Ðaptq ´ 1h wnptoqn, aptq, xptq, yptq ` 1aptq

(4) (5)

where

1



0

is

the

step

size

parameter,

ptq
wnon

and

aptq

are

uniformly

sampled

a

unit

sphere

and

at

iteration

we

use

a

fresh

sample

`xptq, yptq.

Here

we

add

isotropic

noise

ptq
wnon

and

aptq

because

the objective function for learning a linear CNN is non-convex and there may exist saddle points.

Adding noise can help escape from these saddle points. We refer readers to Ge et al. (2015) for more

technical details regarding this. As will be apparent in Section 4, after sufficient iterations, we obtain a pair `wpT1q, apT1q such that either it is close to the truth pwn°on, a°q or close to the negative of the truth p´wn°on, ´a°q.

Remark 3.1 (Non-overlapping Patches). If there is no overlap between patches, we can skip Stage

2 because after Stage 1 we have already learned a and wnon " w.

Stage 2: Convotron with fixed Linear Layer In Stage 1 we have learned a good approximation to the second layer (either apT1q or ´apT1q). Therefore, the problem reduces to learning a convolutional filter. We run Convotron (Algorithm 3) proposed in Goel et al. (2018) using apT1q and ´apT1q to obtain corresponding weight vectors wp`q and wp´q. We show that the Convotron analysis can be extended to handle approximately known outer layer weight vectors.
Stage 3: Validation In stage 2 we have obtained two possible solutions `wp`q, apT q and `wp´q, ´apT q. We know at least one of them is close to the ground truth. Closeness in ground truth implies small squared loss (c.f. Lemma A.1). In the last stage we use a validation set to choose

4

Under review as a conference paper at ICLR 2019

Algorithm 2 Double Convotron

Initialize wnp0oqn P R2s´r and ap0q P Rk randomly

for t " 1 to T do Draw pxptq, yptqq ,, Z

Compute g `wnon, a, xptq, yptq and h `wnon, a, xptq, yptq according to equation 2 and equa-

tion 3.

Set

wnpto`n1q

"

wnptoqn

´

1g

´wnptoqn,

aptq,

xptq,

¯ yptq

`

1

ptq
wnon

Set

apt`1q

"

aptq

´

1h

´wnptoqn, aptq,

xptq,

¯ yptq

`

1aptq

Return apT `1q

Algorithm 3 Convotron (Goel et al., 2018)

Initialize w1 :" 0 P Rr.

for t " 1 to T do Draw pxptq, yptqq ,, Z

Let

gptq

"

pyptq

´

f

pwptq,

a,

xptq

qq

´k
i"1

¯ aiPixptq

Set wpt`1q " wptq ` gptq

Return wT `1

the right one. To do this, we simply use T3 " poly `k, B, 1  fresh samples and output the solution which gives lower squared error.

pw,

aq

"

arg min pw,aqPtpwp`q,apT qq,pwp´q,´apT qqu

1 T3

T3
ÿ
i"1

´ ypiq

´

f

´ w,

a,

¯¯2 xpiq

.

(6)

Since we draw many samples, the empirical estimates will be close to the true loss using standard concentration bounds and choosing the minimum will give us the correct solution.

3.3 MAIN THEOREM

The following theorem shows that Algorithm 1 is guaranteed to learn the target convolutinoal neural

network in polynomial time. To our knowledge, this is the first polynomial time proper learning

algorithm for convolutional neural network with overlapping patches.

Theorem 3.1 (Theorem 1.1 (Formal)). Under Assumptions 3.1-3.3, if we set |S1| , |S2| , |S3| "



`poly

`k, B,

1 

and

1,

2

"

O

`poly

`

1 k

,

1 B

,

 then with high probability, Algorithm 1 returns

a pair pw, aq which satisfies

Ex,,Z

" pf

pw,

a,

xq

´

f

pw°,

a°,

xqq2i



.

4 PROOFS AND TECHNICAL INSIGHTS

In this section we list our key ideas used for designing the Algorithm 1 and proving its correctness. We discuss the analysis stage-wise for ease of understnading.

4.1 ANALYSIS OF STAGE 1

Learning a non-overlapping CNN with linear activation. We first consider the problem of learning a convolutional neural network with linear activation function and non-overlapping patches. For this setting, we can write the prediction function in a compact form:

f pw, a, xq " wJP pxq a " xP pxq , waJy.

The label also admits this form y " xP pxq , w° pa°qJy. A natural way to learn w° and a° is to consider solving a square loss minimization problem:

min

pw,

a,

xq

"

´ xP

pxq

,

waJy

´

xP

pxq

,

w°

pa°

qJ

¯2 y

.

5

Under review as a conference paper at ICLR 2019

Now, taking expectation with respect to x, we have

L

pw,

aq

"

> >waJ >

´

w°

pa°qJ>>>2F

(7)

where the last step we used our assumptions that patches are non-overlapping and the covariance of
x is the identity. From equation 7, it is now apparent that the population L2 loss is just the standard loss for rank-1 matrix factorization problem.

Recent advances in non-convex optimization shows the following regularized loss function

Lreg

pw, aq

"

1 2

> >waJ >

´

w°

pa°qJ>>>F2

`

1 8

´}w}22

´

}a}22¯2

.

(8)

satisfies all local minima are global and all saddles points and local maxima has a negative curvature Ge et al. (2017a) and thus allows simple local search algorithm to find a global minimum. Though the objective function in equation 8 is a population risk, we can obtain its stochastic gradient by our samples if we use fresh sample at each iteration. We define

gwptq

´ "f

´¯ wptq, aptq, xptq

´

¯ yptq P `xt aptq

`

1 2

^> >2 >wptq> > >2

´



>>at

>2 >2

wptq

gaptq

´ "f

´¯ wptq, aptq, xptq

´

¯ yptq

P

`xt

J

wptq

`

1 2

^ >>at>>22

´

> >2 >wptq> aptq > >2

(9) (10)

where `xptq, yptq is the sample we use in the t-th iteration. In expectation this is the standard gradient descent algorithm for solving equation 8:

"i Ex gwptq

"

BLreg `wptq, aptq , Bwptq

Ex

"i gaptq

"

BLreg

`wptq, aptq .
Baptq

With this stochastic gradient oracle at hand, we can implement the noise-injected stochastic gradient descent proposed in Ge et al. (2015).

wpt`1q " wptq ´ gwptq ` wptq, apt`1q " wptq ´ gaptq ` aptq

where wptq and aptq are sampled from a unit sphere. Theorem 6 in Ge et al. (2015) implies after polynomial iterations, this iterative procedure returns an -optimal solution of the objective function equation 8 with high probability.

Learning non-overlapping part of a CNN with piece-wise linear activation function Now we consider piece-wise linear activation function. Our main observation is that we can still obtain a stochastic gradient oracle for the linear convolutional neural network using equation 2 and equation 3. Formally, we have the following theorem.

Lemma 4.1 (Properties of Stochastic Gradient for Linear CNN). Define

Lreg

pwnon, aq

"

1 2

> >>wnonaJ

´

wn°on

pa°

qJ

>2 >
>F

`

1 8

´}wnon}22

´

}a}22¯2 .

Under Assumption 3.1, we have

Ex

rg pwnon, a, x, yqs

"

BLreg pwnon Bwnon

,

aq

,

Ex

rh pwnon, a, x, yqs

"

BLreg

pwnon, aq Ba

where g pwnon, a, x, yq and h pwnon, a, x, yq are defined in equation 2 and equation 3, respectively. Further, if }wnon}2 " Oppoly p1qq, }a}2 " Oppoly p1qq, then the differences are also bounded

>
>>g >

pwnon, a,

x,

yq

´

BLreg pwnon, Bwnon

aq

> >

>

>



>
D, >>h >

pwnon,

a, x, yq

´

BLreg

pwnon, Ba

aq

> >

>

>2



D

for some D " O ppoly pB, k, 1qq.

Here the expectation of g and h are equal to the gradient of the objective function for linear CNN because we assume the input distribution is symmetric and the activation function is piece-wise linear. This observation has been stated in Goel et al. (2018) and based on this property, Goel et al.

6

Under review as a conference paper at ICLR 2019

(2018) proposed Convotron algorithm (Algorithm 3), which we use in our stage 2. Lemma 4.1 is a natural extension of Lemma 2 of Goel et al. (2018) that we show even for one-hidden-layer CNN, we can still obtain an unbiased estimate of the gradient descent for linear CNN.

Now with Lemma 4.1 at hand, we can use the theory from non-convex matrix factorization. Ge

et al.

(2017a) has

shown if 1

"

O

`poly

`

1 k

,

1 B

,



then

for all

iterates,

with

high probability,

>>>wnptoqn

> > >2

"

Oppoly

p1qq,

}at}2

"

Oppoly

p1qq.

Therefore,

we

can

apply

the

algorithmic

result

in

Ge et al. (2015) and obtain the following convergence theorem.

Theorem 4.1 (Convergence of Stage 1). If wp0q " O `?1, ap0q " O `?1, and 1 "

O

`poly

`1
k

,

1 B

,

 then after T1 " O `poly `r, k, B, 1  we have

> >> >

> apT1q

a° >

> apT1q

a° >

>

> >

>>apT1

q

> >2

´

>

}a°}2

> >2



.

or

>

> >

>>apT1

q

> >2

`

>

}a°}2

> >2



.

4.2 ANALYSIS OF STAGE 2

After Stage 1, we have approximately recovered the outer layer weights. We use these as fixed weights and run Convotron to obtain the filter weights. The analysis of Convotron inherently handles average pooling as the outer layer. Here we extend the analysis of Convotron to handle any fixed outer layer weights and also handle noise in these outer layer weights. Formally, we obtain the following theorem:

Theorem 4.2. (Learning the Convolutional Filter) Suppose }a ´ a°}2 

for



1 k3 }w° }2

and

without loss of generality1 let }a}2

"

}a°}2

" 1.

For suitably chosen 

"

O

`poly

`

1 k

,

1 B

,

Convotron (modified) returns w such that with a constant probability, }w ´ w°}2  Opk3 }w°} q

in polypk, }w°} , B, logp1{ qq iterations.

Note that we present the theorem and proof for covariance being identity and no noise in the label but it can be easily extended to handle non-identity convariance with good condition number and bounded (in expectation) probabilistic concept noise.

Our analysis closely follows that from Goel et al. (2018). However, in contrast to the known second

layer setting considered in Goel et al. (2018), we only know an approximation to the second layer

and a robust analysis is needed. Another difficulty arises from the fact that the convergence rate

depends on

the least eigenvalue of Pa

:"


1i,jk

ai

aj

Pi

PjT

.

By simple

algebra, we can show

that the matrix has the following form:

$

'1 &

Papi, jq "

k´1
i"1

aiai`1

'%0

if i " j if |i ´ j| " s otherwise.

Using property of Toeplitz matrices, we show the least eigenvalue of Pa is lower bounded by 1 ´

´¯

cos

 k`1

(c.f. Theorem A.2) for all a with norm 1.

4.3 ANALYSIS OF STAGE 3
In section we just need to show we can pick the right hypothesis. Under our assumptions, the individual loss pypiq ´ f pw, a, xpiqqq2 is bounded. Thus, a direct application of Hoeffding inequality gives the following guarantee. Theorem 4.3. Suppose T3 "  `poly `r, k, B, 1  and let pw, aq. If either `wp`q, aT1  or `wp´q, ´aT1  has population risk smaller than 2 , then let pw, aq be the output according to equation 6, then with high probability
Ex,,Z "f pw, a, x ´ f pw°, a°, xqq2  .
1Note that we can assume that the outer layers have norm 1 by using the normalized weight vectors since the activations are scale invariant.

7

Under review as a conference paper at ICLR 2019

Angle between a and a* Logarithm of Test Loss Angle between a and a*
Logarithm of Test Loss

1.5

1

0.5 0 0

s=9 s=12 s=16
2000

4000 6000 Iterations

8000 10000

(a) G: Stage 1

-4.5 s=9 s=12
-5 s=16

-5.5

-6

-6.5 0

2000

4000 6000 Iterations

8000 10000

(b) G: Stage 2

2 s=9 s=12
1.5 s=16

1

0.5

0 0123 Iterations
(c) U: Stage 1

4 104

-4 s=9 s=12
-6 s=16
-8

-10

-12

-14 02468 Iterations
(d) U: Stage 2

10 104

Figure 1: Evaluation of Algorithm 1. Gaussian input (G): 1a and 1b. Uniform input (U): 1c and 1d.

4.4 PUTTING THINGS TOGETHER: PROOF OF THEOREM 3.1

Now we put our analyses for Stage 1-3 together and prove Theorem 3.1. By Theorem 4.1, we know

´¯ we have apT1q such that >>aT1 ´ a°>>  O r1{2k5{21 (without loss of generality, we assume a and

´´

¯¯

a° are normalized) with 1 " O

poly

1 k

,

1 B

,

1 1

,

and |S1| " poly `r, k, B, 1, 1 .

Now

with

Theorem

4.2,

we

know

with



"

O

`poly

`

1 k

,

1 
B

and

|S2|

"

O `poly `k, 1, log

1 

we have either >>wp`q ´ w°>>2  1r1{2k3{2 or >>wp´q ´ w°>>2  .1r1{2k3{2 Lastly, the following

lemma bounds the loss of each instance in terms of the closeness of parameters.

Lemma 4.2. For any a and w, we have

´ f

pw°,

a°,

xq

´

f

´ wptq,

a,

¯¯2 x



2k

´}a}22

}w

´

w°}22

`

}a

´

a°}22

}w°}22¯

}x}22

.

Therefore, we know either `wp`q, apT1q or `wp´q, ´apT1q achieves prediction error. Now combining Theorem 4.3 and Lemma A.1 we obtain the desired result.

5 EXPERIMENTS

In this section we use simulations to verify the effectiveness of our proposed method. We fix input

dimension d " 160 and filter size r " 16 for all experiments and vary the stride size s " 9, 12, 16.

For all experiments, we generate w° and a° from a standard Gaussian distribution and use 10, 000

samples to calculate the test error. Note in Stage 2 of Algorithm we need to test a " apT1q and

´apT1q. Here we only report the one with better performance in the Stage 2 because in Stage 3 we

can decide which one is better. To measure the performance of Stage 1, we use the angle between

at an a° (in radians). We first test Gaussian input distribution x ,, N p0, Iq. Figure 1a shows the

convergence in Stage 1 of Algorithm 1 with T1 " 10000 and 1 " 0.0001. Figure 1b shows the

convergence in Stage input distribution x ,,

2UonfifAr´lg?or3it,h?m3s1d

with (this

T2 " 10000 and 2 " 0.0001. We then test uniform distribution has identity covariance). Figure 1c shows

the convergence in Stage 1 of Algorithm 1 with T1 " 40000 and 1 " 0.0001. Figure 1d shows the

convergence in Stage 2 of Algorithm 1 with T2 " 100000 and 2 " 0.00001. Note for both input

distributions and all choices of stride size, our algorithm achieves low test error..

6 CONCLUSION AND FUTURE WORK
In this paper, we propose the first efficient algorithm for learning a one-hidden-layer convolutional neural network with possibly overlapping patches. Our algorithm draws ideas from isotonic regression, landscape analysis of non-convex problem and spectral analysis of Toeplitz matrices. These findings can inspire further development in this field. Our next step is extend our ideas to design provable algorithms that can learn complicated models consisting of multiple filters. To solve this problem, we believe the recent progress on landscape design (Ge et al., 2017b) may be useful.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Eric B Baum. A polynomial time algorithm that learns two hidden unit nets. Neural Computation, 2(4):510­522, 1990.
Albrecht Boeo´ttcher and Sergei M Grudsky. Spectral properties of banded Toeplitz matrices, volume 96. Siam, 2005.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192­204, 2015.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. arXiv preprint arXiv:1611.01540, 2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points ´ online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pp. 797­842, 2015.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In Proceedings of the 34th International Conference on Machine Learning, pp. 1233­1242, 2017a.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017b.
Surbhi Goel and Adam Klivans. Eigenvalue decay implies polynomial-time learnability for neural networks. arXiv preprint arXiv:1708.03708, 2017a.
Surbhi Goel and Adam Klivans. Learning depth-three neural networks in polynomial time. arXiv preprint arXiv:1709.06010, 2017b.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the ReLU in polynomial time. arXiv preprint arXiv:1611.10258, 2016.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches. arXiv preprint arXiv:1802.02547, 2018.
Benjamin D Haeffele and Rene´ Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning, pp. 1724­1732, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information Processing Systems, pp. 586­594, 2016.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation. arXiv preprint arXiv:1705.09886, 2017.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint arXiv:1607.06534, 2016.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017a.
Quynh Nguyen and Matthias Hein. The loss surface and expressivity of deep convolutional neural networks. arXiv preprint arXiv:1710.10928, 2017b.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, pp. 774­782, 2016.
9

Under review as a conference paper at ICLR 2019
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv preprint arXiv:1712.08968, 2017.
Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse connectivity. arXiv preprint arXiv:1412.2693, 2014.
Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591, 2017.
Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Eric W Weisstein. Gershgorin circle theorem. 2003. Yuchen Zhang, Jason D Lee, Martin J Wainwright, and Michael I Jordan. Learning halfspaces and
neural networks with random initialization. arXiv preprint arXiv:1511.07948, 2015. Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural
networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a. Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b. Pan Zhou and Jiashi Feng. The landscape of deep learning algorithms. arXiv preprint
arXiv:1705.07038, 2017.
10

Under review as a conference paper at ICLR 2019

A USEFUL LEMMAS/THEOREMS

In this section we present a few lemmas/theorems that are useful for our analysis.

Proof of Lemma 4.2. Observe that,

´ ´ ¯¯2 f pw°, a°, xq ´ f wptq, a, x



2

´ pf

pw°,

a°,

xq

´

f

pw°,

a,

xqq2

`

pf

pw°,

a,

xq

´

f

pw,

a,

xqq2¯

since pa ` bq2  2 `a2 ` b2 for all a, b P R.

The first term can be bounded as follows,

~ k ¸2

pf pw°, a°, xq ´ f pw°, a, xqq2 "

ÿ

pa°i

´

aiq



´ pw°

qT

¯ Pix

i"1

~ k ¸2  ÿ |a°i ´ ai|| pw°qT Pix|

i"1

~ k ¸2  ÿ |ai° ´ ai| }w°}2 }x}2

i"1

 k }a° ´ a}22 }w°}22 }x}22 .

?Here the k }v}2

first inequality for all v P Rk.

follows

from

observing

that

 paq



|a|

and

the

last

follows

from

}v}1



Similarly, the other term can be bounded as follows,

~ k ¸2

pf pw°, a, xq ´ f pw, a, xqq2 "

ÿ

ai

´ 

´pw°qT

¯ Pix

´



`wT

¯ Pix

i"1

~ k ¸2  ÿ |ai|| pw° ´ wqT Pix|

i"1

~ k ¸2  ÿ |ai| }w° ´ w}2 }x}2

i"1

 k }a}22 }w° ´ w}22 }x}22 .

Here we use the Lipschitz property of  to get the first inequality. The lemma follows from combining the above two.

The following lemma extends this to the overall loss.

Lemma A.1. For any a and w,

´ Er f

pw°,

a°,

xq

´

f

´ wptq,

a,

¯¯2 xs



2kB

´}a}22

}w

´

w°}22

`

}a

´

a°}22

}w°}22¯

.

The following lemma from Goel et al. (2018) is key to our analysis.

Lemma A.2 (Lemma 1 of Goel et al. (2018)). For all a, b P Rn, if Z is symmetric then,

Ex,,Z r

`aT x `bT xs

"

1

` 2

 Ex,,Z r`aT x `bT xs.

The following well-known theorem is useful for bounding eigenvalues of matrices.

Theorem n
j"1,ji

A.1 (Gershgorin Circle |Ai,j|. Each eigenvalue

Theorem Weisstein (2003)). of A must lie in at least one

For a of the

n ^ n matrix A, define Ri :" disks tz : |z ´ Ai,i|  Riu.

11

Under review as a conference paper at ICLR 2019

The following lemma bounds the eigenvalue of the weighted patch matrices.

Theorem A.2. For all a P Sk´1,

min

pPaq



1

´

cos

^

k

 `

1



and

max

pPaq



1

`

cos

^

k

 `

1



.

Proof.

Since s 

t

r 2

u

`

1,

only

adjacent

patches

overlap,

and

it

is

easy

to

verify

that

the

matrix

Pa

has the following structure:

$

'1 &

Pa pi, jq "

k´1
i"1

ai

ai`1

'%0

if i " j if |i ´ j| " s otherwise.

Using the Gershgorin Circle Theorem, stated below, we can bound the eigenvalues, min pPaq 

1

´

k´1  i"1



aiai`1

 

and

max

pPaq



1

`

k´1  i"1

 aiai`1.

To

bound

the

eigenvalues,

we

will

bound

k´1  i"1

 aiai`1

by

maximizing

it

over

all

a

such

that

}a}2

"

1.

We

have

max

!k´1  i"1

)

aiai`1

 

"

max

!k´1
i"1

) aiai`1

since

the

maximum

can

be

achieved

by

setting all ai to be non-negative. This can alternatively be viewed as max}a}2"1 aT Ma " max pMq where M is a tridiagonal symmetric Toeplitz matrix as follows:

M

pi,

jq

"

"1{2 0

if |i ´ j| " 1 otherwise.

´¯

It is well known that the eigenvalues of this matrix are of the form cos

i k`1

for i " 1, . . . , k (c.f.

´¯

Boeo´ttcher & Grudsky (2005)). The maximum eigenvalue is thus cos

 k`1

. This gives us the

result.

B OMITTED PROOFS

B.1 PROOF OF LEMMA 4.1

First by definition, we have

Ex

rg

pwnon,

a,

x,

yqs

"Ex

«

1

2 `



´f^pwnon,

a,

xq

´

¯ y

k
ÿ
j"1

aj Pnj onx

`

1 4

´}wnon}22

´

}a}22¯

ff wnon

.

Because the input distribution is symmetric and the covariance is identity, by Lemma A.2, we have

Ex

« 1

2 `



f^pwnon,

a,

xq

kff ÿ aj Pnj onx
j"1

"Ex

« 1

2 `



k
ÿ
j"1

aj 

k
`wnJonPnj onx ÿ
j"1

ff aj Pnj onx

«k k ff "Ex ÿ aj wnJonPjnonx ÿ aj Pjnonx

j"1

j"1

" }a}22 wnon.

Similarly, we have

«k

ff

Ex y ÿ aj Pjnonx " aJa°w°.

j"1

Also recall

BLreg pw, aq Bw

"

}a}22 wnon

´ aJa°w°

`

1 4

´}wnon}22

´

}a}22¯ wnon.

12

Under review as a conference paper at ICLR 2019

Thus

Ex

rg pwnon, a, x, yqs

"

BLreg pw, aq . Bw

The proof for h pwnon, a, x, yq is similar.

To obtain a bound of the gradient, note that

> >

k
ÿ

>

>

aj 

`wnJonPjnonx

k
ÿ

>

>

aj

Pjnon

x> >



k
ÿ

|aj |

wnJonPjnonx

> >

k
ÿ

>

>

>

>

aj

Pjnon

x> >

>j"1

j"1 > j"1

>j"1

>

k



max
j

|aj |

ÿ

}wnon}2

>>Pjnonx>>2

¨

}a}2

}x}2

j"1

k }a}22 }x}22 }w}2 "poly pk, 1, Bq .

Similar

argument

applies

to

y

k
j"1

aj

Pjnonx.

B.2 PROOF OF THEOREM 4.2

We follow the Convotron analysis and include the changes. Define St " tpx1, y1q , . . . , pxt, ytqu. The modified gradient update is as follows,

~k ¸

´´

¯¯

gptq " yt ´ f wptq, a, xt

ÿ aiPixt

i"1

The dynamics of Convotron can then be expressed as follows:

> >2 >

>2

´ ¯T

Ext,yt r>>wptq

´

w°> >2

´>>wpt`1q

´

w°> >2

|St´1s

"

2Ext,yt r

w° ´ wptq

gptq|St´1s´2Ext,yt r||gptq||2|St´1s.

We have,

,,´ ¯T  Ext,yt w° ´ wptq gptq|St´1

"

Ext ,yt

«
´ w°

´

¯T wptq

´ yt

´

f

´ wptq,

a,

¯¯ xt

~k ÿ

¸ ff 
aiPixt St´1

i"1 

"

Ext

«
´ w°

´

¯T wptq

´ f

pw°,

a°,

xtq

´

f

´ wptq,

a,

¯¯ xt

~k ÿ

¸ ff 
aiPixt St´1

i"1 

"

ÿ

^ Ext r a°i  ´pw°qT

¯

^ ´

¯T

Pixt ´ ai wptq

 ^

Pixt

aj

pw°qT

´ aj

´ ¯T 

wptq

Pj xt|St´1s

1i,jk

1` "2

ÿ

Ext

,,^^ ai° pw°qT

´ ¯T   ^

´ ai wptq

Pixt aj

pw°qT

´ aj

´ ¯T 



wptq

Pj xt|St´1

1i,jk

(11)

1` "2

ÿ

^ a°i pw°qT

´ ¯T 

´ ai wptq

PiExt rxtxTt sPjT

´ aj w°

¯ ´ aj wptq

1i,jk

(12)

1` "2

ÿ

^ ai° pw°qT

´

ai pw°qT

`

ai pw°qT

´

´ ¯T 

ai wptq

PiPjT

´ aj w°

´

¯ aj wptq

1i,jk

~¸

1` "2

´ pw°

qT

´

pw°qT ¯ Pa

pw°

´

w°q

`

ÿ

pa°i

´

aiq

aj

pw°qT

PiPjT

´ w°

´

¯ wptq

1i,jk

(13)

13

Under review as a conference paper at ICLR 2019

¨

>

>> >





1` 2

>

min

pPa

q

>wptq >

´

>2 w°>
>2

´

}w°}2

> > >

ÿ

pai°

>1ik

>>

´ aiq Pi>>

> >

ÿ

>>

aj

Pj

> >

>wptq >

>
2

>1jk

>
2

>

´

w°

> ,
>2

(14)



1` 2

^

>

min

pPa

q

>wptq >

´

>2 w°>
>2

´

k }w°} }a°

>

´

a}2

}a}2

>wptq >



>

´

w°

> >2



1

` 2



^ min

pPaq

> >wptq >

´

>2 w°>
>2

´

k



>>

}w°}2

>wptq >

´

w°> >2

(11)

follows

from

using

Lemma

A.2,

(13)

follows

from

defining

Pa

:"


1i,jk

aiaj PiPjT ,

(12)

follows from setting the covariance matrix to be identity and (14) follows from observing that Pa is

symmetric, thus @x, xT Pax  min pPaq }x}22 as well as lower bounding the second term in terms

of the norms of the corresponding parts.

Now we bound the variance of gptq.

Ext,yt r||gptq||2|St´1s

"

Ext ,yt

»
´ ­ yt

´

f

´ wptq,

a,

¯¯2 xt

   

k
ÿ

2 fi





aiPixt

 

St´1fl



i"1

 

,, ´

´ ¯¯2  

 max pPaq Ext f pw°, a°, xtq ´ f wptq, a, xt ||xt||2St´1





2kmax

pPaq

^ }a}22

> >wptq >

´

>2 w°>
>2

`

}a

´

a°}22

 }w°}22

Ext

" }xt

}24

i

^> >2

 2kBmax pPaq

>wptq >

´

w°> >2

`

 2 }w°}22

(15) (16) (17)

(15)

follows

from

observing

that

k  i"1

2 aiPix



max

pPaq

}x}22

for

all

x

and

(16)

follows

from

Lemma 4.2.

Combining the above equations and taking expectation over St´1, we get

> >2

> >2

>>

ESt r>>wpt`1q ´ w°>>2s



`1 ´ 2

` 2 ESt´1 r>>wptq

´ w°>>2s ` 2

ESt´1 r>>wptq

´

w°

> >2

s

`

2

2

> >2  `1 ´ 2 ` 2 ESt´1 r>>wptq ´ w°>>2s ` 2

b

ESt´1 r>>wptq

´

w°

>2 >2

s

`

2

2

for



"

1` 2

min

pPaq,



"

2max

pPaq kB,



"

1` 2

k

}w°

}2

and



"

2max

pPaq kB

}w°}22.

´¯

From Theorem A.2, we have that min pPaq " 1 ´ cos

 k`1

"  `1{k2 (by Taylor expansion)

implying



"



`1{k2

and



"

O

pkBq,



"

O

´ kB

}w°}22¯.

We

set



"

´¯

 min

1 

,

1 

.

First

we

show

that

ESt´1 r>>wptq

´

w°

>2 >2s



1 for all

iterations t.

We

prove this inductively. For t " 1, since w1 " 0, this is satisfied. Let us assume it holds for iteration

t, then we have that,

> >2

> >2

ESt r>>wpt`1q ´ w°>>2s  `1 ´ 2 ` 2 ESt´1 r>>wptq ´ w°>>2s ` 2

b

ESt´1 r>>wptq

´

w°

>2 >2

s

`

2

2

 1 ´ 2 ` 2 ` 2 ` 2 2

 1 ´ 2 `  ` 2 ` 

 1 ´  p ´ p ` q q  1

The last inequality follows from



1 k3 }w° }2



 `

.

Thus we have that for each iteration,

ESt´1 r>>wptq

´

w°

>2 >2

s



1.

Substituting this in the recurrence and solving the recurrence gives

14

Under review as a conference paper at ICLR 2019

us,

> >2

> >2

ESt r>>wpt`1q

´

w°

> >2

s



`1

´

2

`

2

ESt´1 r>>wptq

´

w°

> >2

s

`

2

` 2 2

> >2  p1 ´ q ESt´1 r>>wptq ´ w°>>2s ` 2 `  2

t´1
 p1 ´ qt }w1 ´ w°}22 ` `2 `  2 ÿ p1 ´ qi

i"0



p1

´

qt

`

2 

`

2

´¯

Thus for T " O

1 

log

`1

, we have,

> ESt r>>wpt`1q

´

>2 w°>>2s



O

^

 



"

O

`k3

}w°}2

.

Now using Markov's inequality, we know that the above holds for some constant probability.

B.3 PROOF OF THEOREM 4.3
For i " T2 ` 1, . . . , T3, define zpiq " `ypiq ´ f `w, a, xpiq2. Using our assumptions, we know zpiq  O ppoly pr, k, Bqq almost surely. Now applying Hoeffding inequality we obtain our desired result.

15

