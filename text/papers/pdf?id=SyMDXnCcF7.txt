Under review as a conference paper at ICLR 2019
A MEAN FIELD THEORY OF BATCH NORMALIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. We find that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamical range.
1 INTRODUCTION
Deep neural networks have been enormously successful across a broad range of disciplines. These successes are often driven by architectural innovations. For example, the combination of convolutions (LeCun et al., 1990), residual connections (He et al., 2015), and batch normalization Ioffe & Szegedy (2015) has allowed for the training of very deep networks and these components have become essential parts of models in vision (Zoph et al.), language Chen & Wu (2017), and reinforcement learning Silver et al. (2017). However, a fundamental problem that has accompanied this rapid progress is a lack of theoretical clarity. An important consequence of this gap between theory and experiment is that two important issues become conflated. In particular, it is generally unclear whether novel neural network components improve generalization or whether they merely increase the number of hyperparameter configurations where good generalization can be achieved. Resolving this confusion has the promise of allowing researchers to more effectively and deliberately design neural networks.
Recently, progress has been made (Poole et al., 2016; Schoenholz et al., 2016; Daniely et al., 2016; Pennington et al., 2017) in this direction by considering neural networks at initialization, before any training has occurred. In this case, the parameters of the network are random variables which induces a distribution of the activations of the network as well as the gradients. Studying these distributions is equivalent to understanding the prior over functions that these random neural networks compute. Picking hyperparameters that correspond to well-conditioned priors ensures that the neural network will be trainable and this fact has been extensively verified experimentally. However, to fulfill its promise of making neural network design less of a black box, these techniques must be applied to neural network architectures that are used in practice. Over the past year, this gap has closed significantly and theory for networks with skip connections (Yang & Schoenholz, 2017; 2018), convolutional networks (Xiao et al., 2018), and gated recurrent networks (Chen et al., 2018) have been developed.
Before state-of-the-art models can be studied in this framework, a slowly-decreasing number of architectural innovations must be studied. One particularly important component that has thus-far remained illusive is batch normalization. In this paper, we develop a theory of random, fully-connected networks with batch normalization. A significant complication in the case of batch normalization (compared to e.g. layer normalization or weight normalization) is that the statistics of the network
1

Under review as a conference paper at ICLR 2019
depend non-locally on the entire batch. Thus, our first main result is to recast the theory for random fully-connected networks so that it can be applied to batches of data. We then extend the theory to include batch normalization explicitly and validate this theory against Monte-Carlo simulations. We show that as in previous cases we can leverage our theory to predict valid hyperparameter configurations.
In the process of our investigation, we identify a number of previously unknown properties of batch normalization that make training unstable. In particular, we show that for any choice of nonlinearity, gradients of fully-connected networks with batch normalization explode exponentially in the depth of the network. This imposes strong limits on the maximum trainable depth of batch normalized networks that can be ameliorated by pushing activation functions to be more linear at initialization. It might seem that such gradient explosion ought to lead to learning dynamics that are unfavorable. However, we show that networks with batch normalization causes the scale of the gradients to naturally equilibrate after a single step of gradient descent (provided the gradients are not so large as to cause numerical instabilities).
Finally, we note that there is a related vein of research that has emerged that leverages the prior over functions induced by random networks to perform exact Bayesian inference (Lee et al., 2017). One of the natural consequences of this work is that the prior for networks with batch normalization can be computed exactly in the wide network limit. As such, it is now possible to perform Bayesian inference in the case of wide neural networks with batch normalization.
2 RELATED WORK
Batch normalization has rapidly become an essential part of the deep learning toolkit. Since then, a number of similar modifications have been proposed including layer normalization Ba et al. (2016) and weight normalization Salimans & Kingma (2016). Comparisons of performance between these different schemes have been challenging and inconclusive Gitman & Ginsburg (2017). Since the original introduction of batchnorm in Ioffe & Szegedy (2015), which proposed that batchnorm prevents "internal covariate shift" as the explanation for their effectiveness. Since then, several papers have approached batchnorm from a theoretical angle, especially following Ali Rahimi's famous call to action at NIPS 2018. Balduzzi et al. (2017) found that batchnorm in resnets allow deep gradient signal propagation in contrast to the case without batchnorm. Santurkar et al. (2018) found that batchnorm does not help covariate shift but helps by smoothing loss landscape. Bjorck et al. (2018) reached the opposite conclusion as our paper for residual networks with batchnorm, that batchnorm works in this setting because it induces beneficial gradient dynamics and thus allows a much bigger learning rate. Luo et al. (2018) explores similar ideas that batchnorm allows large learning rates and likewise uses random matrix theory to support their claims. Kohler et al. (2018) identified situations in which batchnorm can provably induce acceleration in training. Of the above that mathematically analyze batchnorm, all but Santurkar et al. (2018) make simplifying assumptions on the form of batchnorm and typically do not have gradients flowing through the batch variance. Even Santurkar et al. (2018) only analyzes a deep linear network which gets added a batchnorm layer at a single moment in training. Our analysis here works for arbitrarily deep batchnorm networks with any activation function used in practice1. It is an initialization time analysis, but we use such insight to predict training and test time behavior.
3 THEORY
We begin with a brief recapitulation of mean field theory in the fully-connected setting. In addition to recounting earlier results, we rephrase the formalism developed previously to compute statistics of neural networks over a batch of data. Later, we will extend the theory to include batch normalization. A fully-connected network of depth L whose layers have width Nl is defined by an activation function2  along with weights, W l  RNl-1ОNl , and biases, bl  RNl . Given a batch of B inputs3
1upper bounded by an exponential function, for example. 2The activation function may be layer dependent, but for ease of exposition we assume that it is not. 3Throughout the text, we assume that all elements of the batch are unique.
2

Under review as a conference paper at ICLR 2019

{xi : xi  RN0 }i=1,иии ,B, the pre-activations of the network are defined by the recurrence relation,

zi1 = W 1xi + b1 and zil = W l(zil-1) + bl  l > 1.

(1)

At initialization, we choose the weights and biases to be i.i.d. as Wl   N (0, w2 /Nl-1) and
bl  N (0, b2). We will be concerned with understanding the statistics of the pre-activations and the gradients induced by the randomness in the weights and biases. For ease of exposition we will
typically take the network to have constant width Nl = N .

In the mean field approximation, we iteratively replace the pre-activations in eq. (2) by Gaussian
random variables with matching first and second moments. In the infinite width limit this approxi-
mation becomes exact Lee et al. (2017). Since the weights are i.i.d. with zero mean it follows that
the mean of each pre-activation is zero and the covariance between distinct neurons are zero. The pre-activation statistics are therefore given by (zl 11, и и и , zl BB) -N-l---1-- N (0, l1иииB ) where l are B О B covariance matrices. The covariance matrices are defined by the recurrence relation,

l = w2 V(l-1) + b211T

(2)

where V() = E[(h)(h)T : h  N (0, )] computes the matrix of uncentered second moments of (z) for z  N (0, ). At first eq. (2) may seem challenging since the expectation involves a Gaussian integral in RB. However, each term in the expectation of V involves at most a pair of pre-activations and so the expectation may be reduced to the evaluation of O(B2) two-dimensional
integrals. For many choices of activation function these integrals may be done analytically and so
eq. (2) defines a computationally efficient method for computing the statistics of neural networks
after random initialization. This theme of dimensionality reduction will play a prominent role in the
forthcoming discussion on batch normalization.

Eq. (2) defines a dynamical system over the space of covariance matrices. Studying the statistics of random feed-forward networks therefore amounts to investigating this dynamical system and is an enormous simplification compared with studying the pre-activations of the network directly. As is common in the dynamical systems literature, a significant amount of insight can be gained by investigating the behavior of eq. (2) in the vicinity of its fixed points. For most common activation functions, eq. (2) has a fixed point at . Moreover, when the inputs are non-degenerate, this fixed point generally has a simple structure with  = q[(1 - c)I + c11T ] owing to permutation symmetry among elements of the batch. We refer to fixed points with such symmetry as Batch Symmetry Breaking 1 (BSB1) fixed points. As we will discuss later, in the context of batch normalization other fixed points with fewer symmetries may become preferred. In the fully-connected setting fixed points may efficiently be computed by solving the fixed point equation induced by eq. (2) in the special case B = 2. The structure of this fixed point implies that in asymptotically deep feed-forward neural networks all inputs yield pre-activations of identical norm with identical angle between them. Neural networks that are deep enough so that their pre-activation statistics lie in this regime have been shown to be untrainable.

Notation As we often talk about matrices and also linear operator over matrices, we write T {} for an operator T applied to a matrix , and matrix multiplication is still written as juxtaposition. Composition of matrix operators are denoted with T1  T2.

To understand the behavior of eq. (2) near its fixed point we can consider the Taylor series in the deviation from the fixed point, l = l - . To lowest order we generically find,

l =

dV d

{l-1}
=

(3)

where J

=

dV d

=

is the B2 О B

non-linearity one could consider the

2 Jacobian of V. special case of B

In most prior work where  was a = 2 which naturally gave rise to

pointwise linearized

dynamics in ql = E[(zil)2] and cl = E[zilzjl ]/ql. However, in the case of batch normalization we will see that one must consider the evolution of eq. (3) as a whole. This is qualitatively reminiscent of the

case of convolutional networks studied in Xiao et al. (2018) where the evolution of the entire pixel

О pixel covariance matrix had to be evaluated. The dynamics induced by eq. (3) will be controlled

by the eigenvalues of J. Suppose J has eigenvalues i - ordered such that 1  2  и и и  B2 with associated eigen"vectors" ei (note that the ei will themselves be B О B matrices). It follows

3

Under review as a conference paper at ICLR 2019

that if 0 = i ciei for some choice of constants ci then l = i ciilei. Thus, if i < 1 for all i, l will approach zero exponentially and the fixed-point will be stable. The number of layers over which  will approach  will be given by -1/ log(1). By contrast if i > 1 for any i then the fixed point will be unstable. In this case, there is typically a different, stable, fixed point that must be identified. It follows that if the eigenvalues of J can be computed then the dynamics will follow immediately.
At face value, J is a complicated object since it simultaneously has large dimension and possesses an intricate block structure. However, the permutation symmetry of  induces strong symmetries in J that significantly simplify the analysis [B.4]. In particular Jijkl is a four-index object, however Jijkl = J(i)(j)(k)(l) for all permutations  on B and Jijkl = Jjilk. We call linear operators possessing such symmetries ultrasymmetric and show that all ultrasymmetric matrices admit an eigen-decomposition that contains three distinct eigenspaces with associated eigenvalues [B.37].
Theorem 1. Let T be an ultrasymmetric matrix operator. Then it has the following eigenspaces,

1. Two 1-dimensional eigenspaces whose eigenvectors have identical structure to ,

e1i = (i1 - 1)I + (1 + 1 - i1)11T

(4)

with eigenvalue 1i .

2. Two (B - 1)-dimensional eigenspaces whose eigenvectors are permutations of the matrix,

2i - 2 0 -2 -2 и и и

0 

-(i2 - 2) 2

2 и и и 

e2i

=

 



-2 -2

2 2

0 0 и и и 
0 0 и и и

 ...

...

...

...

 ...

(5)

with eigenvalues 2i .
3. An eigenspace of dimension B(B - 3)/2 whose eigenvectors are of the form e3 = GG such that e3 is symmetric and Diag(e3) = 0. The eigenvalue of all such eigenvectors is 3.

The eigenvalues as well as  and  are not arbitrary but depend on the specific choice of ultrasymmetric matrix. In the case of fully-connected networks, the number of distinct eigenspaces reduces to two whose eigenvalues are identical to those found via the simplified analysis presented in Schoenholz et al. (2016) [B.62].

Similar arguments allow us to develop a theory for the statistics of gradients. The backpropogation algorithm gives an efficient method of propagating gradients from the end of the network to the earlier layers as,

L Wl =

il(zil-1)T

i

l i =  (zl i)

Wl+1l+i 1.



(6)

Here il

=

L  zil

are Nl-dimensional vectors that describe the error signal from neurons in the l'th

layer due to the i'th element of the batch. The preceding discussion gave a precise characterization

of the statistics of the zil that we can leverage to understand the statistics of il. It is easy to see that

E[l i] = 0 and E[l il j] = ~ lij where ~ l is a covariance matrix and we may once again drop

the neuron index. We can construct a recurrence relation to compute ~ l,

~ l = w2 V (l) ~ l+1.

(7)

Typically, we will be interested in understanding the dynamics of ~ l when l has converged exponentially towards its fixed point. Thus, we study the approximation,

~ l  w2 V () ~ l+1.

(8)

Since these dynamics are linear, explosion and vanishing of gradients will be controlled by the eigenvalues of V ().

4

Under review as a conference paper at ICLR 2019

3.1 BATCH NORMALIZATION

We now extend the mean field formalism to include batch normalization. Here, the definition for the neural network is modified to the coupled equations,

zil = W l(z~il-1) + bl

z~l i

=



zl i - 

х

+ 

(9)

where х

=

1 Nl

i zi and 2 =

1 Nl

i(zi - х)2 + are the per-neuron batch statistics. In

practice  10-5 or so to prevent division by zero, but in this paper, unless stated otherwise (in the

last few sections), is assumed to be 0. Unlike in the case of vanilla fully-connected networks, here

the pre-activations and b2 = 0 for the

are invariant to w2 and b2. Without a loss of generality, we therefore set w2 remainder of the text. In principal, batch normalization additionally yields a

=1 pair

of hyperparameters  and  which are set to be constants. However, these may be incorporated into

the nonlinearity and so without a loss of generality we set  = 1 and  = 0.

The arguments from the previous section can proceed identically and we conclude that as the width

of the network grows, the pre-activations will be jointly Gaussian with identically distributed neu-

rons. Thus, we arrive at an analogous expression to eq. (2),

l = V~(l-1)

where

~(h) = 

 BGh
||Gh||

.

(10)

Here

we

have

introduced

the

projection

operator

G

=

I

-

1 B

11T

which

is

defined

such

that

Gx

=

x - х1 with х = i xi/N . Unlike , ~ is does not act component-wise on h. It is therefore not

obvious whether V~ can be evaluated without performing a B dimensional Gaussian integral.

We present a pair of results that simplify eq. (10) to a small number of integrals - independent of B
- over V by finding integral transforms to relate the two functions. From previous work Poole et al. (2016), V can be expressed in terms of a two-dimensional Gaussian integrals independent of B. When  is degree- positive homogeneous (e.g. rectified linear activations) we can relate V and V~ by the Laplace transform [B.1.1].
Theorem 2. Suppose  : R  R is degree- positive homogeneous. For any positive semi-definite matrix  define the projection G = GG. Then

V~()

=

B ()

 dss-1 V(G(I + 2sG)-1) . 0 det(I + 2sG)

(11)

Using this parameterization when V has a closed form solution V~ involves only a single integral. We further show that for any , V~ can be related to V by Fourier transform at the expense of an
additional integral to perform the change of variables r = ||Gh|| [B.1.2].

Theorem 3. For general  : R  R with finite Gaussian moments,

V~() =


d(r2)
0

 -

d 2

eir2

V(G(-1 + 2iG/B)Gr-2) det(I + 2iG/B)

(12)

Together these theorems provide analytic recurrence relations for random neural networks with batch normalization over a wide range of activation functions. By analogy to the fully-connected case we would like to study the dynamical system over covariance matrices induced by these equations.

We begin by investigating the fixed point structure of eq. (10). As in the case of feed-forward networks, permutation symmetry implies that there exist fixed points of the form  = q[(1 - c)I + c11T . A low-dimensional integral expression for q and c can be obtained by transforming to hyperspherical coordinates [B.3.1]. Theorem 4. For B  4 the fixed point  = q[(1 - c)I + c11T ] satisfies,

q =  B2-B22-1

 d1 sinB-3 1( B1(1))2
0

qc

=

B- 2

3


d1
0

  d2 sinB-3 1 sinB-4 2( B1(1))( B2(1, 2))
0

(13) (14)

5

Under review as a conference paper at ICLR 2019

where

1() = -

B

- B

1

cos

,

2(1, 2)

=

1 B-

1

1 B

cos

1

-

 B

-

2

sin

1

cos

2

.

(15)

While these equations allow for the efficient computation of fixed points for arbitrary activation functions, significant simplification occurs when the activation functions are -homogeneous [B.3.2]. In particular, for rectified linear activations we arrive at the following result.
Theorem 5. When  = ReLU, there is a unique fixed point of the form  = uI + v11T with,

q

=

B- 1 2



3 2




B-1 2

B+1

2

c = J

-1 B-1



where

J (c)

=

1 

(

1 - c2 + ( - arccos(c))c) is the arccosine kernel Cho & Saul (2009a).

(16)

Together, these results describe the fixed points for most commonly used activation functions.

In the presence of batch normalization, when the activation function grows quickly, a winner-take-
all phenomenon can occur where a subset of samples in the batch have much bigger activations
than others. This causes the covariance matrix to form blocks of differing magnitude, breaking the BSB1 symmetry. One observes this, for example, as the degree  of -relu increases past a point transition(B) depending on the batch size B. We examine this in more detail and give concrete examples in the appendix. However, by far most of the nonlinearities used in practice, like ReLU,
leaky ReLU, tanh, sigmoid, etc, all lead to BSB1 fixed points. Thus from here on, we assume that any nonlinearity  mentioned induces l to converge to BSB1 fixed points.

3.1.1 LINEARIZED DYNAMICS

With the fixed point structure for batch normalized networks having been described, we now in-
vestigate the linearized dynamics of eq. (10) in the vicinity of these fixed points. As in the vanilla
setting, we leverage the properties of ultrasymmetric matrices; however, as a consequence of mean subtraction with batch normalization here there are only three unique eigenspaces with 11 = 12, and 12 = 22 and in this case we label them G, L, and M respectively. These eigenspaces have an intuitive interpretation and in particular G captures the size of the batch; L captures the fluctu-
ation between norms of the elements of the batch; M captures the correlation subject to zero mean
constraint.

To determine the eigenvalues of

dV~ d

=

it is helpful to consider the action of batch normalization

in more detail. In particular, we notice that ~ can be decomposed into the composition of three sepa-

rate operations, ~ =   r G. As discussed above, Gh subtracts the mean from h and we introduce the new function r(h) = Bh/||h|| which normalizes h by its standard deviation. Applying the

chain rule, we can rewrite the Jacobian as,

dV~ d

=

dVr d

 G2

(17)

where  denotes composition and G2 is the natural extension of G to act on matrices as G2{} =

GG

=

G.

It ends up being advantageous to study G2 

dVr d

and to note that the nonzero

eigenvalues of this object are identical to the nonzero eigenvalues of the Jacobian [B.42].

As in the previous section there are two distinct ways to make progress on the spectrum of eq. (17). For arbitrary nonlinearity one can transform to hyperspherical coordinates which leads to tractable integral equations for the eigenvalues. The resulting equations for the eigenvalues can be evaluated, but are complicated and the specific form is relatively unenlightening [B.47] . In the case of positivehomogeneous activation functions we arrive at a relatively compact representation for the different eigenvalues [B.68]. Here, we summarize the results for rectified linear networks.

6

Under review as a conference paper at ICLR 2019

Theorem 6. Let  = ReLU and B > 3. The eigenvalues for the different eigenspaces outlined above are

G = 0

M = 2Bv 

3 2




B+1 2

B+3

2

J

-1 B-1

L = 21v 

3 2




B+1 2

B+3

2

(B - 2) 1 - J

-1 B-1

+ B J -1 B-1 B-1

(18) (19) . (20)

Together these eigenvalues along with the fixed point outlined in Theorem 5 completely characterize the statistics of pre-activations in deep networks with batch normalization.

3.1.2 GRADIENT BACKPROPAGATION

With a mean field theory of the pre-activations of feed-forward networks with batch normalization having been developed, we turn our attention to the backpropagation of gradients. In contrast to the case of networks without batch normalization, we will see that exploding gradients at initialization are a severe problem here. To this end, one of the main results from this section will be to show that fully-connected networks with batch normalization feature exploding gradients for any choice of nonlinearity such that l  a BSB1 fixed point. Below, by "rate of gradient explosion" we mean the rate at which the gradient norm squared grows with depth.

As a starting point we seek an analog of eq. (8) in the case of batch normalization. However, because the activation functions no longer act point-wise on the pre-activations, the backpropagation equation becomes,

l i =

j

~(zl j zl i

)

Wl+1

l+j 1

(21)

where we observe the additional sum over the batch. Computing the resulting covariance matrix ~ l, we arrive at the recurrence relation,

 ~ l = w2 E 

~(h) h

T

~ l+1



~(h) h

:

h



 N (0, l)

=:

w2 V~

(l){~ l+1}

(22)

where and we have defined the linear operator ~  VF (){~ } = E[FhT ~ Fh : h  N (0, )] for any vector-indexed linear operator Fh. As in the case of vanilla feed-forward networks, here we will be concerned with the behavior of gradients when l is close to its fixed point. We therefore study the asymptotic approximation to eq. (22) given by ~ l = V~ (){~ l+1}. In this case the dynamics of ~ are linear and are therefore naturally determined by the eigenvalues of V~ ().
As in the forward case, batch normalization is the composition of three operations ~ =   r  G. Applying the chain rule, eq. (22) can be rewritten as,

 V~ () = G2  E 

(  r)(z) T z z=Gh

2  : h  N (0, ) = G2  F ()

(23)

with F () appropriately defined. Note that (V~ ())n = (G2  F ())n = (G2  F ()  G2)n-1  G2  F (), so that it suffices to study the eigendecomposition of G2  F ()  G2. Due to the symmetry of , this operator is ultrasymmetric, so that its eigenspaces are G, L, M and
we can compute its eigenvalues as in Section 3.1.1. However, this computation is not so enlightening
as to the dependence of these eigenvalues on the nonlinearity. We instead use the Laplace and Fourier
methods to derive more explicit representations of the eigenvalues. Here we highlight our results on the max eigenvalue, max = G, which determines the asymptotic dynamics of ~ l.

7

Under review as a conference paper at ICLR 2019

Theorem 7. For any well-behaved nonlinearity  such that l converges to a BSB1 fixed point with depth l  , the gradient explodes asymptotically at the rate of

(B(B - 2))2-B/2 q(1 - c)(2)

(B - 1 - z12) (z1)2 + (1 + z1z2) (z1) (z2) Z(B-5)/2 dz1 dz2
Z >0

(24)

where Z = Z(z1, z2) = B(B - 2) - B(z12 + z22) + (z1 - z2)2. Theorem 8. In a ReLU-batchnorm network, gradients explode asymptotically at the rate

(B - 3 + 2) (2 - 1)J

-1 B-1

+ 2(B - 1)J (1)

-

2

(2 - 1)(B - 3)(B - 1)(J (1) - J

-1 B-1

)

B-3

(25)

which decreases to

 -1

as B



.

In contrast, for a linear batchnorm network, the gradients

explode

asymptotically

at

the

rate

B-2 B-3

,

which

goes

to

1

as

B



.

Section 3.1.2 shows theory and simulation for ReLU gradient dynamics.

By noticing that the integral in Theorem 7 diagonalizes over the Gegenbauer basis, we obtain the

following lower bound on the gradient explosion rate:

Theorem 9 (Batchnorm causes gradient explosion). Suppose (z) has the Gegenbauer expansion

(z) =

 k=0

ak

B-3+2k

( )(B-3)

B-4+k k

Ck(B-3)/2

z B-1

, normalized so that

(B

-

1)(B-3)/2



B 2

-1

B2 

-

1 2

 B-1



 dz(z)2((B - 1) - z2)(B-4)/2 = a2k.

- B-1

k=0

(26)

Then

max =

 k=1

k

B-3+k B-3

ck

a2k

 k=1

ck

a2k

(27)

where

ck

=

1

-

(-1)k2F1(-k, B

-

3

+

k,

B 2

-

1;

B-2 2(B-1)

)

>

0

for

all

k

>

0.

Consequently,

for

any non-constant  (i.e. there is a j > 0 such that aj = 0), max > 1;  minimizes max iff it is

linear

(i.e.

ai

=

0i



2),

in

which

case

gradients

explode

at

the

rate

of

B-2 B-3

.

This contrasts starkly with the case of nonnormalized fully-connected networks, which can use the weight and bias variances to control its mean field network dynamics Poole et al. (2016); Schoenholz et al. (2016). As a corollary, we disprove the conjecture of the original batchnorm paper Ioffe & Szegedy (2015) that "Batch Normalization may lead the layer Jacobians to have singular values close to 1" in the initialization setting, and in fact prove the exact opposite, that batchnorm forces the layer Jacobian singular values away from 1.

as a hyperparameter In practice, is usually treated as small constant and is not regarded as a hyperparameter to be tuned. Nevertheless, we can investigate its effect on gradient explosion. A straightforward generalization of the analysis presented above to to the case of > 0 suggests somewhat larger values than typically used can ameliorate (but not eliminate) gradient explosion problems. See Fig. 4(c,d).

4 EXPERIMENTS
Having developed a theory for neural networks with batch normalization at initialization, we now explore the relationship between the properties of these random networks and their learning dynamics. We will see that the trainability of networks with batch normalization is controlled by gradient explosion. We quantify the depth scale over which gradients explode by  = 1/ log G where, as above, G is the largest eigenvalue of the jacobian. Across many different experiments we will see strong agreement between  and the maximum trainable depth.
We first investigate the relationship between trainability and initialization for rectified linear networks as a function of batch size. The results of these experiments are shown in fig. 2 where in

8

Under review as a conference paper at ICLR 2019

eLgenvalues lognorm

a1.8
1.6 1.4 1.2 1.0 0.8

G L 0

20 40 60 80 100 batchsLze

b60
50

40

30

20

10

0 0

20

40 60 layer

batch 8 16 32 64 128

10 c
1 0.100 0.010 0.001
10-4

80 100

Var( aa) Var( ab)
transition
0.5 1.0

1.5

2.0

1 2 3 4 5 6 7 8 9 10

d1 2 3

1 2 3

44

55

66

77

88

99

10 10

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10
11 22 33 44 55 66 77 88 99 10 10
1 2 3 4 5 6 7 8 9 10

6.06.1
4.6
3.1
1.6
1.0 0.50.51
0.45 0.38 0.32 0.25 0.18 0.12
0.1

Figure 1: Numerical confirmation of theoretical predictions. (a,b) Comparison between theoretical prediction (dashed lines) and Monte Carlo simulations (solid lines) for the eigenvalues of backwards jacobian as a function of batch size and the magnitude of gradients as a function of depth respectively for rectified linear networks. In each case Monte Carlo simulations are averaged over 200 sample networks of width 1000 and shaded regions denote 1 standard deviation. (c,d) Demonstration of the existence of a BSB1 to BSB2 symmetry breaking transition as a function of  for -homogeneous activation functions. In (c) we plot the empirical variance of the eigenvalues of the covariance matrix which clearly shows a jump at the transition. In (d) we plot representative covariance matrices for the two phases (BSB1 bottom, BSB2 top).

0.1 1.0
ab

0.1 0.55
cd

Figure 2: Batch normalization strongly limits the maximum trainable depth. Colors show test accuracy for rectified linear networks with batch normalization and  = 1,  = 0, = 10-3, N = 384, and  = 10-5B. (a) trained on MNIST for 10 epochs (b) trained with fixed batch size 1000 and batch statistics computed over sub batches of size B. (c) trained using RMSProp. (d) Trained on CIFAR10 for 50 epochs.
each case we plot the test accuracy after training as a function of the depth and the batch size and overlay 16 in white dashed lines. In fig. 2 (a) we consider networks trained using SGD on MNIST where we observe that networks deeper than about 50 layers are untrainable regardless of batch size. In (b) we compare standard batch normalization with a modified version in which the batch size is held fixed but batch statistics are computed over subsets of size B. This removes subtle gradient fluctuation effects noted in Smith & Le (2018). In (c) we do the same experiment with RMSProp and in (d) we train the networks on CIFAR10. In all cases we observe a nearly identical trainable region.
It is counter intuitive that training can occur at intermediate depths where there is significant gradient explosion. To gain insight into the behavior of the network during learning we record the magnitudes of the weights, the gradients with respect to the pre-activations, and the gradients with respect to the weights for the first 10 steps of training for networks of different depths. The result of this experiment is shown in fig. 3. Here we see that before learning, as expected, the norm of the weights is constant and independent of layer while the gradients feature exponential explosion. However, we observe that two related phenomena occur after a single step of learning: the weights grow exponentially in the depth and the magnitude of the gradients are stable up to some threshold after which they vanish exponentially in the depth. Thus, it seems that although the gradients of batch normalized networks at initialization are ill-conditioned, the gradients appear to quickly reach a stable dynamical equilibrium. Pathologically, in very high depth settings, the relative gradient vanishing can in fact be so severe as to cause lower layers to mostly stay constant during training.
9

Under review as a conference paper at ICLR 2019

a

t
<latexit sha1_base64="USoNv0fCIvi1lLEl3uP/p3RZN/c=">AAAB+HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbQM2FgmYD4gOcLeZi9Zsrt37M4JMeQX2GpvJ7b+G1t/iZvkCk18YeDhnRlmeKNUCou+/+UVNja3tneKu6W9/YPDo/LxScsmmWG8yRKZmE5ELZdC8yYKlLyTGk5VJHk7Gt/N++1HbqxI9ANOUh4qOtQiFoyisxrYL1f8qr8QWYcghwrkqvfL371BwjLFNTJJre0GforhlBoUTPJZqZdZnlI2pkPedaip4jacLh6dkQvnDEicGFcaycL9vTGlytqJitykojiyq725+W8vUiuXMb4Np0KnGXLNlofjTBJMyDwFMhCGM5QTB5QZ4X4nbEQNZeiyKrlQgtUI1qF1VQ0cN64rNT+PpwhncA6XEMAN1OAe6tAEBhye4QVevSfvzXv3PpajBS/fOYU/8j5/ABK4k4w=</latexit>
0 2 4 6 10

b

c

Figure 3: Gradients in networks with batch normalization quickly achieve dynamical equilibrium. Plots of the relative magnitudes of (a) the weights (b) the gradients of the loss with respect to the pre-activations and (c) the gradients of the loss with respect to the weights for rectified linear networks of varying depths during the first 10 steps of training. Colors show step number from 0 (black) to 10 (green).

0.1 1.0
ab

c

d

Figure 4: Three techniques for counteracting gradient explosion. Test accuracy on MNIST as a function of different hyperparameters along with theoretical predictions (white dashed line) for the maximum trainable depth. (a) tanh network changing the overall scale of the pre-activations, here   0 corresponds to the linear regime. (b) Rectified linear network changing the mean of the preactivations, here    corresponds to the linear regime. (c,d) tanh and rectified linear networks respectively as a function of , here we observe a well defined phase transition near  1. Note that in the case of rectified linear activations we use  = 2 so that the function is locally linear about 0. We also find initializing  and/or setting > 0 having positive effect on VGG19 with batchnorm. See Figs. 5 and 6
As discussed in the theoretical exposition above, batch normalization necessarily features exploding gradients for any nonlinearity that converges to a BSB1 fixed point. We performed a number of experiments exploring different ways of ameliorating this gradient explosion. These experiments are shown in fig. 4 with theoretical predictions for the maximum trainable depth overlaid; in all cases we see exceptional agreement. In fig. 4 (a,b) we explore two different ways of tuning the degree to which activation functions in a network are nonlinear. In fig. 4 (a) we tune   [0, 2] for networks with tanh-activations and note that in the   0 limit the function is linear. In fig. 4 (b) we tune   [0, 2] for networks with rectified linear activations and we note, similarly, that in the    limit the function is linear. As expected, we see the maximum trainable depth increase significantly with decreasing  and increasing . In fig. 4 (c,d) we vary for tanh and rectified linear networks respectively. In both cases, we observe a critical point at large where gradients do not explode and very deep networks are trainable.
5 CONCLUSION
In this work we have presented a theory for neural networks with batch normalization at initialization. In the process of doing so, we have uncovered a number of counter intuitive aspects of batch normalization and - in particular - the fact that at initialization it causes gradients to explode necessarily. We have introduced several methods to reduce the degree of gradient explosion enabling the training of significantly deeper networks in the presence of batch normalization. Finally, this work paves the way for future work on more advanced, state-of-the-art, network topologies.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv:1607.06450 [cs, stat], July 2016. URL http://arxiv.org/abs/1607.06450. 00329 arXiv: 1607.06450.
David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The Shattered Gradients Problem: If resnets are the answer, then what is the question? In PMLR, pp. 342Г350, July 2017. URL http://proceedings.mlr.press/v70/ balduzzi17b.html.
Johan Bjorck, Carla Gomes, and Bart Selman. Understanding Batch Normalization. June 2018. URL https://arxiv.org/abs/1806.02375.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 873Г882, Stockholmsmssan, Stockholm Sweden, 10Г15 Jul 2018. PMLR. URL http://proceedings.mlr.press/ v80/chen18i.html.
Q. Chen and R. Wu. CNN Is All You Need. ArXiv e-prints, December 2017.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pp. 342Г350, 2009a.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pp. 342Г350, 2009b. URL http://papers.nips.cc/ paper/3628-kernel-methods-for-deep-learning.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity. arXiv:1602.05897 [cs, stat], February 2016. URL http://arxiv.org/abs/1602.05897. arXiv: 1602.05897.
Igor Gitman and Boris Ginsburg. Comparison of Batch Normalization and Weight Normalization Algorithms for the Large-scale Image Classification. arXiv:1709.08145 [cs], September 2017. URL http://arxiv.org/abs/1709.08145. 00002 arXiv: 1709.08145.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/ 1512.03385. arXiv: 1512.03385.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv:1502.03167 [cs], February 2015. URL http: //arxiv.org/abs/1502.03167. arXiv: 1502.03167.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas Hofmann. Towards a Theoretical Understanding of Batch Normalization. arXiv:1805.10694 [cs, stat], May 2018. URL http://arxiv.org/abs/1805.10694. 00000 arXiv: 1805.10694.
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, pp. 396Г404, 1990.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.
Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Understanding Regularization in Batch Normalization. arXiv:1809.00846 [cs, stat], September 2018. URL http://arxiv.org/ abs/1809.00846. arXiv: 1809.00846.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pp. 4785Г4795, 2017.
11

Under review as a conference paper at ICLR 2019
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. arXiv:1606.05340 [cond-mat, stat], June 2016. URL http://arxiv.org/abs/1606.05340. arXiv: 1606.05340.
Tim Salimans and Diederik P. Kingma. Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. February 2016. URL https://arxiv.org/ abs/1602.07868. 00149.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). arXiv:1805.11604 [cs, stat], May 2018. URL http://arxiv.org/abs/1805.11604. 00000 arXiv: 1805.11604.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information Propagation. arXiv:1611.01232 [cs, stat], November 2016. URL http://arxiv.org/abs/ 1611.01232. arXiv: 1611.01232.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient descent. 2018.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5393Г5402, Stockholmsmssan, Stockholm Sweden, 10Г15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/xiao18a.html.
Greg Yang and Samuel S. Schoenholz. Meanfield Residual Network: On the Edge of Chaos. In Advances in neural information processing systems, 2017.
Greg Yang and Samuel S Schoenholz. Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion. 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition.
12

Under review as a conference paper at ICLR 2019

A VGG19 WITH BATCHNORM ON CIFAR100

We find acceleration effects, especially in initial training, due to setting > 0 and/or initializing  > 0. See Figs. 5 and 6.

lr 0.2 0.10.050.020.010.001 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.2 0.10.050.020.010.001 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc1
BNeps vacc1
BNeps tacc1
betainit vacc1
betainit

10 8 6 4 2
15 12 9 6 3
10 8 6 4 2
15 12 9 6 3

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc4
BNeps vacc4
BNeps tacc4
betainit vacc4
betainit

36 30 24 18 12 6
36 30 24 18 12 6
36 30 24 18 12 6
36 30 24 18 12 6

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc10
BNeps vacc10
BNeps tacc10
betainit vacc10
betainit

50 40 30 20 10
50 40 30 20 10
50 40 30 20 10
50 40 30 20 10

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc20
BNeps vacc20
BNeps tacc20
betainit vacc20
betainit

60 45 30 15
60 50 40 30 20 10
60 45 30 15
60 50 40 30 20 10

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc40
BNeps vacc40
BNeps tacc40
betainit vacc40
betainit

90 75 60 45 30 15
60 45 30 15
90 75 60 45 30 15
60 45 30 15

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc80
BNeps vacc80
BNeps tacc80
betainit vacc80
betainit

80 60 40 20
60 45 30 15
80 60 40 20
60 45 30 15

lr 0.2 0.10.050.020.010.001 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.2 0.10.050.020.010.001 0.0 0.05 0.1 0.2 0.5 1.0 2.0

Figure 5: We sweep over different values of learning rate,  initialization, and , in training VGG19 with batchnorm on CIFAR100 with data augmentation. We use 8 random seeds for each combina-
tion, and assign to each combination the median training/validation accuracy over all runs. We then
aggregate these scores here. In the first row we look at training accuracy with different learning rate vs  initialization at different epochs of training, presenting the max over . In the second row we do the same for validation accuracy. In the third row, we look at the matrix of training accuracy for learning rate vs , taking max over . In the fourth row, we do the same for validation accuracy.

In what follows, we adopt a slightly different notation from the main text in order to express the mean field theory of batchnorm more faithfullly.

B FORWARD DYNAMICS

Definition B.1. Let SB be the space of PSD matrices of size B О B. Given a measurable function  : RB  RB, define the integral transform V : SB  SB by V() = E[(h)2 : h  N (0, )].4 When  : R  R and B is clear from context, we also write V for V applied to the function acting coordinatewise by .

Definition B.2. For any  : R  R, let B : RB  RB be batchnorm (applied to a batch of neuronal activations) followed by coordinatewise applications of , B(x)j =

 (xj - Avg x) /

1 B

B i=1

(xi

-

Avg

x)2

(here

Avg x

=

1 B

B i=1

xi).

When



=

id

we

will

also write B = Bid. We write  for ReLU, so that B is batchnorm followed by ReLU.

4This definition of V absorbs the previous definitions of V and W in Yang & Schoenholz (2017) for the scalar case

13

Under review as a conference paper at ICLR 2019

lr 0.2 0.10.050.020.010.001 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.2 0.10.050.020.010.001 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc1
BNeps vacc1
BNeps tacc1
betainit vacc1
betainit

9.0 7.5 6.0 4.5 3.0 1.5
15.0 12.5 10.0 7.5 5.0 2.5
10 8 6 4 2
15 12 9 6 3

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc4
BNeps vacc4
BNeps tacc4
betainit vacc4
betainit

30 24 18 12 6
30 24 18 12 6
30 24 18 12 6
36 30 24 18 12 6

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc10
BNeps vacc10
BNeps tacc10
betainit vacc10
betainit

50 40 30 20 10
50 40 30 20 10
50 40 30 20 10
50 40 30 20 10

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc20
BNeps vacc20
BNeps tacc20
betainit vacc20
betainit

60 45 30 15
50 40 30 20 10
60 45 30 15
50 40 30 20 10

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc40
BNeps vacc40
BNeps tacc40
betainit vacc40
betainit

90 75 60 45 30 15
60 45 30 15
75 60 45 30 15
60 45 30 15

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.0 0.05 0.1 0.2 0.5 1.0 2.0

tacc80
BNeps vacc80
BNeps tacc80
betainit vacc80
betainit

80 60 40 20
60 45 30 15
80 60 40 20
60 45 30 15

lr 0.2 0.10.050.020.010.001 0.0 0.05 0.1 0.2 0.5 1.0 2.0

lr 0.2 0.10.050.020.010.001 0.0 0.05 0.1 0.2 0.5 1.0 2.0

Figure 6: In the same setting as Fig. 5, except we don't take the max over the unseen hyperparameter but rather set it to 0 (the default value).

Definition B.3.

Define the matrix GB

=I-

1 B

11T

.

Let SBG be the space of PSD matrices of size

B О B with
1 = 0}.

zero

mean

across

rows

and

columns,

SBG

:=

{



SB

:

GB GB

=

}

=

{



SB

:

When B is clear from context, we will suppress the subscript/superscript B. In short, for h  RB, Gh zeros the sample mean of h. G is a projection matrix to the subspace of vectors h  RB of zero coordinate sum. With the above definitions, we then have B(h) = ( BGh/ Gh ).

In this section we will be interested in studying the dynamics on PSD matrices of the form 
(l) = VB((l-1)) = E[( BGh/ Gh )2 : h  N (0, (l-1))]

(28)

where (l)  SB and  : R  R.

B.1 SIMPLIFICATION
On the face of it, the iteration map VB requires one to do an B dimensional integral. However, one can reduce this down to a constant number of dimension (independent of B) if the operator V has a closed form. There are two ways to do this: 1) The Laplace method, which reduces the integral down to 1 dimension but requires the assumption that  is positive homogeneous, and 2) The Fourier method, which reduces the integral down to 2 dimensions but allows  to be any function.

B.1.1 LAPLACE METHOD

The key insight in the Laplace Method is to apply Schiwinger parametrization to deal with normalization.

Lemma B.4 (Schwinger parametrization). For z > 0 and c > 0,



c-z = (z)-1

xz-1e-cx dx

0

14

Under review as a conference paper at ICLR 2019

The following is the key lemma in the Laplace method.

Lemma B.5 (The Laplace Method Master Equation). For A, A  N, let f : RA  RA and let k  0. Suppose f (y)  h( y ) for some nondecreasing function h : R0  R0 such that E[h(r y ) : y  N (0, IA)] exists for every r  0. Define () := E[ y -2kf (y) : y  N (0, )]. Then on {  SA : rank  > 2k}, () is well-defined and continuous, and furthermore satisfies


() = (k)-1 ds sk-1 det(I + 2s)-1/2 E[f (y) : y  N (0, (I + 2s)-1)]
0

(29)

Proof. () is well-defined for full rank  because the y -2k singularity at y = 0 is Lebesgueintegrable in a neighborhood of 0 in dimension A > 2k.
We prove Eq. (29) in the case when  is full rank and then apply a continuity argument.
Proof of Eq. (29) for full rank . First, we will show that we can exchange the order of integration


ds
0

dy

sk-1

f

(y)e-

1 2

yT

(-1

+2sI

)y

=

dy

RA RA



ds

sk-1f

(y)e-

1 2

yT

(-1 +2sI )y

0

by Fubini-Tonelli's theorem. Observe that



(2)-A/2 det -1/2

ds sk-1

dy

f (y)

e-

1 2

yT

(-1

+2sI

)y

0 RA 
= ds sk-1 det((-1 + 2sI))-1/2 E[ f (y) : y  N (0, (-1 + 2sI)-1)]

0 
= ds sk-1 det(I + 2s)-1/2 E[ f (y) : y  N (0, (I + 2s)-1)]

0

For  =  2,

f ( (I + 2s)-1y)  h( (I + 2s)-1y  h 
Because E[h(  y ) : y  N (0, I)] exists by assumption,

1

 + 2s

y

  h(  y )

E[ f (y) : y  N (0, (I + 2s)-1)]
= E[ f ( (I + 2s)-1y) : y  N (0, I)]  E[ f (0) : y  N (0, I)] = f (0)

 as s  , by dominated convergence with dominating function h( 

y

)e-

1 2

y

2 (2)-A/2.

By

the same reasoning, the function s  E[ f (y) : y  N (0, (I + 2s)-1)] is continuous. In

particular this implies that sup0s E[ f (y) : y  N (0, (I + 2s)-1)] < . Combined with

the fact that det(I + 2s)-1/2 = (s-A/2) as s  ,


ds sk-1 det(I + 2s)-1/2 E[ f (y) : y  N (0, (I + 2s)-1)]
0 
 ds (sk-1-A/2)
0

which is bounded by our assumption that A/2 > k. This shows that we can apply Fubini-Tonelli's theorem to allow exchanging order of integration.

15

Under review as a conference paper at ICLR 2019

Thus,

E[ y -2kf (y) : y  N (0, )]



= E[f (y)

ds (k)-1sk-1e- y 2s : y  N (0, )]

0
= (2)-A/2 det -1/2



dy

e-

1 2

yT

-1yf (y)

ds (k)-1sk-1e- y 2s

= (2)-A/2(k)-1 det -1/2


ds sk-1

0

dy

f

(y

)e-

1 2

yT

(-1

+2sI

)y

0

(by Fubini-Tonelli)



= (k)-1

ds sk-1 det((-1 + 2sI))-1/2 E[f (y) : y  N (0, (-1 + 2sI)-1)]

0 
= (k)-1 ds sk-1 det(I + 2s)-1/2 E[f (y) : y  N (0, (I + 2s)-1)]

0

Domain and continuity of (). The LHS of Eq. (29), (), is defined and continuous on rank /2 > k. Indeed, if  = M ICM T , where M is a full rank A О C matrix with rank  = C  A, then
E[ y -2k f (y) : y  N (0, )]
= E[ M z -2k f (M z) : z  N (0, IC )].
This is integrable in a neighborhood of 0 iff C > 2k, while it's always integrable outside a ball around 0 because f by itself already is. So () is defined whenever rank  > 2k. Its continuity can be established by dominated convergence.

Proof of Eq. (29) for rank  > 2k. Observe that det(I + 2s)-1/2 is continuous in  and, by an application of dominated convergence as in the above, E[f (y) : y  N (0, (I + 2s)-1)] is continuous in . So the RHS of Eq. (29) is continuous in  whenever the integral exists. By the reasoning above, E[f (y) : y  N (0, (I + 2s)-1)] is bounded in s and det(I + 2s)-1/2 = (s- rank /2), so that the integral exists iff rank /2 > k.
To summarize, we have proved that both sides of Eq. (29) are defined and continous for rank  > 2k. Because the full rank matrices are dense in this set, by continuity Eq. (29) holds for all rank  > 2k.

If  is degree- positive homogeneous, i.e. (ru) = r(u) for any u  R, r  R+, we can then

compute 
VB() = E[( BGh/ Gh )2 : h  N (0, )] = B E[(Gh)2/ Gh 2 : h  N (0, )]

= B(2)-B/2(det )-1/2

(Gh)2 Gh 2

e-hT

-1 h/2

dh



= B(2)-B/2(det )-1/2 (Gh)2e-hT -1h/2

()-1s-1e-s Gh 2 ds dh

0
(30)



= B(2)-B/2(det )-1/2()-1

s-1

(Gh)2e-

1 2

(hT

-1 h+2shT

Gh)

dh

ds

0



= B(det )-1/2()-1

ds s-1(det(-1 + 2sG)-1)1/2 E[(y)2 : y  N (0, G(-1 + 2sG)-1G)]

0 
= B()-1 ds s-1 det(I + 2sG)-1/2V(G(I + 2sG)-1G)

0

where in Eq. (30) we applied Schwinger's Parametrization.

If  = , then

V()ij =

1 2

ii

1 2

J1

(ij

/

iijj )

iijj

if i = j otherwise

16

Under review as a conference paper at ICLR 2019

and, more succinctly, V() = D1/2V(D-1/2D-1/2)D1/2 where D = Diag(). Here

J1(c)

:=

1 

(

1 - c2 + ( - arccos(c))c) Cho & Saul (2009b).

Matrix simplification. We can simplify the expression G(I + 2sG)-1G, leveraging the fact that G is a projection matrix.

Definition B.6. Let e be an B О (B - 1) matrix whose columns form an orthonormal basis of

im an

G := {Gv orthogonal

: v  RB} matrix. For

= {w much

 of

RB this

:i paper

wi =
e can

0}. Then the B О be any such basis,

B matrix e~ = (e|B-1/21) is
but at certain sections we will

consider specific realizations of e for explicit computation.

From easy computations it can be seen that G = e~

IB-1 0

0 0

e~T . Suppose  = e~

 vT

v a

e~T

where  is (B - 1) О (B - 1), v is a column vector and a is a scalar. Then  = eT e and

G = e~

 vT

0 0

e~T is block lower triangular, and

(I + 2sG)-1 = e~

(I + 2s )-1 

0 1

e~T

(I + 2sG)-1G = e~

(I + 2s )-1 

0 0

e~T

G(I + 2sG)-1G = e~

(I + 2s )-1 0

0 0

e~T

= e(I + 2s )-1 eT

= GG(I + 2sGG)-1

=: G(I + 2sG)-1

where Definition B.7. For any matrix , write G := GG.

Similarly, det(IB + 2sG) = det(IB-1 + 2s ) = det(IB + 2sG). So, altogether, we have Theorem B.8. Suppose  : R  R is degree- positive homogeneous. Then for any B О (B - 1)
matrix e whose columns form an orthonormal basis of im G := {Gv : v  RB} = {w  RB : i wi = 0}, with  = eT e,

VB() = B()-1 = B()-1


ds s-1 det(I + 2s )-1/2V(e(I + 2s )-1 )-1eT )
0 
ds s-1 det(I + 2sG)-1/2V((I + 2sG)-1G)-1)
0

(31) (32)

B.1.2 FOURIER METHOD
The Laplace method crucially used the fact that we can pull out the norm factor Gh out of , so that we can apply Schwinger parametrization. For general  this is not possible, but we can apply
17

Under review as a conference paper at ICLR 2019

some wishful thinking and proceed as follows

 VB() = E[( BGh/ Gh )2 : h  N (0, )]


= E[(Gh/r)2 d(r2)(r2 - Gh 2/B) : h  N (0, )]

0

= E[(Gh/r)2


d(r2)
0

 -

d 2

ei(r2

-

Gh

2 /B )

:

h



N (0, )]

"="
RB

dh

e

-1 2

hT

-1

h

(Gh/r)2

det(2)


d(r2)
0

 -

d 2

ei(r2

-

Gh

2 /B )

"="


d(r2)
0

 d - 2

RB

dh

e

-1 2

hT

-1

h

(Gh/r)2

ei(r2

-

Gh

2 /B )

det(2)

(33)

=

1 det(2)


d(r2)
0

 -

d 2

eir2

dh(Gh/r)2

e

-1 2

(hT

-1

h+2i

Gh

2 /B )

RB

(34)

1 =


d(r2)

 d eir2

dh(Gh/r)2

e

-1 2

(hT

(-1

+2iG/B

)h)

det(2) 0

- 2

RB

"="

1


d(r2)

 d eir2

det(2) 0

- 2

det(2(-1 + 2iG/B)-1) E[(Gh/r)2 : h  N (0, -1 + 2iG/B)]

=


d(r2)
0

 -

d 2

eir2

det(I

+

2iG/B)-1/2

E[(h)2

:

h



N (0,

G(-1

+

2iG/B)Gr-2))]

(35)

=


d(r2)
0

 -

d 2

eir2

det(I

+

2iG/B)-1/2V(G(-1

+

2iG/B)Gr-2)

Here all steps are legal except for possibly Eq. (33), Eq. (34), and Eq. (35). In Eq. (33), we "Fourier expanded" the delta function -- this could possibly be justified as a principal value integral, but this cannot work in combination with Eq. (34) where we have switched the order of integration, integrating over h first. Fubini's theorem would not apply here because ei(r2- Gh 2/B) has norm 1 and is not integrable. Finally, in Eq. (35), we need to extend the definition of Gaussian to complex covariance matrices, via complex Gaussian integration.
Thus the above derivation is not correct mathematically. However, its end result can be justified rigorously by carefully expressing the delta function as the limit of mollifiers.
Theorem B.9. Let  be a positive-definite D О D matrix. Let  : R  R. Suppose for any b > a > 0,

1.

b a

d(r2

)|(za/r)(zb/r)|

exists

and

is

finite

for

any

za, zb



R

(i.e.

(za/r)(zb/r)

is

locally integrable in r).

2. there exists an > 0 such that any   [- , ] and for each a, b  [D],

dz

e-

1 2

zT

-1 z |(za /

z 2[a,b]

z 2 + )(zb/

z 2 + )|

exists and is uniformly bounded by some number possibly depending on a and b.

3. for each a, b  [D],

RD dz

b a

d(r2)e-

1 2

zT

-1 z

|(za/r)(zb/r)|

exists

and

is

finite.

4.

 -

d

det(I + 2i)-1/2V((-1 + 2iI)-1r-2)

exists and is finite for each r > 0.

18

Under review as a conference paper at ICLR 2019

If

e-

1 2

zT

-1 z (z /

z

)2 is integrable over z  RD, then

(2)-D/2 det -1/2

dz

e-

1 2

zT

-1 z (z /

z

)2

1 = lim

b
ds d eis det(I + 2i)-1/2V((-1 + 2iI)-1s-1).

2 a 0,b  a

-

Similarly, (2)-D/2 det -1/2

dz

e-

1 2

zT

-1

z

 ( Dz

/

z

)2

1 = 2 a

lim
0,b 

b
ds
a


d eis det(I + 2i)-1/2V((-1 + 2iI/D)-1s-1).
-

If G is the mean-centering projection matrix and G = GG, then by the same reasoning as in Thm B.8,

VB() = (2)-D/2 det -1/2

dz

e-

1 2

zT

-1

z

 ( DGz

/

Gz

)2

1 =
2 a

lim
0,b 

b
ds
a


d eis det(I + 2iG/D)-1/2V(G(I + 2iG/D)-1s-1).
-

(36)

Note that assumption (1) is satisfied if  is continuous; assumption (4) is satisfied if D  3 and for all , V()    for some   0; this latter condition, as well as assumptions (2) and (3), will be satisfied if  is polynomially bounded. Thus the common coordinatewise nonlinearities
ReLU, identity, tanh, etc all satisfy these assumptions.

Warning: in general, we cannot swap the order of integration as

 -

d

 -

ds.

For

example,

if

 = id, then

(2)-D/2 det -1/2

dz

e-

1 2

zT

-1

z

(z/

z

)2

1 =
2 a

lim
0,b 

b
ds
a


d eis det(I + 2i)-1/2(-1 + 2iI)-1s-1
-

1 = 2


d
-


ds eis det(I + 2i)-1/2(-1 + 2iI)-1s-1
0

because the s-integral in the latter diverges (in a neighborhood of 0).

Proof. We will only prove the first equation; the second follows similarly. By dominated convergence,

dz

e-

1 2

zT

-1

z

(z/

z

)2

= lim

dze-

1 2

zT

-1

z

(z

/

z

)2I(

z

2  [a, b]).

a 0,b 

Let  : R  R be a nonnegative bump function (i.e. compactly supported and smooth) with support [-1, 1] and integral 1 such that  (0) =  (0) = 0. Then its Fourier transform ^(t) decays like O(t-2). Furthermore,  (x) := -1(x/ ) is a mollifier, i.e. for all f  L1(R), f    f in L1
and pointwise almost everywhere.

Now, we will show that

dze-

1 2

z

T

-1

z

(za

/r)(zb

/r

)I(

z

2  [a, b]) = lim

0

b

dze-

1 2

z

T

-1

z

d(r2)(za/r)(zb/r) ( z 2 - r2)

a

19

Under review as a conference paper at ICLR 2019

by dominated convergence. Pointwise convergence is immediate, because

b

d(r2)(za/r)(zb/r) (r2 - z 2) =

d(r2)(za/r)(zb/r)I(r2  [a, b]) ( z 2 - r2)

a -

= [(r2  (za/r)(zb/r)I(r2  [a, b]))   ]( z 2)

 (za/ z )(zb/ z )I( z 2  [a, b])

as  0 (where we used assumption (1) that (za/r)(zb/r)I(r2  [a, b]) is L1).

Finally, we construct a dominating integrable function. Observe

b

dze-

1 2

z

T

-1

z

d(r2)|(za/r)(zb/r)| ( z 2 - r2)

a

b

=

dz

e-

1 2

zT

-1

z

d(r2)|(za/r)(zb/r)| ( z 2 - r2)

a

b

=

dz

e-

1 2

zT

-1

z

d( z 2 + )|(za/ z 2 + )(zb/ z 2 + )| (-)

a

b- z 2

=

dz

e-

1 2

zT

-1

z

d |(za/ z 2 + )(zb/ z 2 + )| (-)

a- z 2

b- z 2

=

dz

e-

1 2

zT

-1

z

d |(za/

z 2[a- ,b+ ]

a- z 2

z 2 + )(zb/

z 2 + )| (-) since supp  = [- , ]



dz

e-

1 2

zT

-1

z

d |(za/

z 2[a- ,b+ ]

-

z 2 + )(zb/

z 2 + )| (-)

= d  (-)

dz

e-

1 2

zT

-1

z

|(za

/

- z 2[a- ,b+ ]

z 2 + )(zb/

z 2 + )|

For small enough then, this is integrable by assumption (2), and yields a dominating integrable function for our application of dominated convergence.

In summary, we have just proven that

dz

e-

1 2

zT

-1

z

(z

/

z

)2

= lim lim
a 0,b  0

b

dz

e-

1 2

zT

-1

z

d(r2)(za/r)(zb/r) ( z 2 - r2)

a

Now,

b

dz

e-

1 2

zT

-1

z

d(r2)(za/r)(zb/r) ( z 2 - r2)

a

=

dz

e-

1 2

zT

-1

z

a

b

d(r2

)(za/r)(zb/r)

1 2


d ^ ()e-i( z 2-r2)
-

1 = 2

b

dz

e-

1 2

zT

-1

z

d(r2)(za/r)(zb/r)

d ^( )e-i( z 2-r2)

a -

Note that the absolute value of the integral is bounded above by

1 2 1 = 2
C

b

dze-

1 2

zT

-1

z

d(r2)|(za/r)(zb/r)|

d |^( )e-i( z 2-r2)|

a -

b

dze-

1 2

zT

-1

z

d(r2)|(za/r)(zb/r)|

d |^( )|

a -

b

dz

e-

1 2

zT

-1

z

d(r2)|(za/r)(zb/r)|

a

20

Under review as a conference paper at ICLR 2019

for some C, by our construction of  that ^(t) = O(t-2) for large |t|. By assumption (3), this integral exists. Therefore we can apply the Fubini-Tonelli theorem and swap order of integration.

1 2

b

dz

e-

1 2

zT

-1

z

d(r2)(za/r)(zb/r)

d ^( )e-i( z 2-r2)

a -

1 = 2

b
d(r2)
a


d ^( )e-i( z 2-r2)
-

dz(za

/r)(zb

/r)e-

1 2

zT

-1

z

1 =

b

d(r2)

d ^( )eir2

2 a

-

dz

(za

/r)(zb

/r)e-

1 2

zT

(-1

+2iI

)z

1 =

b

d(r2)rD

d ^( )eir2

2 a

-

dz

(za

)(zb

)e-

1 2

r2

zT

(-1

+2iI

)z

b

= (2)D/2-1 det 1/2 d(r2)

d ^( )eir2 det(I + 2i)-1/2V((-1 + 2iI)-1r-2)

a -

By assumption (4),

b

d(r2)

d ^( )eir2 det(I + 2i)-1/2V((-1 + 2iI)-1r-2)

a -

b

 C d(r2)

d det(I + 2i)-1/2V((-1 + 2iI)-1r-2)

a -

is integrable (where we used the fact that ^ is bounded -- which is true for all   C0(R)), so by dominated convergence (applied to 0),

dze-

1 2

z

T

-1

z

(z

/

z

)2

= lim lim
a 0,b  0

b

dz

e-

1 2

zT

-1

z

d(r2)(za/r)(zb/r) ( z 2 - r2)

a

b

= (2)D/2-1 det 1/2 lim

d(r2)

d eir2 det(I + 2i)-1/2V((-1 + 2iI)-1r-2)

a 0,b  a

-

where we used the fact that ^(0) = (x) dx = 1 by construction. This gives us the desired result
R
after putting back in some constants and changing r2  s.

B.2 GLOBAL CONVERGENCE
Basic questions regarding the dynamics Eq. (28) are 1) does it converge? 2) What are the limit points? 3) How fast does it converge? Here we answer these questions definitively when  = id.

The  = id case. First we prove a lemma
Lemma B.10. Consider the dynamics (l) = E[(h/ h )2 : h  N (0, (l-1))] on (l)  SA. Suppose (0) is full rank. Then

1.

liml (l) =

1 A

I

.

2. This convergence is exponential in the sense that, for any full rank (0), there is a constant K < 1 such that 1((l)) - A((l)) < K(1((l-1)) - A((l-1))) for all l  2. Here
1 (resp. A) denotes that largest (resp. smallest) eigenvalue.

3.

Asymptotically, 1((l)) - A((l)) = O((1 -

2 A+2

)l).

Proof. Let 1(l)  (2l)  и и и  A(l) be the eigenvalues of (l). It's easy to see that i (il) = 1 for all l  1. So WLOG we assume l  1 and this equality holds.

We

will

show

the

"exponential

convergence"

statement;

that

liml (l)

=

1 A

I

then

follows

from

the trace condition above.

21

Under review as a conference paper at ICLR 2019

Proof of Item 2. Using the overbar/underbar notation for brevity, we will now compute the eigenvalues 1  и и и  A of .
First, notice that  and  can be simultaneously diagonalized, and by induction all of {(l)}l0 can be simultaneous diagonalized. Thus we will WLOG assume that  is diagonal,  = Diag(1, . . . , A), so that  = Diag(1, . . . , A) for some {i}i. These {i}i form the eigenvalues of  but a priori we don't know whether they fall in decreasing order; in fact we will soon see that they do, and i = i.
We have

i = = = = = = =

 -A/2 A 2 0A

ixi2 j j xj2

e-

x

2/2 dx

 2

-A/2

A
ix2i e- x 2/2 dx
0A


e-s
0

j j xj2 ds

 2

-A/2

A+1

i

xi2

e-

1 2

0A+1

j (1+2sj )x2j dx ds

 2

-A/2

2

i

xi2

e-

1 2

(1+2si

)xi2

и

02

 2

A-1
2 (1 + 2sj )-1/2 dxi ds
j=i

 2

-1/2

2

i

x2i

e-

1 2

(1+2si

)xi2

dxi

(1 + 2sj )-1/2 ds

02 j=i


i(1 + 2si)-3/2 (1 + 2sj )-1/2 ds

0 j=i

A
(1 + 2sj )-1/2i(1 + 2si)-1 ds
0 j=1

(37)

Therefore,

A

i - k =

(1 + 2sj)-1/2(i(1 + 2si)-1 - k(1 + 2sk)-1) ds

0 j=1

A

= (i - k)

(1 + 2sj )-1/2(1 + 2si)-1(1 + 2sk)-1 ds

0 j=1

Since the RHS integral is always positive, i  k = i  k and thus i = i for each i.

Define T () :=

 0

Aj=1(1 + 2s1)-1/2(1 + 2s1)-1(1 + 2sA)-1 ds, so that 1 - A =

(1 - A)T ().

Note first that

A j=1

(1

+

2sj )-1/2(1

+

2si)-1(1

+

2sk )-1

is

(strictly)

log-convex

and

hence

(strictly) convex in . Furthermore, T () is (strictly) convex because it is an integral of (strictly)

convex functions. Thus T is maximized over any convex region by its extremal points, and only by

its extremal points because of strict convexity.

The convex region we are interested in is given by

A := {(i)i : 1  2  и и и  A  0 & i = 1}.
i

22

Under review as a conference paper at ICLR 2019

k times

The extremal points of A are {k := (1/k, 1/k, и и и , 1/k, 0, 0, и и и , 0)}Ak=1. For k = 1, . . . , A - 1, we have


T (k) = (1 + 2s/k)-k/2-1 ds

0

=

2/k -k/2 (1

+

2s/k)-k/2

 0

= 1.

But for k = A,


T (A) = (1 + 2s/A)-A/2-2 ds

0

=

2/A -A/2 -

(1 + 1

2s/A)-k/2

 0

=

A A+2

This shows that T ()  1, with equality iff  = k for k = 1, . . . , A - 1. In fact, because every

point   A is a convex combination of k for k = 1, . . . , A,  =

A k=1

ak



k

,

by

convexity

of

T

,

we must have

A
T ()  akT (k)

k=1

=

1

-

aA

A

2 +

2

2 = 1 - A A + 2

(38)

where the last line follows because A is the only point with last coordinate nonzero so that aA = A.

We now show that the gap 1(l) - A(l)  0 as l  . There are two cases: if A(l) is bounded away from 0 infinitely often, then T () < 1 - infinitely often for a fixed > 0 so that the gap indeed vanishes with l. Now suppose otherwise, that (Al) converges to 0; we will show this leads to a contradiction. Notice that

A

A =

(1 + 2sj )-1/2A(1 + 2sA)-1 ds

0 j=1

A

A/A 

(1 + 2s(1 - A)/(A - 1))-(A-1)/2(1 + 2sA)-3/2 ds

0 j=1

where the first lines is Eq. (37) and the 2nd line follows from the convexity of jA=-11(1 + 2sj)-1/2 as a function of (1, . . . , A-1). By a simple application of dominated convergence, as A  0, this integral converges to a particular simple form,

A

A/A 

(1 + 2s/(A - 1))-(A-1)/2 ds

0 j=1

2 =1+ A-3

Thus for large enough l, A(l+1)/(Al) is at least 1 + for some > 0, but this contradicts the convergence of A to 0.

Altogether, this proves that (1l) - A(l)  0 and therefore (l)  A as l  . Consequently, A(l) is bounded from below, say by K (where K > 0 because by Eq. (37), (Al) is never 0), for all l,

23

Under review as a conference paper at ICLR 2019

and by Eq. (38), we prove Item 2 by taking K

=

1-K

2 A+2

.

In

addition,

asymptotically,

the

gap

l

decreases exponentially as T (A)l =

A A+2

, proving Item 3.

Theorem B.11. Consider the dynamics (l) = E[(h/ h )2 : h  N (0, (l-1))] on (l)  SA. Suppose (0) = M T DM where M is orthogonal and D is a diagonal matrix. If D has rank C and Dii = 0, 1  i  C, then

1.

liml (l) =

1 C

MT

D

M

where

D

is the diagonal matrix with Dii = I(Dii = 0).

2. This convergence is exponential in the sense that, for any (0) of rank C, there is a constant K < 1 such that 1((l)) - C ((l)) < K(1((l-1)) - C ((l-1))) for all l  2. Here i denotes the ith largest eigenvalue.

3.

Asymptotically, 1((l)) - C ((l)) = O((1 -

2 C+2

)l).

Proof. Note that (l) can always be simultaneously diagonalized with (0), so that (l) = M T D(l)M for some diagonal D(l) which has Di(il) = 0, 0  i  C. Then we have (D(l)) = E[(h/ h )2 : h  N (0, (D(l-1)) )], where (D(l)) means the diagonal matrix obtained from D(l) by deleting the dimensions with zero eigenvalues. The proof then finishes by Lemma B.10.

From this it easily follows the following characterization of the convergence behavior.
Corollary B.12. Consider the dynamics of Eq. (28) for  = id: (l) = B E[(Gh/ Gh )2 :
h  N (0, (l-1))] on (l)  SB. Suppose GG has rank C < B and factors as e^De^T where D  RCОC is a diagonal matrix with no zero diagonal entries and e^ is an B О C matrix whose
columns form an orthonormal basis of a subspace of im G. Then

1.

liml (l)

=

B C

e^IC

e^T

.

2. This convergence is exponential in the sense that, for any G(0)G of rank C, there is a constant K < 1 such that 1((l)) - C ((l)) < K(1((l-1)) - C ((l-1))) for all l  2. Here i denotes the ith largest eigenvalue.

3.

Asymptotically, 1((l)) - C ((l)) = O((1 -

2 C+2

)l).

General  case. We don't (currently) have a proof of any characterization of the basin of attraction for Eq. (28) for general . Thus we are forced to resort to finding its fixed points manually and
characterize their local convergence properties.

B.3 LIMIT POINTS
Batchnorm B is permutation-equivariant, in the sense that B(h) = B(h) for any permtuation matrix . Along with the case of  = id studied above, this suggests that we look into fixed points  that are invariant under permutation. These are matrices of the form
Definition B.13. We say a matrix   SB is BSB1 (short for "1-Step Batch Symmetry Breaking" or "1-Block Symmetry Breaking") if  has one common entry on the diagonal and one common entry on the off-diagonal, i.e.
a b b и и и b a b иии
 b b a и и и  ... ... ... . . .
We will denote such a matrix as BSB1(a, b). Note that BSB1(a, b) can be written as (a-b)I +b11T .
Its spectrum is given below.
Lemma B.14. A B О B matrix of the form I + 11T has two eigenvalues  and B + , each with
eigenspaces {x : i xi = 0} and {x : x1 = и и и = xB}. Equivalently, if it has a on the diagonal and b on the off-diagonal, then the eigenvalues are a - b and (B - 1)b + a.

24

Under review as a conference paper at ICLR 2019

The following simple lemma will also be very useful Lemma B.15. Let  := BSB1B(a, b). Then GG = (a - b)G.
Proof. G and BSB1B(a, b) can be simultaneously diagonalized by Lemma B.14. Note that G zeros
out the eigenspace R1, and is identity on its orthogonal complement. The result then follows from
easy computations.

What are the BSB1 fixed points of Eq. (28)? We will exhibit two method in this section to compute fixed points of BSB1 form. The spherical coordinates method works for any nonlinearity  and reduces the computation of such fixed points to 2-dimensional integrals. On the other hand, the
Laplace method sheds more light for positive-homogenous nonlinearities (like ReLU), where we
can give closed form solutions for the fixed points.

B.3.1 SPHERICAL COORDINATES METHOD
The main result (Thm B.18) of this section is an expression of the BSB1 fixed point diagonal and offdiagonal entries in terms of 1- and 2-dimensional integrals. This allows one to numerically compute such fixed points.
By a simple symmetry argument, we have the following
Lemma B.16. Suppose X is a random vector in RB, symmetric in the sense that for any permutation matrix  and any subset U of RB measurable with respect to the distribution of X, P (X  U ) = P (X  (U )). Let  : RB  RB be a symmetric function in the sense that (x) = (x) for
any permutation matrix . Then E (X)(X)T = I + 11T for some  and .

Then

Lemma B.17. Suppose Y is a random Gaussian vector in RB with zero mean and covariance
matrix  := I + 11T . Then E B(Y )2 = E B(Y )B(Y )T = I + 11T where



+



=

(2)

1-B 2

1-B
 = (2) 2

 ( Bz1(xB-1)/

x

)2e-

x

2/2 dx

RB-1

 ( Bz1(xB-1)/

x

 )( Bz2(xB-1, xB-2)/

x

)e-

x

2/2 dx

RB-1

where z1(xB-1) = -

B-1 B

xB-1

and

z2(xB-1, xB-2)

=

1
B(B-1)

xB-1

-

B-2 B-1

xB-2.

Proof. By Lemma B.16 and the permutation symmetry of B and Y , the expected outer product has
the form I + 11T for some  and , satisfying  +  = E B(Y )2i ,  = E B(Y )iB(Y )j for
any i = j. So it suffices to show that these expectations, for some i and j, are equal to the integrals
in the statement of the theorem.

By Lemma B.14, G has eigenvalues 1 and 0 respectively with eigenspaces V0 := {y : i yi = 0} and V1 := {y : y1 = и и и = yB}, and  has eigenvalues  and  + B with the same eigenspaces. Since the linear image of a Gaussian is a Gaussian, Z := GY is normally distributed with covariance
GG; in other words, Z is isotropic on V0, with variance .

i

Define ei := (i(i + 1))-1/2(1, 1, . . . , 1, -i, 0, . . . , 0)T , for each i = 1, . . . , B - 1. Then {ei}i
forms an orthonormal basis of V0. Let x  RB-1 be the random coordinates of V0 in this basis, so
that Z = eX, where e is the matrix with {ei} as columns. A short computation shows that ZB =

-

B-1 B

XB-1

=

z1(XB-1) and ZB-1

=

1
B(B-1)

XB-1

-

B-2 B-1

XB-2

=

z2(XB-1, XB-2).

25

Under review as a conference paper at ICLR 2019

Thus



E

B(Y

)B2

=

E

( B 

ZB

/

Z

)2

=

E

( B 

z1

(XB-1

)/

X 

)2

E

B(Y

)B B (Y

)B-1

=

E

( B 

ZB

/

Z

)(

BZB-1/ Z )

= E ( Bz1(XB-1)/ X )( Bz2(XB-1, XB-2)/ X )

which correspond to the integrals in the theorem statement, after noting that X is isotropic on RB with variance  because it is related to Z by an orthogonal linear transformation.

Note that, by rescaling, the integrals in Lemma B.17 actually do not depend on  and . Thus they yield the fixed point BSB1 . We can simplify the integrals further into 1- and 2-dimensions, which give the following
Theorem B.18. The BSB1 fixed point  = I + 11T to Eq. (28) satisfies



+

 

= =

(2B1B2(-2-B3202-2)10)dd1(0-0dd21s2sinisninB)B--3(3si1n1s(/inBB2--14(c2o1)s()2B3/1(21)),)(B2(1, 2)),

if B  4 if B = 3

where

1() = z1(cos ) = -

B

- B

1

cos



2(1, 2) = z2(cos 1, sin 1 cos 2) =

1 B(B

-

1)

cos

1

-

B B

- -

2 1

sin

1

cos

2

with z1, and z2 as defined in Lemma B.17,

Proof. We change the integrals in Lemma B.17 to spherical coordinates (r, 1, . . . , B-2)  [0, ) О [0, ]B-3 О [0, 2], defined by

xB-1 = r cos 1 xB-2 = r sin 1 cos 2 xB-3 = r sin 1 sin 2 cos 3
... x2 = r sin 1 и и и sin B-3 cos B-2 x1 = r sin 1 и и и sin B-3 sin B-2.

Then

dx1 . . . dxB-1 = rB-2 sinB-3 1 sinB-4 2 и и и sin B-3 dr d1 и и и dB-2.

and z1(xB-1)/ x = z1(cos 1) and z2(xB-1, xB-2) x -1 = z2(cos 1, sin 1 cos 2). Thus,



+



=

(2

)

1-B 2



=

(2

)

1-B 2


rB-2e-r2/2 dr
0 
rB-2e-r2/2 dr
0

 ( Bz1)2 sinB-3 1 и и и sin B-3 dr d1 и и и dB-2
A
 ( Bz1)( Bz2) sinB-3 1 и и и sin B-3 dr d1 и и и dB-2
A

where A = [0, ]B-3 О [0, 2]. By Lemma B.21 and Lemma B.22 (below), for B  4, we can integrate out r, 3, . . . , B-2:

26

Under review as a conference paper at ICLR 2019



+ 

=

(2)

1-B 2

О 2  B-3 B-1 22

B-1 2

О



B-4 2

+1

B-2 2

-1
(2)

 d1 sinB-3 1( Bz1)2
0

= (B(2-B22-)1) 

 d1 sinB-3 1( Bz1)2
0



=

(2)

1-B 2

О 2  B-3 B-1 22

B-1

О



B-5 2

+1

B-3

-1
(2)О

22





d1 d2 sinB-3 1 sinB-4 2( Bz1)( Bz2)

00

=

B- 2

3


d1
0

  d2 sinB-3 1 sinB-4 2( Bz1)( Bz2)
0

For B = 3 we can similarly obtain the result for .

Lemma B.19. For j, k  0 and 0  s  t  /2,

t sinj  cosk  d = 1 Beta cos2 t, cos2 s; k + 1 , j + 1 .

s2

22

By antisymmetry of cos with respect to    - , if /2  s  t  ,

t sinj  cosk  d = 1 (-1)k Beta cos2 s, cos2 t; k + 1 , j + 1

s2

22

.

Proof. Set x := cos2  = dx = -2 cos  sin  d. So the integral in question is

-1

cos2 t

(1

-

j-1
x) 2

x k-1 2

dx

2 cos2 s

=

1 (Beta(cos2

s;

k

+

1,

j

+

1 )

-

Beta(cos2

t;

k

+

1,

j

+

1 ))

2 22

22

As consequences,

Lemma B.20. For j, k



0,

 0

sinj



cosk



d

=

1+(-1)k 2

Beta(

j

+1 2

,

k+1 2

)

.1+(-1)k

(

1+j 2

)(

1+k 2

)

2

(

2+j+k 2

)

Lemma B.21.


sin 1 d1
0


sin2 2 d2 и и и
0

 0

sink

k

dk

=

k/2/( k

+ 2

2 ).

=

Proof. By Lemma B.20,

 0

sinr

r

dr

=

Beta(

r+1 2

,

1 2

)

=

.(

r+1 2

 )

(

r+2 2

)

Thus

this

product

of

inte-

grals is equal to

k r=1

 

( (

r+1 2
r+2 2

) )

=

k/2(1)/( k

+ 2

2 )

=

k/2/( k

+ 2

2 ).

Lemma B.22. For B > 1,  > 0,

 0

rB e-r2/2

dr

=

2

B-1 2



B+1 2

(

B+1 2

)

Proof. Apply change of coordinates z = r2/2.

Case of ReLU. One can obtain an analytical solution of the fixed point for ReLU by playing with the integrals above. We present such a derivation in the appendix, but the Laplace method yields a much quicker and cleaner way to do so (Thm B.25).

27

Under review as a conference paper at ICLR 2019

B.3.2 LAPLACE METHOD

Definition B.23. For any   0, define K,B := cP

B-1 2

,



-1

B-1 2



where P (a, b)

:=

(a + b)/(a) is the Pochhammer symbol.

Note that

Proposition B.24. limB K,B = c.

Theorem B.25. Suppose  : R  R is degree  positive-homogeneous. For any BSB1   SB, VB() is BSB1. The diagonal entries are K,BJ(1) and the off-diagonal entries

are K,BJ

-1 B-1

.

Here c is as defined in Defn H.2 and J is as defined in Defn I.4.

Thus a

BSB1 fixed point of Eq. (28) exists and is unique.

Proof. Let e be an B О (B - 1) matrix whose columns form an orthonormal basis of im G := {Gv :
v  RB} = {w  RB : i wi = 0}. Let BSB1(a, b) denote a matrix with a on the diagonal and b on the off-diagnals. Note that V is positive homogeneous of degree , so for any ,

V(eIB-1(IB-1 + 2sIB-1)-1eT )

=

V(

1

 + 2s

eeT

)

=

 1 + 2s


V(eeT )

=

 1 + 2s


V(G)

=

 B-1 1 + 2s B


V

BSB1

1,

-1 B-

1

=

 B-1 

1 + 2s B

cJ

BSB1

1,

-1 B-1

by P roposition I.8 by Corollary I.3

So by Eq. (31),



VB(eIeT ) = B()-1

ds s-1(1 + 2s)-(B-1)/2

0

= ()-1(B - 1)cJ

BSB1

1,

-1 B-

1

 B-1 1 + 2s B


cJ

BSB1

1,

-1 B-

1


ds s-1(1 + 2s)-(B-1)/2-

0

= ()-1(B - 1)cJ

BSB1

1,

-1 B-

1

Beta(, (B - 1)/2)2-

((B - 1)/2) = c ( + (B - 1)/2)

B-1 2


J

BSB1

1,

-1 B-1

Corollary B.26. Suppose  : R  R is degree  positive-homogeneous. If  is the BSB1 fixed point of Eq. (28) as given by Thm B.25, then GG = IB-1 where  = K,B J(1) - J(-1/(B - 1))

By setting  = 1 and  = , we get

Corollary B.27.

For any BSB1 



SB, VB() is BSB1 with diagonal entries

1 2

and off-diagonal

entries

1 2

J1

-1 B-1

, so that G2{VB()} = G(VB())G =

1 2

-

1 2

J1

-1 B-1

G.

By setting  = 1 and  = id = x  (x) - (-x), we get

Corollary B.28. For any BSB1   SB, VBid() is BSB1 with diagonal entries 1 and off-diagonal

entries

-1 B-1

,

so

that

G2{VBid()}

=

B/(B

-

1)G.

Remark B.29. One might hope to tweak the Laplace method for computing the fixed point to work for the Fourier method, but because there is no nice relation between V(c) and V() in general, we cannot simplify Eq. (36) as we can Eq. (31) and Eq. (32).

28

Under review as a conference paper at ICLR 2019

B.4 LOCAL CONVERGENCE

In this section we consider linearization of the dynamics given in Eq. (28). Thus we must consider linear operators on the space of PSD linear operators SB. To avoid confusion, we use the following notation: If T : SB  SB (for example the Jacobian of VB) and   SB, then write T {} for the image of  under T .
A priori, the Jacobian of VB at its BSB1 fixed point may seem like a very daunting object. But because of the BSB1 symmetries, the corresponding Jacobian will also have many symmetries that significantly simplify its analysis. We formalize such symmetries as below. Definition B.30. Let T : RBОB  RBОB be a linear operator. Let i  RB be the vector with 0 everywhere except 1 in coordinate i; then ijT is the matrix with 0 everywhere except 1 in position (i, j). Write [kl|ij] := T (ijT )kl. Suppose T has the property that for all i, j, k, l  [B] = {1, . . . , B}
и [kl|ij] = [(k)(l)|(i)(j)] for all permutation  on [B], and
и [ij|kl] = [ji|lk].

Then we say T is ultrasymmetric.
Remark B.31. In what follows, we will often "normalize" the representation "[ij|kl]" to the unique "[i j |k l ]" that is in the same equivalence class according to Defn B.30 and such that i , j , k , l  [4] and i  j , k  l unless i = l , j = k , in which case the normalization is [12|21]. Explicitly, we have the following equivalence classes and their normalized representations

class
i=j=k=l i = j = k or i = j = l i = j and k = l i=j i = k = l or j = k = l i = k and j = l i = k or i = l i = l and j = k k=l
all different

repr. [11|11] [11|12] [11|22] [11|23] [12|11] [12|12] [12|13] [12|21] [12|33] [12|34]

It's straightforward to verify that

Proposition B.32.

The Jacobian

dVB d

 is ultrasymmetric for any BSB1 .

The Jacobian of VB will be a linear operator from HB to HB where
Definition B.33. Denote by HB the space of symmetric matrices of dimension B. Also write HBG for the space of symmetric matrices  of dimension B such that GG =  (which is equivalent to saying rows of  sum up to 0).

As in the case of S, we omit subscript B when it's clear from context.

Definition B.34. Let LB := {GDG : D diagonal, tr D = 0}  HBG and MB := {  HBG :

Diag

=

0}.

Note

that

dim LB

=

B

-

1, dim MB

=

B(B-3) 2

and

RGB



LB

 MB

=

HBG

is

an

orthogonal decomposition w.r.t Frobenius inner product.

Definition B.35. For any nonzero a, b  R, set

 a 0 -b -b и и и

0 -a b



LB (a,

b)

:=

-b -b

b b

0 0

b иии



0 0

и и

и и

и и



HB

.

 ...

...

...

...

.

.

 .

29

Under review as a conference paper at ICLR 2019

In general, we say a matrix M is LB(a, b)-shaped if M = P LB(a, b)P T for some permutation matrix P .

It's not hard to see that L(B - 2, 1)-shaped matrices span LB.

Lemma B.36.

Let L = LB(a, b).

Then G2{L} = GLG =

a+2b B

LB (B

-

2,

1).

Proof. GLG can be written as the sum of outer products

B

GLG = aG1  G1 - aG2  G2 + -bG1  Gi + bG2  Gi - bGi  G1 + bGi  G2

i=3

= aLB

B-1 B

2

-

1 B2

,

B B2

B
+ b (-1 + 2)  Gi + Gi  (-1 + 2)

i=3

=

a B

LB

(B

- 2, 1)

+ b((-1

+ 2)  v

+

v



(-1

+ 2))

with v =

-

B

- B

2

,

-

B

- B

2

,

2 B

,

2 B

,

.

.

.

=

a B

LB

(B

- 2, 1)

+

2b B

LB

(B

- 2, 1)

=

a

+ B

2b

LB

(B

- 2, 1)

Lemma B.37. Let T : RBОB  RBОB be an ultrasymmetric linear operator. Then T has the following eigendecomposition.
1. Two 1-dimensional eigenspace R и BSB1(BT SB1,i - 22, 21) with eigenvalue BT SB1,i for i = 1, 2, where
11 = [11|11] + [11|22](B - 1) 12 = 2(B - 1)[11|12] + (B - 2)(B - 1)[11|23] 21 = 2[12|11] + (B - 2)[12|33] 22 = [12|12] + 4(B - 2)[12|13] + [12|21] + (B - 2)(B - 3)[12|34]
and BT SB1,1 and TBSB1,2 are the roots to the quadratic
x2 - (11 + 22)x + 1122 - 1221.

2. Two (B - 1)-dimensional eigenspaces SB и L(TL,i - 22, 21) with eigenvalue LT,i for i = 1, 2. Here SB и W denotes the linear span of the orbit of matrix W under simultaneous permutation of its column and rows (by the same permutation), and
11 = [11|11] - [11|22] 12 = 2(B - 2)([11|23] - [11|12]) 21 = -[12|11] + [12|33] 22 = [12|21] + [12|12] + 2(B - 4)[12|13] - 2(B - 3)[12|34].
and LT,1 and TL,2 are the roots to the quadratic
x2 - (11 + 22)x + 1122 - 1221.

3. Eigenspace M (dimension B(B - 3)/2) with eigenvalue TM. The proof is by careful, but ultimately straightforward, computation.
30

Under review as a conference paper at ICLR 2019
Proof. We will use the bracket notation of Defn B.30 to denote entries of T , and implicitly simplify it according to Remark B.31. Item 1. Let U  RBОB be the BSB1 matrix . By ultrasymmetry of T and BSB1 symmetry of A, T {U } is also BSB1. So we proceed to calculate the diagonal and off-diagonal entries of T {G}.
We have T {BSB1(a, b)}11 = [11|11]a + 2(B - 1)[11|12]b + [11|22](B - 1)a + [11|23](B - 2)(B - 1)b
= ([11|11] + [11|22](B - 1))a + (2(B - 1)[11|12] + (B - 2)(B - 1)[11|23])b T {BSB1(a, b)}12 = [12|12]b + 2[12|11]a + (B - 2)[12|33]a + 2(B - 2)[12|13]b
+ [12|21]b + 2(B - 2)[12|23]b + (B - 2)(B - 3)[12|34]b = (2[12|11] + (B - 2)[12|33])a
+ ([12|12] + 2(B - 2)[12|13] + [12|21] + 2(B - 2)[12|13] + (B - 2)(B - 3)[12|34])b = (2[12|11] + (B - 2)[12|33])a
+ ([12|12] + 4(B - 2)[12|13] + [12|21] + (B - 2)(B - 3)[12|34])b
Thus BSB1(1, 1) and BSB1(2, 2) are the eigenmatrices of T , where (1, 1) and (2, 2) are the eigenvectors of the matrix
11 12 21 22 with 11 = [11|11] + [11|22](B - 1) 12 = 2(B - 1)[11|12] + (B - 2)(B - 1)[11|23] 21 = 2[12|11] + (B - 2)[12|33] 22 = [12|12] + 4(B - 2)[12|13] + [12|21] + (B - 2)(B - 3)[12|34] The eigenvalues are the two roots TBSB1,1, BT SB1,2 to the quadratic x2 - (11 + 22)x + 1122 - 1221 and the corresponding eigenvectors are (1, 1) = (1 - 22, 21) (2, 2) = (2 - 22, 21).
Item 2. We will study the image of LB(a, b) (Defn B.35) under T . We have T {L(a, b)}11 = -T {L(a, b)}22
= [11|11]a + 2(B - 2)[11|12](-b) + [11|22](-a) + 2(B - 2)[11|23]b = ([11|11] - [11|22])a + 2(B - 2)([11|23] - [11|12])b T {L(a, b)}12 = T {L(a, b)}21 = [12|12]0 + [12|21]0 + [12|11](a - a) + [12|13](b - b) + [12|31](b - b) + [12|33]0 + [12|34]0 =0 T {L(a, b)}33 = T {L(a, b)}ii, i  3 = [11|11]0 + [11|12](2b - 2b) + [11|22](a - a) + [11|23](2(B - 3)b - 2(B - 3)b) =0 T {L(a, b)}34 = T {L(a, b)}ij, i = j & i, j  3 = [12|12]0 + [12|11]0 + [12|13](2b - 2b) + [12|21]0
+ [12|31](2b - 2b) + [12|33](a - a) + [12|34](2(B - 4)b - 2(B - 4)b) =0 T {L(a, b)}13 = T {L(a, b)}1j, j  3 = T {L(a, b)}j1, j  3 = [12|12](-b) + [12|13](B - 3 - 1)(-b) + [12|11]a + [12|21](-b)
+ [12|31](B - 3 - 1)(-b) + [12|33](-a) + [12|34]2(B - 3)b = [12|11] - [12|33] a + 2(B - 3)[12|34] - 2(B - 4)[12|13] - [12|12] - [12|21] b
31

Under review as a conference paper at ICLR 2019

Thus L(a, b) transforms under T by the matrix

L(a, b)  L(

11 21

12 22

a b

)

with

11 = [11|11] - [11|22] 12 = 2(B - 2)([11|23] - [11|12]) 21 = -[12|11] + [12|33] 22 = [12|21] + [12|12] + 2(B - 4)[12|13] - 2(B - 3)[12|34].
So if LT,1 and LT,2 are the roots of the equation
x2 - (11 + 22)x + 1122 - 1221
then T {L(TL,1 - 22, 21)} = TL,1L(LT ,1 - 22, 21) T {L(LT ,2 - 22, 21)} = LT ,2L(TL,2 - 22, 21)
Similarly, any image of these eigenvectors under simultaneous permutation of rows and columns remains eigenvectors with the same eigenvalue. This derives Item 2.

Item 3. Let M  M. We first show that T {M } has zero diagonal. We have

B BB

T {M }11 = [11|12]( M1i + Mi1) + [11|23]( Mij -

M1i + Mi1

i=2

i,j=1

i=2

=0+0=0

which follows from M 1 = 0 by definition of M. Similarly T {M }ii = 0 for all i.

)

Now we show that M is an eigenmatrix.

BB

T {M }12 = [12|12]M12 + [12|11]0 + [12|33]0 + [12|13]

M1i + Mi2

i=3 i=1

BB

+ [12|21]M21 + [12|31]

Mi1 + M2i + [12|34]

Mij

i=3 i=1

i3,j3

B
= [12|12]M12 - 2M12[12|13] + M21[12|21] + [12|31](-2M12) + [12|34]( -Mi1 - M1i)
i=3
= M12([12|12] - 2[12|13] + [12|21] - 2[12|31] + 2[12|34]) = M12([12|12] + [12|21] - 4[12|13] + 2[12|34]) = TMM12 Similarly T {M }ij = MT Mij for all i = j.

Lemma B.38. Let T : RBОB  RBОB be a linear operator. We write G2  T HBG for the operator   T {}  G(T {})G, restricted to   HBG. Then G2  T HBG has the following eigendecomposition.
1. Eigenspace RG with eigenvalue GRG,T := B-1 ((B - 1)(11 - 21) - (12 - 22)), where as in Lemma B.37,
11 = [11|11] + [11|22](B - 1) 12 = 2(B - 1)[11|12] + (B - 2)(B - 1)[11|23] 21 = 2[12|11] + (B - 2)[12|33] 22 = [12|12] + 4(B - 2)[12|13] + [12|21] + (B - 2)(B - 3)[12|34]

32

Under review as a conference paper at ICLR 2019

2. Eigenspace L with eigenvalue LG,T := B-1((B - 2)11 + 12 + 2(B - 2)21 + 222), where as in Lemma B.37,
11 = [11|11] - [11|22] 12 = 2(B - 2)([11|23] - [11|12]) 21 = -[12|11] + [12|33] 22 = [12|21] + [12|12] + 2(B - 4)[12|13] - 2(B - 3)[12|34].

3.

Eigenspace

M

with

eigenvalue

G,T
M

:= MT

= [12|12] + [12|21] - 4[12|13] + 2[12|34].

Proof. Item 1 As in the proof of Lemma B.37, we find

T {BSB1(a, b)} = BSB1

11 12 21 22

a b

where

11 = [11|11] + [11|22](B - 1) 12 = 2(B - 1)[11|12] + (B - 2)(B - 1)[11|23] 21 = 2[12|11] + (B - 2)[12|33] 22 = [12|12] + 4(B - 2)[12|13] + [12|21] + (B - 2)(B - 3)[12|34].
For a = B - 1, b = -1 so that BSB1(B - 1, -1) = BG, we get

T {BSB1(B - 1, -1)} = BSB1 ((B - 1)11 - 12, (B - 1)21 - 22) G2  T {BSB1(B - 1, -1)} = G BSB1 ((B - 1)11 - 12, (B - 1)21 - 22) G
= ((B - 1)(11 - 21) - (12 - 22)) G = B-1 ((B - 1)(11 - 21) - (12 - 22)) BSB1(B - 1, -1) = GRG,T BSB1(B - 1, -1)
by Lemma B.15.
Item 2. It suffices to show that L(B - 2, 1) is an eigenmatrix with the eigenvalue G,T .
L
As in the proof of Lemma B.37, we find

T {L(a, b)} = L

11 12 21 22

a b

where

11 = [11|11] - [11|22] 12 = 2(B - 2)([11|23] - [11|12]) 21 = -[12|11] + [12|33] 22 = [12|21] + [12|12] + 2(B - 4)[12|13] - 2(B - 3)[12|34].
So with a = B - 2, b = 1, we have

T {L(B - 2, 1)} = L((B - 2)11 + 12, (B - 2)21 + 22) G2  T {L(B - 2, 1)} = G L((B - 2)11 + 12, (B - 2)21 + 22) G
= B-1((B - 2)11 + 12 + 2((B - 2)21 + 22))L(B - 2, 1) = LG,T L(B - 2, 1)
by Lemma B.36.

Item 3. The proof is exactly the same as in that of Lemma B.37.

Noting that the eigenspaces of Lemma B.38 are orthogonal, we have

33

Under review as a conference paper at ICLR 2019

Proposition B.39. G2  HG for any ultrasymmetric operator T is self-adjoint.

Now we prepare several helper reults in order to make progress understanding batchnorm. 
Definition B.40. Define N(x) = Bx/ x , i.e. division by sample standard deviation.

Batchnorm B can be decomposed as the composition of three steps,   N  G, where G is meancentering, N is division by standard deviation, and  is coordinate-wise application of nonlinearity.
We have, as operators H  H,

dVB() = d E[B(z)2 : z  N (0, )] d d

d E[(  N)(x)2 : x  N (0, GG)] =
d

=

dV(  N)(G) dG



dG d

=

dV(  N)(G) dG

 G2

Definition B.41.

Write U

:=

G2



dV(N)(G ) dG

G=GG : HG  HG.

It turns out to be advantageous to study U first and relate its eigendecomposition back to that of

dVB () d

= , where



is the BSB1 fixed point, by applying Lemma B.42.

Lemma B.42. Let X and Y be two vector spaces. Suppose linear operators A : X  Y, B : Y 

X. Then

1. rank AB = rank BA
2. If v  Y is an eigenvector of AB with nonzero eigenvalue, then X Bv = 0 and Bv is an eigenvector of BA of the same eigenvalue
3. Suppose AB has k = rank AB linearly independent eigenvectors {vi}ik=1 of nonzero eigenvalues {i}ik=1. Then BA has k linearly independent eigenvectors {Bvi}ik=1 with the same eigenvalues {i}ki=1 which are all eigenvectors of BA with nonzero eigenvalues, up to linear combinations within eigenvectors with the same eigenvalue.

With A

=

G2

and B

=

dV(N)(G dG

)

,

Lemma

B.37

implies that AB

and BA can both be diag-

onalized,

and

this

lemma

implies

that

all

nonzero

eigenvalues

of

dVB () d

can

be

recovered

from

those of U.

Proof. (Item 1) Observe rank AB = rank ABAB  rank BA. By symmetry the two sides are in fact equal.
(Item 2) Bv cannot be zero or otherwise ABv = A0 = 0, contradicting the fact that v is an eigenvector with nonzero eigenvalue. Suppose  is the eigenvalue associated to v. Then BA(Bv) = B(ABv) = B(v) = Bv, so Bv is an eigenvector of BA with the same eigenvalue.
(Item 3) Item 2 shows that {Bvi}ik=1 are eigenvectors BA with the same eigenvalues {i}ik=1. The eigenspaces with different eigenvalues are linearly independent, so it suffices to show that if {Bvij }j are eigenvectors of the same eigenvalue s, then they are linearly independent. But j ajBvij = 0 = j ajvij = 0 because B is injective on eigenvectors by Item 2, so that aj = 0 identically. Hence {Bvj}j is linearly independent.
Since rank BA = k, these are all of the eigenvectors with nonzero eigenvalues of BA up to linearly combinations.

34

Under review as a conference paper at ICLR 2019

Lemma B.43. Let f : RB  RA be measurable, and   SB be invertible. Then for any   RBОB ,
d E[f (z) : z  N (0, )]{} = 1 E[f (z) -1zzT -1 - -1,  : z  N (0, )] d 2 If f is in addition twice-differentiable, then

d E[f (z) : z  N (0, )] = 1 E d 2

d2f (z) d2z

:

z



N (0, )

whenever both sides exist.

Proof.

Let t, t  (- ,

)

be

a

smooth

path

in

SB ,

with

0

=

.

Write

d dt

t

=

 t.

Then

d dt

E[f (z)

:

z



N

(0,

t)]

=

d dt

(2)-B/2

det

t-1/2

dz

e-

1 2

zT

t-1z f (z)

t=0

=

(2)-B/2

-1 2

det

0-1/2

tr

0-1 0

dz

e-

1 2

zT

-0 1

z

/2

f

(z

)

+ (2)-B/2 det 0-1/2

dz

e-

1 2

zT

-0 1

z

-1 zT 2

(--0 1 00-1)zf (z)

=

-1 2

(2)-B/2

det

-0 1/2

dz

e-

1 2

zT

0-1zf (z)(tr

0-1 0

-

zT

0-1 0-0 1z)

= 1 (2)-B/2 det -1/2 2

dz

e-

1 2

zT

-1zf (z)

-1zzT -1 - -1,  0

= -1 E[f (z) 2

-1 - -1zzT -1,  0

: z  N (0, )]

Note that

vT

d dz

e

-1 2

zT

-1

z

=

-vT

-1

z

e

-1 2

z

T

-1

z

wT

d2 dz2

e

-1 2

zT

-1

z

v=

wT -1zzT -1v - wT -1v

e -1 2

zT

-1

z

=

-1zzT -1 - -1, vwT

e -1 2

zT

-1

z

so that as a cotensor,

d2 dz2

e

-1 2

zT

-1z {}

=

-1zzT -1 - -1, 

e -1 2

zT

-1

z

for any   RBОB. Therefore,

d E[f (z) : z  N (0, )] {} = 1 (2)-B/2 det -1/2 d 2

dz

f

(z

)

d2

e-

1 2

z

T

d2z

-1

z

{}

= 1 (2)-B/2 det -1/2 2

dz

e-

1 2

zT

-1

z

d2f (z d2z

)

{}

(by integration by parts)

1 =E
2

d2f (z) d2z

:

z



N (0, )

{}

35

Under review as a conference paper at ICLR 2019

B.4.1 SPHERICAL COORDINATES

Note that for any   HB with  2 < ,

E[  N(z)2 : z  N (0, G + )] = E[  N(Gz)2 : z  N (0, I + )] = E[B(z)2 : z  N (0, I + )]

so that we have, for any   HB,

U {} = dVB()

{}

d =I

=

1 2

E[B(z)2

-1zzT

- -1I, 

: z  N (0, I)]

(by Lemma B.43)

=

1 2

-2

E[B(z)2

zzT , 

: z  N (0, I)] - 1 -1 I,  2

(by Thm B.25)

= (2)-1 E[B(z)2 zzT ,  : z  N (0, I)] -  I, 

(B is scale-invariant)

= (2)-1 E[B(z)2 GzzT G,  : z  N (0, I)] -  I, 

( = GG)

Let's extend to all matrices by this formula: Definition B.44. Define
U~ : RBОB  SB,   (2)-1 E[B(z)2 GzzT G,  : z  N (0, I)] -  I, 

So U~ HB = U HB. Ultimately we will apply Lemma B.37 to G2  U~ HB = G2 = U

Definition B.45.

Write ij

=

1 2

ijT + j iT

.

Then
U~{ij}kl = (2)-1 (E[B(z)kB(z)l(Gz)i(Gz)j : z  N (0, I)] - klI(i = j)) = (2)-1 (E[(N(y))k(N(y))lyiyj : y  N (0, G)] - klI(i = j))
= (2)-1 E[(N(y))k(N(y))lyiyj : y  N (0, eeT )] - klI(i = j) (See Defn B.6 for defn of e)
= (2)-1 (E[(N(ex))k(N(ex))l(ex)i(ex)j : x  N (0, IB-1)] - klI(i = j))

Here we will realize e as the matrix whose columns are eB-m := (m(m + m
1))-1/2(0, . . . , 0, -m, 1, 1, . . . , 1)T , for each m = 1, . . . , B - 1.

 1



B(B-1)

-(B - 1) 1 1 и и и 1 1 1





eT

=

 







1
(B-1)(B-2)
... 6-1/2

      

2-1/2

0 ... 0 0

-(B - 2) 1 и и и 1 1 1



...

... . . . ...

...

...

 



0 0 и и и -2 1 1

0 0 и и и 0 -1 1

(39)

Definition B.46. Define Wij|kl := E[(N(ex))k(N(ex))l(ex)i(ex)j : x  N (0, IB-1)].

36

Under review as a conference paper at ICLR 2019

Then U~{ij}kl = (2)-1(Wij|kl - klI(i = j)). If we can evaluate Wij|kl then we can use Lemma B.37 to compute the eigenvalues of G2  U . It's easy to see that Wij|kl is ultrasymmetric. Thus WLOG we can take i, j, k, l from {1, 2, 3, 4}.

Lemma B.47. Let f : R4  RA for some A  N. Suppose B  6. Then for any k < B - 1

E[r-kf (v1, . . . , v4) : x  N (0, IB-1)]

=

((B - ((B

1 -

- k)/2) 5)/2)

2-k/2

-2


d1 и и и
0


d4 f (v1, . . . , v4) sinB-3 1 и и и sinB-6 4
0

where vi = xi/ x and r = x , and

v1 = cos 1

v2 = sin 1 cos 2

v3 = sin 1 sin 2 cos 3

v4 = sin 1 sin 2 sin 3 cos 4.

Lemma B.48. Let f : R2  RA for some A  N. Suppose B  4. Then for any k < B - 1

E[r-kf (v1, v2) : x  N (0, IB-1)]

=

((B - ((B

1 - k)/2) - 3)/2)

2-k/2-1


d1
0


d2 f (v1, v2) sinB-3 1 sinB-4 2
0

where vi = xi/ x and r = x , and

v1 = cos 1

v2 = sin 1 cos 2

Lemma B.49. Let f : R  RA for some A  N. Suppose B  3. Then for any k < B - 1

E[r-kf (v) : x  N (0, IB-1)]

=

((B - ((B

1 - k)/2) - 2)/2)

2-k/2-1/2


d f (cos ) sinB-3 
0

where vi = xi/ x and r = x .

Lemma B.47 implies Lemma B.48 and Lemma B.49 by integrating out the appropriate is, so we will just prove Lemma B.47.

Proof of Lemma B.47.

E[r-kf (v1, . . . , v4) : x  N (0, IB-1)] = (2)-(B-1)/2 dx r-kf (v1, . . . , v4)e-r2/2

Now we change into spherical coordinates (r, 1, . . . , B-2)  [0, ) О [0, ]B-3 О [0, 2], defined by

x1 = r cos 1 x2 = r sin 1 cos 2 x3 = r sin 1 sin 2 cos 3

...

xB-2 = r sin 1 и и и sin B-3 cos B-2 xB-1 = r sin 1 и и и sin B-3 sin B-2.

Then

 cos 1 

 sin 1 cos 2 



v

=

 



sin 1 sin 2 cos 3 ...

   



sin 1 и и и sin B-3 cos B-2

sin 1 и и и sin B-3 sin B-2

37

Under review as a conference paper at ICLR 2019

and dx1 . . . dxB-1 = rB-2 sinB-3 1 sinB-4 2 и и и sin B-3 dr d1 и и и dB-2.
So E[r-kf (v1, . . . , v4) : x  N (0, IB-1)]
= (2)-(B-1)/2 dx r-kf (v1, . . . , v4)e-r2/2

= (2)-(B-1)/2 rB-2 sinB-3 1 и и и sin B-3 dr d1 и и и dB-2 r-kf (v1, . . . , v4)e-r2/2

= (2)-(B-1)/2 = (2)-(B-1)/2 = (2)-(B-1)/2

f (v1, . . . , v4) sinB-3 1 и и и sin B-3 d1 и и и dB-2 rB-2-ke-r2/2 dr



f (v1, . . . , v4) sinB-3 1 и и и sin B-3 d1 и и и dB-2

rB-2-ke-r2/2 dr

0

f (v1, . . . , v4) sinB-3 1 и и и sin B-3 d1 и и и dB-22(B-3-k)/2((B - 1 - k)/2)

(change of coordinate r  r2 and definition of  function)

= ((B - 1 - k)/2)2-(2+k)/2-(B-1)/2 f (v1, . . . , v4) sinB-3 1 и и и sin B-3 d1 и и и dB-2

Because v1, . . . , v4 only depends on 1, . . . , 4, we can integrate out 5, . . . , B-2. By applying

Lemma B.21 to 5, . . . B-3 and noting that

2 0

dB-2

=

2,

we

get

E[r-kf (v1, . . . , v4) : x  N (0, IB-1)]

= ((B - 1 - k)/2)2-(2+k)/2-(B-1)/2 f (v1, . . . , v4) sinB-3 1 и и и sinB-6 4(B-7)/2((B - 5)/2)-1(2)

=

((B - ((B

1 - k)/2) - 5)/2)

2-k/2-2

f (v1, . . . , v4) sinB-3 1 и и и sinB-6 4

Assuming {i, j, k, l}  {1, 2, 3, 4},

(ev)1 = -

B

- B

1

cos

1

(ev)2 = cos 1 -

B B

- -

2 1

sin

1

cos

2

(ev)3 = cos 1 + sin 1 cos 2 -

B B

- -

3 2

sin

1

sin

2

cos

3

(ev)4 = cos 1 + sin 1 cos 2 + sin 1 sin 2 cos 3 -

B B

- -

4 3

sin

1

sin

2

sin

3

cos

4

so in particular they only depend on 1, . . . , 4. By Lemma B.47, and the fact that x  ex is an

isometry,


Wij|kl = E[r2( Bev)k( Bev)l(ev)i(ev)j : x  N (0, IB-1)]







= (B - 5)(B - 3)(B - 1)(2)-2 d1 и и и d4 ( Bev)k( Bev)l(ev)i(ev)j sinB-3 1 и и и sinB-6 4

00

If WLOG we further assume that k, l  {1, 2} (by ultrasymmetry), then there is no dependence on
3 and 4 inside . So we can expand (ev)i and (ev)j in trigonometric expressions like above and
integrate out 3 and 4 via Lemma B.20.

38

Under review as a conference paper at ICLR 2019

B.4.2 LAPLACE METHOD

Differentiating Eq. (31) In what follows, let  be a positive-homogeneous function of degree .

We

begin

by

studying

dV(N)(G dG

)

.

We

will

differentiate

Eq.

(31)

directly

at

G2{}

=

GG

where  is the BSB1 fixed point given by Thm B.25. To that end, consider a smooth path (t) 

SBG, t  (- , ) for some > 0, with (0) = GG. Set  (t) = eT (t)e  SB-1, so that

(t) = e

(t)eT and 

(0)

=

IB-1

where



=

K,B (J (1)

-

J

(

-1 B-1

))

as

in

Thm

B.25.

If

we write  and  for the time derivatives, we have

B

-

()

d dt

V(



N)(e

(t)eT )
t=0

=

B-()

d dt

VB(e

(t)eT )
t=0

=


ds s-1
0

d dt

det(I

+

2s

(t))-1/2 О V e
t=0

(0)(I + 2s

(0))-1eT

+ det(I + 2s

(0))-1/2

d dt

V(e

(t)(I + 2s

(t))-1eT )
t=0

=


ds s-1
0

-

1

s + 2s

det(I

+

2s

(0))-1/2 tr



(0)

О

 1 + 2s


V(G)

+ det(I + 2s (0))-1/2 dV() d =e (0)(I+2s (0))-1eT

e(I + 2s (0))-1 (0)(I + 2s (0))-1eT

(apply Lemma B.51 and Lemma B.52)

=


ds s-1
0

-

1

s + 2s

(1

+

2s)-(B-1)/2

tr



(0)

О

 1 + 2s


V(G)

+ (1 + 2s)-(B-1)/2

 1 + 2s

-1 dV()

(1 + 2s)-2e (0)eT

d =G

(using fact that dV/d is degree ( - 1)-positive homogeneous)

=-



(1

+

2s)-

B-1 2

-1-s

tr  (0) V(G)

0

+



(1

+

2s)-

B-1 2

-1--1s-1

dV()

0 d =G

= --12-1- Beta B - 1 ,  + 1 tr  (0) V(G) 2

e (0)eT

+ -12- Beta B + 1 ,  dV()

 (0)

2 d =G

(apply Lemma B.53)

if

B-1 2

+

2

>



(precondition

for

Lemma

B.53).

With some trivial simplifications, we obtain the following

39

Under review as a conference paper at ICLR 2019

Lemma B.50. Let  be positive-homogeneous of degree . Consider a smooth path (t)  SBG

with

(0)

=

G.

If

B-1 2

+

2

>

,

then

d dt

V(



N)((t))

t=0

= dV(  N)()

{ (0)}

d =G

= -B-12-1-P

B - 1, + 1

-1
tr



(0)

V(G)

2

+ B-12-P B + 1 ,  -1 dV()

 (0)

2 d =G

(40)

where P (a, b) = (a + b)/(a) is the Pochhammer symbol.

Lemma B.51.

For any  (0),

d dt

det(I

+ 2s

)-1/2

=

-s det(I + 2s )-1/2 tr((I +

2s )-1d /dt).

For 

(0)

=

I,

this

is

also

equal

to

-

s 1+2s

det(I

+

2s

)-1/2

tr(

d dt



).

Proof. Straightforward computation.

Lemma B.52. For any 

(0),

d dt



(I + 2s

)-1 = (I + 2s

(0))

d dt

(I + 2s

(0))-1.

Proof. Straightforward computation. Lemma B.53. For a > b + 1, 0(1 + 2s)-asb ds = (2)-1-b Beta(b + 1, a - 1 - b).

Proof.

Apply change of variables x =

2s 1+2s

.

Definition B.54. Let T : HB  HB be such that for any   HB and any i = j  [B],

T {}ii = uii T {}ij = vii + vjj + wij

Then we say that T is diagonal-off-diagonal semidirect, or DOS for short. We write more specifically T = DOSB(u, v, w).
Lemma B.55. Let T := DOSB(u, v, w). Then T {LB(a, b)} = LB(ua, wb - va).

Proof. Let L := LB(a, b). It suffices to verify the following, each of which is a direct computation.

1. T {L}3:B,3:B = 0. 2. T {L}1,2 = T {L}2,1 = 0 3. T {L}1,1 = -T {L}2,2 = ua 4. T {L}1,i = -T {L}2,i = T {L}i,1 = -T {L}i,2 = va - wb.

Lemma B.56. Let T := DOSB(u, v, w). Then LB(w - u, v) and LB(0, 1) are its eigenvectors:
T {LB(w - u, v)} = uLB(w - u, v) T {LB(0, 1)} = wLB(0, 1)

Proof. The map (a, b)  (ua, wb - va) has eigenvalues u and w with corresponding eigenvectors (w - u, v) and (0, 1). By Lemma B.55, this implies the desired results.

Lemma B.57.

Let T

:= DOSB(u, v, w). Then for any L  LB, G2 T {L} =

(u-2v)(B-2)+2w B

L.

40

Under review as a conference paper at ICLR 2019

Proof. By Lemma B.36 and Lemma B.55, G2  T {L(B - 2, 1)} = G2{L(u(B - 1), w - v(B -

1))}

=

(u-2v)(B-2)+2w B

LB

(B

- 2, 1).

By

permutation symmetry,

we

also have the general result

for any LB(B - 2, 1)-shaped matrix L. Since they span LB, this gives the conclusion we want.

Lemma B.58. Let T := DOSB(u, v, w). Then T {BSB1B(a, b)} = BSB1B(ua, wb + 2va).

Proof. Direct computation.
Lemma B.59. Let T := DOSB(u, v, w). Then BSB1B(w - u, wv) and BSB1B(0, 1) are eigenvectors of T .
T {BSB1B(u - w, 2v)} = uBSB1B(u - w, 2v) T {BSB1B(0, 1)} = wBSB1B(0, 1)

Proof. The linear map (a, b)  (ua, wb + 2va) has eigenvalues u and w with corresponding eigenvectors (u - w, 2v) and (0, 1). The result then immediately follows from Lemma B.58.

Lemma B.60.

Let T

:= DOSB(u, v, w). Then G2  T {G} =

(B-1)(u-2v)+w B

G.

Proof. Direct computation with Lemma B.58 and Lemma B.15

Definition B.61. Define MB := {  SB : Diag = 0}.

For any a, b  R, set

 a -b -b и и и

-b 0 0 и и и

LB (a,

b)

:=

 -b

0

0

и

и

 и



HB

.

 ...

...

...

 ...

Define LB(a, b) := span(P1TiLB(a, b)P1i : i  [B]) where P1i is the permutation matrix that swap the first entry with the ith entry, i.e. LB(a, b) is the span of the orbit of LB(a, b) under permuting rows and columns simultaneously.

Note that

LB(a, b) = LB(a, b) - P1T2LB(a, b)P1T2
B
BSB1B(a, -2b) = P1TiLB(a, b)P1i
i=1

So LB(a, b), BSB1B(a, -2b)  LB(a, b). Theorem B.62. Let T := DOSB(u, v, w). Then T HB has the following eigendecomposition:
и MB has eigenvalue w (dim MB = B(B - 1)/2) и LB(w - u, v) has eigenvalue of u (dim LB(w - u, v) = B)

Proof. The case of MB is obvious. We will show that LB(w-u, v) is an eigenspace with eigenvalue u. Then by dimensionality consideration they are all of the eigenspaces of T .
Let L := LB(w - u, v). Then it's not hard to see T {L} = LB(a, b) for some a, b  R. It follows that a has to be u(w - u) and b has to be -(v(w - u) - wv) = uv, which yields what we want.
Theorem B.63. Let T := DOSB(u, v, w). Then G2  T HBG : HBG  HBG has the following eigendecomposition:

и

RG has eigenvalue

(B-1)(u-2v)+w B

(dim RG = 1)

41

Under review as a conference paper at ICLR 2019

и MB has eigenvalue w (dim MB = B(B - 3)/2)

и

LB

has eigenvalue

(B-2)(u-2v)+2w B

(dim LB

= B - 1)

Proof. The case of MB is obvious. The case for RG follows from Lemma B.60. The case for LB follows from Lemma B.57. By dimensionality considerations these are all of the eigenspaces.

This immediately gives the following consequence.

Theorem B.64. Let  : R  R be any function with finite first and second Gaussian moments.

Then

G2



dV d

=BSB1(a,b)

:

HBG



HBG

has the following eigendecomposition:

и

RG has eigenvalue

(B-1)(u-2v)+w B

(dim RG = 1)

и MB has eigenvalue w (dim MB = B(B - 3)/2)

и

LB

has eigenvalue

(B-2)(u-2v)+2w B

(dim LB

= B - 1)

where

u=

V()11 11

=BSB1(a,b)

v=

V()12 11

=BSB1(a,b)

w = V()12 12 =BSB1(a,b)

Theorem B.65. Let  : R  R be positive-homogeneous of degree . Then for any p = 0, c  R,

G2



dV d

=BSB1(p,cp)

:

HBG



HBG

has the following eigendecomposition:

и MB has eigenvalue cp-1J (c)

и

LB

has

eigenvalue

cp-1

1 B

(B - 2) [J(1) - J (c)] + (2 + c(B - 2)) J (c)

и

RG

has

eigenvalue

c

p-1

1 B

(B - 1) [J(1) - J (c)] + (1 + c(B - 1)) J (c)

Proof.

By Proposition I.6,

dV d

=BSB1(p,cp)

is DOS(u, v, w) with

u = V()11 11 =BSB1(p,cp)

= cp-1J(1)

v=

V()12 11

=BSB1(p,cp)

=

1 2

ci12i(-2)j12j(J(cij

)

-

cij

J(cij

))

=

1 2

cp-1

J (c) - cJ (c)

w=

V()12 12

=BSB1(p,cp)

= ci(i-1)/2j(j -1)/2J(cij )

= cp-1J (c)

With Thm B.63, we can do the computation:

42

Under review as a conference paper at ICLR 2019

и MB has eigenvalue w = cp-1J (c)

и LB has eigenvalue

(u - 2v)(B - 2) + 2w

B

1 =B

(B - 2) cp-1J(1) - cp-1

J (c) - cJ (c)

+ 2cp-1J (c)

=

cp-1

1 B

(B - 2) [J(1) - J (c)] + (2 + c(B - 2)) J (c)

и RG has eigenvalue

(B - 1)(u - 2v) + w

B

1 =B

(B - 1)

cp-1J(1) - cp-1

J (c) - cJ (c)

+ cp-1J (c)

=

cp-1

1 B

(B - 1) [J(1) - J (c)] + (1 + c(B - 1)) J (c)

We record the following consequence which will be used frequently in the sequel.

Theorem B.66.

Let 

:

R



R be positive-homogeneous of degree .

Then G2 

dV d

=G

:

HBG  HBG has the following eigendecomposition:

и

MB

has

eigenvalue

G,(B,
M

)

:=

c

B-1 B

-1 J

-1 B-1

и LB has eigenvalue LG,(B, ) := c

B-1 -1 1 BB

(B - 2) J(1) - J

-1 B-1

+

B B-1

J

-1 B-1

и RG has eigenvalue RGG,(B, ) := c

B-1 B



J(1) - J

-1 B-1

Proof.

Plug in p =

B-1 B

and c = -1/(B - 1) for Thm B.65.

Theorem B.67. Let  : R  R be a degree  positive-homogeneous function. Then for p = 0, c 

R,

dV d

=BSB1(p,cp)

:

HB



HB

has

the

following eigendecomposition:

и MB has eigenvalue cp-1J (c)

и LB(a, b) has eigenvalue cp-1J(1) where a = 2 J (c) - J(1) and b = J (c) - cJ (c)

Proof. Use Thm B.62 with the computations from the proof of Thm B.65 as well as the following computation

w - u = cp-1J (c) - cp-1J(1)

= cp-1 J (c) - J(1)

v

=

1 2

cp-1

J (c) - cJ (c)

Theorem B.68.

Let  be positive-homogeneous with degree .

Assume

B-1 2

>

.

The operator

U

=

G2



dV(N)(G ) dG

G =  G

:

HBG



HBG

has 3 distinct eigenvalues.

They are as follows:

43

Under review as a conference paper at ICLR 2019

1. RGG,B (B, ) := 0 with dimension 1 eigenspace RG.

2.

G,B (B, ) := B-12-P

B + 1,

-1
G,(B, )

M 2M

= cB(B - 1)-1-12-P

B

+ 2

1

,



-1
J

-1 B-1

with

dimension

B(B-3) 2

eigenspace

M.

3.

G,B (B, ) := B-12-P

B + 1,

-1
G,(B, )

L 2L

= c(B - 1)-1-12-P

B + 1 ,  -1 2

(B - 2) J(1) - J

-1 B-1

B + B - 1 J

-1 B-1

with dimension B - 1 eigenspace L.

Proof. Item 1. The case of G. Since (  N)(G) = (  N)(G + G) for any , the operator

dV(N)(G ) dG

G =  G

sends G to 0.

Item 2. The case of 2. Assume  (0)  M, Thm B.66 gives (with denoting Hadamard product, i.e. entrywise multiplication)

dV() d

{ (0)}
=G

=

GM,(B, ) (0)

B - 1 -1

= c B

J

-1 B-1

 (0)

Since tr( (0)) = 0, Eq. (40) gives

G2



dV(  N)(G) dG

{ (0)}
G =  G

=

G2



d dt

V(



N)((t))

{ (0)}
t=0

= B-12-P B + 1 ,  -1 G2  dV()

 (0)

2 d =G

= B-12-P

B + 1, 2

-1
c

B - 1 -1 B J

-1 B-1

G2{ (0)}

= cB(B - 1)-1-12-P

B + 1, 2

-1
J

-1 B-1

 (0)

So

the

eigenvalue

for

M

is

G,B
L

=

cB(B

-

1)-1-12-P

B+1 2

,



-1 J

-1 B-1

.

Item 3. The case of L. Let  (0) = L(B - 2, 1). Thm B.66 gives

G2  dV()

{ (0)} = G,(B, ) (0)

d =G

L

44

Under review as a conference paper at ICLR 2019

where

LG,(B, )

:=

1 B c

B - 1 -1 B

(B - 2) J(1) - J

-1 B-1

B -1 + B - 1 J B - 1

Since tr( (0)) = 0, Eq. (40) gives

G2



dV(  N)(G) dG

{ (0)}
G =  G

=

G2



d dt

V(



N)((t))

t=0

= B-12-P B + 1 ,  -1 G2  dV() 2 d =G

= B-12-P

B + 1,

-1
~ (0)

2

= (B - 1)-1c-12-P

B + 1 ,  -1 2

 (0)

О (B - 2) J(1) - J О  (0)

-1 B-1

B + B - 1 J

-1 B-1

So G,B
L

=

(B-1)-1c-12-P

B+1 2

,



-1

(B - 2) J(1) - J

-1 B-1

+

B B-1

J

-1 B-1

Proposition B.69. With  and  as above, as B  ,

L





+

B-1(23

-

42

-



+

J(0) J(1) - J(0)

)

+

O(B-2)

M



J(0) J(1) - J(0)

+ B-1

22 - 4 + 1

J(0) - J(0) - J(1) - J(0) J(1) - J(0)

J(0)

2

J(1) - J(0)

+ O B-2

We can compute, by Proposition I.7,

J(0) J(1) - J(0)

=

(a2

(a

+

b)2

1 

(

 2

+1)2

(+

1 2

)

+

b2)

-

(a

-

b)2

1 2

(

 2

+

1 2

)2

(+

1 2

)



2 

 2



+1 +

2
1 2

where the last part is obtained by optimizing over a and b. On   (-1/2, ), this is greater than  iff  < 1. Thus for   1, the maximum eigenvalue of U is always achieved by eigenspace L (for large enough B)

Corollary B.70. L > M > G.

C BACKWARD DYNAMICS

We adopt the matrix convention that, for a multivariate function f : Rn  Rm, the Jacobian is

 f1
x1

f1 x2

иии

f1 
xn

df dx

=

    

f2 x1
...

f2 x2

иии

... . . .

f2 

xn
...

   

,

fm x1

fm x2

иии

fm xn

45

Under review as a conference paper at ICLR 2019

so that f (x + x)

=

df dx

x

+

o(x),

where x and x are treated as column vectors.

In what

follows we further abbreviate B(x) =

dB (z ) dz

z=x, B(x)x =

dB (z ) dz

z=xx.

Let's extend the definition of the V operator:

Definition C.1. Suppose Tx : Rn  Rm is a linear operator parametrized by x  Rk. Then for a PSD matrix   Sk, define VT () := E[Tx  Tx : x  N (0, )] : RnОn  RmОm, which acts on n О n matrices  by

VT (){} = E[TxTxT : x  N (0, )]  RmОm.

Under this definition, VB() is a linear operator RBОB  RBОB. Recall the notion of adjoint: Definition C.2. If V is a (real) vector space, then V , its dual space, is defined as the space of linear functionals f : V  R on V . If T : V  W is a linear operator, then T , its adjoint, is a linear operator W   V , defined by T (f ) = v  f (T (v)).

If a linear operator is represented by a matrix, with function application represented by matrixvector multiplication (matrix on the left, vector on the right), then the adjoint is represented by matrix transpose.

The backward equation. In this section we are interested in the backward dynamics, given by the following equation

(l) = VB((l)){(l+1)}

where (l) is given by the forward dynamics. Particularly, we are interested in the specific case when we have exponential convergence of (l) to a BSB1 fixed point. Thus we will study the asymptotic approximation of the above, namely

(l) = VB(){(l+1)}

(41)

where  is the BSB1 fixed point. This is a linear system, so its dynamics is entirely determined by the eigendecomposition of the operator VB(). It turns out to be much more convenient to study VB(), which has the same eigenvalues as its adjoint.

Let us start then.

C.1 EIGENDECOMPOSITION OF VB().

By chain rule, B(x)

=

dN(z) dz

dGx z=Gx dx

=

dN(z) dz

z=GxG, B(x)x

=

dN(z) dz

z=GxGx

This

VB() = E B(x)2 : x  N (0, )

=E

d  N(z) dz

G
z=Gx

2
: x  N (0, )

=E

d  N(z) dz

2 z=Gx

 G2

:

x



N (0, )

=E

d  N(z) dz

2 z=Gx

:

x



N (0, )

 G2

=V

x



d  N(z) dz

z=Gx

()  G2

If we let F () := V

x

dN(z) dz z=Gx

(), then VB() = F ()  G2. By Lemma B.42,

to obtain the eigendecomposition of VB() it suffices to do the same for G2  F ().

46

Under review as a conference paper at ICLR 2019

Proposition C.3.

dN(z) dz

z=y

=

 Br-1(I

- vvT ),

 where r = y , v = y/ y = N(y)/ B.

Proof.

We have

dN(z) dz

z=y

 =B

dz/ z dz

, and
z=y

(yi/ y yj

)

= =

ij
ij y

y - ( y /yj)yi

y2

-

yi y и y y 2 yj

= ij - y

yi yj y2 y

=

ij y

-

yiyj y3

so that

dz/ z dz

= r-1(I - vvT ).
z=y

By chain rule, this easily gives

Proposition C.4.

d  N(z) dz

z=y

=

 BDr-1(I

- vvT ),

 where D = Diag( (N(y))), r = y , v = y/ y = N(y)/ B.

With v = y/ y , r = y , and D~ = Diag( (N(y))), we then have

F () = E D~ 2 
 = E D~ 2 

 B

dz/ z dz

 B

dz/ z dz

2
: x  N (0, )
z=Gx
2 
: y  N (0, GG)
z=y

= B E r-2 D~ 2 + (D~ vvT )2 - D~  (D~ vvT ) - (D~ vvT )  D~ : y  N (0, GG) (42)
F (){} = B E r-2 D~ D~ + D~ vvT vvT D~ - D~ vvT D~ - D~ vvT D~ : y  N (0, GG)

47

Under review as a conference paper at ICLR 2019

C.1.1 SPHERICAL INTEGRAL

F (){} = B E r-2 D~ D~ + D~ vvT vvT D~ - D~ vvT D~ - D~ vvT D~ : y  N (0, G)

= -1B E r-2 D~ D~ + D~ vvT vvT D~ - D~ vvT D~ - D~ vvT D~ : y  N (0, G)

F (){ij }kl = -1B E[r-2



 ( Bvk)

 ( Bvl)

I(k

=

i

&

l = j) + I(k = j 2

& l = i)



+ vivjvk ( Bvk)vl ( Bvl)

1 



- 2

I(i = k) (

Bvk)vjvl (

Bvl) + I(j = k) (

Bvk)vivl (

Bvl)





+ I(i = l) ( Bvl)vjvk ( Bvk) + I(j = l) ( Bvl)vivk ( Bvk)

: rv  N (0, G)]

=

-1B

((B ((B

- -

3)/2) 5)/2)

2-2/2-2


d1 и и и
0


d4 f (v1, . . . , v4) sinB-3 1 и и и sinB-6 4
0



= -1B(B - 5)(2)-2 d1 и и и d4 f (v1, . . . , v4) sinB-3 1 и и и sinB-6 4

00

by Lemma B.47, where we assume, WLOG by ultrasymmetry, k, l  {1, 2}; i, j  {1, . . . , 4}, and

f (v1, . . . , v4)

:=

1 2


( Bev)k


( Bev)l

I(k = i & l = j) + I(k = j & l = i) + 2(ev)i(ev)j(ev)k(ev)l

- (I(i = k)(ev)j(ev)l + I(j = k)(ev)i(ev)l + I(i = l)(ev)j(ev)k + I(j = l)(ev)i(ev)k)

v1 = cos 1 v2 = sin 1 cos 2 v3 = sin 1 sin 2 cos 3 v4 = sin 1 sin 2 sin 3 cos 4.
and e as in Eq. (39).
We can integrate out 3 and 4 symbolically by Lemma B.20 since their dependence only appear outside of  . This reduces each entry of F (){ij}kl to 2-dimensional integrals to evaluate numerically.

C.1.2 LAPLACE METHOD

In this section suppose that  is degree  positive-homogeneous. Set D = Diag( (y)) (and recall v = y/ y , r = y ). Then D is degree  - 1 positive-homogeneous in x (because  is).
Consequently we can rewrite Eq. (42) as follows,

F () = B

E

r-2 r-2(-1)D2 + r-2(+1)(DvvT )2 - r-2D  (DvvT ) - r-2(DvvT )  D

yN (0,GG)

= B E

r-2D2 + r-2(+2)(DyyT )2 - r-2(+1) D  (DyyT ) + (DyyT )  D

yN (0,G)

= B(A + B - C)

(43)

where  is the BSB1 fixed point of Eq. (28), GG = G, and A = E r-2D2 : y  N (0, G)

B = E r-2(+2)(DyyT )2 : y  N (0, G)

C = E r-2(+1) D  (DyyT ) + (DyyT )  D : y  N (0, G)

48

Under review as a conference paper at ICLR 2019

Each term in the sum above is ultrasymmetric, so has the same eigenspaces RG, M, L (this will also become apparent in the computations below without resorting to Lemma B.37). We can thus compute the eigenvalues for each of them in order.
In all computation below, we apply Lemma B.5 first to relate the quantity in question to V.

Computing A. For two matrices , , write   for entrywise multiplication. We have by Lemma B.5

A{} = E r-2DD : y  N (0, G)

= E r-2  (y) (y)T : y  N (0, G)



= ()-1

ds s-1 det(I + 2sG)-1/2

0

V

1

 + 2s

G



= ()-1

ds s-1(1 + 2s)-(B-1)/2

0

 1 + 2s

-1
V (G)



= ()-1 V (G)

ds (s)-1(1 + 2s)-(B-1)/2-+1

0

= ()-1 V (G) Beta B - 3 ,  -12- 2

=

c-1

B - 1 -1 B BSB1

J (1), J

-1 B-1

P

B - 3,

-1
-12-

2

Then Thm B.63 gives the eigendecomposition for BG2  A HBG

Theorem C.5. Let RB, := (2-1)c-1B(B -1)-1P

B-3 2

,



-1 2-.

Then BG2 A

has the following eigendecomposition.

HBG

AL = RB,-1

B-2

2

B J(1) + B J

-1 B-1

(2 + B - 3) 2(B - 2)J (1) + 2(2 - 1)J

=

(2 - 1)(B - 3)(B - 1)

J(1) - J

1 1-B

MA = RB,-1J

-1 B-1

1 1-B

B(2 + B - 3)J

1 1-B

=

(B - 3)(B - 1)

J(1) - J

1 1-B

GA = RB,-1

B-1

1

B J(1) + B J

-1 B-1

(2 + B - 3)  2(B - 1)J (1) + (2 - 1)J

1 1-B

=

(2 - 1)(B - 3)(B - 1)

J(1) - J

1 1-B

Proof. We have BA = (2 - 1)-1RB,-1DOS J (1), 0, J

-1 B-1

. This allows us to

apply Thm B.63. We make a further simplification J (c) = (2 - 1)J(c) via Proposition I.6.

Computing B.

B{} = E r-2(+2)DyyT yyT D : y  N (0, G) = E r-2(+2)(y)yT y(y)T : y  N (0, G)

49

Under review as a conference paper at ICLR 2019

where (y) := y (y), which, in this case of  being degree  positive-homogeneous, is also equal to (y).

By Lemma B.5, B{} equals

( + 2)-1


ds
0

s+1 det(I

+

2sG)-1/2 E[(y)yT y(y)T

:

y



N (0,

1

 + 2s



G)]

Definition C.6. Let  : R  R be measurable and   SB. Define V(4)() : HB  HB by

V(4)(){} = E[(y)yT y(y)T : y  N (0, )]

For two matrices , , write ,  := tr(T ).

Proposition C.7.

V(4)(){} = V() , 

+

2

dV() d

{}.

Proof. We have

dV() {} = (2)-B/2

dz (z)2

d

det()-1/2

e-

1 2

zT

-1

z

{}

d d

= (2)-B/2 dz (z)2

-1 det -1/2-1 + det -1/2 1 -1zzT -1

e-

1 2

zT

-1

z

,



22

= 1 (2)-B/2 det -1/2

dz

(z

)2

e-

1 2

zT

-1 z

zzT , -1-1 - -1, 

2

= 1 V(4)(){-1-1} - 1 V() -1,  22

Making the substitution   , we get the desired result.

If  is degree  positive-homogeneous, then V(4) is degree  + 1 positive-homogeneous. Thus,



B{} = ( + 2)-1

ds s+1 det(I + 2sG)-1/2V(4)

0

1

 + 2s

G

{}



= ( + 2)-1

ds s+1(1 + 2s)-(B-1)/2

0

 1 + 2s

+1
V(4)(G){}



= ( + 2)-1V(4)(G){}

ds (s)+1(1 + 2s)-(B-1)/2--1

0

= ( + 2)-1V(4)(G){} Beta( B - 3 ,  + 2)2--2-1 2

=P

B - 3, + 2

-1
2--2-1V(4)(G){}

2

By

Proposition

C.7,

V(4)(G){}

=

V(G)

tr



+

2

dV() d

{}

if





HBG.

Thus

Theorem C.8. BG2  B HBG has the following eigendecomposition (note that here  is still with respect to , not  = )

1. Eigenspace RG with eigenvalue

BG

:=

2 B-

3

2. Eigenspace M with eigenvalue

MB := BP

B - 3, + 2 2

-1
2--2-12GM,(B, )

22BJ

-1 B-1

=

(B - 3)(B - 1)(B - 1 + 2)(J(1) - J

-1 B-1

)

50

Under review as a conference paper at ICLR 2019

3. Eigenspace L with eigenvalue

LB := BP

B - 3, + 2 2

-1
2--2-12LG,(B, )

23(B - 2)

22BJ

-1 B-1

= (B - 3)(B - 1)(B - 1 + 2) + (B - 3)(B - 1)2(B - 1 + 2)(J(1) - J

-1 B-1

)

Proof. The only thing to justify is the value of BG. We have

BG = BP

B - 3, + 2

-1
2--2-1

2

(B - 1)c

B-1 B



J(1) - J

-1 B-1

+ 2MG,(B, )

= BP

B - 3, + 2 2

-1
2--2-1(B - 1 + 2)c

B-1  B

J(1) - J

-1 B-1

= c(B - 1)(B - 1 + 2)P

B - 3, + 2

-1
2--2-1

2

J(1) - J

-1 B-1

= c(B - 1)P

B - 3, + 1

-1
2--1-1

2

J(1) - J

-1 B-1

= c(B - 1)P

B - 3, + 1 2

-1
2--1K-,1B 2

(since J = 2J)

=

2c(B - 1)P

B-3 2

,



+

1

-1 2--1

cP

B-1 2

,



-1

B-1  2

(Defn B.23)

22-1 = (B - 3)/2

2 = B-3

Computing C. By Lemma B.5,

C{} = E r-2(+1) DyyT D + DyyT D : y  N (0, G)

= ( + 1)-1 = ( + 1)-1

ds

s

det(I

+

2sG)-1/2

E[DyyT

D

+

DyyT

D

:

y



N

(0,

1

 + 2s

G)]

ds s(1 + 2s)-(B-1)/2

 1 + 2s


E[DyyT D + DyyT D : y  N (0, G)]

= ( + 1)-1 ds (s)(1 + 2s)-(B-1)/2- E[DyyT D + DyyT D : y  N (0, G)]

= ( + 1)-1 Beta B - 3 ,  + 1 2--1-1 E[DyyT D + DyyT D : y  N (0, G)] 2

=P

B - 3, + 1

-1
2--1-1 E[DyyT D + DyyT D : y  N (0, G)]

2

Lemma C.9. Suppose  is degree  positive-homogeneous. Then for   HB,

E[D  (DyyT ) + DyyT  D : y  N (0, )] =  dV() d =I
E[DyyT D + DyyT D : y  N (0, )] =  dV() { + } d

51

Under review as a conference paper at ICLR 2019

where D = Diag( (y)).

Proof. Let t, t  (- , ) be a smooth path in HB with 0 = I. Write Dt = Diag( (ty)), so that D0 = D. Then, using  t to denote t derivative,

d dt

V(t

t

)

=

d dt

E[(ty)(ty)T

:

y



N (0, )]

= E[Dt ty(ty)T + (ty)yT  tDt : y  N (0, )]

d dt

V(tt)

t=0

=

E[D 0y(y)T

+

(y)yT  0D

:

y



N (0, )]

Because for x  R, (x) = x (x), for y  RB we can write (y) = -1Diag( (y))y. Then



dV() d

{ 0

+

 0}

=



dV() d

dtt dt

=



d dt

V(t

t

)

t=0

= E[D 0yyT D + DyyT  0D : y  N (0, )]

Therefore, for   HBG,

C{} = P

B - 3, + 1

-1
2--1-1

dV()

{G + G}

2 d =G

= 2P

B - 3, + 1

-1
2--1-1

dV()

{}

2 d =G

So Thm B.66 gives Theorem C.10. BG2  C has the following eigendecomposition

1. eigenspace RG with eigenvalue

GC := 2BP

B

- 2

3

,



+

1

-1
2--1-1GRG,(B, )

22 =
B-3

2. eigenspace L with eigenvalue

LC := 2BP

B - 3, + 1

-1
2--1-1G,(B, )

2L

22(B - 2)

2BJ

-1 B-1

= (B - 3)(B - 1) + (B - 3)(B - 1)2(J(1) - J

-1 B-1

)

3. eigenspace M with eigenvalue

CM := 2BP

B - 3, + 1 2

-1
2--1-1GM,(B, )

2BJ

-1 B-1

=

(B - 3)(B - 1)(J(1) - J

-1 B-1

52

Under review as a conference paper at ICLR 2019

Altogether, by Eq. (43) and Thms C.5, C.8 and C.10, this implies
Theorem C.11. G2  F () : HBG  HBG has eigenspaces RG, M, L respectively with the following eigenvalues

GG,B

=

(B

- 3 + 2) (2 - 1)J (2 - 1)(B - 3)(B -

-1 B-1

+ 2(B

1)(J(1) - J

- 1)J(1)

-1 B-1

)

- 2 B-3

2(B - 3 + 2)

J(1)

+

1 B-1

J-1



-1 B-1

=

(2 - 1)(B - 3)(J(1) - J

-1 B-1

)

- 2 B-3

2

(B

-

2)J(1)

+

(B

-

3

+

2)

1 B-1

J-1 

-1 B-1

+ (2 - 1)(J(1) - J

-1 B-1

=

(2 - 1)(B - 3)(J(1) - J

-1 B-1

)

G,B
M

=

B(B2 + 2( - 2)B + 2( - 3) + (B - 3)(B - 1)(B - 1 + 2)

3)

J J(1) -

-1 B-1

J

-1 B-1

G,B
L

=

- 22(B - 2)(B - 1 + ) (B - 3)(B - 1)(B - 1 + 2)

2(3B - 4) +  3B2 - 11B + 8 + (B - 3)(B - 1)2

J

-1 B-1

+2

(B - 3)(B - 1)2(B - 1 + 2)

J(1) - J

-1 B-1

2(B - 2)(B - 3 + 2) +

J(1)

(2 - 1)(B - 3)(B - 1) J(1) - J

-1 B-1

Proposition C.12.

GG,B

 G,B
L

 G,B
M

D CROSS BATCH: FORWARD ITERATION

Definition D.1. For linear operators Ti : Xi  Yi, i = 1, . . . , k, we write i Ti : for the operator (x1, . . . , xk)  (T1(x1), . . . , Tk(xk)). We also write T1n := direct sum of n copies of T1.

inXi
j=1

 T1

i Yi for the

For k  2, now consider the extended ("k-batch") dynamics on   SkB defined by

(l) = VB k((l-1))

(44)

= E[(B(z1:B), B(zB+1:2B), . . . , B(z(k-1)B+1:kB))2 : z  N (0, (l-1))]

If we restrict the dynamics to just the upper left B О B submatrix (as well as any of the diagonal

B О B blocks) of (l), then we recover Eq. (28).

D.1 LIMIT POINTS

Definition D.2. We say a matrix   SkB is CBSB1 (short for "1-Step Cross-Batch Symmetry Breaking") if  in block form (k О k blocks, each of size B О B) has one common BSB1 block on the diagonal and one common constant block on the off-diagonal, i.e.

BSB1(a, b)

 c11T

 

c11T

 ...

c11T
BSB1(a, b)
c11T
...

c11T c11T
BSB1(a, b) ...

и и и
и и и и и и ...

We will study the fixed point  to Eq. (44) of CBSB1 form.

53

Under review as a conference paper at ICLR 2019

D.1.1 SPHERICAL COORDINATES

Theorem D.3. Let   SkB. If  is CBSB1 then VBk() is CBSB1 and equals  =

 c c 

 where  is the BSB1 fixed point of Thm B.18 and c  0 with c =

((B-1)/2) ((B-2)/2)

-1/2

 0

d

 (- B

-

1

cos

)

sinB-3

.

Proof. We will prove for k = 2. The general k cases follow from the same reasoning.

Let  =


c11T

c11T


where  = BSB1(a, b). As remarked below Eq. (44), restricting to any

diagonal blocks just recovers the dynamics of Eq. (28), which gives the claim about the diagonal blocks being  through Thm B.18.

We now look at the off-diagonal blocks.

VB2()1:B,B+1:2B = E[B(z1:B)  B(zB+1:2B) : z  N (0, )] = E[  N(y1:B)    N(yB+1:2B) : y  N (0, G2 )]

where G2 := G2G2 =

G 0

0 G



G 0

0 G

=

GG cG11T G cG11T G GG

=

(a - b)G 0

0 (a - b)G

(the last step follows from Lemma B.15). Thus y1:B is independent from

yB+1:2B , and

VB 2()1:B,B+1:2B = E[  N(x) : x  N (0, (a - b)G)]2

By symmetry,


E[  N(x) : x  N (0, (a - b)G)] = c1 E[  N(x) : x  N (0, (a - b)G)]2 = c11T

 where c := E[(N(x)1) : x  N (0, (a - b)G)]. We can compute

 c = E[(N(x)1) : x  N (0, G)]

because N is scale-invariant

= E[(N(ex)1) : x  N (0, IB-1)] where e is as in Eq. (39)

= E[((eN(x))1) : x  N (0, IB-1)] because e is an isometry

= E[( B(ev)1) : x  N (0, IB-1)]

with v = x/ x

=

((B ((B

- -

1)/2) 2)/2)



-1/2



d

 (- B

-

1

cos

)

sinB-3



0

Corollary D.4. (x) = a(x)

With the notationas - b(-x), then c

in =

Thm (a -

D.3, if 

b)

1 2

(B

is -

positive-homogeneous of degree

1)/2

((B-1)/2)((+1)/2) ((+B-1)/2)

.



and

54

Under review as a conference paper at ICLR 2019

Proof. We compute

/2 d (cos ) sinB-3  = 1 Beta 0, 1;  + 1 , B - 2 0 2 22
by Lemma B.19

1 = Beta

 + 1, B - 2

2 22

 /2

d (- cos ) sinB-3  =

d (cos ) sinB-3 

/2 0

= 1 Beta  + 1 , B - 2 2 22

So for a positive homogeneous function (x) = a(x) - b(-x),

 c

=

((B ((B

- -

1)/2) 2)/2)

-1/2

a



d

 (- B

-

1

cos

)

sinB-3



-

b

/2

d

 (B

-

1

cos

)

sinB-3



/2 0

=

(a

-

b)

((B ((B

- -

1)/2) 2)/2)

-1/2(B

-

1)/2

1 2

Beta

 + 1, B - 2 22

=

(a

-

b)

1 2

(B

-

1)/2

((B - 1)/2)(( + 1)/2) (( + B - 1)/2)

Expanding the beta function and combining with Thm D.3 gives the desired result.

D.1.2 LAPLACE METHOD

While we don't need to use the Laplace method to compute c for positive homogeneous functions (since it is already given by Corollary D.4), going through the computation is instructive for the machinery for computing the eigenvalues in a later section.

Lemma D.5 (The Laplace Method Cross Batch Master Equation). For A, B, C  N, let f :
RA+B  RC and let a, b  0. Suppose for any y  RA, z  RB, f (y, z)  h( y 2 + z 2) for some nondecreasing function h : R0  R0 such that E[h(r z ) : z  N (0, IA+B)] exists for every r  0. Define () := E[ y -2a z -2bf (y, z) : (y, z)  N (0, )]. Then on {  SA+B : rank  > 2(a + b)}, () is well-defined and continuous, and furthermore satisfies



() = (a)-1(b)-1 ds dt sa-1tb-1 det(IA+B + 2)-1/2 E f (y, z) (45)

00

(y,z)N (0,)



where D =

sIA 0

0 tIB

,  = DD, and  = D-1(I + 2)-1D-1.

Proof. If  is full rank, we can show Fubini-Tonelli theorem is valid in the following computation by the same arguments of the proof of Lemma B.5.

E[ y -2a z -2bf (y, z) : (y, z)  N (0, )]



= E[f (y, z)

ds (a)-1sa-1e- y 2s

dt (b)-1tb-1e- y 2t : (y, z)  N (0, )]

00



=

(2)-

A+B 2

(a)-1(b)-1

det

-1/2

ds


dt sa-1tb-1

dy

dz

f

(y,

z)e-

1 2

(y,z)(-1

+2D2

)(y,z)T

00

RA+B

(Fubini-Tonelli)


= (a)-1(b)-1 ds dt sa-1tb-1 det((-1 + 2D2))-1/2 E[f (y, z) : (y, z)  N (0, (-1 + 2D2)-1)]

00

55

Under review as a conference paper at ICLR 2019

We recover the equation in question with the following simplifications. (-1 + 2D2)-1 = (IA+B + 2D2)-1 = (D-1 + 2D)-1D-1 = D(IA+B + 2DD)-1D-1 = D-1(IA+B + 2)-1D-1
det((-1 + 2D2)) = det(IA+B + 2D2) = det(IA+B + 2DD) = det(IA+B + 2)
The case of general  with rank  > 2(a + b) is given by the same continuity arguments as in Lemma B.5.

Let   S2B,  =

 T

 

where ,   SB and   RBОB. Consider the off-diagonal block

of VB 2().

E[B(z)  B(z ) : (z, z )  N (0, )]

= E[(N(y))  (N(y )) : (y, y )  N (0, G2 )]

= B E[ y - y -(y)  (y ) : (y, y )  N (0, G2 )]



= B(/2)-2 ds dt (st)/2-1 det(I2B + 2)-1/2 E (y)  (y )

00

(y,y )N (0,)

(by Lemma D.5)



= B(/2)-2 ds

dt (st)/2-1 det(I2B + 2)-1/2V()1:B,B+1:2B

(46)

0

where  = 

 sG st(G)T

sIB 0

0 tIB

.

0 stG t G

and



=

D-1(I

+

2)-1D-1

with

D

=

 sIB



 tIB

=

Theorem D.6 (Rephrasing and adding to Corollary D.4). Let   SkB and  be positive homoge-

neous of degree . If  is CBSB1 then VBk() is CBSB1 and equals  =


c11T

c11T


where  is the BSB1 fixed point of Thm B.18 and

c = c

B-1  P
2

B - 1,  22

-2
J(0)

=

(a

-

b)2

1 4

(B

-

1)

((B - 1)/2)(( + 1)/2) 2 (( + B - 1)/2)

Proof. Like in the proof of Corollary D.4, we only need to compute the cross-batch block, which is

given above by Eq. (46). Recall that  =


c11T

c11T


where  = BSB1(a, b). Set  := a - b.

Note that because the off-diagonal block  = c11T by assumption, G = 0, so that

 = sG  tG

= sG  tG

(I

+

2)-1

=

1

s + 2s

G



1

t + 2t

G



=

1

 + 2s

G



1

 + 2t

G

56

Under review as a conference paper at ICLR 2019

Therefore,

det(I + 2) = (1 + 2s)B-1(1 + 2t)B-1

V() = 

B-1 B


V

BSB1(1,

-1 B-1

)

0

0

BSB1(1,

-1 B-1

)



where  =

(1 + 2s)-/2IB 0

0 (1 + 2t)-/2IB

. In particular, the off-diagonal block is con-

stant with entry c

B-1 B

 (1 + 2s)-/2(1 + 2t)-/2J(0).

Finally, by Eq. (46),

E[B(z)  B(z ) : (z, z )  N (0, )]



= B(/2)-2

ds

dt (st)/2-1 det(I2B + 2)-1/2V()1:B,B+1:2B

00



= B(/2)-2

ds

dt

(st)/2-1[(1

+

2s)(1

+

2t)]-

B-1 2

00

О

c

B-1 B


[(1 + 2s)(1 + 2t)]-/2J(0)11T



= c(B - 1)(/2)-2J(0)11T

d

d

( )/2-1(1

+

2)-

B-1+ 2

(1

+

2

)-

B-1+ 2

00

(with  = s,  = t)

= c(B - 1)(/2)-2J(0)11T

2-/2 Beta

B

- 2

1,

 2

2

= c

B-1  P
2

B

- 2

1

,

 2

-2
J(0)11T

Simplification with Defn H.2 and Proposition I.5 gives the result.

E LOCAL CONVERGENCE

We now look at the linearized dynamics around CBSB1 fixed points of Eq. (44). Theorem E.1. eigenvalue of deviation with zero diagonal blocks is

B 2

B - 1 -1 P
2

B,  22

-2
-1cJ(0)

=

BP B-1 P

B-1 2

,



B 2

,

 2

2

J(0) J(1) - J

-1 B-1

Proof. Let  =


c11T

c11T


be the CBSB1 fixed point in Thm D.6. Take a smooth path

 =

 T

 



S2B, 



(-

,

) such that 0

=

,  0

:=

d d



=



0 T

 0

for some

  RBОB. Then with  =

 sG st(G )T

stG t G

and  = D-1 (I + 2 )-1D-1 with

57

Under review as a conference paper at ICLR 2019

 D = sIB  tIB =

 sIB 0

0 tIB

,

d d

E[B(z)

 B(z

)

:

(z, z

)



N (0, )]

 =0

=

d d

B

(/2)-2


ds
0


dt (st)/2-1 det(I2B + 2)-1/2V()1:B,B+1:2B
0  =0


= B(/2)-2 ds dt (st)/2-1
00

d d

det(I2B

+ 2)-1/2

V()1:B,B+1:2B
 =0



= B(/2)-2

ds

0


dt (st)/2-1
0

+ det(I2B

+ 2)-1/2

d d

V()1:B,B+1:2B

 =0

- det(I + 20)-1/2 tr (I + 20)-1 0 V()

+ det(I

+ 20)-1/2(D-)2 

dV d

=0 (I +20 )-1

d d

(I

+

2)-1

 =0

(47)

1:B,B+1:2B

(by chain rule and Lemma B.51)

We compute

0 = (sG  tG)

 0

=

 st

0 (G)T

G 0

(I

+

20)-1

=

((1

+

2s)-1G

+

1 B

11T )



((1

+

2t)-1G

+

1 B

11T

)

det(I + 20) = (1 + 2s)B-1(1 + 2t)B-1

0(I

+ 20)-1

=

1

s + 2s

G



1

t + 2t

G

dV =
d =0(I+20)-1

s 1 + 2s

(-1)/2
IB 

t 1 + 2t

(-1)/2
IB

2


B-1 B

-1 dV

d =BSB1(1,

-1 B-1

)2

(because dV/d is degree  - 1 positive homogeneous)

d d

(I

+

2)-1

 =0

=

(I

+

20)-1 0(I

+

20)-1



st = (1 + 2s)(1 + 2t)

0 (G)T

G 0

(by Lemma B.52)

dV d =0(I+20)-1

d d

(I

+

2)-1

 =0

B - 1 -1

 st

=B

(1 + 2s)(1 + 2t) cJ(0)

s (-1)/2 1 + 2s

t (-1)/2 1 + 2t

=

B - 1 -1

(st)/2-1cJ(0)

B (1 + 2s)(+1)/2(1 + 2t)(+1)/2

0 (G)T

G 0

0 G (G)T 0

58

Under review as a conference paper at ICLR 2019

Thus the product (I + 20)-1 0 has zero diagonal blocks so that its trace is 0. Therefore, only the second term in the sum in Eq. (47) above survives, and we have

d d

E[B(z)



B(z

)

:

(z, z

)



N (0, )]

 =0



= B(/2)-2 ds

dt

(st)/2-1(1

+

2s)-

B-1 2

(1

+

2t)-

B-1 2

О

00

(st)-/2

B-1 B

-1

(1

+

(st)/2-1cJ(0) 2s)(+1)/2(1 + 2t)(+1)/2

G



= B(B - 1)-1(/2)-2

ds

0

 0

dt

(st)/2-1

(1

+

-1cJ(0) 2s)(B+)/2(1 + 2t)(B+)/2

G

= B(B - 1)-1(/2)-2-1cJ(0)

 0

d(s)

(1

(s)/2-1 + 2s)(B+)/2

2
G

= B(B - 1)-1(/2)-2-1cJ(0)

2-/2 Beta

B,  22

2
G

=B 2

B - 1 -1 P
2

B,  22

-2
-1cJ(0)G

F CROSS BATCH: BACKWARD DYNAMICS

For k  2, we study the linear equation ~ (l) = V (B k) (){~ (l+1)}

(48)

Its dynamics is essentially given by the eigendecomposition of V (Bk) ().

Consider the case k = 2 (for illustration purposes).

V (B2) =

E [(B(x)  B(y))2]

(x,y)N (0,)

=

V (B 2)

 T 

=E
(x,y)N (0,)

B(x)B(x)T B(x)B(y)T B(y)T B(x)T B(y)B(y)T

From this one sees that V (B 2) acts independently on each block, and consequently so does

its adjoint. The diagonal blocks of Eq. (48) evolves according to Eq. (41) which we studied in

Appendix C. In this section we will study the evolution of the off-diagonal blocks, which is given

by

(l) =

E [B(x)  B(y)]{(l+1)} =

E [B(y)T  B(x)T ]{(l+1)}

(x,y)N (0,)

(x,y)N (0,)

(49)

= E [B(y)T (l+1)B(x)T ]
(x,y)N (0,)

Define U := E(x,y)N (0,)[B(x)  B(y)]. Then

U= E
(,)N (0,G2G2)

d  N(z)  d  N(z)

dz z=

dz z=

 G2

=E

(,)N (0,(

G 0

0 G

))

d  N(z)  d  N(z)

dz z=

dz z=

 G2

59

Under review as a conference paper at ICLR 2019

since  is CBSB1 with diagonal blocks  (Corollary D.4). Then  is independent from , so this is just

E [B(x)
(x,y)N (0,)



B(y)]

=

E
N (0,G)

2

d  N(z) dz z=

 G2

=E

 BDr-1(I

- vvT )

2  G2

N (0,G)

(by Proposition C.4)



where D = Diag( ( Bv)), r =  , v = /  .

But notice that R :=

EN (0,G) Dr-1(I - vvT ) is actually BSB1, with diagonal 
E[r-1(1 - vi2) ( Bvi) : rv  N (0, G)] = E[r-1(1 - v12) ( Bv1) : rv  N (0, G)], i  [B]

and off-diagonal 
E[r-1(-vivj) ( Bvi) : rv  N (0, G)] = E[r-1(-v1v2) ( Bv1) : rv  N (0, G)], i = j  [B].

Thus by Lemma B.14, R has two eigenspaces, {x : Gx = x} and R1, with respective eigenvalues
R11 - R12 and R11 + (B - 1)R12. However, because of the composition with G2, only the the former eigenspace survives with nonzero eigenvalue in U.

Theorem F.1. U has two eigenspaces { :

tive

eigenvalues

1 2

B

-1

 0

d1

sinB-1

G 1

= } and (- B -

its orthogonal complement, 1 cos 1) 2 and 0.

with

respec-

Proof. By the above reasoning, the sole eigenspace with nonzero eigenvalue is {x : Gx = x}2 = { : G = }. It has eigenvalue

B

 E[r-1(1 - v12 + v1v2) ( Bv1) : rv  N (0, G)]

2

= B-1

 E[r-1(1 - v12 + v1v2) ( Bv1) : rv  N (0, G)]

2

2
= B-1 E[r-1(1 - (ew)21 + (ew)1(ew)2) ( B(ew)1) : rw  N (0, IB-1)]

= B-1

((B ((B

- -

2)/2) 3)/2)

2-1/2

-1


d1
0

 2 d2 (1 - (1)2 + (1)(1, 2)) ( B(1)) sinB-3 1 sinB-4 2
0

where we applied Lemma B.48 with () = -

B-2 B-1

sin

1

cos

2.

B-1 B

cos



and

(1,

2)

=

1
B(B-1)

cos 1

-

We can further simplify





d1 d2 (1 - (1)2 + (1)(1, 2)) ( B(1)) sinB-3 1 sinB-4 2

00

 = d1 sinB-3 1 ( B(1))
0

1 - (1)2 + (1)

1 B(B

-

1)

cos

1


d2 sinB-4 2
0

-

B B

- -

2 1

sin

1

(1)


d2 sinB-4 2 cos 2
0

 = d1 sinB-3 1 ( B(1))
0

1 - (1)2 + (1)

1 B(B

-

1)

cos

1

Beta

B - 3, 1 22

=

 d1 sinB-3 1 ( B(1))
0

sin2 1 Beta

B - 3, 1 22

= Beta B - 3 , 1 22


d1

sinB-1 1

 (- B

- 1 cos 1)

0

(by Lemma B.20)

60

Under review as a conference paper at ICLR 2019

so the cross-batch backward off-diagonal eigenvalue is

B-1

1 2


d1

sinB-1 1

 (- B

- 1 cos 1)

0

2

=

1 2

B



-1


d1

sinB-1 1

 (- B - 1 cos 1)

2

0

Theorem F.2. If  is positive-homogeneous of degree  with (c) = a(c) - b(-c), then the

cross-batch backward off-diagonal eigenvalue is

1 8

B

(B

-

1)-1

-12(a

+

b)2

Beta

 2

,

B 2

2
.

Proof. As in the proof of Corollary D.4,

/2
d (cos )-1 sinB-1  =

 d (- cos )-1 sinB-1  = 1 Beta

0 /2 2

So

1 2

B

-1

 2 d1 sinB-1 1 (- B - 1 cos 1)
0

, B 22

=

1 2

B

-1(B

- 1)-12

 /2

a d (- cos )-1 sinB-1  + b

d (cos )-1 sinB-1 

/2 0

= 1 B-1(B - 1)-12(a + b)22-2 Beta  , B 2 2 2 2

=

1 8

B(B

- 1)-1-12(a +

b)2

Beta

, B 22

2

2

G WEIGHT GRADIENTS

xiи = (hiи),

N
hik = wij xjk + bi.
j=1

Suppose we have a loss function E which induces, on a minibatch of inputs x(и0и), a minibatch of gra-

dient vectors at the layer l, / .E xи(lи)

Then

E  hi(иl)

=

E  xi(иl)

d dh

(hi(иl))

and

E  xi(иl-1)

=

j

E  h(jlи)

wj

i

.

The

latter is again a sum of a large number of random variables, so converges to a Gaussian (with zero

mean) as width N  . Write gjb for E/x(jlb). Then (with gradient independence assumption)

E[xiagjbxi cgj d] = E[xiaxi c] E[gjbgj d]

= acI(i = i )bdI(j = j )

So

E



E 2

wij

= E

2
xiagja 
a



= E  xiagjaxibgjb
a,b
= abab
a,b
= , 

61

Under review as a conference paper at ICLR 2019

Since  is BSB1 with GG = G and   SBG, we get

E

E wij

2

= , 

= , G2{}

= G2{},  = G,  =  tr .

H -RELU

Recall that -ReLUs (Yang & Schoenholz (2017)) are, roughly speaking, the th power of ReLU.
Definition H.1. The -ReLU function  : R  R sends x  x when x > 0 and x  0 otherwise.

This is a continuous function for  > 0 but discontinuous at 0 for all other .

We briefly review what is currently known about the V and W transforms of  Cho & Saul (2009b); Yang & Schoenholz (2017).

Definition H.2.

For

any



>

-

1 2

,

define

c

=

1 

2-1(

+

1 2

).

When considering only 1-dimensional Gaussians, V is very simple.

Proposition H.3.

If



>

-

1 2

,

then

for

any

q

 S1

= R0, V(q) = cq

To express results of V on SB for higher B, we first need the following Definition H.4. Define

J()

:=

1 2c

(sin

)2+1(

+

1)

/2 d cos  0 (1 - cos  cos )1+

and J(c) = J(arccos c) for  > -1/2.

Then Proposition H.5. For any   SB, let D be the diagonal matrix with the same diagonal as . Then
V() = cD/2J(D-1/2D-1/2)D/2 where J is applied entrywise.

For example, J and J for the first few integral  are

J0()

=



- 



J1()

=

sin 

+

( - 

) cos 

J2()

=

3 sin  cos 

+

( - 3

)(1

+

2 cos2

)

J0(c)

=

- 

arccos c 

J1(c) =

1 - c2 + ( - arccos c)c 

J2(c) = 3c

1 - c2 + ( - arccos c)(1 + 2c2) 3

One can observe very easily that Daniely et al. (2016); Yang & Schoenholz (2017)

62

Under review as a conference paper at ICLR 2019

Proposition H.6. For each  > -1/2, J(c) is an increasing and convex function on c  [0, 1],

and

is

continous

on

c



[0, 1]

and

smooth

on

c



(0, 1).

J(1)

=

1,

J(0)

=

,1
2

(

 2

+

1 2

)2

(+

1 2

)

and

J(-1) = 0.

Yang & Schoenholz (2017) also showed the following fixed point structure Theorem H.7. For   [1/2, 1), J(c) = c has two solutions: an unstable solution at 1 ("unstable" meaning J(1) > 1) and a stable solution in c  (0, 1) ("stable" meaning J(c) < 1).

The -ReLUs satisfy very interesting relations amongst themselves. For example, Lemma H.8. Suppose  > 1. Then
J() = cos J-1() + ( - 1)2(2 - 1)-1(2 - 3)-1 sin2 J-2() J(c) = cJ-1(c) + ( - 1)2(2 - 1)-1(2 - 3)-1(1 - c2)J-2(c)

In additional, surprisingly, one can use differentiation to go from  to  + 1 and from  to  - 1! Proposition H.9. Yang & Schoenholz (2017) Suppose  > 1/2. Then

J() = -2(2 - 1)-1J-1() sin  J(c) = 2(2 - 1)-1J-1(c)

so that






J-n(c) = 

-2(2 - 1) (/c)nJ(c)

=-n+1

We have the following from Cho & Saul (2009b)

Proposition H.10. Cho & Saul (2009b) For all   0 and integer n  1

Jn+(c)

=

c cn+

(1

-

c2)n++

1 2

(

/c)n(J(c)/(1

-

c2)+

1 2

)


+n-1

-1

=

(2 + 1)

(1

-

c2)n++

1 2

(/c)n(J(c)/(1

-

c2)+

1 2

)

=

This implies in particular that we can obtain J from J and J+1. Proposition H.11. For all   0,
J(c) = (2 + 1)(1 - c2)-1(J1+(c) - cJ(c)))

Proof.

J1+(c)

=

c c1+

(1

-

c2)1++

1 2

(/

c)(J(c)/(1

-

c2)+

1 2

)

= (2 + 1)-1(1 - c2)+3/2(J(c)/(1 - c2)+1/2 + 2c( + 1/2)J(c)/(1 - c2)+3/2)

= (2 + 1)-1J(c)(1 - c2) + cJ(c)

J(c) = (2 + 1)(1 - c2)-1(J1+(c) - cJ(c))

Note that we can also obtain this via Lemma H.8 and Proposition H.9. 63

Under review as a conference paper at ICLR 2019

I POSITIVE-HOMOGENEOUS FUNCTIONS IN 1 DIMENSION

Suppose for some   R,  : R  R is degree  positive-homogeneous, i.e. (rx) = r(x) for any x  R, r > 0. The following simple lemma says that we can always express  as linear combination of powers of -ReLUs.
Proposition I.1. Any degree  positive-homogeneous function  : R  R with (0) = 0 can be written as x  a(x) - b(-x).

Proof. Take a = (1) and b = (-1). Then positive-homogeneity determines the value of  on R \ {0} and it coincides with x  a(x) - b(-x).

As a result we can express the V and W transforms of any positive-homogeneous function in terms of those of -ReLUs.
Proposition I.2. Suppose  : R  R is degree  positive-homogeneous. By Proposition I.1,  restricted to R \ {0} can be written as x  a(x) - b(-x) for some a and b. Then for any PSD 2 О 2 matrix M ,
V(M )11 = (a2 + b2)V(M )11 = c(a2 + b2)M11 V(M )22 = (a2 + b2)V(M )22 = c(a2 + b2)M22 V(M )12 = V(M )21 = (a2 + b2)V(M )12 - 2abV(M )12
= c(M11M22)/2((a2 + b2)J(M12/ M11M22) - 2abJ(-M12/ M11M22))

where M :=

1 0

0 -1

M

1 0

0 -1

.

Proof. We directly compute, using the expansion of  into s:

V(M )11 = E[(x)2 : x  N (0, M11)]

= E[a2(x)2 + b2(-x)2 + 2ab(x)(-x)]

= a2 E[(x)2] + b2 E[(-x)2]

= (a2 + b2) E[(x)2 : x  N (0, M11)]

(50)

= c(a2 + b2)M11

where in Eq. (50) we used negation symmetry of centered Gaussians. The case of V(M )22 is similar.

V(M )12 = E[(x)(y) : (x, y)  N (0, M )] = E[a2(x)(y) + b2(-x)(-y) - ab(x)(-y) - ab(-x)(y)] = (a2 + b2)V(M )12 - 2abV(M )12

= c(M11M22)/2((a2 + b2)J(M12/ M11M22) - 2abJ(-M12/ M11M22))

where in the last equation we have applied Proposition H.5.

This then easily generalizes to PSD matrices of arbitrary dimension:
Corollary I.3. Suppose  : R  R is degree  positive-homogeneous. By Proposition I.1,  restricted to R \ {0} can be written as x  a(x) - b(-x) for some a and b. Let   SB. Then
V() = cD/2J(D-1/2D-1/2)D/2
where J is defined below and is applied entrywise. Explicitly, this means that for all i, V()ii = ciiJ(1) V()ij = cJ(ij / iijj )ii/2jj/2
Definition I.4. Suppose  : R  R is degree  positive-homogeneous. By Proposition I.1,  restricted to R \ {0} can be written as x  a(x) - b(-x) for some a and b. Define J(c) := (a2 + b2)J(c) - 2abJ(-c).

64

Under review as a conference paper at ICLR 2019

Let us immediately make the following easy but important observations.

Proposition I.5.

J(1)

=

a2

+

b2

and

J(0)

=

(a2

+

b2

-

2ab)J(0)

=

(a

-

b)2

1 2

(

 2

+

1 2

)2

(+

1 2

)

,

Proof.

Use the fact that J(-1) = 0 and J(0) =

1 2

(

 2

+

1 2

)2

(+

1 2

)

by Proposition H.6.

As a sanity check, we can easily compute that Jid(c) = 2J1(c) - 2J1(-c) = 2c because id(x) =

(x) - (-x).

By

Corollary

I.3

and

c1

=

1 2

,

this

recovers

the

obvious

fact

that

V(id)()

=

.

We record the partial derivatives of V.

Proposition I.6. Let  be positive homogeneous of degree . Then for all i with ii = 0,

For all i = j with ii, jj = 0,

V()ii ii

=

cii-1J(1)

V()ii ij

=

0

V()ij ij

= ci(i-1)/2j(j -1)/2J(cij )

V()ij ii

=

1 2

ci21i(-2)j12j(J(cij

)

-

cij

J(cij

))

where cij = ij/ iijj and J denotes its derivative. Proposition I.7. If (c) = a(c) - b(-c) with  > 1/2 on R \ {0}, then

J(c) = (2 - 1)-1J (c)

= (2 + 1)(1 - c2)-1((a2 + b2)J+1(c) + 2abJ+1(-c)) - cJ(c)

J(0)

=

(a

+

b)2

1 

(

 2

(

+ +

1)2

1 2

)

Proof. We have  (c) = a-1(c) + b-1(-c). On the other hand, J = (a2 + b2)J(c) +

(22abJ-(1-)-c)1Jwh(icc)h.

by Proposition This proves the

H.9 first

is 2(2 equation.

-

1)-1((a2

+ b2)J-1(c) + 2abJ-1(-c))

=

With Proposition H.11,

J(c) = (a2 + b2)J(c) + 2abJ(-c) = (a2 + b2)(2 + 1)(1 - c2)-1(J+1(c) - cJ(c)) + 2ab(2 + 1)(1 - c2)-1(J+1(-c) + cJ(-c)) = (2 + 1)(1 - c2)-1((a2 + b2)J+1(c) + 2abJ+1(-c)) - cJ(c)

This gives the second equation.

Expanding J+1(0) With Proposition H.6, we get

J(0) = (2 + 1)((a2 + b2)J+1(0) + 2abJ+1(0))

=

(2

+

1)(a

+

b)2

1 2

(

 2

(

+ +

1)2

3 2

)

Unpacking

the

definition

of

(

+

3 2

)

then

yields

the

third

equation.

In general, we can factor diagonal matrices out of V.
Proposition I.8. For any   SB, D any diagonal matrix, and  positive-homogeneous with degree ,
V(DD) = DV()D

65

Under review as a conference paper at ICLR 2019

Proof. For any i,

V(DD)ii = E[(x)2 : x  N (0, Di2iii)] = E[(Diix)2 : x  N (0, ii)] = E[Di2i(x)2 : x  N (0, ii)] = Di2iV()ii

For any i = j,

V(DD)ij = E[(x)(y) : (x, y)  N (0,

Di2iii DiiDjj ji

= E[(Diix)(Djjy) : (x, y)  N (0,

ii ji

DiiDjj ij Dj2j jj

)

ij jj

)]

= DiiDjj E[(x)(y) : (x, y)  N (0,

ii ji

ij jj

)]

= DiiDjj V()ij

66

