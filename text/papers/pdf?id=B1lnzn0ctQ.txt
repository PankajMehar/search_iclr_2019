Under review as a conference paper at ICLR 2019

ALISTA: ANALYTIC WEIGHTS ARE AS GOOD AS LEARNED WEIGHTS IN LISTA
Anonymous authors Paper under double-blind review

ABSTRACT
Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven "black-box" training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This significantly simplifies the training. Specifically, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.

1 INTRODUCTION

Sparse vector recovery, or sparse coding, is a classical problem in source coding, signal reconstruction, pattern recognition and feature selection. There is an unknown sparse vector x = [x1, · · · , xM ]T  RM . We observe it noisy linear measurements:

M
b = dmxm +  = Dx + ,
m=1

(1)

where b  RN , D = [d1, · · · , dM ]  RN×M is the dictionary, and   RN is additive Gaussian white noise. For simplicity, each column of D, named as a dictionary kernel, is normalized, that is, dm 2 = D:,m 2 = 1, m = 1, 2, · · · , M . Typically, we have N M , so Equation (1) is an under-determined system.

However, when x is sufficiently sparse, it can be recovered faithfully. A popular approach is to solve the LASSO problem below (where  is a scalar):

1 minimize
x2

b - Dx

2 2

+



x

1

using iterative algorithms such as the iterative shrinkage thresholding algorithm (ISTA):

(2)

x(k+1) = /L

x(k) + 1 DT (b - Dx(k)) L

,

k = 0, 1, 2, . . .

(3)

where  is the soft-thresholding function1 and L is usually taken as the largest eigenvalue of DT D.

Inspired by ISTA, the authors of (Gregor & LeCun, 2010) proposed to learn the weights in the matrices in ISTA rather than fixing them. Their methods is called Learned ISTA (LISTA) and resembles a recurrent neural network (RNN). If the iteration is truncated to K iterations, LISTA becomes a K-layer feed-forward neural network with side connections. Specifically, LISTA is:

x(k+1) = (k) (W1(k)b + W2(k)x(k)), k = 0, 1, · · · , K - 1.

(4)

1Soft- thresholding function is defined in a component-wise way: (x) = sign(x) max(0, |x| - )

1

Under review as a conference paper at ICLR 2019

If we Given

set W1(k) each pair



1 L

DT

,

W2(k)



of sparse vector and

I

-

1 L

DT

D,

(k)



its noisy measurements

(L1x,,

then LISTA b), applying

recovers ISTA. (4) from some

initial point x(0) and using b as the input yields x(k). Our goal is to choose the parameters

 = {W1(k), Ww(k), (k)}k=0,1,...,K-1 such that x(k) is close to x for all sparse x following some distribution P. Therefore, given the distribution P, all parameters in  are subject to learning:

2
minimize Ex,bP x(K) , b, x(0) - x .
2

(5)

This problem is approximately solved over a training dataset {(xi, bi)}Ni=1 sampled from P.
Many empirical results, e.g., (Gregor & LeCun, 2010; Sprechmann et al., 2015; Wang et al., 2016), show that a trained K-layer LISTA (with K usually set to 10  20) or its variants can generalize more than well to unseen samples (x , b ) from the same distribution and recover x from b to the same accuracy within one or two order-of-magnitude fewer iterations than the original ISTA. Additionally, the accuracies of the outputs {x(k)} of the layers k = 1, .., K gradually improve. However, such networks will generalize worse when the input deviates from the training distribution (e.g., when D varies), in contrast to the classical iterative algorithms such as ISTA that are trainingfree and thus agnostic to the input distribution.
More recently, the convolutional sparse coding (CSC), an extension of the sparse coding (1), gains increasingly attention in the machine learning area. (Sreter & Giryes, 2018) showed that the CSC could be similarly approximated and accelerated by a LISTA-type feed-forward network. (Tolooshams et al., 2018) designed a structure of sparse auto-encoder inspired by multi-layer CSC. (Papyan et al., 2016; Sulam et al., 2017) also revealed CSC as a potentially useful tool for understanding general convolutional neural networks (CNNs).

1.1 RELATED WORK
Despite the empirical success (Sprechmann et al., 2015; Wang et al., 2016; Zhang & Ghanem, 2018; Zhou et al., 2018; Ito et al., 2018) in constructing fast trainable regressors for approximating iterative sparse optimization solvers, the theoretical understanding of such approximations remains limited.
A handful of recent works have been investigating the theory of LISTA. (Moreau & Bruna, 2017) re-factorized the Gram matrix of dictionary, by trying to nearly diagonalize the Gram matrix with a basis, subject to a small 1 perturbation. They thus re-parameterized LISTA a new factorized architecture that achieved similar acceleration gain to LISTA, hence ending up with an "indirect" proof. They concluded that LISTA can converge faster than ISTA, but still sublinearly. (Giryes et al., 2018) interpreted LISTA as a projected gradient descent descent (PGD) where the projection step was inaccurate, which enables a trade-off between approximation error and convergence speed. The latest work (Chen et al., 2018) presented the more related results to ours: they introduced necessary conditions for the LISTA weight structure in order to achieve asymptotic linear convergence of LISTA, which also proved to be a theoretical convergence rate upper bound. They also introduced a thresholding scheme for practically improving the convergence speed. Note that, none of the above works extended their discussions to CSC and its similar LISTA-type architectures.
Several other works examined the theoretical properties of some sibling architectures to LISTA. (Xin et al., 2016) studied the model proposed by (Wang et al., 2016), which unfolded/truncated the iterative hard thresholding (IHT) algorithm instead of ISTA, for approximating the solution to 0-minimization. They showed that the learnable fast regressor can obtained a transformed dictionary with improved restricted isometry property (RIP). However, their discussions are not applicable to LISTA directly, although IHT is linearly convergent (Blumensath & Davies, 2009) under rather strong assumptions. Their discussions were also limited to linear sparse coding and resulting fully-connected networks only. (Borgerding et al., 2017; Metzler et al., 2017) studied a similar learning-based model inspired from another LASSO solver, called approximated message passing (AMP). (Borgerding et al., 2017) showed the MMSE-optimality of an AMP-inspired model, but not accompanied with any convergence rate result. Also, the popular assumption in analyzing AMP algorithms (called "state evolution") does not hold when analyzing ISTA.
2

Under review as a conference paper at ICLR 2019

1.2 MOTIVATION AND CONTRIBUTIONS
This paper presents multi-fold contributions in advancing the theoretical understanding of LISTA, beyond state-of-the-art results. Firstly, we show that the layer-wise weights in LISTA need not being learned from data. That is based on decoupling LISTA training into a data-free analytic optimization stage followed by a lighter-weight data-driven learning stage without compromising the optimal linear convergence rate proved in (Chen et al., 2018). We establish a minimum-coherence criterion between the desired LISTA weights and the dictionary D, which leads to an efficient algorithm that can analytically solve the former from the latter, independent of the distribution of x. The data-driven training is then reduced to learning layer-wise step sizes and thresholds only, which will fit the distribution of x. The new scheme, called Analytic LISTA (ALISTA), provides important insights into the working mechanism of LISTA. Experiments shows ALISTA to perform comparably with previous LISTA models (Gregor & LeCun, 2010; Chen et al., 2018) with much lighter-weight training. Then, We extend the above discussions and conclusions to CSC, and introduce an efficient algorithm to solve the convolutional version of coherence minimization. Further, we introduce a new robust LISTA learning scheme benefiting from the decoupled structure, by adding perturbations to D during training. The resulting model is shown to possess much stronger robustness when the input distribution varies, even when D changes to some extent, compared to classical LISTA models that learn to (over-)fit one specific D.

2 ANALYTIC LISTA: CALCULATING WEIGHTS WITHOUT TRAINING

We theoretically analyze the LISTA form defined in (Chen et al., 2018):

x(k+1) = (k) x(k) - (W(k))T (Dx(k) - b) ,

(6)

where W(k) = [w1(k), · · · , wM(k)]  RN×M is a linear operator with the same dimensionality with D, x(k) = x(1k), · · · , xM(k) is the kth layer node. In (6),  = {W(k), (k)}k are parameters to train. Model (6) can be derived from (4) with W1(k) = (W(k))T , W2(k) = I - W1(k)D. (Chen et al., 2018) showed that (6) has the same representation capability with (4) on the sparse recovery problem, with
a specifically light weight structure.

Our theoretical analysis will further define and establish properties of "good" parameters  in (6), and then discuss how to analytically computer those good parameters rather than relying solely on black-box training. In this way, the LISTA model could be further significantly simplified, without little performance loss. The proofs of all the theorems in this paper are provided in the appendix.

2.1 RECOVERY ERROR UPPER BOUND

We start with an assumption on the "ground truth" signal x and the noise . Assumption 1 (Basic assumptions). Signal x is sampled from the following set:
x  X (B, s) x |xi|  B, i, x 0  s .
In other words, x is bounded and s-sparse2 (s  2). Furthermore, we assume  = 0.

(7)

The zero-noise assumption is for simplicity of the proofs. Our experiments will show that our models are robust to noisy cases.
Definition 1. Given D  RN×M with each of its column normalized, a weight matrix W is "good" if it belongs to

W(D) =

arg min

WRN ×M (W:,i)T D:,i=1,1iM

max (W:,i)T D:,j
i=j 1i,jM

Taking a W  W(D), we define the generalized mutual coherence:

,

(8)

µ~(D) = max (W:,i) D:,j .
i=j 1i,jM
2A signal is s-sparse if it has no more than s non-zero entries.

(9)

3

Under review as a conference paper at ICLR 2019

Theorem 1 (Recovery error upper bound). Take any x  X (B, s), any W  W(D), and any

sequence

 (k)



(0,

2 2µ~s-µ~+1

).

Using

them,

define

the

parameters

{W(k),

(k)}:

W(k) = (k)W, (k) = (k)µ~(D) sup

x(k)(x) - x 1 ,

xX (B,s)

(10)

while the sequence {x(k)(x)}k=1 is generated by (6) using the above parameters and x(0) = 0. (Each x(k)(x) depends only on (k-1), (k-2), . . . and defines (k).) Let Assumption 1 hold with any B > 0 and s < (1 + 1/µ~)/2. Then, we have

support(x(k)(x))  S,

k-1
x(k)(x) - x 2  sB exp - c() ,
 =0

k = 1, 2, . . .

(11)

where S is the support of x and c(k) = - log (2µ~s - µ~)(k) + |1 - (k)| is a positive constant.

In Theorem 1, Eqn. (10) defines the properties of "good" parameters:

· The weights W(k) can be separated as the product of a scalar (k) and a matrix W independent of layer index k.
· W has small coherence with D. · (k) is bounded in an interval. · (k)/(k) is proportional to the 1 error of the output of the kth layer.

The factor c(k) takes the maximum at (k) = 1. If (k)  1, the recovery error converges to zero in a linear rate (Chen et al., 2018):
x(k)(x) - x 2  sB exp - ck ,
where c = - log(2µ~s - µ~)  c(k). Although (k)  1 gives the optimal theoretical upper bound if there are infinitely many layers k = 0, 1, 2, · · · , it is not the optimal choice for finite k. Practically, there are finitely many layers and (k) obtained by learning is bounded in an interval.

2.2 RECOVERY ERROR LOWER BOUND

In this subsection, we introduce a lower bound of the recovery error of LISTA, which illustrates that the parameters analytically given by (10) are optimal in the convergence order (linear) too.

Assumption 2. The signal x is a random variable following the distribution PX . Let S =

support(x). PX satisfies: 2  | S |  s; S uniformly distributes on the whole index set; non-

zero

part

x
S

satisfies

the

uniform

distribution

with

bound

B:

|xi |



B, i



S.

Moreover, the

observation noise  = 0.

Definition 2. Given D  RN×M , s  2, ¯min > 0, we define a set that W(k) are chosen from:

W¯ (D, s, ¯min) = W  RN×M min I - (W:,S)T D:,S  ¯min,  S s.t. 2  | S |  s . (12)

Theorem 1 tells that an ideal weight W  W(D) satisfies I - WT D  0. But this cannot be met exactly in the overcomplete D case, i.e., N < M . Definition 2 addresses this point. Definition 3. Based on Definition 2, we define a set that  = {W(k), (k)}k=0 are chosen from:
T = {W(k), (k)}k=0 W(k)  W¯ (D, s, ¯min), support(x(k)(x))  S, x  X (B, s), k (13)
The conclusion (11) demonstrates that T is nonempty because "support(x(k)(x))  S" (no false positive) is satisfied as long as (k) large enough. Actually, T contains almost all "good" parameters because considerable false positives lead to large recovery errors. With T defined, we have:

4

Under review as a conference paper at ICLR 2019

Theorem 2 (Recovery error lower bound). Let the sequence {x(k)(x)}k=1 be generated by (6) with {W(k), (k)}k=0 and x(0) = 0. Under Assumption 2, for all parameters {W(k), (k)}k=0  T and any sufficient small > 0, we have

x(k)(x) - x 2  x 2 exp(-c¯k),

(14)

with probability at least (1 - s3/2 - 2), where c¯ = s log(3) - log(¯min).

This theorem illustrates that, with high probability, the convergence rate of LISTA cannot be faster than a linear rate. Thus, the parameters given in (10), that leads to the linear convergence if k is
bounded within an interval near 1, are optimal with respect to the order of convergence of LISTA.

2.3 ANALYTIC LISTA: LESS PARAMETERS TO LEARN

Following Theorems 1 and 2, we set W(k) = (k)W, where (k) is a scalar, and propose Tied

LISTA:

x(k+1) = (k) x(k) - (k)WT (Dx(k) - b) ,

(15)

where  = {(k)}k, {(k)}k, W are parameters to train. The matrix W is tied over all the layers. Further, we notice that the selection of W from W(D) depends on D only. Hence we propose the analytic LISTA (ALISTA) that decomposes tied-LISTA into two stages:

x(k+1) = (k) x(k) - (k)W~ T (Dx(k) - b) ,

(16)

where W~ is pre-computed by solving the following problem (Stage 1):

W~  arg min
WRN ×M

WT D

2 F

,

s.t. (W:,m)T D:,m = 1, m = 1, 2, · · · , M.

(17)

Then with W~ fixed, {(k), (k)}k in (16) are learned from end to end (Stage 2). (17) reformulates (8) to minimizing the Frobenius norm of WT D (a quadratic objective), over linear constraints. This is a standard convex quadratic program, which is easy to solve.

3 CONVOLUTIONAL ANALYTIC LISTA

We extend the analytic LISTA to the convolutional case in this section, starting from discussing the convolutional sparse coding (CSC). Many works studied CSC and proposed efficient algorithms for that (Bristow et al., 2013; Heide et al., 2015; Wohlberg, 2014; 2016; Papyan et al., 2017; GarciaCardona & Wohlberg, 2018; Wang et al., 2018; Liu et al., 2017; 2018). In CSC, the general linear transform is replaced by convolutions in order to learn spatially invariant features:

M
b = dm  xm + ,
m=1

(18)

where each dm is a dictionary kernel (or filter). {dm}mM=1 is the dictionary of filters, M denotes the number of filters. {xm }mM=1 is the set of coefficient maps that are assumed to have sparse structure, and  is the convolution operator. Now we consider 2D convolution. Let b  RN2 , dm  RD2 , xm  R(N+D-1)2 . Equation (18) is pointwisely defined as3:

D-1 D-1 M

b(i, j) =

dm(k, l)xm(i + k, j + l) + (i, j), 0  i, j  N - 1.

k=0 l=0 m=1

(19)

We vectorize b, dm, xm and let d = [d1, · · · , dM ]T and x = [x1, · · · , xM ]T . Then the above transform can be written as

M

b=

DcNonv,m(dm)xm +  = DNconv(d)x + ,

m=1

(20)

3Strictly speaking, (19) is the cross-correlation rather than convolution. However in TensorFlow, that operation is named as convolution, and we follow that convention to be consistent with the learning community.

5

Under review as a conference paper at ICLR 2019

where DNconv(d) = [DNconv,1(d1), · · · , DcNonv,M (dM )]  RN2×(N+D-1)2M is a matrix depending on the signal size N and the dictionary d.

From (18), the convolutional LISTA becomes a natural extension of the fully-connected LISTA (6):

M

x(mk+1) = (k) xm(k) - wm(k) 

dm¯  xm(¯k) - b , m = 1, 2, · · · , M,

m¯ =1

(21)

where {wm(k)}Mm=1 share the same sizes with {dm}mM=1. Let w(k) = [w1(k), · · · , wM(k)]T  RD2M . Parameters to train are  = {w(k), (k)}k.
Let WcNonv(w(k)) be the matrix induced by dictionary w(k) with the same dimensionality as DcNonv(d). Since convolution can be written as a matrix form (20), (21) is equivalent to

x(k+1) = (k) x(k) - (WcNonv(w(k)))T (DcNonv(d)x(k) - b) .

(22)

Then by just substituting D, W(k) with DcNonv(d), WcNonv(w(k)) respectively, Theorems 1 and 2 can be applied to the convolutional LISTA.
Proposition 1. Let D = DNconv(d) and W(k) = WcNonv(w(k)). With Assumption 1 and other settings the same with those in Theorem 1, (11) holds. With Assumption 2 and other settings the same with those in Theorem 2, (14) holds.

Similar to the fully connected case (16), based on the results in Proposition 1, we should set wm(k) = m(k)w~ m, m = 1, 2, · · · , M , where w~ = [w~1, · · · , w~ M ]T is chosen from

w~



W

N conv

=

arg min

wRD2 M

wm·dm=1, 1mM

WcNonv (w)

T DNconv(d)

2
.
F

(23)

However, (23) is not as efficient to solve as (17). To see that, matrices DNconv(d) and WcNonv(w) are both of size N 2 × (N + D - 1)2M , the coherence matrix WcNonv(w) T DNconv(d) is thus of size (N + D - 1)2M × (N + D - 1)2M . In the typical application setting of CSC, b is usually an image
rather than a small patch. For example, if the image size is 100 × 100, dictionary size is 7 × 7 × 64, N = 100, D = 7, M = 64, then (N + D - 1)2M × (N + D - 1)2M  5 × 1011.

3.1 CALCULATING CONVOLUTIONAL WEIGHTS ANALYTICALLY AND EFFICIENTLY

To overcome the computational challenge of solving (23), we exploit the following circular convolution as an efficient approximated way:

D-1 D-1 M

b(i, j) =

dm(k, l)xm (i+k)modN , (j+l)modN +(i, j), 0  i, j  N -1, (24)

k=0 l=0 m=1

where b  RN2 , dm  RD2 , xm  RN2 . The corresponding matrix form of (24) is:

M
b = DNcir,m(dm)xm +  = DcNir(d)x + ,
m=1
where DcNonv(d) : RN2M  RN2 is a matrix depending on the signal size N and the dictionary d. Then the coherence minimization with the circular convolution is given by

W

N cir

=

arg min

wRD2 M

wm·dm=1, 1mM

WcNir(w)

T DcNir(d)

2
.

F

(25)

The following theorem motivates us to use the solution to (25) to approximate that of (23). Theorem 3. The solution sets of (23) and (25) satisfy the following properties:

6

Under review as a conference paper at ICLR 2019

1.

W

N cir

=

W

2D-1 cir

,

N

 2D - 1.

2.

If

at

least

one

of

the

matrices

{Dc2iDr,-1 1, · · ·

, Dc2iDr,-M1}

is

non-singular,

W

2D-1 cir

involves

only a unique element. Furthermore,

lim
N 

W

N conv

=

W

2D-1 cir

.

(26)

The solution

set

W

N cir

is not

related

with the

image

size

N

as

long as

N



2D - 1,

thus one can

deal with a much smaller-size problem (let N = 2D - 1). Further, (26) indicates that as N gets

(much)

larger

than

D,

the

boundary

condition

becomes

less

important.

Thus,

one

can

use

W

2D-1 cir

to approximate WcNonv. In Appendix D, we introduce the algorithm details of solving (25).

Based on Proposition 1 and Theorem 3, we obtain the convolutional ALISTA:

M

xm(k+1) = (k) x(mk) - m(k)w~ m 

dm¯  x(m¯k) - b , m = 1, 2, · · · , M,

m¯ =1

(27)

where w~

=

[w~ 1, · · ·

, w~ M ]T



W

2D-1 cir

and



=

{{m(k)}m,k, {(k)}k} are

the parameters

to train.

(27) is a simplified form, compared to the empirically unfolded CSC model recently proposed in

(Sreter & Giryes, 2018)

4 ROBUST ALISTA TO MODEL PERTURBATION
Many applications, such as often found in surveillance video scenarios (Zhao et al., 2011; Han et al., 2013), can be formulated as sparse coding models whose dictionaries are subject to small dynamic perturbations (e.g, slowly varied over time). Specifically, the linear system model (1) may have uncertain D: D~ = D + D, where D is some small stochastic perturbation. Classical LISTA entangles the learning of all its parameters, and the trained model is tied to one static D. The important contribution of ALISTA is to decompose fitting W w.r.t. D, from adapting other parameters {(k), (k)}k to training data.
In this section, we develop a robust variant of ALISTA that is a fast regressor not only for a given D, but all its randomly perturbations D~ to some extent. Up to our best knowledge, this approach is new. Robust ALISTA can be sketched as the following empirical routine (at each iteration):
· Sample a perturbed dictionary D~ . Sample x and  to generate b w.r.t. D~ .
· Apply Stage 1 of ALISTA w.r.t. D~ and obtain W~ ; however, instead of an iterative minimization algorithm, we use a neural network that unfolds that algorithm to produce W~ .
· Apply Stage 2 of ALISTA w.r.t. W~ , D, x, and b to obtain {(k), (k)}k.
In Robust ALISTA above, D~ becomes a part of the data for training the neural network that generates W~ . This neural network is faster to apply than the minimization algorithm. One might attempt to use D~ in the last step, rather than D, but D~ makes training less stable, potentially because of larger weight variations between training iterations due to the random perturbations in D~ . We observe that using D stabilizes training better and empirically achieves a good prediction. More details of training Robust ALISTA are given in Appendix E.

5 NUMERICAL RESULTS
In this section, we conduct extensive experiments on both synthesized and real data to demonstrate: · We experimentally validate Theorems 1 and 2, and show that ALISTA is as effective as classical LISTA (Gregor & LeCun, 2010; Chen et al., 2018)but is much easier to train. · Similar conclusions can be drawn for convolutional analytic LISTA. · The robust analytic LISTA further shows remarkable robustness in sparse code prediction, given that D is randomly perturbed within some extent.
7

Under review as a conference paper at ICLR 2019

Notation For brevity, we let LISTA denote the vanilla LISTA model (4) in (Gregor & LeCun, 2010); LISTA-CPSS refers to the lately-proposed fast LISTA variant (Chen et al., 2018) with weight coupling and support selection; TiLISTA is the tied LISTA (15); and ALISTA is our proposed Analytic LISTA (16). If the model is for convolutional case, then we add "Conv" as the prefix for model name, such as "Conv ALISTA" that represents the convolutional analytic LISTA.
5.1 VALIDATION OF THEOREMS 1 AND 2 (ANALYTIC LISTA)
We follow the same N = 250, M = 500 setting as (Chen et al., 2018) by default. We sample the entries of D i.i.d. from the standard Gaussian distribution, Dij  N (0, 1/N ) and then normalize its columns to have the unit 2 norm. We fix a dictionary D in this section. To generate sparse vectors x, we decide each of its entry to be non-zero following the Bernoulli distribution with pb = 0.1. The values of the non-zero entries are sampled from the standard Gaussian distribution. A test set of 1000 samples generated in the above manner is fixed for all tests in our simulations. The analytic weight W that we use in the ALISTA is obtained by solving (17).
All networks used (vanilla LISTA, LISTA-CPSS, TiLISTA and ALISTA) have the same number of 16 layers. We also include two classical iterative solvers: ISTA and FISTA. We train the networks with four different levels of noises: SNR (Signal-to-Noise Ratio) = 20, 30, 40, and . While our theory mainly discussed the noise-free case (SNR = ), we hope to empirically study the algorithm performance under noise too. As shown in Figure 1, the x-axes denotes is the indices of layers for the networks, or the number of iterations for the iterative algorithms. The y-axes represent the NMSE (Normalized Mean Squared Error) in the decibel (dB) unit:
NMSEdB(x^, x) = 10 log10 E x^ - x 2/E x 2 , where x is the ground truth and x^ is the estimated one.

NMSE (dB)

-5

-15

-25

-35

-45

-55 ISTA FISTA
-65 LISTA

LISTA-CPSS TiLISTA ALISTA

-75

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

(a) Noiseless Case: SNR = 

NMSE (dB)

0 -5 -10 -15 -20 -25 -30 -35 -40 ISTA -45 FISTA -50 LISTA -55

LISTA-CPSS TiLISTA ALISTA

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

(b) Noisy Case: SNR = 40dB

NMSE (dB)

0 -5 -10 -15 -20 -25 -30 -35 ISTA -40 FISTA -45 LISTA -50

LISTA-CPSS TiLISTA ALISTA

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

(c) Noisy Case: SNR = 30dB

NMSE (dB)

0

-5

-10

-15

-20

-25

-30

ISTA FISTA

-35 LISTA

LISTA-CPSS TiLISTA ALISTA

-40

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

(d) Noisy Case: SNR = 20dB

Figure 1: Validation of Theorems 1 and 2: comparision among LISTA variants.

In Figure 1 (a) noise-less case, all four learned models apparently converge much faster than two iterative solvers (ISTA/FISTA curves almost overlap in this y-scale, at the small number of iterations). Among the four networks, classical-LISTA is inferior to the other three by an obvious margin. LISTA-CPSS, TiLISTA and ALISTA perform comparably: ALISTA is observed to eventually achieve the lowest NMSE. Figure 1(a) (a) also supports Theorem 2, that all networks have at most linear convergence, regardless of how freely their parameters can be end-to-end learned.
Figure 1 (b) - (d) further show that even in the presence of noise, ALISTA can empirically perform comparably with LISTA-CPSS and TiLISTA, and stay clearly better than LISTA and ISTA/FISTA. Always note that ALISTA the smallest amount of parameters to learn from the end-to-end training

8

Under review as a conference paper at ICLR 2019

(Stage 2). The above results endorse that: i) the optimal LISTA layer-wise weights could be structured as W(k) = (k)W; and ii) W could be analytically solved rather than learned from data, without incurring performance loss. We also observe the significant reduction of training time for ALISTA: while LISTA-CPSS of the same depth took 6.5 hours to train, ALISTA was trained with 1.5 hours, on the same hardware (one 1080 Ti on server).

100
10
1
0.1
0.01 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
(a) k

10 2 10 1 10 0 10 -1 10 -2 10 -3
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
(b) k/k

Figure 2: Validation of Theorem 1 (noiseless case): the parameters obtained by training satisfy (10).

We further supply Figures 2 and 3 to support Theorem 1 from differ-

1.2 1

ent perspectives. Figure 2 plots

0.8

the learned parameters {(k), (k)} in ALISTA (Stage 2), showing

0.6 0.4 0.2

that they satisfy the properties pro-

0

posed in Theorem 1: (k) bounded;

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

(k) and (k) is proportional to

supx x(k)(x) - x 1 ("supx " is Figure 3: Validation of Theorem 1 (noiseless case): Proportaken over the test set). Figure 3 re- tion of false positives vs true positives in xk(x).

ports the average magnitude of the

false positives and the true positives in xk(x) of ALISTA: the "true positives" curve draws the

values

of

E{

xk (x )
S

22/

xk (x )

22}

w.r.t.

k (the expectation is taken over the test set), while

"false positives" for E{ xSkc (x) 22/ xk(x) 22}. The results support the Theorem 1 conclusion

that support(xk(x))  S.

5.2 VALIDATION OF THEOREM 3 (CONVOLUTIONAL ANALYTIC LISTA)

For convolutional cases, we use real image data to verify Theorem 3. We train a convolutional dictionary d with D = 7, M = 64 on the BSD500 training set (400 images), using the Algorithm 1 in (Liu et al., 2018). We then use it for problems (23) and (25) and solve them with different N s.

In

Table

1,

we

take

wcNir



W Ncir,

w



W

50 cir

(consider

50

as

large

enough)

For

this

example,

W

N cir

has only one element. Table 1 shows that wcNir = w for N  13, i.e., the solution of the problem

(25) is independent of N if N  2D - 1, justifying the first conclusion in Theorem 3. In Table

2, we take wcNonv



W

N conv

and

w



wc1i3r,

where

W

N conv

also

has

only

one

element.

Table 2

shows wcNonv validating the

 w, second

i.e., the solution of the problem (23) converges to that of (25) as N increases, conclusion of Theorem 3. Visualized w  wc1i3r is displayed in Appendix F.

Besides validating Theorem 3, we also present a real image denoising experiment to verify the effectiveness of Conv ALISTA. The detailed settings and results are presented in Appendix G.

Table

1:

Validation

of

Conclusion

1

in

Theorem

3.

D

=

7.

wcNir



W

N cir

and

w



W

50 cir

.

N = 10 2.0 × 10-2

N = 11 9.3 × 10-3

wcNir - w 2/ w 2

N = 12

N = 13

3.9 × 10-3 1.4 × 10-12

N = 15 8.8 × 10-13

N = 20 5.9 × 10-13

9

Under review as a conference paper at ICLR 2019

NMSE (dB) NMSE (dB)

Table

2:

Validation

of

Conclusion

2

in

Theorem

3.

D

=

7.

wcNonv



W

N conv

and

w



wc1i3r.

wcNonv - w 2/ w 2 N = 3 N = 5 N = 10 N = 15 N = 20
0.1892 0.0850 0.0284 0.0161 0.0113

50 30 10 -10 -30 -50 -70
-0.005

50

30

10

-10

-30

-50

-70

0 0.005 0.01 0.015 0.02 0.025 -0.005 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035

(a) max = 0.02

(b) max = 0.03

Figure 4: Validation of Robust ALISTA.

5.3 VALIDATION OF ROBUST ALISTA
We empirically verify the effectiveness of Robust ALISTA, by sampling the dictionary perturbation D entry-wise i.i.d. from another Gaussian distribution N (0, m2 ax). We choose max = 0.02 and 0.03. Other simulation settings are by default the same as in Section 5.1. We then build the Robust ALISTA model, following the strategy in Section 4 and using a 4-layer encoder for approximating its second step (see Appendix E for details). Correspondingly, we compare Robust ALISTA with TiLISTA and ALISTA with specific data augmentation: we straightforwardly augment their training sets, by including all data generated with randomly perturbed D~ s when training Robust ALISTA. We also include the data-free FISTA algorithm into the comparison.
Figure 4 plots the results when the trained models are applied on the testing data, generated with the same dictionary and perturbed by N (0, t). We vary t from zero to slightly above max. Not surprisingly, FISTA is unaffected, while the other three data-driven models all slight degrade as t increases. Compared to the augmented TiLISTA and ALISTA whose performance are both inferior to FISTA, the proposed Robust ALISTA appears to be much more favorable in improving robustness to model perturbations. In both max cases, it consistently achieves much lower NMSE than FISTA, even when t has slightly surpassed max. Although the NMSE of ALISTA may decrease faster if t continues growing larger, such decrease could be alleviated by improving max in training, e.g., by comparing max = 0.02 and 0.03. Robust ALISTA demonstrates remarkable robustness and maintains the best NMSE performance, within at least the [0, max] range.
6 CONCLUSIONS AND FUTURE WORK
Based on the recent theoretical advances of LISTA, we have made further steps to reduce the training complexity and improve the robustness of LISTA. Specifically, we no longer train any matrix for LISTA but directly use the solution to an analytic minimization problem to solve for its layer-wise weights. Therefore, only two scalar sequences (stepsizes and thresholds) still need to be trained. Excluding the matrix from training is backed by our theoretical upper and lower bounds. The resulting method, Analytic LISTA or ALISTA, is not only faster to train but performs as well as the state-of-the-art variant of LISTA by (Chen et al., 2018). This discovery motivates us to further replace the minimization algorithm by its unfolding neural network, and train this neural network to more quickly produce the weight matrix. The resulting algorithm is used to handle perturbations in the model dictionary -- we only train once for a dictionary with all its small perturbations. Our future work will investigate the theoretical sensitivity of ALISTA (and its convolutional version) to noisy measurements.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3):265 ­ 274, 2009.
Mark Borgerding, Philip Schniter, and Sundeep Rangan. AMP-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing, 65(16):4293­4308, 2017.
Hilton Bristow, Anders P. Eriksson, and Simon Lucey. Fast convolutional sparse coding. 2013 IEEE Conference on Computer Vision and Pattern Recognition, pp. 391­398, 2013.
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of unfolded ista and its practical weights and thresholds. arXiv preprint arXiv:1808.10038, 2018.
Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over learned dictionaries. IEEE Transactions on Image processing, 15(12):3736­3745, 2006.
Cristina Garcia-Cardona and Brendt Wohlberg. Convolutional dictionary learning: A comparative review and new algorithms. IEEE Transactions on Computational Imaging, 2018.
Raja Giryes, Yonina C Eldar, Alex Bronstein, and Guillermo Sapiro. Tradeoffs between convergence speed and reconstruction accuracy in inverse problems. IEEE Transactions on Signal Processing, 2018.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 399­406. Omnipress, 2010.
Sheng Han, Ruiqing Fu, Suzhen Wang, and Xinyu Wu. Online adaptive dictionary learning and weighted sparse coding for abnormality detection. In Image Processing (ICIP), 2013 20th IEEE International Conference on, pp. 151­155. IEEE, 2013.
Felix Heide, Wolfgang Heidrich, and Gordon Wetzstein. Fast and flexible convolutional sparse coding. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5135­ 5143, 2015.
Daisuke Ito, Satoshi Takabe, and Tadashi Wadayama. Trainable ista for sparse signal recovery. 2018 IEEE International Conference on Communications Workshops (ICC Workshops), pp. 1­6, 2018.
Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, and Wotao Yin. Online convolutional dictionary learning. 2017 IEEE International Conference on Image Processing (ICIP), pp. 1707­1711, 2017.
Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, and Wotao Yin. First-and second-order methods for online convolutional dictionary learning. SIAM Journal on Imaging Sciences, 11(2):1589­ 1628, 2018.
Christopher A Metzler, Ali Mousavi, and Richard G Baraniuk. Learned D-AMP: Principled neural network based compressive image recovery. In Advances in Neural Information Processing Systems, pp. 1770­1781, 2017.
Thomas Moreau and Joan Bruna. Understanding trainable sparse coding with matrix factorization. In ICLR, 2017.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via convolutional sparse coding. arXiv preprint arXiv:1607.08194, 2016.
Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Convolutional dictionary learning via local processing. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5306­5314, 2017.
R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science & Business Media, 2009.
11

Under review as a conference paper at ICLR 2019
Pablo Sprechmann, Alexander M Bronstein, and Guillermo Sapiro. Learning efficient sparse and low rank models. IEEE transactions on pattern analysis and machine intelligence, 37(9):1821­ 1833, 2015.
Hillel Sreter and Raja Giryes. Learned convolutional sparse coding. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2191­2195. IEEE, 2018.
Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multi-layer convolutional sparse modeling: Pursuit and dictionary learning. arXiv preprint arXiv:1708.08705, 2017.
Bahareh Tolooshams, Sourav Dey, and Demba Ba. Scalable convolutional dictionary learning with constrained recurrent sparse auto-encoders. arXiv preprint arXiv:1807.04734, 2018.
Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Scalable online convolutional sparse coding. IEEE Transactions on Image Processing, 2018.
Zhangyang Wang, Qing Ling, and Thomas Huang. Learning deep l0 encoders. In AAAI Conference on Artificial Intelligence, pp. 2194­2200, 2016.
Brendt Wohlberg. Efficient convolutional sparse coding. 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7173­7177, 2014.
Brendt Wohlberg. Efficient algorithms for convolutional sparse representations. IEEE Transactions on Image Processing, 25:301­315, 2016.
Brendt Wohlberg. Convolutional sparse representations with gradient penalties. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6528­6532. IEEE, 2018.
Bo Xin, Yizhou Wang, Wen Gao, David Wipf, and Baoyuan Wang. Maximal sparsity with deep networks? In Advances in Neural Information Processing Systems, pp. 4340­4348, 2016.
Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing. In IEEE CVPR, 2018.
Bin Zhao, Li Fei-Fei, and Eric P Xing. Online detection of unusual events in videos via dynamic sparse coding. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 3313­3320. IEEE, 2011.
Joey Tianyi Zhou, Kai Di, Jiawei Du, Xi Peng, Hao Yang, Sinno Jialin Pan, Ivor W Tsang, Yong Liu, Zheng Qin, and Rick Siow Mong Goh. SC2Net: Sparse LSTMs for sparse coding. In AAAI Conference on Artificial Intelligence, 2018.
12

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 1

In this proof, we use the notion x(k) to replace x(k)(x) for simplicity. We fix D in the proof, µ~(D) can be simply written as µ~.

Before proving Theorem 1, we present and prove a lemma. Lemma 1. With all the settings the same with those in Theorem 1, we have

support(x(k))  S, k. In another word, there are no false positives in x(k): x(ik) = 0, i / S, k.

(28)

Proof. Take arbitrary x  X (B, s). We prove Lemma 1 by induction. As k = 0, (28) is satisfied since x(0) = 0. Fixing k, and assuming support(x(k))  S, we have

xi(k+1) =(k) x(ik) - (k)(W:,i)T (Dx(k) - b)

=(k) - (k) (W:,i)T D:,j (xj(k) - xj) , i / S .
jS

By (10), the thresholds are taken as (k) = µ~(k) supx { x(k) - x 1}. Also, since W  W(D), we have |(W:,i)T D:,j|  µ~ for all j = i. Thus, for all i / S,

(k) µ~(k) x(k) - x 1 =

µ~(k) x(jk) - xj =

µ~(k) xj(k) - xj

j support(x(k) )

jS

 - (k) (W:,i)T D:,j (xj(k) - xj) ,
jS

which implies xi(k+1) = 0, i / S by the definition of (k) , i.e., support(x(k+1))  S

By induction, (28) is proved.

With Lemma 1, we are able to prove Theorem 1 now.

Proof of Theorem 1. Take arbitrary x  X (B, s). For all i  S, by (28), we obtain

x(ik+1) = (k) x(ik) - (k)(W:,i)T D:,S(xS(k) - xS)  x(ik) - (k)(W:,i)T D:,S(x(Sk) - xS) - (k) 1(xi(k+1)),
where  1(x) is the sub-gradient of |x|, x  R:

{sign(x)}  1(x) = [-1, 1]

if x = 0, if x = 0.

The choice of W (8) gives (W:,i)T D:,i = 1. Thus,

xi(k) - (k)(W:,i)T D:,S(xS(k) - xS)

=x(ik) - (k)

(W:,i)T D:,j (xj(k) - xj) - (k)(xi(k) - xi )

jS,j=i

=xi - (k)

(W:,i)T D:,j (xj(k) - xj ) + (1 - (k))(xi(k) - xi).

jS,j=i

Then the following inclusion formula holds for all i  S,

xi(k+1) - xi  -(k)

(W:,i)T D:,j (x(jk) - xj ) - (k) 1(x(ik+1)) + (1 - (k))(xi(k) - xi ).

jS,j=i

13

Under review as a conference paper at ICLR 2019

By the definition of  1, every element in  1(x), x  R has a magnitude less than or equal to 1. Thus, for all i  S,

|xi(k+1) - xi| 

(k) (W:,i)T D:,j |x(jk) - xj| + (k) + |1 - (k)| xi(k) - xi

jS,j=i

µ~(k)

|x(jk) - xj | + (k) + |1 - (k)| x(ik) - xi .

jS,j=i

Equation (28) implies

x(k) - x 1 =

x(k) - x
SS

1 for all k. Then

x(k+1) - x 1 =

|x(ik+1) - xi |

iS

 µ~(k)

|x(jk) - xj | + (k) + |1 - (k)||xi(k) - xi |

iS jS,j=i

=µ~(k)(|S| - 1) |x(ik) - xi | + (k)|S| + |1 - (k)| x(k) - x 1
iS

=µ~(k)(|S| - 1) x(k) - x 1 + (k)|S| + |1 - (k)| x(k) - x 1.

Taking supremum of the above inequality over x  X (B, s), by |S|  s, sup{ x(k+1) - x 1}  µ~(k)(s - 1) + |1 - (k)| sup{ x(k) - x 1} + (k)s.
x x
By the value of (k) given in (10), we have sup{ x(k+1) - x 1}  (k)(2µ~s - µ~) + |1 - (k)| sup{ x(k) - x 1}.
x x
Let c() = - log (2µ~s - µ~)() + |1 - ()| . Then, by induction,

kk

sup{ x(k+1) - x 1}  exp - c() sup{ x(0) - x 1}  exp - c() sB.

x

 =0

x

 =0

Since x 2  x 1 for any x  Rn, we can get the upper bound for 2 norm:

k

sup{ x(k+1) - x 2}  sup{ x(k+1) - x 1}  sB exp - c() .

x x

 =0

The assumption s < (1 + 1/µ~)/2 gives 2µ~s - µ~ < 1. If 0 < (k)  1, we have c(k) > 0. If 1 < (k) < 2/(1 + 2µ~s - µ~), we have

(2µ~s - µ~)(k) + |1 - (k)| = (2µ~s - µ~)(k) + (k) - 1 < 1,

which implies c(k) > 0. Theorem 1 is proved.

B PROOF OF THEOREM 2

Proof of Theorem 2. We fix D and sample a x  PX .

If we can prove

P (14) does not hold support(x) = S  |S| + |S|,

then the lower bound (14) in Theorem 2 is proved by

P (14) holds =

P (14) holds support(x) = S P support(x) = S

S,2|S|s

(1 - s3/2 - 2)

P support(x) = S

2|S|s

=1 - s3/2 - 2.

(29)

14

Under review as a conference paper at ICLR 2019

Now we fix k and prove inequality (29) by three steps:

Step 1: If (14) does not hold, then what condition x should satisfy?

Fixing k, we define a set X (k)( ), which involves all the x that does not satisfy (14):

X (k)( ) = {(14) does not hold} = x x(k)(x) - x 2 <

x 2

¯min 3s

k

.

Let S = support(x). For x  X (k)( ), we consider two cases:

1. |xi| > 2. |xi| 

x 2(¯min/3s)k, i  S. x 2(¯min/3s)k, for some i  S.

If case 1 holds, we obtain that the support of x(k) is exactly the same with that of x:

support(x(k)(x)) = S . Then the relationship between x(k) and x(k-1) can be reduced to an affine transform:

x(k)
S

=(k)

x(k-1)
S

-

(W:(,kS-1))T

(Dx(k-1)

-

b)

=x(k-1)
S

-

(W:(,kS-1))T

D:,S

(x(k-1)
S

-

xS)

-

(k-1)sign(x(k)).
S

Subtracting x from the two sides of (30), we obtain

(30)

I - (W:(,kS-1))T D:,S

(x(k-1)
S

-

xS)

-

(k-1)sign(x(Sk))

=
2

x(k)
S

-

xS

2=

x(k) - x 2,

where the last equality is due to Definition 3. Thus, for all x  X (k)( ), if case 1 holds, we have

I - (W:(,kS-1))T D:,S

(x(k-1)
S

-

xS)

-

(k-1)sign(xS(k))


2

x 2(¯min/3s)k.

Multiplying both sides of (31) by (I - (W:(,kS-1))T D:,S)-1, we have

x(k-1)
S

-

xS

-

(k-1)(I

-

(W:(,kS-1))T

D:,S

)-1sign(x(k))
S

2

 (I - (W:(,kS-1))T D:,S)-1 2 · x 2(¯min/3s)k  x 2(¯min)k-13-ks,

where the last inequality is due to (12). Let x~(k-1) denote the bias of x(k-1):

(31)

x~(k-1) (k-1)(I - (W:(,kS-1))T D:,S)-1sign(x(Sk)), then we get a condition that x satisfies if case 1 holds:

X (k-1)( ) =

x

x(k-1)(x)
S

-

xS

-

x~(k-1)(x)

2

If case 2 holds, x belongs to the following set:

x 2(¯min)k-13-ks .

X~ (k)( ) = x |xi |  x 2 ¯min/3s k, for some i  S .

Then for any x  X (k)( ), either x  X (k-1)( ) or x  X~ (k)( ) holds. In another word,

X (k)(

)



X~

(k)
(

)  X (k-1)(

).

Step 2: By imitating the construction of X (k)( ), we construct
X (k-2)( ), X (k-3)( ), · · · .
Similar to Step 1, we divide X (k-1)( ) into two sets: X~ (k-1)( ) and X (k-2)( ), then we divide X (k-2)( ) into X~ (k-2)( ) and X (k-3)( ). Repeating the process, until dividing X (1)( ) into X~ (1)( ) and X (0)( ).

15

Under review as a conference paper at ICLR 2019

By induction, we have

X (k)( )  X~ (k)( )  X~ (k-1)( )  X~ (k-2)( )  · · ·  X~ (1)( )  X (0)( ),

where the sets are defined as follows for all j = 0, 1, 2, · · · , k:

X~

(k-j)
(

)

=

x |xi + x~i(k-j)(x)| <

x 2 ¯min k-j 3-ks, for some i  S. ,

X (k-j)( ) = x

x(k-j)(x)
S

-

xS

-

x~(k-j)(x)

2

x 2 ¯min k-j 3-ks

and the bias is defined as following for all j = 0, 1, 2, · · · , k:

j
x~(k-j)(x) =
t=1

I-

W:(,kS-j+t-1)

T
D:,S

-t
(k-j+t-1)sign x(k-j+t)(x) .
S

(32) (33) (34)
(35)

Step 3: Estimating the probabilities of all the sets in (32). By (32), we have
P x  X (k)( ) support(x) = S
k-1
 P x  X~ (k-j)( ) support(x) = S + P x  X (0)( ) support(x) = S .
j=1
Now we have to prove that each of the above terms is small, then P (x  X (k)( )|support(x) = S) is small and (29) will be proved. Define a set of n-dimensional sign numbers

Si(n) = (s1, s2, · · · , sn) si  {0, -1, 1}, i = 1, · · · , n .

Since sign

x(k-j+t)
S

 Si(|S|) for all t = 1, 2, · · · , j, {sign(xS(k-j+t))}jt=1 has finitely possible

values.

Let

sign(x(k-j+t))
S

=

s(t)

for

t

=

1, 2, · · ·

, j.

Then

x~i(k-j)(x)

is

independent

of

x

and

can be written as x~i(k-j)(s(1), s(2), · · · , s(j)). Thus, we have

P (x



X~

(k-j

)
(

)|support(x)

=

S)

= ···
iS s(1)Si(|S|) s(2)Si(|S|) s(j)Si(|S|)
P |xi + x~(ik-j)(x)| < x 2(¯min)k-j 3-ks, sign(xS(k)) = s(1), · · · , sign(xS(k-j+1)) = s(j) support(x) = S

 ···
iS s(1)Si(|S|) s(2)Si(|S|) s(j)Si(|S|)
P |xi + x~(ik-j)(s(1), s(2), · · · , s(j))| <

|S|B(¯min)k-j 3-ks support(x) = S

 ···
iS s(1)Si(|S|) s(2)Si(|S|) s(j)Si(|S|)

|S|B(¯min)k-j 3-ks B

=|S|3j|S|( |S| (¯min)k-j 3-ks  |S|3/2(¯min)k-j 3(j-k)|S|

where the second inequality comes from the uniform distribution of xS (Assumption 2), the last inequality comes from |S|  s.

16

Under review as a conference paper at ICLR 2019

The last term, due to the uniform distribution of xS and x(0) = 0, can be bounded by P (x  X (0)( )|support(x) = S)
=P x + x~(0)(x) 2  x 23-ks support(x) = S

= ···

s(1)Si(|S|) s(2)Si(|S|) s(k)Si(|S|)

P

x + x~(0)(x) 2 

x

2

3-ks

,

sign(x(1))
S

=

s(1),

··

·

,

sign(x(k))
S

=

s(k)

support(x) = S

3k|S| ( 3-ks)|S|  |S|.

Then we obtain P (x  X (k)( )|support(x) = S)

k-1

k

 |S|3/2(¯min)k-j 3(j-k)|S| + |S| = |S|3/2(¯min)j 3-j|S| + |S|

j=0

j=1

=

|S|3/2

1

¯min3-|S| - ¯min3-|S|

Then (29) is proved.

1 - (¯min3-|S|)k

+ |S|  |S|3/2 + |S|.

C PROOF OF THEOREM 3

There are two conclusions in Theorem 3. We prove the two conclusions in the following two subsections respectively.

C.1 PROOF OF CONCLUSION 1.

Before proving Conclusion 1, we analyze the operator DcNir in detail.

The circular convolution (24) is equivalent with:

N -1 N -1 M

b(i, j) =

DcNir(i, j; k, l, m)xm(k, l),

k=0 l=0 m=1

0  i, j  N - 1,

where the circulant matrix is element-wise defined as:

DcNir(i, j; k, l, m) =

dm (k - i)modN , (l - j)modN , 0,

0  (k - i)modN , (l - j)modN  D - 1 others

Similarly, the corresponding circulant matrix WcNir(i, j; k, l, m) of dictionary w is:

(36)

WcNir(i, j; k, l, m) =

wm (k - i)modN , (l - j)modN , 0,

0  (k - i)modN , (l - j)modN  D - 1 others

(37)

If we vectorize b and x = [x1, · · · , xM ]T , operator DNcir is a matrix, where (i, j) is its row index and (k, l, m) is its column index.

Define a function measuring the difference between i and k:

I(i, k) (k - i)modN , 0  i, k  N - 1.
The coherence between DcNir(i, j; k, l, m) and WcNir(i, j; k, l, m): Bcoh = (DcNir)T WcNir is elementwise defined by:

N -1 N -1

Bcoh(k1, l1, m1; k2, l2, m2) =

DcNir(i, j; k1, l1, m1)WcNir(i, j; k2, l2, m2)

i=0 j=0

= dm1 I(i, k1), I(j, l1) wm2 I(i, k2), I(j, l2) .
iI(k1,k2) jJ (l1,l2)

17

Under review as a conference paper at ICLR 2019

where I(k1, k2) = {i|0  i  N - 1, 0  I(i, k1)  D - 1, 0  I(i, k2)  D - 1}, J (l1, l2) = {j|0  j  N - 1, 0  I(j, l1)  D - 1, 0  I(j, l2)  D - 1}.
Lemma 2. Given N  2D - 1, it holds that:
(a) I(k1, k2) =  if and only if " 0  (k1 -k2)modN  D -1" or " 0 < (k2 -k1)modN  D -1" holds.
(b) J (l1, l2) =  if and only if " 0  (l1 - l2)modN  D - 1" or " 0 < (l2 - l1)modN  D - 1" holds.

Proof. Now we prove Conclusion (a). Firstly, we prove "if." If 0  (k1 - k2)modN  D - 1 and N  2D - 1, we have

I(k1, k2) = (k1 - )modN   Z, (k1 - k2)modN    D - 1 = .

(38)

If 0 < (k2 - k1)modN  D - 1 and N  2D - 1, we have

I(k1, k2) = (k2 - )modN   Z, (k2 - k1)modN    D - 1 = .

(39)

Secondly, we prove "only if." If I(k1, k2) = , we can select an i  I(k1, k2). Let r1 = (k1 - i)modN and r2 = (k2 - i)modN . By the definition of I(k1, k2), we have 0  r1, r2  D - 1. Two cases should be considered here. Case 1: r1  r2. Since 0  r1 - r2  D - 1  N - 1, it holds that r1 - r2 = (r1 - r2)modN . Thus,

r1 - r2 = (r1 - r2)modN = (k1 - i)modN - (k2 - i)modN modN = (k1 - i) - (k2 - i) modN =(k1 - k2)modN .

The equality "0  r1 - r2  D - 1" leads to the conclusion "0  (k1 - k2)modN  D - 1". In case 2 where r1 < r2, we can obtain 0 < (k2 - k1)modN  D - 1 with the similar arguments.

Conclusion (b) can be proved by the same argument with the proof of (a). Lemma 2 is proved.

Now we fix k1, l1 and consider what values of k2, l2 give I(k1, k2) =  and J (l1, l2) = . Define four index sets given 0  k1, l1  N - 1:
K(k1) ={k|0  (k1 - k)modN  D - 1} K¯(k1) ={k|0 < (k - k1)modN  D - 1}
L(l1) ={l|0  (l1 - l)modN  D - 1} L¯(l1) ={l|0 < (l - l1)modN  D - 1} Lemma 3. If N  2D - 1, we have:
(a) The cardinality of K(k1), K¯(k1): | K(k1)| = D, | K¯(k1)| = D - 1. (b) K(k1)  K¯(k1) = . (c) The cardinality of L(l1), L¯(l1): | L(l1)| = D, | L¯(l1)| = D - 1. (d) L(l1)  L¯(l1) = .

Proof. Now we prove Conclusion (a). The set K(k1) can be equivalently written as

K(k1) = {(k1 - rk)modN |rk = 0, 1, · · · , D - 1}

(40)

Let k(rk) = (k1 - rk)modN . We want to show that k(rk1) = k(rk2) as long as rk1 = rk2. Without loss of generality, we assume 0  rk1 < rk2  D - 1. By the definition of modulo operation, There exist two integers q, q such that

k(rk1) = qN + k1 - rk1, k(rk2) = q N + k1 - rk2.

18

Under review as a conference paper at ICLR 2019

rSDk1up-=po1(sqe k-N(rqk1-))N1=,, iw.keh,(irNck2h)d.cioTvniadtkreaisndgrick2tths-ewdrikt1ihf.fe"HrNeonwcdeeivvbieder,itnw0geernk2r-tk1he<rk1a.br"ko2TvehutDws,oi-tehq1ouliadmtsipotlnhiesas,t wk1(erko1)brkt2=ai-nkrr(kr1k2k2-). Then we have | K(k1)| = D.

In the same way, we have

K¯(k1) = {(k1 + rk)modN |rk = 1, 2, · · · , D - 1}

(41)

and | K¯(k1)| = D - 1. Conclusion (a) is proved.

Now we prove Conclusion (b). Suppose K(k1)  K¯(k1) = . Pick a k2  K(k1)  K¯(k1). Let r3 = (k1 - k2)modN and r4 = (k2 - k1)modN . Then we have 0  r3  D - 1 and 0 < r4  D - 1.
By the definition of modulo operation, There exist two integers q, q such that

k1 - k2 = qN + r3, k2 - k1 = q N + r4
which imply r3 + r4 + (q + q )N = 0.
However, 0 < r3 +r4  2D -2 contradicts with "q  Z, q  Z, N  Z, N  2D -1." Conclusion (b) is proved.

Conclusions (c) and (d) are actually the same with Conclusions (a) and (b) respectively. Thus, it holds that

L(l1) ={(l1 - rl)modN |rl = 0, 1, · · · , D - 1} L¯(l1) ={(l1 + rl)modN |rl = 1, 2, · · · , D - 1}

(42) (43)

and | L(l1)| = D, | L¯(l1)| = D - 1. Lemma 3 is proved.

With the preparations, we can prove Conclusion 1 of Theorem 3 now.

Proof of Theorem 3, Conclusion 1. Firstly we fix k1  {0, 1, · · · , N - 1} and consider k2  K(k1). Let rk = (k1 - k2)modN . Then equation (38) implies that, for any i  I(k1, k2), there exists a  (rk    D - 1) such that

I(i, k1) = k1 - (k1 - )modN modN = ()modN = , I(i, k2) = k2 - (k1 - )modN modN = ( - rk)modN =  - rk.

(44)

Now we consider another case for k2: k2  K¯(k1), rk = (k2 - k1)modN . Equation (39) implies that, for any i  I(k1, k2), there exists a  (rk    D - 1) such that

I(i, k1) = k1 - (k2 - )modN modN = ( - rk)modN =  - rk, I(i, k2) = k2 - (k2 - )modN modN = ()modN = .

(45)

Similarly, for any l1  {0, 1, · · · , N - 1} and l2  L(l1), we denote rl = (l1 - l2)modN . For any j  J (l1, l2), there exists a  (rl    D - 1) such that

I(j, l1) = l1 - (l1 - )modN modN = ()modN = , I(j, l2) = l2 - (l1 - )modN modN = ( - rl)modN =  - rl.

(46)

Another case for l2: l2  L¯(l1), rl = (l2 - l1)modN . For any j  J (l1, l2), there exists a  (rl    D - 1) such that

I(j, l1) = l1 - (l2 - )modN modN = ( - rl)modN =  - rl, I(j, l2) = l2 - (l2 - )modN modN = ()modN = .

(47)

Now let us consider the following function. By results in Lemmas 2 and 3, we have

N -1 N -1

2

f (k1, l1, m1, m2) =

Bcoh(k1, l1, m1; k2, l2, m2)

k2=0 l2=0

=f1 + f2 + f3 + f4,

19

Under review as a conference paper at ICLR 2019

where

f1 =
k2K(k1) l2L(l1)
f2 =
k2K¯(k1) l2L(l1)
f3 =
k2K(k1) l2L¯(l1)
f4 =
k2K¯(k1) l2L¯(l1)

2
Bcoh(k1, l1, m1; k2, l2, m2)
2
Bcoh(k1, l1, m1; k2, l2, m2)
2
Bcoh(k1, l1, m1; k2, l2, m2)
2
Bcoh(k1, l1, m1; k2, l2, m2) .

Combining equations (40), (42), (44) and (46), we obtain

D-1 D-1 D-1 D-1
f1 =
rk=0 rl=0 k=rk l=rl

2
dm1 (k, l)wm2 (k - rk, l - rl) .

Combining (41), (42), (45) and (46), we obtain

D-1 D-1 D-1 D-1
f2 =
rk=1 rl=0 k=rk l=rl

2
dm1 (k - rk, l)wm2 (k, l - rl) .

Combining (40), (43), (44) and (47), we obtain

D-1 D-1 D-1 D-1
f3 =
rk=0 rl=1 k=rk l=rl

2
dm1 (k, l - rl)wm2 (k - rk, l) .

Combining (41), (43), (45) and (47), we obtain

D-1 D-1 D-1 D-1
f4 =
rk=1 rl=1 k=rk l=rl

2
dm1 (k - rk, l - rl)wm2 (k, l) .

By the above explicit formulas of fi, 1  i  4, we have f1, f2, f3, f4 are all independent of k1, l1 and N . They are only related with m1, m2 for fixed d and m. Thus, we are able to denote f (k1, l1, m1, m2) as f (m1, m2) for simplicity. Consequently,

1 N2

(DNcir)T WcNir

2 F

1 =N2

N -1 N -1 N -1 N -1

M

M

2
Bcoh(k1, l1, m1; k2, l2, m2)

k1=0 l1=0 k2=0 l2=0 m1=1 m2=1

1 N -1 N -1 M M

=N2

f (k1, l1, m1, m2)

k1=0 l1=0 m1=1 m2=1

1 N -1 N -1 M M

=N2

f (m1, m2)

k1=0 l1=0 m1=1 m2=1

1 =N2

· N2

·

M

MM
f (m1, m2) =

M
f (m1, m2)

m1=1 m2=1

m1=1 m2=1

Thus,

1 N2

(DcNir)T WcNir

2 F

is

dependent

of

N:

1 N2

(DcNir)T WcNir

2 F

=

1 (2D - 1)2

(Dc2iDr -1)T Wc2iDr -1

2 F

,

N  2D - 1,

(48)

which

implies

W

N cir

=

W 2ciDr -1, N



2D

-

1.

20

Under review as a conference paper at ICLR 2019

C.2 PROOF OF CONCLUSION 2.
Before proving Conclusion 2, let us analyze the relationship between DcNonv and DcNir+D-1. Similar to Dcir, we use (i, j) as the row index and (k, l, m) as the column index of Dconv. For 0  i, j  N - 1, 1  m  M ,

DNcir+D-1(i, j; k, l, m) = DcNonv(i, j; k, l, m) =

dm(k - i, l - j), 0,

0  k - i, l - j  D - 1 k, l taken as others

Matrix DNcir+D-1 is of dimension (N + D - 1)2 × (N + D - 1)2M , where 0  i, j  N + D - 2; matrix DNconv is of dimension (N )2 × (N + D - 1)2M , where 0  i, j  N - 1. Thus, DNconv is a block in DcNir+D-1, i.e.,

DcNir+D-1 =

DcNonv DN

.

The matrix ND is of dimension ((N + D - 1)2 - N 2) × (N + D - 1)2M :

DN = DcNir+D-1(i, j; :, :, :) , (i, j)  I

where

I = I1  I2  I3 I1 ={(i, j)|N  i  N + D - 2, 0  j  N - 1} I2 ={(i, j)|0  i  N - 1, N  j  N + D - 2} I3 ={(i, j)|N  i  N + D - 2, N  j  N + D - 2}.

Similarly,

WcNir+D-1 =

WcNonv WN

,

WN = WcNir+D-1(i, j; :, :, :) ,

(i, j)  I .

Then,

(DNcir+D-1)T WcNir+D-1 = (DcNonv)T WcNonv + (DN )T WN .

Lemma 4. For any (i, j)  I, one has

DcNir+D-1(i, j; :, :, :)

2 2

=

d

2 2

,

WcNir+D-1(i, j; :, :, :)

2 2

=

w

2 2

.

(49)
(50) (51)

Proof. Equation (36) implies that, for (i, j)  I1, 1  m  M ,

dm(k - i, l - j),

i  k  N + D - 2, j  l  j + D - 1

DNcir+D-1(i, j; k, l, m) = dm(k - i + N + D - 1, l - j), 0  k  i - N, j  l  j + D - 1

0, k, l taken as others

Thus, for any (i, j)  I1,

N +D-2 N +D-2 M

DNcir+D-1(i, j; :, :, :)

2 2

=

DcNir+D-1(i, j; k, l, m) 2 =

d

2 2

k=0

l=0 m=1

Similarly,

DNcir+D-1(i, j; :, :, :)

2 2

=

d

2 2

,

(i, j)  I2  I3 .

Equation (50) is proved. With the same argument, equation (51) is also proved.

Lemma 5. If N  2D - 1, we have

(ND)T NW

2 F



2N (D - 1) + (D - 1)2 (2D - 1)2

d

2 2

w 22.

(52)

21

Under review as a conference paper at ICLR 2019

Proof. For simplicity, we denote two row vectors:

Then,

di,j wi,j

DNcir+D-1(i, j; :, :, :)  R1×(N+D-1)2M WcNir+D-1(i, j; :, :, :)  R1×(N+D-1)2M

(ND)T NW

2 F

=

diT,j wi,j
(i,j)I

2
=
F (i1,j1)I (i2,j2)I

dTi1,j1 wi1,j1 , dTi2,j2 wi2,j2

,
F

where

dTi1,j1 wi1,j1 , dTi2,j2 wi2,j2

= trace
F

wiT1,j1 di1,j1 diT2,j2 wi2,j2

= (di1,j1 dTi2,j2 ) · (wi1,j1 wiT2,j2 ).

Since

N -1 N -1 M

di1,j1 diT2,j2 =

= DcNir+D-1(i1, j1; k, l, m)DcNir+D-1(i2, j2; k, l, m),

k=0 l=0 m=1

with the same argument in Lemma 2, we have: di1,j1 dTi2,j2 = 0 implies

i2  I {i|0  (i1 - i)mod(N+D-1)  D - 1 or 0  (i - i1)mod(N+D-1)  D - 1}

j2  J  {j|0  (j1 - j)mod(N+D-1)  D - 1 or 0  (j - j1)mod(N+D-1)  D - 1}

Then

(DN )T NW

2 F

=

(di1,j1 diT2,j2 ) · (wi1,j1 wiT2,j2 )

(i1,j1)I i2I j2J 



d

2 2

w

2 2

(i1,j1)I i2I j2J 

=| I | · | I | · | J  | ·

d

2 2

w

2 2

=

2N (D - 1) + (D - 1)2

(2D - 1)2

d

2 2

w

2 2

,

where the inequality in the second line follows from (50) and (51). Inequality (52) is proved.

With these preparations, we can prove Theorem 3, Conclusion 2 now.

Proof of Theorem 3, Conclusion 2. Define set

Wnormal = w  RD2M wm · dm = 1, m = 1, · · · , M .

Since d  Wnormal, the set is nonempty:

W normal = .

Define functions FcNonv : RD2M  R, FcNir : RD2M  R.

FcNonv (w)

=

N

1 +D

-

1

DNconv(d) T WcNonv(w) F + Wnormal (w)

FcNir(w)

=

1 N

(DNcir(d))T WcNir(w) F + Wnormal (w)

By

the

definitions

of

W Nconv,

W

N cir

,

we

have

W

N conv

=

arg

min

FcNonv (w),

W

N cir

=

arg

min

FcNir(w)

ww

22

(53) (54)

Under review as a conference paper at ICLR 2019

Step 1: Proving FcNonv(w) uniformly converges to Fc2iDr -1(w) on X  Wnormal for any compact set X  RD2M .

We arbitrarily choose such a compact set X. Based on (48), (49) and (52), one has, for all w  X  W normal,

|FcNonv(w) - Fc2iDr -1(w)| =|FcNonv(w) - FcNir+D-1(w)|

1 =N + D - 1

(DcNir+D-1)T WcNir+D-1

-
F

(DcNonv)T WcNonv

F

1 N +D-1

(DN )T NW

F

2N (D - 1) + (D - 1)2 (2D - 1)

 N +D-1

d2w2

 (2D- 1) 2(D - 1) N +D-1

d

2

w

2.

Thus, there exists a constant B > 0, which is independent of N , such that

|FcNonv(w) - Fc2iDr -1(w)|



B N

sup
wX W normal

w,

 w  X  Wnormal .

(55)

Step 2: Proving FcNonv(w) epigraphically converges4 to Fc2iDr -1(w).

We want to show, at each point w it holds that

lim inf
N 

FcNonv (wN

)



Fc2iDr -1(w)

lim sup FcNonv(wN )  Fc2iDr -1(w)
N 

for every sequence wN  w for some sequence wN  w

(56) (57)

Firstly, we prove (56). We arbitrarily pick a sequence {wN }N=0 such that wN  w.

If w / Wnormal, Fc2iDr -1(w) = +. Since Wnormal is a closed set, there exists a N + such that wN / Wnormal for all N  N +. Thus, one has FcNonv(wN ) = + for all N  N +, i.e.,

lim inf
N 

FcNonv(wN )

=

Fc2iDr -1(w)

=

+.

If w  Wnormal, two cases should be considered. The first case is that any subsequences of {wN }N=0 are not kept within Wnormal, i.e., there exists a N + such that wN / Wnormal for all N  N +. Then we have

lim inf
N 

FcNonv(wN )

=

+

>

Fc2iDr -1(w).

The second case is that there exists a subsequence {wNk }k=0  {wN }N=0 such that

wNk  Wnormal, k = 0, 1, 2, · · · .

Since wN converges to w, any subsequences should be Cauchy. Given any Cauchy sequence {wNk }k=0 in finite dimensional Euclidean space, there exists a compact set X such that

wNk  X, k = 0, 1, 2, · · ·

Let B = supwXWnormal w . By (55), we obtain

|FcNonkv(wNk ) - Fc2iDr -1(w)| |FcNonkv(wNk ) - Fc2iDr -1(wNk )| + |Fc2iDr -1(wNk ) - Fc2iDr -1(w)|

 BB Nk

+ |Fc2iDr -1(wNk ) - Fc2iDr -1(w)|.

4Epigraphic convergence is a standard tool to prove the convergence of a sequence of minimization problems. The definition of epigraphic convergence refers to Definition 7.1 and Proposition 7.2 in (Rockafellar & Wets, 2009).

23

Under review as a conference paper at ICLR 2019

For any > 0, by the continuity of Fc2iDr -1, we are able to find a K > 0 such that |Fc2iDr -1(wNk ) - Fc2iDr -1(w)| < for all k  K. Pick a K such that NK  (BB / )2. Then, for all k  max(K, K ), we have |FcNonkv(wNk ) - Fc2iDr -1(w)| < 2 , i.e.,

lim
k

FcNonkv(wNk )

=

Fc2iDr -1(w).

The above conclusion holds for all subsequences {wNk }k=0  Wnormal. Fc2iDr -1(w) is an accumulation point of {FcNonv(wN )}N=0. All the other accumulation points of {FcNonv(wN )}N=0 must be + because FcNonv(w) = Fc2iDr -1(w) = + for all w / Wnormal. Thus,

lim inf
N 

FcNonv(wN )

=

Fc2iDr -1(w)

<

+.

Secondly, we prove (57). We set wN = w for all N = 0, 1, 2, · · · . Then (57) is a direct result of (55).

Step 3: proving (26). Define

G(w) =

(Dc2iDr -1)T Wc2iDr -1

2 F

.

We want to show that G(w) is strongly convex.

Let w~ i  R(2D-1)2 be the ith column of Wc2iDr -1, i.e.,

Wc2iDr -1 = w~ 1, w~ 2, · · · , w~ (2D-1)2M

Then

(2D-1)2 M

G(w) =

(w~ i)T

i=1

Let w~  R(2D-1)4M vectorize Wc2iDr -1, i.e.,

D2ciDr -1(Dc2iDr -1)T

w~ i.

T
w~ = (w~ 1)T , (w~ 2)T , · · · , (w~ (2D-1)2M )T .

Then G(w) can be written as a quadratic form of w~ :

G(w) = w~ T Qw~ ,

where

 D2ciDr -1(Dc2iDr -1)T



Q

=

 

···

 

.

 Dc2iDr -1(Dc2iDr -1)T 

totally (2D-1)2M diagonal blocks
As long as at least one of the matrices {Dc2iDr,-0 1, · · · , D2ciDr,-M1-1} is non-singular, Dc2iDr -1 is full row rank, which implies that D2ciDr -1(Dc2iDr -1)T is non-singular. Then Q is positive definite.
The transform between w and w~ is linear. We denote the transform as T , i.e.,

w~ = T w.

It's trivial that

w~

2 2

= 0 implies

Wc2iDr -1

2 F

= 0. By the definition of Wc2iDr -1,

Wc2iDr -1

2 F

=0

implies

w

2 2

=

0.

Thus, linear operator T

is full column rank.

Thus, T T QT

is positive definite,

and

G(w) = wT (T T QT )w

is strongly convex. Then Fc2iDr -1(w) =

G(w)+W

normal

(w)

has

only

one

minimizer,

i.e.,

W

2D-1 cir

involves only a unique element.

Now we check the conditions of Propositions 7.32(c) and 7.33 in (Rockafellar & Wets, 2009) to apply them.

24

Under review as a conference paper at ICLR 2019

1. FcNonv -e Fc2iDr -1. This is proved in Step 2.

2. Fc2iDr -1 is level bounded. Since G(w) is strongly convex, Fc2iDr -1(w) = Wnormal (w) must be level bounded.

G(w) +

3. Fc2iDr -1  +. Since Wnormal is nonempty (54), dom Fc2iDr -1 = , Fc2iDr -1 is not constantly +.

4. All the level set of FcNonv are connected. This can be derived from the convexity of FcNonv. 5. Fc2iDr -1 and FcNonv are all lower semi-continuous and proper. This condition follows from
the fact that the functions Fc2iDr -1 and FcNonv are all continuous functions defined on a nonempty closed convex domain Wnormal.

Applying Proposition 7.32(c), we have {FcNonv} is eventually level bounded. If we arbitrarily pick

a

wN



W

N conv

and

let

wcir

be

the unique

point in Wc2iDr -1.

Applying Proposition

7.33,

we

have

wN  wcir. By Definition 4.1 in (Rockafellar & Wets, 2009), we obtain the convergence of the

sequence

of

sets

{W

N conv

}:

limN 

W

N conv

=

W

2D-1 cir

.

D AN EFFICIENT ALGORITHM TO SOLVE (25)

In this section, we introduce an algorithm to solve (25) (we copy (25) below to facilitate reading):

min
wRD2 M wm·dm=1, 1mM

WcNir(w)

T DNcir(d)

2
.
F

By the definition of the Frobenius norm,

WT D

2 F

=

(WT D)T

2 F

=

DT W

2 F

.

Thus, the

above problem is equivalent with

min
wRD2 M dm·wm=1, 1mM

DNcir(d)

T WcNir(w)

2
.
F

(58)

Since the circular convolution is very efficient to calculate in the frequency domain, we consider solving (58) utilizing the fast Fourier transform (FFT).
Firstly, we introduce the operators DNcir(d), WcNir(w) in the frequency domain. To simplify the notation, we denote the operators as DcNir and WcNir respectively. Let F be the FFT operator. Thus, b = DcNirx is equivalent with
F b = F DcNir F H F x.
Let b^ = F b, x^ = F x be the frequency domain signals, let D^ Ncir = F DNcir F H be the frequency domain operator. The above equation is:

b^ = D^ cNirx^.
The frequency domain operator D^ cNir is much cheaper to calculate than the operator DcNir in the spacial domain because it is block diagonal (Wohlberg, 2016). Specifically, we zero pad d to N × N and do FFT: d^m = FFT zeropad(dm, N - D) , then the above operator can be explicitly written as:
M
b^ = d^m x^m.
m=1

Further, since

D^ Ncir

H W^ cNir

2
=
F

F DcNir F H

H F WcNir F H

2
=
F
=

F DNcir T WcNir F H

DcNir

T WcNir

2
,
F

2 F

25

Under review as a conference paper at ICLR 2019

problem (58) is equivalent with

min
wRD2 M dm·wm=1, 1mM

D^ Ncir

H W^ cNir

2
,

F

which can be efficiently solved by the frequency domain FISTA in (Liu et al., 2017). The details are

outlined in Algorithm 1.

Algorithm 1: Frequency-domain FISTA for solving (25)
Input: Dictionary d = [d1, · · · , dM ]T , dm  RD2 , m. Initialize: Let w0 = d , wa0ux = d, 0 = 1. 1 for j = 0, 1, 2, . . . until convergence do 2 Zeropad and FFT:

w^ ajux,m = FFT zeropad wajux,m, N - D , m = 1, · · · , M.

3 Compute frequency domain gradient:

M

(f )m =

d^ m

m=1

d^¯ m

w^ ajux,m, m = 1, · · · , M,

where ¯· represents the conjugate of a complex number. 4 Compute the next iterate:

wmj+1 = ProjWnormal IFFT w^ ajux,m - (f )m , m = 1, · · · , M,

where the set Wnormal is defined in (53). 5 Let j+1 = 1 + 1 + 4(j)2 /2, then compute the auxiliary variable:

waju+x1

=

wj+1

+

j - 1  j +1

(wj+1

-

wj )

.

6 end Output: wJ , where J is the last iterate.

E ALGORITHM DETAILS OF TRAINING ROBUST ALISTA

E.1 MODEL ARCHITECTURE

Inspired by the a similar unrolling and truncating fashion in LISTA, we can approximately solve the

coherence minimization problem (17) using a similar finite-layer neural network that is unfolded

from iterative algorithms. Because the linear constraints in (17) are hard to enforce in deep neural

networks, we first relax it to the following form:

where

arg min Q
WRN ×M

(DT W - IM )

2 F

,

(59)

is the Hadamard product and Q is a weight matrix that put more penalty on errors on

diagonals, because entries on the diagonal will be far smaller than off-diagonal. The above relaxed

coherence minimization can be solved using the gradient descent algorithm:

W(k+1) = W(k) - (k)DT (Q (DT W(k) - IM )).

(60)

By unfolding (60) and truncate to K steps, and considering the (k)DT outside the residual as

learnable parameters B, we will have a deep neural network W = E(D) as a coherence minimizer.

We call it a Stage 1 encoder as it encodes a dictionary D into a weight matrix, that can be used in

the Stage 2 of ALISTA, refered as a decoder. One layer of this model is shown in Fig. 5(a).

The illustration of the whole feed-forward robust model is shown in Fig. 5(b). The two parts, the encoder and the decoder, can be jointly trained to gain the most from data-driven learning. We further adopt pre-training and curriculum learning to stabilize training, as to be discussed below.

26

Under review as a conference paper at ICLR 2019

(a) One Layer of the Encoder

(b) Cascaded Encoder-Decoder Structure.

Figure 5: Feed-Forward Analytic LISTA.

E.2 MODEL TRAINING
To stabilize the training process, we train the model in two stages: the pre-training stage and curriculum (joint) training stage.

Pre-Training Stage. We first pre-train the encoder and the decoder individually. The pre-training
of the decoder, e.g., ALISTA, follows the standard training procedure in Section 5.1, without both-
ering the perturbations of D. On the other hand, the encoder will always see perturbed dictionaries D~ = D + D, where D's entries are sampled from i.i.d. normal distribution with zero mean and p2re variance, and update its weight to minimize loss function defined by (59). The pre is a hyperparameter that we manually select for the pre-training stage, with a default value of 0.01.

Curriculum (Joint) Training Stage. After the pre-training stage, we concatenate these two parts and do joint training. However, a direct end-to-end tuning was observed to cause much instability, due to the randomness in weights. Inspired by the curriculum learning technique, we first perturb the dictionaries with smaller standard deviations and gradually increase the perturbation level during training. Specifically, starting from a small standard deviation t = 0, the curriculum joint training procedure repeats the routine below:

· First uniformly sample a batch of standard deviations {i}Bi=D1 from [0, t], where BD is the batch size for perturbations of the original dictionary D. Use the sampled standard deviations to sample BD perturbations and apply to D and then normalized them to get {D~ }iB=D1.
· Sample a batch of sparse codes {xj}Bj=x1 from a pre-defined Gaussian-Bernoulli distribution; the supports of the sparse codes are decided i.i.d. by a Bernoulli distribution to have around 10% non-zero entries; and the magnitudes are sampled from i.i.d. standard Gaussian. Bx is the batch size for sparse codes in training.
· Measure yi,j = D~ ixj. Then (yi,j, xj, D~ i) forms a tripelet of training sample. Note that only Robust ALISTA needs D~ i as part of the training samples.
· Feed in the data and update the encoder and decoder with learning rates e and d, using the Adam Optimizer, respectively.
· Increase t to the next larger value, after C training batches.
· Repeat the above steps, until the value of t exceeds the pre-defined max, that represents the maximal standard deviation to sample the dictionary perturbation.

In the experiment, we have BD = 4, Bx = 16, C = 50000, e = 10-6, d = 10-4 and max 

{0.02, 0.03} and t is obtained by linearly interpolating between [0, max] for L - 1 times. We

choose

L

=

5;

hence

t

takes

i 5

max,

i

=

1, 2, . . . , 5

in

order.

F VISUALIZATION OF THE ANALYTIC CONVOLUTIONAL WEIGHTS
Fig. 6 visualizes the dictionary d (7 × 7 × 64) and the weights w~  W1ci3r, used in the convolutional A-LISTA simulation of Section 5.2.

27

Under review as a conference paper at ICLR 2019

(a) The dictionary d.

(b) The w obtained by solving (25).

Figure 6: A visualization of d and w~ : w~ keeps the high-frequency texture in d. The support of w is small, most of the pixels in w are zeros. Then the coherence between shifted d and w is nearly 0.

G RESULTS OF NATURAL IMAGE DENOISING USING CONV ALISTA
The natural image denoising experiment is conducted on the same BSD 500 dataset using the 400image training set, 50-image validation set and 50-image test set. We convert them all to grayscale, and then add  = 20 Gaussian i.i.d. noise. We train both Conv LISTA (i.e., model (21)) and Conv ALISTA (i.e., model (27)). Both networks have 5 layers, with the same dictionary D obtained from the training set by solving (25). We reconstruct the denoised images using by convolving the learned feature maps with the original dictionary D. The mean-square-error (MSE) between denoised and clean images are adopted as the network training loss, as inspired by (Zhou et al., 2018).
Six popular benchmark images (adding  = 20 noise) are tested and reported in Table 3. The APSNR denotes the average PSNR over all images and the A-Times represents the average inference time (in seconds) for denoising one image. We compare Conv LISTA and Conv ALISTA, as well as the classical KSVD denoising algorithm (Elad & Aharon, 2006) and the recent CSC denoising algorithm with gradient regularization (CSC-GR) (Wohlberg, 2018). The results show that Conv LISTA and Conv ALISTA (without heavy tuning done for their optimal performance) can perform comparably with KSVD and outperforms CSC-GR, but with tremendously faster inference speeds than KSVD/CSC-GR. More importantly, Conv LISTA and Conv ALISTA only have marginal performance differences, validating again the analytic weights in convolutional cases.
Table 3: Peak Signal to Noise Ratio (PSNR) Comparision between Conv LISTA and Conv ALISTA.

Model
KSVD CSC-GR Conv LISTA Conv ALISTA

Lenna 31.03 28.41 31.26 31.01

House 33.24 29.11 32.77 32.46

Image PSNR (dB) Pepper Couple Boats 30.97 31.71 31.00 27.39 29.31 28.35 31.00 31.89 30.78 30.81 31.85 30.58

Barbara 30.47 27.19 29.53 29.72

A-PSNR
31.40 28.29 31.21 31.07

A-Time
24.70 7.56 0.012 0.014

28

