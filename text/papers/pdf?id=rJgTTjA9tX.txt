Under review as a conference paper at ICLR 2019
THE COMPARATIVE POWER OF RELU NETWORKS AND POLYNOMIAL KERNELS IN THE PRESENCE OF SPARSE LATENT STRUCTURE
Anonymous authors Paper under double-blind review
ABSTRACT
There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worstcase setting: e.g. characterizing the best L1 or L approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used. However, in typical applications we expect data to be high dimensional, but structured ­ so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now. With this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting.
1 INTRODUCTION
The concept of representational power has been always of great interest in machine learning. In part the reason for this is that classes of "universal approximators" abound ­ e.g. polynomials, radial bases, rational functions, etc. Some of these were known to mathematicians as early as Bernstein and Lebesgue1 ­ yet it is apparent that not all such classes perform well empirically.
In recent years, the class of choice is neural networks in tasks as simple as supervised classification, and as complicated as reinforcement learning ­ inspiring an immense amount of theoretical study. Research has focus on several angles of this question, e.g. comparative power to other classes of functions (Yarotsky, 2017; Safran and Shamir, 2017; Telgarsky, 2017), the role of depth and the importance of architecture (Telgarsky, 2016; Safran and Shamir, 2017; Eldan and Shamir, 2016), and many other topics such as their generalization properties and choice of optimization procedure (Hardt et al., 2016; Zhang et al., 2017; Bartlett et al., 2017).
Our results fall in the first category: comparing the relative power of polynomial kernels and ReLU networks ­ with a significant twist, that makes our results more relevant to real-life settings. The flavor of existing results in this subject is roughly the following: every function in a class C1 can be approximately represented as a function in a different class C2, with some blowup in the
1 Lebesgue made use of the universality of absolute value and hence ReLu ­ see the introduction of (Newman et al., 1964).
1

Under review as a conference paper at ICLR 2019
size/complexity of the function (e.g. degree, number of nodes, depth). The unsatisfying aspect of such results is the "worst-case" way in which the approximation is measured: typically, one picks a domain coarsely relevant for the approximation (e.g. an interval or a box), and considers the L, L2, L1, . . . norm of the difference between the two functions on this domain. In some of the constructions (e.g. (Eldan and Shamir, 2016; Safran and Shamir, 2017)), the evaluation is even more adversarial: it's the mean-square error over a specially-designed measure.
Instead, in practically relevant settings, it's reasonable to expect that approximating a predictor function well only on some "relevant domain" would suffice, e.g. near the prediction boundary or near a lower-dimensional manifold on which the data lives, as would be the case in settings like images, videos, financial data, etc. A good image classifier need not care about "typical" data points from the -ball, which mostly look like white noise.
The difficulty with the above question is that it's not immediate how to formalize what the "relevant domain" is or how to model the data distribution. We tackle here a particularly simple (but natural) incarnation of this question: namely, when the data distribution has sparse latent structure, and all we ask is to predict a linear function of the latent variables based upon (noisy) observations. The assumption of sparsity is very natural in the context of realistic, high-dimensional data: sparsity under the correct choice of basis is essentially the reason that methods such as lossy image compression work well, and it is also the engine behind the entire field of compressed sensing (Donoho, 2006).
2 OVERVIEW OF RESULTS
We will be considering a regression task where the data has a sparse latent structure. More precisely, we wish to fit pairs of (observables, labels) (X, Y ) generated by a (latent-variable) process:
· Sample a latent vector Z  Rm from H, where H is a distribution over sparse vectors. · To produce X  Rn, set X = AZ + , where the noise   subG(2) is a subgaussian
random vector with variance proxy 2 (e.g. N (0, 2I)). · To produce Y  R, we set Y = w, Z .
We hope the reader is reminded of classical setups like sparse linear regression, compressive sensing and sparse coding: indeed, this distribution on the data distribution X is standard in these setups. In our setting, we additionally attach a regression task to this data distribution, wherein the labels Y are linearly generated2 by a predictor w from the latent vector Z.
Note our interest is slightly different than usual: in the traditional setup, we are interested in the statistical/algorithmic problem of inferring Z, given X as input (the former studying the optimal rates of "reconstruction" for Z, the latter efficient algorithms for doing so). In particular, we do not typically care about the particular form of the predictor as long as it is efficiently computable.
By contrast, we want to understand how well different subsets of universal approximator families can fit the data points (X, Y ). Namely, regardless of the specifics of the training procedure, the end will be an element of some function class like a linear function of a kernel embedding of X, or a neural network. Therefore, we ask if these classes are rich enough to reconstruct Y given X accurately (i.e. compared to the Bayes-optimal estimator E[Y |X]): if the answer is negative, then we know our predictor will perform poorly, no matter the training method. We measure the performance of these estimators in the natural3 distributional sense: expected reconstruction error, E[(Y^ - Y )2]. Informally, what we will show is the following. Theorem (Informal). For the problem of predicting Y given X in the generative model for data described above, it holds that: (1) Small two-layer ReLU networks achieve close to the statistically optimal rate. (2) Polynomial predictors of degree lower than log m achieve a statistical rate which is substantially worse. (In fact, in a certain sense, close to "trivial".) Conversely, polynomial predictors of degree O((log n)2) achieve close to the statistically optimal rate.
2One could also imagine producing discrete labels by applying a softmax operation. We stick to the regression setting for reasons of technical simplicity and leave generalizing our results to future work.
3This is natural because it is the (only) loss which the conditional expectation E[Y |X] minimizes. In statistical language, we are asking about the minimum excess risk achievable by estimators in our function class.
2

Under review as a conference paper at ICLR 2019
The lower bound in (2) is relevant since fitting a polynomial to data points of the form (xi, yi) requires4 searching through the space of multivariate polynomials of degree (log m) which has dimension m(log(m)), and thus even writing down all of the variables in this optimization problem takes super-polynomial time. Practical aspects of using polynomial kernels even with much lower degree than this have been an important concern and topic of empirical research; see for example (Chang et al., 2010) and references within. On the other hand, the upper bound in (2) shows that our analysis is essentially tight: greater than polylog(m) degree is not required to achieve good statistical performance, which is qualitatively different from the situation in worst-case analyses (see Section 4.2.2 for more details). Our mathematical analysis closely matches the observed behavior in experiments: see Section 6.
For formal statements of the theorems, see Section 4.
3 PRIOR WORK
There has been a large body of work studying the ability of neural networks to approximate polynomials and various classes of well-behaved functions, such as recent work (Yarotsky, 2017; Safran and Shamir, 2017; Telgarsky, 2017; Poggio et al., 2017). These results exclusively focus on the worst-case setting where the goal is to find a network close to some function in some norm (e.g. L or L1-norm, often under an adversarially chosen measure).
In contrast there is little work on the problem of approximating ReLU networks by polynomials, mostly because it is well-known by classical results of approximation theory (Newman et al., 1964; DeVore and Lorentz, 1993) that polynomials of degree (1/ ) are required to approximate even a single ReLU function within error in L-norm on [-1, 1]. On the other hand, we will show that if we do not seek to achieve -error everywhere for the ReLU (in particular not near the nonsmooth point at 0) we can build good approximations to ReLU using polynomials of degree only O(log2(1/ )) (see discussion in Section 4.2.2 and Theorem 5.2).
Because of the trivial (1/ ) lower bound for worst-case approximation of ReLU networks by polynomials, (Telgarsky, 2017) studied the related problem of approximating a neural network by rational functions. (A classical result of approximation theory (Newman et al., 1964) shows that rational functions of degree O(log2(1/ )) can get within -error of the absolute value function.) In particular, (Telgarsky, 2017) shows that rational functions of degree polylog(1/ ) can get within distance in L-norm of bounded depth ReLU neural networks.
Somewhat related is also the work of (Livni et al., 2014) who considered neural networks with quadratic activations and related their expressivity to that of sigmoidal networks in the depth-2 case building on results of (Shalev-Shwartz et al., 2011) for approximating sigmoids. The result in (Shalev-Shwartz et al., 2011) is also proved using complex-analytic tools, though the details are substantially different.
There is a vast literature on high dimensional regression and compressed sensing which we do not attempt to survey, since the main goal of our paper is not to develop new techniques for sparse regression but rather to analyze the representation power of kernel methods and neural networks. Some relevant references for sparse recovery can be found in (Vershynin, 2018; Rigollet, 2017). We only emphasize that the upper bound via soft thresholding we show (Theorem 4.1) is implicit in the literature on high-dimensional statistics; we include the proofs here solely for completeness.
4We note that for kernel ridge regression, one can use the kernel trick to reduce to train in a space with dimension equal to the size of the training dataset. However, since training data sets are often very large, this is not necessarily helpful. More usefully, in the classification context, the optimization may become feasible due to the existence of a smaller number of support vectors. In this case, the ability of algorithms to efficiently find a good function depends on more complex interactions between the particular choice of kernel and the margin properties of the data; we leave analyzing these kinds of quantities for future work.
3

Under review as a conference paper at ICLR 2019
4 MAIN RESULTS
In this section we will give formal statements of the results and give some insight into the techniques used.
First, we state the assumptions on the parameters of our generative model:
· Z is sparse: more precisely, |supp(Z)|  k and Z 1  M with high probability.5 · A is a µ-incoherent n × m matrix, which means that A A - I   µ for some µ  0. · w  = 1 (w.l.o.g., since changing the magnitude of w rescales Y )
The assumption on A is standard in the literature on sparse recovery (see reference texts (Rigollet, 2017; Moitra, 2018)). In general one needs an assumption like this (or a stronger one, such as the RIP property) in order to guarantee that standard algorithms such as LASSO actually work for sparse recovery. For the reader not familiar with this literature, this property is a proxy for the matrix being "random-like" ­ e.g. a matrix with i.i.d. entries of the form ±1/ n has µ = O(1/ n), even when m >> n. We also note that for notational convenience, we will denote A  = maxi,j |Ai,j|. Before proceeding to the results, we note that the first-time reader may freely assume that µ = 0 and n = m; the results are still interesting in this setting and no important technical idea is needed for the more general case. For the upper bounds, we have included results for the more general setting (with µ  0) to show that our results are relevant even to very high-dimensional settings where m >> n. We have only proven the lower bound in the case µ = 0: this is the easiest setting for algorithms, so this makes the lower bounds the strongest.
4.1 REGRESSION USING RELU NETWORKS
We prove the following theorem, which shows that small 2-layer ReLU networks can achieve an almost optimal statistical rate. Let us denote the soft threshold function with threshold  as  (x) := sgn(x) min(0, |x| -  ) = ReLU(x -  ) - ReLU(-x +  ). Consider the following estimator (for y), corresponding to a 2-layer neural network:
Z^NN :=  m(A X) Y^NN := w, Z^NN We can prove the following result for the estimator (see Appendix A of the supplement): Theorem 4.1 (2-layer ReLU). With high probability, the estimator Y^NN satisfies (Y^NN - Y )2 = O((1 + µ)2k2 log(m) + µ2k2M 2)
Notice that the size of the ReLU net is comparable to the input: one of the layers has the same dimension as A, the other the same dimension as w. Furthermore, to interpret this result, recall that we think of µ as quite small ­ in particular µ 1. Thus the error of the estimator is essentially O(2k2 log(m)), i.e. essentially || error "per-nonzero-coordinate". It can be shown that this upper bound is nearly information-theoretically optimal (see Remark B.1), except that there is an additional factor of k. This additional factor is artificial and can be removed with added technical effort; we show how to do this in the µ = 0 case in Theorem A.1.
We emphasize that the analysis of this kind of soft thresholding estimator is implicit in much of the literature on sparse linear regression. For completeness, we include a complete and self-contained proof of Theorem 4.1 in Section A.
4.2 REGRESSION USING POLYNOMIALS
4.2.1 LOWER BOUNDS FOR LOW-DEGREE POLYNOMIALS
We first show that polynomials of degree smaller than O(log m) essentially cannot achieve a "nontrivial" statistical rate. This holds even in the easiest case for the dictionary A: when it's the identity matrix.
5The assumed 1-norm bound M plays a minor role in our bounds and is only used when the incoherence µ > 0.
4

Under review as a conference paper at ICLR 2019

More precisely, we consider the situation in which A is an orthogonal matrix (i.e. µ = 0, m = n), w  {±1}m, the noise distribution is Gaussian N (0, 2I), and the entries of Z are independently 0 with probability 1 - k/m and N (0, 2) with probability k/m. Then we show

Theorem 4.2. Suppose k < m/2 and f is a multivariate degree d polynomial. Then

E[(f (X) - Y )2]  (1/4)

2k
2

1 + k/m(d + 1)3d+2 (1 + (/)d)

To parse the result, observe that the numerator is of order 2k which is the error of the trivial

estimator6 and the denominator is close to 1 unless d is sufficiently large with respect to m. More

precisely, assuming the that the denominator is

signal-to-noise ratio  close to 1 unless dd =

/ does not grow too quickly with respect to m, ( m), i.e. unless d is of size ((log m)/ log

we log

see m).

On a technical note we observe that this statement is given with respect to expectation but a similar

one can be made with high probability, see Remark B.2.

4.2.2 NEARLY MATCHING UPPER BOUNDS

The lower bound of the previous section leaves open the possibility that polynomials of degree O(polylog(m)) still do not suffice to perform sparse regression and solve our inference problem; Indeed, it is a well-known fact (see e.g. (Telgarsky, 2017)) that to approximate a single ReLU to -closeness in infinity norm in [-1, 1] requires polynomials of degree poly(1/ ); this follows from standard facts in approximation theory (DeVore and Lorentz, 1993) since ReLU is not a smooth function.

Proceeding with this "worst-case" way of thinking: our upper bound follows by designing a polynomial approximation to ReLU into our neural network construction; since estimates for Y typically accumulate error from estimating each of the m coordinates of Z, to guarantee accurate reconstruction we would need m to be small. Plugging in the the best approximation to ReLU in infinity norm, we would need a ( m)-degree polynomial for this to yield a multivariate polynomial with similar statistical performance to the 2-layer ReLU network which computes Y^NN . Thus, naively, we might suspect that the degree of the kernel needs to be as high as m to get a reasonable approximation.

Surprisingly, we show this intuition is incorrect! In fact, we show how using only a polylog(m)

degree polynomial, our converted ReLU network has similar statistical performance. Formally this is summarized by the following theorem, where Y^d,M is the corresponding version of Y^NN formed by replacing each ReLU by our polynomial approximation.

Theorem 4.3. Suppose  = (

(1

+

µ)

log

m+µM )

and

d



d0

=

((2 +

M 

)

log2(M m/ 2)).

With high probability, the estimator Y^d,M satisfies7

(Y^d,M - Y )2 = O(k2((1 + µ)2 log(m) + µ2M 2))

The idea behind our construction is described in Section 5.3. Our methods are novel and may be of independent interest; we are not aware of a way to get this result using only generic techniques such as FT-Mollification (Diakonikolas et al., 2010).

5 OVERVIEW OF PROOFS
In this section, we will sketch the ideas behind the proofs of our results. The full proofs are relegated to the appropriate appendices. We proceed with each of our results in turn.
5.1 UPPER BOUND FOR RELU NETWORKS
As previously mentioned, this kind of result is well-known in the literature on sparse regression and we include a proof primarily for completeness. The intuition is simple: the estimator Z^NN can make
6I.e. the estimator which always returns 0, without looking at the data. 7As in Theorem 4.1, there is a spurious factor of k in this bound which can be removed with additional technical effort. In particular in the µ = 0 case we can remove it using the same argument as Theorem A.1; details are omitted.

5

Under review as a conference paper at ICLR 2019
use of the non-linearity in the soft threshold to zero out the coordinates in the estimate A X which are small and thus "reliably" not in the support of the true z. Thus, the estimator only makes mistakes on the non-zero coordinates. The full proofs are in Section A.
5.2 LOWER BOUND: PROOF SKETCH OF THEOREM 4.2
The proof of Theorem 4.2 has two main ideas: (1) A structural lemma, which shows that the optimal predictor has a "decoupled" structure along the coordinates of the latent variable. (2) An analysis of this decoupled estimator using a bias-variance calculation in an appropriately chosen basis. The full proofs of this Section are in Appendix B.
5.2.1 STRUCTURE OF THE OPTIMAL ESTIMATOR
To parse the structural lemma more easily, observe first that the optimal estimator for Y = w, Z given X has a particularly simple structure. Concretely, the optimal estimator is the conditional expectation E[ w, Z |X] = i wiE[Zi|X], so the optimal estimator for Y simply reconstructs Z as well as possible coordinate-wise, then takes an inner product with w.
With this in mind, note the coordinates of Z are independent in our setting, which allows us to show that the optimal polynomial of degree d to estimate Y has no "mixed monomials" in an appropriate basis. This is the content of the next lemma, whose proof is in Appendix B. Lemma 5.1. Suppose X = AZ +  where A is an orthogonal m × m matrix, Z has independent entries and   N (0, 2Id). Then there exists a unique minimizer fd over all degree d polynomials fd of the square-loss,
E[(fd(A X) - w, Z )2] and furthermore fd has no mixed monomials. In other words, we can write fd(A X) =
i fd,i((A X)i) where each of the fd,i are univariate degree d polynomials.
5.2.2 FOURIER ANALYSIS AND BIAS-VARIANCE TRADE-OFF
The second idea of the proof is to perform a bias-variance calculation in an appropriately chosen basis. More precisely, we will use Fourier analytic methods, so we will switch to an orthonormal basis. Since the noise we chose for the lower bound instance is Gaussian8, a natural choice is the Hermite polynomials.
We review the definition of the Hermite polynomials in Appendix B, but for the purposes of this proof overview, the Hermite polynomials are polynomials Hn(x) indexed by multi-indices n  Nm0 with the important property that they are orthogonal with respect to the standard m-variate Gaussian distribution, namely
0, if n = n EXN (0,2I )Hn(X/)Hn (X/) = 1, otherwise From this, we can derive Plancherel's Theorem in this basis: Theorem 5.1 (Plancherel's Theorem in Hermite Basis). Let f (x) = n f (n)Hn(x/), then
EXN (0,2I )[|f (X)|2] = |f (n)|2
n
We use this theorem, along with the structural Lemma 5.1) to perform a bias-variance tradeoff analysis of any predictor: namely, we show (1) If the Fourier coefficients |f (n)| are large, then the estimator will be very sensitive to noise (i.e. has too high of a variance). (2) On the other hand, if |f (n)| is small and f is low-degree, then the estimator cannot match the
8The proof does not rely heavily on this choice; as long as we choose the correct orthogonal basis of polynomials, the proof would go through with minor modifications.
6

Under review as a conference paper at ICLR 2019

correct mean well (i.e. has too high of a bias).

Note when f is sufficiently high-degree, it effectively can take advantage of the difference in scales between the noise and the signal: see the following upper bound section for details.

5.3 UPPER BOUND: PROOF SKETCH OF THEOREM 4.3

As previously mentioned, it's a classical approximation theory result that no low-degree polynomial is close to the ReLU function on all of [-1, 1]. The crux of these results is that it's hard to approximate
ReLU well at 0, its point of non-smoothness.

However, in our setting precisely approximating ReLU everywhere is not important for getting a good regression rate: instead, the approximation needs to be very close to 0 when the input is negative, and only very coarsely accurate otherwise. The reason for this is the intuition we described for 2-layer ReLU networks: the property of ReLU that is useful in this setting is it's "denoising" ability ­ the fact that it zeroes out negative inputs.

Consequently, we design a polynomial approximation to ReLU of degree O(log2 n) which sacrifices accuracy near the point of non-smoothness in favor of closeness to 0 in the negative region.

More precisely, we prove the following theorem, in which the parameter  in our theorem controls

the trade-off between the polynomial pd being close to 0 for x < 0 and being close to x for x > 0.

Theorem 5.2. Suppose R > 0, 0 <  < 1/2 and d  7. Then there exists a polynomial pd = pd,,R of degree d such that for x  [-R, 0]

|pd(x) - ReLU(x)|  14R

d

 e-  d/4



and for x  [0, R],

|pd(x) - ReLU(x)|  2R + 2R

4 + 12R
d

d

 e-  d/4.



The proof of this theorem proceeds in two steps:

(1)

First,

one

takes

a

"soft-max"

mollification

of

ReLU

of

the

form

g (x)

:=

1 

log(1 + ex)

with

an

appropriately tuned , so that g is sufficiently close to ReLU.

(2) Second, if  is not too large, we prove that the poles (in the complex plane) of the function g are

not too close to the origin. This, it turns out, governs the polynomial approximability of g due to a

powerful theorem of Bernstein in complex analysis. (See Theorem C.1, and it's quantitative analogue

we prove as Theorem C.2.) Once we have this approximation to ReLU, we directly plug it into our

2-Layer ReLU network estimator from Section 4.1 to prove Theorem 4.3.

The full proofs are in Appendix C.

6 SIMULATIONS
Finally, we provide synthetic experiments to verify the predictions from Theorem 4.2 and Theorem 4.3. The setup is as follows: we generate a large synthetic data set (with n = m and µ = 0) in the following fashion:
· A is a random orthogonal matrix and w is sampled from a n-dimensional standard Gaussian. · Z  Rn is sampled by including each coordinate with probability k/n, and sampling a
standard Gaussian for each included coordinate. · X and Y are sampled according to the generative model in Section 2, using Gaussian noise
with standard deviation .
For each fixed degree, we fit a polynomial using least-squares regression, and evaluate the performance on a corresponding test set9 generated in the same fashion (reusing the same A and w). Solving the
9Small technical remark: our upper bound (Theorem 4.3) is with-high-probability and not in expectation, so we drop the outlier 1 percent of test results which had largest error. The reason is that we may draw a

7

Under review as a conference paper at ICLR 2019

Dimension (n)

32 45 64 91 128

2-Layer ReLU Network 6.500282 6.969625 7.479449 8.324109 8.969790

Degree 17 Polynomial 7.258900 7.723297 8.727798 9.993010 10.256913

Table 1: Test errors of baseline ReLU network (Section 4.1) and Degree 17 polynomial kernel; error is unnormalized. Experiments were run for n up to 4096 and the error between the two methods continued to be similar ­ results are omitted for concision.

regression problem for large degrees is intractable using standard training methods; to overcome this issue, we used structural observation in Lemma 5.1 to reduce the regression problem for estimating Y from X to that of estimating Zi given Xi, which is a much lower dimensional problem. 10
The results of the experiment are in Figure 1, graphed on a log scale. All experiments were run with k = 5 and  = 0.06. We see that for low degrees, i.e. before our prediction error is close to the information-theoretic limit, the log-error decays roughly linearly with respect to polynomial degree. This matches the prediction of the lower bound in Theorem 4.2 after taking a log of the right-hand-side.
For completeness, we also evaluate the baseline 2-Layer ReLU network described in Section 4.1 in the same experimental setup. Table 1 shows the test error of the baseline 2-Layer ReLU network and, for comparison, the best polynomial of degree 17 in the same experiment. Despite the high degree, the ReLU network is still slightly better.
4.0

Log L2 Error

3.5 log(n) 8 7
3.0 6 5 4
2.5

2.0

5 10 Polynomial Degree

15

Figure 1: Degree vs Log L2 Error on test set for different values of n, the dimensionality of the problem. This plot was generated using a training set of 8000 examples from the generative model and a test set of 1000 additional examples; error is unnormalized.

7 CONCLUSIONS
In this paper, we considered the problem of providing representation lower and upper bounds for different classes of universal approximators in a natural statistical setup that exhibits sparse latent structure. We hope this will inspire researchers to move beyond the worst-case setup when considering the representational power of different function classes.
The techniques we develop are interesting in their own right: unlike standard approximation theory setups, we need to design polynomials which may only need to be accurate in certain regions. Conceivably, in classification setups, similar wisdom may be helpful: the approximator needs to only be accurate near the decision boundary.
rare tail event where the input is unusually large/non-sparse and then the polynomial predictor may make an exponentially large error in the degree.
10Another technical remark: we only run the experiment for odd degrees, since the optimal estimator is an odd function.
8

Under review as a conference paper at ICLR 2019
Finally, we conclude with a tantalizing open problem: In general it is possible to obtain non-trivial sparse recovery guarantees for LASSO even when the sparsity k is nearly of the same order as n under assumptions such as RIP. Since LASSO can be computed quickly using iterated soft thresholding (ISTA and FISTA, see Beck and Teboulle (2009)), we see that sufficiently deep neural networks can compute a near-optimal solution in this setting as well. It would be interesting to determine whether shallower networks and polynomials of degree polylog(n) can achieve a similar guarantees.
REFERENCES
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6241­6250, 2017.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183­202, 2009.
Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. Training and testing low-degree polynomial data mappings via linear svm. Journal of Machine Learning Research, 11(Apr):1471­1490, 2010.
Ronald A DeVore and George G Lorentz. Constructive approximation, volume 303. Springer Science & Business Media, 1993.
Ilias Diakonikolas, Daniel M Kane, and Jelani Nelson. Bounded independence fools degree-2 threshold functions. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 11­20. IEEE, 2010.
David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289­1306, 2006.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pages 907­940, 2016.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, pages 1225­1234, 2016.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855­863, 2014.
Ankur Moitra. Algorithmic aspects of machine learning. Preprint. Cambridge University Press (to appear), 2018.
Donald J Newman et al. Rational approximation to |x|. The Michigan Mathematical Journal, 11(1): 11­14, 1964.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5):503­519, 2017.
Phillippe Rigollet. High-dimensional statistics. Lecture notes (MIT), 2017.
Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural networks. In International Conference on Machine Learning, pages 2979­2987, 2017.
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM Journal on Computing, 40(6):1623­1646, 2011.
Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory, pages 1517­1539, 2016.
Matus Telgarsky. Neural networks and rational functions. In International Conference on Machine Learning, pages 3387­3393, 2017.
Roman Vershynin. High-Dimensional Probability. Cambridge University Press (to appear), 2018.
9

Under review as a conference paper at ICLR 2019

Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94: 103­114, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017.

A UPPER BOUND FOR 2-LAYER RELU NETWORKS
We will first prove a bound on the error of the soft-thresholding estimator Z^NN (Lemma A.2), which corresponds to the hidden layer of the neural network: this is essentially a standard fact in high-dimensional statistics (see reference text (Rigollet, 2017)). The idea is that the soft thresholding will correctly zero-out most of the coordinates in the support while adding only a small additional error to the coordinates outside the support.
From the recovery guarantee for Z^NN , we will then deduce Theorem 4.1.
Towards proving the above result, we first need an estimate on the bias of A x, i.e. the error without noise:
Lemma A.1. Suppose A is µ-incoherent i.e. A A - Id   µ. Then for any z, A Az - z   µ z 1.

Proof. (A Az)i = Ai, zj Aj = zi Ai, Ai + zj Ai, Aj
j j=i
so applying the incoherence assumption we have |(A Az)i - zi|  µ z 1.

Using this we can analyze the error in thresholding.
Lemma A.2. Suppose A is µ-incoherent i.e. A A - I   µ. Let z be an arbitrary fixed vector such that z 1  M and |supp(z)|  k. Suppose x = Az +  where   N (0, 2In×n). Then for some  = ( (1 + µ) log m + µM ) and z^ =  n(A x), with high probability we have z^ - z   2 and supp(z^)  supp(z).

Proof. Observe that

A x = z + (A A - I)z + A .

Note that entry i of A

 is

Ai, 

where

Ai

2 2



(1

+

µ)

so

(At)i

is

subgaussian

with

variance

proxy at most 2(1 + µ).

By concentration and union bound, with high probability all coordinates not in the true support are thresholded to 0. Similarly we see that for each of the coordinates in the support, an error of at most 2 is made.

From the above lemma, we can easily prove the main theorem of this section:

Proof of Theorem 4.1. When the high probability above event happens, we have the following upper bound by Holder's inequality:

|Y^NN -Y |2 =

w|supp(h), (Z^NN -Z)|supp(h) 2  k2

Z^NN -Z

2 

=

O(k2((1+µ)2

log(m)+µ2M 2))

For the lower bounds we will be interested mostly in the case when µ = 0, i.e. A is orthogonal and so m = n, the coordinates of Z are independent and each is nonzero with probability at most k/n, and the noise is Gaussian. Then the error estimate we had in the previous theorem specializes to O(2k2 log(n)), but under these assumptions we know that the information-theoretic optimal is actually 2k log(n). While not very important to the flow of the paper, for completeness we can improve the analysis to eliminate the extra factor of k, without changing the algorithm:
10

Under review as a conference paper at ICLR 2019
Theorem A.1. Suppose A is orthogonal (hence m = n), the coordinates of Z are independent, and   N (0, 2I). Then
E|Y^NN - Y |2 = O(k2 log(m))
Proof. In this case, we have A X = Z +  where   N (0, 2I). Therefore the coordinates of Z^ are independent of each other, and so we see
E|Y^NN - Y |2 = wi2E[(Z^NN - Z)2i ]  E[(Z^NN - Z)i2].
ii
Let Ei denote the event that | |i >  . Then
E[(Z^NN - Z)2i ] = E[(1Ei + 1EiC )(Z^NN - Z)i2]
ii
 4k 2 + E[1EiC (Z^NN - Z)i2]
i
= 4k 2 + Pr(1EiC )E[(Z^NN - Z)i2 1| EiC = 1]
i
 4k 2 + Pr(1EiC )E[( + |Xi - Zi|)2 1| EiC = 1]
i
 4k 2 + Pr(1EiC )(2 2 + 2E[|i|2 1| EiC = 1])
i
 4k 2 + C (2 2 + 2C  2) m
i
where the first inequality follows as in Lemma A.2, the second inequality uses that | (x) - x|   , the third uses that (a + b)2 = a2 + 2ab + b2  2a2 + 2b2 by Young's inequality, and the last inequality follows from standard tail bounds on Gaussians. We see the last expression is O(k2 log(m)) so we have proved the result.
B LOWER BOUNDS FOR POLYNOMIAL KERNELS
In this section, prove the lower bounds for polynomial kernels. We recall the lower bound instance: the noise distribution is N (0, 2Id) and the distribution for Z is s.t. every coordinate is first chosen to be non-zero with probability k/n, and if it is non-zero, it's set as an independent sample from N (0, 2). This construction makes Z approximately k-sparse with high probability while making its coordinates independent. We choose A as an arbitrary orthogonal matrix, so m = n. We choose w to be an arbitrary ±1 sign vector, so wi2 = 1 for every i. As a warmup, we first show that linear predictors, and subsequently fixed low degree polynomials cannot achieve the information-theoretic rate 11 of O(2k log n) ­ in fact, we will show that they achieve a "trivial" rate. Furthermore, we will show that even if the degree of our polynomials is growing with n, if d = o(log n/ log log n) the state of affairs is similar.
B.1 WARMUP: LINEAR PREDICTORS
As a warmup, and to illustrate the main ideas of the proof techniques, we first consider the case of linear predictors. (i.e. kernels of degree 1.)
The main idea is to use a bias-variance trade-off: namely, we show that the linear predictor we use, say f (x) = w~, x either has to have too high of a variance (when w~ is large), or otherwise has too high of a bias. (Recall, the bias captures how well the predictor captures the expectation.)
We prove:
11See Remark B.1 for why this is the optimal information-theoretic rate.
11

Under review as a conference paper at ICLR 2019

Theorem B.1. For any w~  Rn,

E[( w~, X

-

Y

)2]



2k

2 2(k/n)

+

2

Before giving the proof, let us see how the theorem should be interpreted.

The trivial estimator which always returns 0 as thresholding) should instead make error

makes error of order 2K of order 2K log n when

and a good estimator (such  >>  log n. The next

theorem shows that as long as the signal to noise ratio is not too high, more specifically as long as

2(k/n) = o(2), any linear estimator must make square loss of (2k), i.e. not significantly better

than the trivial 0 estimate.

Note that the most interesting (and difficult) regime is when the signal is not too much larger than the noise, e.g. 2 = 2polylog(n) in which case it is definitely true that 2(k/n) << 2.

Proof. Note that

w~, x - y = w~, Az +  - w, z = A w~ - w, z + w~, 

which gives the following bias-variance decomposition for the square loss:

E[( w~, x - y)2] = E[( A w~ - w, z + w~,  )2]

= E[ A w~ - w, z 2 + w~,  2]

= k 2 n

A

w~ - w

2 2

+

2

w~

2 2

=

k 2 n

w~ - Aw

2 2

+

2

w~

2 2

where in the second-to-last step we used that the covariance matrix of Z is 2(k/n)I, and in the last

step we used that A is orthogonal. Now observe that if we fix R = w~ 2, then by the Pythagorean theorem the minimizer of the square loss is given by theprojection of Aw onto the R-dilated unit sphere, so w~ = R2/m(Aw) since Aw 2 = w 2 = m. In this case the square loss is then of

the form

k 2 n

R2/m(Aw) - Aw

2 2

+

2

w~

2 2

=

k 2(R - m)2 + 2R2 n

and the risk is minimized when

0

=

2

k

2(R

-

 m)

+

22R

n

i.e. when

2(k/n)  R = 2(k/n) + 2 m

so the minimum square loss is

 (m

-

R)2R

+

2R2

=

2

2k 2(k/n) +

2

since m = n.

B.2 STRUCTURE OF THE OPTIMAL ESTIMATOR: PROOF OF LEMMA 5.1
Proof of Lemma 5.1. Let X = A X, so by orthogonality X = Z +  where   N (0, 2Id). Observe that if we look at the optimum over all functions f , we see that
min E[(f (X ) - w, Z )2] = E[(E[ w, Z |X ] - w, Z )2]
f
= E[( wiE[Zi|X ] - w, Z )2]
i
= E[( wiE[Zi|Xi] - w, Z )2].
i
12

Under review as a conference paper at ICLR 2019

where where in the first step we used that the conditional expectation minimizes the squared loss, in the second step we used linearity of conditional expectation, and in the last step we used that Zi is independent of X=i.
By the Pythagorean theorem, the optimal degree d polynomial fd is just the projection of i wiE[Zi|Xi] onto the space of degree d polynomials. On the other hand observe that
E[( wiE[Zi|Xi] - w, Z )2] = wi2E[(E[Zi|Xi] - Zi)2]
ii
so the optimal projection fd is just i wifi,d(Xi) where fi,d is just the projection of each of the E[Zi|Xi]. Therefore fd has no mixed monomials.
Remark B.1. The previous calculation shows additionally that the problem of minimizing the squared loss for predicting Y is equivalent to that of minimizing the squared loss for the sparse regression problem of recovering Z. It is a well-known fact that the information theoretic rate for sparse regression (with our normalization convention) is (2k) (see for example (Rigollet, 2017)), and so the information-theoretic rate for predicting Y is the same, and is matched by Theorem A.1.

B.3 BIAS-VARIANCE, FOURIER ANALYSIS AND PROOF OF THEOREM 4.2

We recall that the lower bound for polynomials combines the observation of Lemma 5.1 with a bias-variance tradeoff calculation using Fourier analysis on orthogonal polynomials. Concretely, since the noise we chose for the lower bound instance is Gaussian, the most convenient basis will be the Hermite polynomals.

We recall the probabilist's Hermite polynomial Hen(x), defined by the recurrence relation

Hen+1(x) = xHen(x) - nHen-1(x).

(1)

where He0(x) = 1, He1(x) = x. In terms of this, the normalized Hermite polynomial Hn(x) is

Hn(x)

=

1 n!

He

n

(x).

Let Hn(x) for a vector of indices n  Nm0 denote the multivariate polynomial mi=1Hni (xi). It's easy to see the polynomials Hn(x) form an orthogonal basis with respect to the standard m-variate
Gaussian distribution. As a consequence, we get

0, if n = n EXN (0,2I )Hn(X/)Hn (X/) = 1, otherwise

which gives us Plancherel's theorem:

Theorem B.2 (Plancherel in Hermite basis). Let f (x) = n f (n)Hn(x/), then
EXN (0,2I )[|f (X)|2] = |f (n)|2
n

We can use Plancherel's theorem to get lower bounds on the noise sensitivity of degree d polynomials. This will be an analogue of the variance.

Lemma B.1. [Variance analogue in Hermite basis] Let f (x) = n f (n)Hn(x/) and let f=0 :=

f - f (0). Then

E[(f (A

X) - Y )2]  (1 - k/n)

f=0

2 2

Proof. First suppose Z, and thus y, is fixed. Let S denote the support of Z. Recall that A x = Z + where   N (0, 2In×n). Define fZ () := f (Z + ) - y, then by Plancherel

E[(f (A x) - y)2] = E [fZ ( )2] = |fZ (n)|2
n

Furthermore

|fZ (n)|2 

|f (n)|2

n n:supp(n)S

13

Under review as a conference paper at ICLR 2019

because ( + Z)|Sc =  |Sc so by expanding out fZ in terms of the fourier expansion of f , we see
fZ(n) = f (n) for n such that supp(n)  S. Finally the probability n  S for n = 0 is upper bounded by the probability a single element of its support is in S, which is k/n.

Next we give a lower bound for the bias, showing that if

f=0

2 2

is

small

for

a

low-degree

polynomial,

it cannot accurately predict y. Here we will assume f is of the form given by Lemma 5.1.

Lemma B.2 (Low variance implies high bias). Suppose f is a multivariate polynomial of degree d with no mixed monomials, i.e. f (x) = i fi(xi) where fi is a univariate polynomial of degree d. Expand f in terms of Hermite polynomials as f (x) = n f (n)Hn(x/). Then

n
E[(f (A X) - Y )2]  (k/n) wi2 max(0,  -
i=1

n
|f^(kei)|2(d + 1)3d+2(1 + (/)d))2
i=1

Before proving the lemma, let us see how it proves the main theorem:

Proof of Theorem 4.2. By Lemma 5.1, Lemma B.1, and Lemma B.2 we have that for the f which minimizes the square loss among degree d polynomials, we have a variance-type lower bound

nd

E[(f (A X) - Y )2]  (1 - k/n)

|f^(kei)|2

i=1 k=1

and (using that wi2 = 1 to simplify) a bias-type lower bound

n
E[(f (A X) - Y )2]  (k/n) max(0,  -
i=1

n
|f^(kei)|2(d + 1)3d+2(1 + (/)d))2.
i=1

Let fi 2 := gives

n i=1

|f^(kei)|2.

Then

averaging

these

lower

bounds

and

simplifying

using

k

<

n/2

n

E[(f (A X) - Y )2]  (1/4) max( fi 2, k/n( - fi 2(d + 1)3d+2(1 + (/)d)))2

i=1

n
 (1/4)

2(k/n)

i=1 (1 + k/n(d + 1)3d+2(1 + (/)d))2

 (1/4)

2k

(1 + k/n(d + 1)3d+2(1 + (/)d))2

Returning to the proof of Lemma B.2, we have:
Proof of Lemma B.2. Since f has no mixed monomials, we get for the Hermite expansion that f (n) = 0 unless |supp(n)|  1. Let X := A X = Z +  where   N (0, 2I). Next observe by independence that
E[(f (X ) - Y )2] = wi2E[(fi(Xi) - Zi)2]  (k/n) wi2E[(fi(Xi) - Zi)2|Zi = 0]
ii
where the last inequality follows since there is a k/n chance that Zi  N (0, 2I), equivalently that Zi = 0. By the conditional Jensen's inequality we have
(k/n) wi2E[(fi(Xi) - Zi)2|Zi = 0]  (k/n) wi2E[(E[fi(Xi)|Zi] - Zi)2|Zi = 0].
ii
14

Under review as a conference paper at ICLR 2019

Observe that fi(Xi) =

d k=0

f

(kei )Hk (Zi /

+

/)

and

let

gi(Zi)

:=

E[fi (Xi )|Zi ]

-

Zi,

so

then

gi is a polynomial of degree d in Zi. Write the Hermite polynomial expansion of gi in terms of

Hk(Zi/) as

d

gi(x) = gi(k)Hk(Zi/),

then by Plancherel's formula

k=0

d

(k/n) wi2E[(E[g(Zi) - Zi)2|Zi = 0] = (k/n) wi2 |gi(k)|2  (k/n) wi2|gi(1)|2

i

i k=0

i

and it remains to lower bound |gi(1)|. By orthogonality and direct computation,

gi(1) = EZiN(0,)[(E[fi(Xi)|Zi] - Zi)H1(Zi/)] = - + EZiN(0,)[E[fi(Xi)|Zi](Zi/)].

Now we upper bound the last term

d

EZiN(0,)[E[fi(Xi)|Zi](Zi/)] = f (0)E[Zi/] + f (kei)EZiN(0,)[E[Hk(Zi/ +  /)|Zi](Zi/)]
k=1

d

= f (kei)EZiN(0,)[Hk(Zi/ +  /)(Zi/)]
k=1

d 1/2 d

 |f (kei)|2

EZiN(0,)[Hk(Zi/ +  /)(Zi/)]2

k=1

k=1

where the second equality is by the law of total expectation and the last inequality is Cauchy-Schwarz.

Using the recurrence relation (1), we can bound the sum of the absolute value of the coefficients of Hk(x) by kk/ k!  kk. We can also bound the moments of the absolute value of a Gaussian by EN(0,1)[||k]  kk. Therefore by Holder's inequality

1/2

EZiN(0,)[Hk(Zi/ +  /)(Zi/)]  kk sukp |EZiN(0,)[(Zi/ +  /) (Zi/)]|
=1

 kk sukp EZiN(0,)[(|Zi|/ + | |/) (|Zi|/)]
=1

 2kkk sukp(EZiN(0,)[|Zi| +1/ ] + EZiN(0,)[| | |Zi|/ )])
=1

 2kkk[max(1, (/)k)(k + 1)k+1 + kk]

 (k + 1)3k+1(1 + (/)k). Therefore by reverse triangle inequality

|g^i(1)|2  max(0,  -

n
|f^(kei)|2(d + 1)3d+2(1 + (/)d))2.
i=1

Remark B.2. Remarks on results:
We make a few remarks regarding the results in this section. Recall that 2k is the square loss of the trivial zero-estimator. Suppose as before that  = (2polylog(n)), then we see that if d = o(log n/ log log n) then the denominator of the lower bound tends to 1, hence any such polynomial estimator has a rate no better than that of the trivial zero-estimate.
It is possible to derive a similar statement to Theorem 4.2 that holds with high probability instead of in expectation for polynomials of degree o(log n/ log log n). All that is needed is to bound the contribution to the expectation from very rare tail events when the realization of the noise  is atypically large. Since the polynomials we consider are very low degree o(log n/ log log n), they can only grow at a rate of xd = xo(log(n)/ log log n); thus standard growth rate estimates (e.g. the Remez inequality) combined with the Gaussian tails of the noise can be used to show that a polynomial which behaves reasonably in the high-probability region (e.g. which has small w.h.p. error) cannot contribute a large amount to the expectation in the tail region.
15

Under review as a conference paper at ICLR 2019

C UPPER BOUNDS FOR POLYNOMIAL KERNELS

In this section, we construct polynomials achieving close to the information-theoretic optimal rate of degree only O(log2 m). Recall this is nearly optimal due to our previous lower bound of (log n).

As previously mentioned, the key technical result here will be Theorem 5.2, giving the construction of a new polynomial approximation to ReLU. Before proceeding to the proof of that theorem, we show how it implies the final result, Theorem 4.3.

Towards that, we substitute our polynomial construction for  into our ReLU neural network and derive the analogous version of Lemma A.2. First, define M = M + 2 and let

~d,,M = pd, /M ,M (x -  ) + pd, /M ,M (-x +  )

where p is the polynomial constructed in Theorem 5.2. We then have:

Lemma C.1. Suppose

,

> 0 and M

 1.

Then for all

d



d0

=

(

M 

log2(

M 

)),

for

|x|  (, M ) we have

|~d,,M (x) - x|  3 +

and for |x|   we have

|~d,,M (x)| 

Proof. By the guarantee of Theorem 5.2, we see that for for |x|   that

|~d,,M (x)|  28M

dM

 e-  d/4M

.



Thus

we

see

that

taking

d

=

(

M 

log2(

M 

))

suffices

to

make

the

latter

expression

at

most

.

Similarly for |x| >  we know that

|~d,,M (x)|  2 + 2M

4 M d + 26M

dM

 e- 

d/4M



and

taking

d

=

(

M 

log2

(

M 

))

with

sufficiently

large

constant

guarantees

the

middle

term

is

at

most  and the last term is at most .

Using

this,

we

can

show

that

if

we

use

a

polynomial

of

degree

 ((M/ log

n)

log2

m)

we

can

achieve similar statistical performance to the ReLu network:

Lemma C.2. Suppose A is µ-incoherent i.e. A A - Id   µ. Let z be an arbitrary fixed vector

such that z 1  M and |supp(z)|  k. Suppose x = Az +  where   N (0, 2Idn×n). Then

for some  = (

(1

+

µ) log m

+

µM ),

for

any

d



d0

=

(

M 

log2(M m/ 2)),

if

we

take

z^ := ~d,n,M (A x), then with high probability we have z^ - z 1  6k .

Proof. Apply Lemma C.1 with =  /m. Then we see for |x|  (, M ) we havn

|~d,,M (x) - x|  (3 + 1/m)  4

and for |x|   we have

|~d,,M (x)|   /m

Observe that

A x = z + (A A - Id)z + A .

Note that entry i of A

 is

Ai, 

where

Ai

2 2



(1

+

µ)

so

(At)i

is

Gaussian

with

variance

at

most 2(1 + µ).

By choosing  with sufficiently large constant, then applying the sub-Gaussian tail bound and union bound, with high probability all coordinates not in the true support are thresholded to at most  /m. Similarly we see that for each of the coordinates in the support, an error of at most 5 is made. Therefore z^ - z 1  5k + m( /m)  6k .

Now we have all the ingredients to prove Theorem 4.3:

16

Under review as a conference paper at ICLR 2019

Proof of Theorem 4.3. Define an estimate for Y by taking Z^d,M := ~d,n,M (A X) where  is defined as in the Lemma, and then taking Y^d,M := w, Z^d,M . Applying the previous Lemma, we get analogous versions of Theorem 4.1 by the same argument as in that theorem.

Finally, we return to the proof of the key Theorem 5.2:

Proof of Theorem 5.2. We start with the case where R = 1/2. We build the approximation in two steps. First we approximate ReLu by the following "annealed" version of ReLu, for parameters  > ,  > 0 to be optimized later:

g (x)

=

1 log(1 +


ex)

f, (x) = g(x -  ).

Observe that when we

look

at negative inputs, g(-x)

=

1 

log(1 + e-x)



1 

e-x

.

Therefore

when

x

<

0,

f (x)



1 

e-

.

For the second step,, we need to show f can be well-approximated by low-degree polynomials. In fact, because f is analytic in a neighborhood of the origin, it turns out that its optimal rate of approximation is determined exactly by its complex-analytic properties. More precisely, define D to be the region bounded by the ellipse in C = R2 centered at the origin with equation

x2 y2 a2 + b2 = 1

with

semi-axes

a

=

1 2

(

+

-1)

and

b

=

1 2

|

-

-1|;

the

focii

of

the

ellipse

are

±1.

For

an

arbitrary

function f : [-1, 1]  R, let Ed(f ) denote the error of the best polynomial approximation of degree

d in infinity norm on the interval [-1, 1] of f . Then the following theorem of Bernstein exactly

characterizes the growth rate of Ed(f ):

Theorem C.1 (Theorem 7.8.1, (DeVore and Lorentz, 1993)). Let f be a function defined on [-1, 1]. Let 0 be the supremum of all  such that f has an analytic extension on D. Then

lim sup
d

d

Ed(f )

=

1 0

For our application we need only the upper bound and we need a quantitative estimate for finite n. Following the proof of the upper bound in (DeVore and Lorentz, 1993), we get the following result:

Theorem C.2. Suppose f is analytic on the interior of D1 and |f (z)|  M on the closure of D1 .

Then

Ed(f )



2M 1 -

1

-1 n

We will now apply this theorem to g. First, we claim that g is analytic on D1 where 1 is the solution to this equation for the semi-axis of the ellipse:

1 ( - -1) =  2 2

which is

42 + 2 +  1 = 2 > 1 + /2.

To see this, first extend log to the complex plane by taking a branch cut at (-, 0]. To prove g

is analytic on D1 , we just need to prove that 1 + ez avoids (-, 0] for z  D1 . This follows

because by the definition of 1, for every z  D1 ,

(z)

<

 2

hence

(1 + ez)  1. We also see

that for z  D1 ,

|g (z )|

=

1 | log(1 

+

ez )|



1 

sup
wD1

| log(1

+

ew )|



1 (log(1


+

e )

+

)

<

6.

17

Under review as a conference paper at ICLR 2019

Therefore by Theorem C.2 we have

Ed (g )



12 (1


+ /2)-n



12 e-n/4 

where in the last step we used that 1 + x  exp(x/2) for x < 1/2 and that  > . Let g~,d denote the best polynomial approximation to g of degree d and let f~,,d = g~,d(x -  )

Thus for x  [-1 + , 0],

|ReLu(x)

-

f~,,d(x)|



|f, (x)|

+

|g~,d(x

-

)

-

g, (x

-

 )|



1 e- 

+

12 e-d/4 

Take  = d/4 and require d > 7 so that  > 1, then for x  [-1 + , 0],

|ReLu(x) - f~,,d(x)|  7

d

 e-  d/4



For x  (0, 1 -  ] we have by the 1-Lipschitz property of g and calculus that

|x

-

f, (x)|





+

|x

-

g (x)|





+

log 

2

so

|ReLu(x)

-

f~,,d(x)|



|x

-

f,

(x)|

+

|g~,d(x

-



)

-

g,

(x

-



)|





+

log 

2

+

12 

e-d/4 .

Plugging in our value of  and using log 2  1 gives

|ReLu(x) - f~,,d(x)|   +

4 +6

d

 e-  d/4

d 

Now the result for general R follows by taking pd(x) = 2Rf~,,d(x/2R), since 2R·ReLu(x/2R) = ReLu(x) and [-1/2, 1/2]  [-1 + , 1 -  ].

18

