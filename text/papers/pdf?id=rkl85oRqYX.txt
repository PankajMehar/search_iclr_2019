Under review as a conference paper at ICLR 2019
EXPLOITING INVARIANT STRUCTURES FOR COMPRESSION IN NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Modern neural networks often require deep compositions of high-dimensional nonlinear functions (wide architecture) to achieve high test accuracy, and thus can have overwhelming number of parameters. Repeated high cost in prediction at test-time makes neural networks ill-suited for devices with constrained memory or computational power. We introduce an efficient mechanism, reshaped tensor decomposition, to compress neural networks by exploiting three types of invariant structures: periodicity, modulation and low rank. Our reshaped tensor decomposition method exploits such invariance structures using a technique called tensorization (reshaping the layers into higher-order tensors) combined with higher order tensor decompositions on top of the tensorized layers. Our compression method improves low rank approximation methods and can be incorporated to (is complementary to) most of the existing compression methods for neural networks to achieve better compression. Experiments on LeNet-5 (MNIST), ResNet-32 (CIFAR10) and ResNet-50 (ImageNet) demonstrate that our reshaped tensor decomposition outperforms (5% test accuracy improvement universally on CIFAR10) the state-of-the-art low-rank approximation techniques under same compression rate, besides achieving orders of magnitude faster convergence rates.
1 INTRODUCTION
Modern neural networks achieve unprecedented accuracy over many difficult learning problems at the cost of deeper and wider architectures with overwhelming number of model parameters. The large number of model parameters causes repeated high cost in test-time as predictions require loading the network into the memory and repeatedly passing the unseen examples through the large network. Therefore, the model size becomes a practical bottleneck when neural networks are deployed on constrained devices, such as smartphones and IoT cameras.
Compressing a successful large network (i.e., reducing the number of parameters), while maintaining its performance, is non-trivial. Many approaches have been employed, including pruning, quantization, encoding and knowledge distillation (see appendix A for a detailed survey). A complementary compression technique, on top of which the aforementioned approaches can be used, is low rank approximation. For instance, singular value decomposition (SVD) can be performed on fully connected layers (weights matrices) and tensor decomposition on convolutional layers (convolutional kernels). Low rank approximation methods can work well and reduce the number of parameters by a factor polynomial in the dimension only when the weight matrices or convolutional kernels have low rank structures, which might not always hold in practice.
We propose to exploit additional invariant structures in the neural network for compression. A set of experiments on several benchmark datasets justified our conjecture (Section 4): large neural networks have some invariant structures, namely periodicity, modulation and low rank, which make part of the parameters redundant. Consider this toy example of a vector with periodic structure [1,2,3,1,2,3,1,2,3] or modulated structure [1,1,1,2,2,2,3,3,3] in Figure 1. The number of parameters needed to represent this vector, naively, is 9. However if we map or reshape the vector into a higher order object, for instance, a matrix [1,1,1;2,2,2;3,3,3] where the columns of the matrix are repeated, then apparently this reshaped matrix can be decomposed into rank one without losing information. Therefore only 6 parameters are needed to represent the original length-9 vector.
1

Under review as a conference paper at ICLR 2019

{Invariant structures

[1, 2, 3, 1, 2, 3, 1, 2, 3] Periodic structure
[1, 1, 1, 2, 2, 2, 3, 3, 3]

Modulated structure

reshape

[ ]1, 1, 1 2, 2, 2 3, 3, 3

=

Low-rank structure

[ ]1 2 3

Figure 1: A toy example of invariant structures. The periodic and modulated structures are picked out by exploiting the low rank structure in the reshaped matrix.

Although the invariant structures in large neural networks allow compression of redundant parameters, designing a sophisticated way of storing a minimal representation of the parameters (while maintaining the expressive power of the network) is nontrivial. To solve this problem, we proposed a new framework called reshaped tensor decomposition (RTD) which has three phases:

1. Tensorization. We reshape the neural network layers into higher-order tensors.
· For instance, consider a special square tensor convolutional kernel T  RD×D×D×D, we re-
m
shape T into a higher m-order tensor T   RD × · · · × D, so called tensorization. Tensorization guarantees that the reshaped tensor has a smaller dimension, i.e., D < D ( as Dm = D4 and m > 4). Similar ideas apply to general convolutional kernels.

2. Higher-order tensor decomposition. We deploy tensor decomposition (a low rank approximation technique detailed in section 3) on the tensorized layers to exploit the periodic, modulated as well as low rank structures in the original layers.

· A rank-R tensor decomposition of the above 4-order tensor T will result in R number of

components (each contains 4D parameters), and thus 4DR number of parameters in total -- smaller than the original D4 number of parameters if R is small.

· A rank-R tensor decomposition of the above reshaped m-order kernel tensor T  maps the layer

into m + 1 narrower layers. The decomposition will result in R number of components with

mD

4 m

parameters

and

thus

mD

4 m

R

in

total

--

better

than

the

4DR

number of

parameters

required by doing tensor decomposition on the original tensor T (D is usually large).

Now the weights of the tensorized neural networks are the components of the tensor, i.e., result of the tensor decomposition. However, decomposing higher order tensors is challenging and known methods are not guaranteed to converge to the minimum error decomposition (Hillar & Lim, 2013). Therefore fine tuning is needed to achieve high performance.

3. Data reconstruction-based sequential tuning. We fine-tune the parameters using a data reconstruction-based sequential tuning (Seq) method which minimizes the difference between training output of the uncompressed and compressed, layer by layer. Our Seq tuning is a novel approach inspired by a sequential training method proved to converge faster and achieve guaranteed accuracy using a boosting framework (Huang et al., 2017). Unlike traditional end-to-end (E2E) backpropagation through the entire network, Seq tunes individual compressed "blocks" one at a time, reducing the memory and complexity required during compression.

Summary of Contributions
· Novel compression schemes. We propose new reshaped tensor decomposition methods to exploit invariant structures for compressing the parameters in neural networks. By first tensorizing the kernel/weights into a higher-order tensor, our reshaped tensor decomposition discovers extra invariant structures and therefore outperform existing "low rank approximation methods".
· Efficient computational framework. We introduce a system of tensor algebra that enables efficient training and inference for our compressed models. We show that a tensor decomposition on the parameters is equivalent to transforming one layer into multiple narrower sublayers in the compressed model. Therefore, other compression techniques (e.g. pruning) can be applied on top of our method by further compressing the sublayers returned by our method.
· Sequential knowledge distillation. We introduce Seq tuning to transfer knowledge to a compressed network from its uncompressed counterpart by minimizing the data reconstruction error block by block. With our strategy, only one block of the network is loaded into the GPU "at each

2

Under review as a conference paper at ICLR 2019

time", therefore allowing compression of large networks on moderate devices. Furthermore, we show empirically that our strategy converges much faster than normal end to end tuning.
· Comprehensive experiments. We perform extensive experiments to demonstrate that our reshaped tensor decomposition outperforms state-of-the-art low-rank approximation techniques (obtains 5% higher accuracy on CIFAR10 under same compression rates). Our experiments also show that our method scales to deep residual neural networks on large benchmark dataset, ImageNet.
Organization of the paper Section 2 introduces tensor operations, tensor decompositions and their representations in tensor diagrams. In Section 3, we introduce convolutional layer diagram, review existing low-rank approximation techniques, and propose three new schemes to exploit additional invariant structures. In Section 4 and Appendix B, we demonstrate by extensive experiments that our compression obtains higher accuracy than existing low-rank approximation techniques. Appendix A surveys compression techniques and discuss how our method is related or complementary to existing techniques. For simplicity, we will use tensor diagrams throughout the text. However we provide a detailed appendix where the tensor operations are mathematically defined.

2 TENSOR PRELIMINARIES

a vI

Notations An m-dimensional array T is defined as an m-order tensor T  RI0×···×Im-1 . Its (i0, · · · , in-1, in+1, · · · , im-1)th mode-n fiber, a vector along the nth axis, is denoted as

(a) Scalar

(b) Vector K

T .i0,··· ,in-1,:,in+1,··· ,im-1

J M IJ T I

Tensor Diagrams. Following the convention in quantum physics Cichocki et al. (2016), Figure 2 introduces graphical

(c) Matrix

(d) Tensor

representations for multi-dimensional objects. In tensor diagrams, an array (scalar/vector/matrix/tensor) is represented as a node in the graph, and its order is denoted by the number of edges extending from the node, where each edge corresponds to one mode (whose

Figure 2: Diagram of a  R, v  RI , M  RI×J and T  RI×J×K .

dimension is denoted by the number associated to the edge) of the multi-dimensional array.

I2 J2 I2 J2

I1 X I0

J1 Y

I0 = J1

J0
=

T1

I1 J0

(a)

Mode-(0,1) tensor contraction X ×01 Y  T 1

T1 i1 ,i2,j0 ,j2

=

r Xr,i1,i2 Yj0,r,j2

I2 I2

I1 X I0

J0 M

J1 = I1

T 2 J1

I0 = J0

(b)

Mode-0 tensor product/multiplication

X

×0

M



T

2:

T2 j0 ,i1,i2

=

r Xr,i1,i2 Mj0,r

I2
I1 X I0

J2
J1 Y

J0 = I0

I2 J2 T3

I1 J0

(c)

Mode-(0,1) tensor convolution X 10 Y  T 3

T3 :,i1,i2 ,j0,j2

= X:,i1,i2

 Yj0,:,j2

I2 J2 I2 J2

I1 X I0

J1 Y

I0 = J1

J0 = I0

T4

I1 J0

(d)

Mode-(0,1) tensor partial outer product

X

10

Y



T

4:

T4 r,i1,i2 ,j0 ,j2

=

Xr,i1,i2 Yj0,r,j2

Figure 3: Tensor operation illustration. Examples of tensor operations in which M  RJ0×J1 , X  RI0×I1×I2 and Y  RJ0×J1×J2 are input matrix/tensors, and T 1  RI1×I2×J0×J2 , T 2  RJ0×I1×I2 , T 3  RI0 ×I1×I2×J0×J2 and T 4  RI0×I1×I2×J0×J2 are output tensors of correspond-
ing operations. Similar definitions apply to general mode-(i, j) tensor operations.

Tensor Operations. In Figure 3, we use some simple examples to introduce four types of tensor operations, which are higher-order generalization of their matrix/vector counterparts, on input tensors X and Y and input matrix M. In tensor diagram, an operation is represented by linking edges from the input tensors, where the type of operation is denoted by the shape of line that connects the nodes: solid line stands for tensor contraction / tensor multiplication, dashed line represents

3

Under review as a conference paper at ICLR 2019

tensor convolution, and curved line is for tensor partial outer product. The rigorous definitions of high-order general tensor operations are defined in Appendix D.

Tensor Decompositions. We introduce generalized tensor decomposition as the reverse mapping of the general tensor operations (detailed in Appendix F): given a set of operations and a tensor, the generalized tensor decomposition recovers the factors/components such that the operations on these factors result in a tensor approximately equal to the original one. Several classical types of tensor decompositions (such as CANDECOMP/PARAFAC (CP), Tucker (TK) and Tensor-train (TT) decompositions) are introduced in Appendix F, and their applications on the convolutional kernel in Figure 4a (defined in Section 3) are illustrated as tensor diagrams in Figures 4b, 4c and 4d.

H SKT
W (a) Original kernel

H KC W
R
KS KT ST
(b) CP

H S KS Rs KC Rt KT T
W
(c) Tucker (TK)

S KS Rs

H R
KH

W Rt
KW

T
KT

(d) Tensor-train (TT)

H S0 T0 S1 K T1
S2 T2 W
(e) Tensorized kernel

W H KC
S0 K0

T2 K2 S2
R K1 T1

T0 S1
(f) r-CP (m=3)

S0 T2

P0 R0s H

Q2 R2t

S1 P1

R1s

C R1t Q1

T1

R2s W

R0t

P2 Q0

S2 T0

(g) r-TK (m=3)

S0 K0 R0

S1 K1 R1

S2 K2 R2

H KC

T0 S1 T2
(h) r-TT (m=3)

W

Figure 4: Diagrams of kernel decompositions. Figure (b) (c) and (d) are three types of plain tensor decomposition for a traditional convolutional kernel K in (a). Figure (f) (g) and (h) are three types of reshaped tensor decomposition for our tensorized kernel Kin (e) where the reshaping order m  Z is chosen to be 3 for illustrative simplicity.

3 COMPRESSING CONVOLUTIONAL LAYER BY TENSOR DECOMPOSITIONS

A standard convolutional layer in neural networks is parameterized by a 4-order kernel K  RH×W ×S×T where H, W are height/width of the filters, and S, T are the numbers of input/output channels. The layer maps a 3-order input tensor U  RX×Y ×S (with S number of feature maps of height X and width Y ) to another 3-order output tensor V  RX×Y ×T (with T number of feature maps of height X and width Y ) according to the following equation:

S-1

Vx,y,t =

Ki,j,s,t Ui+dx,j+dy,s

s=0 i,j

(3.1)

where d is the stride of the convolution. With HW ST parameters, it takes O(HW ST XY ) operations (FLOPs) to compute the output V. The diagram of the convolutional layer is in Figure 5a.

Plain Tensor Decomposition (PD) Traditional techniques compress a convolutional layer by directly factorizing the kernel K using tensor decompositions Jaderberg et al. (2014); Lebedev et al.
(2014); Kim et al. (2015), such as CANDECOMP/PARAFAC (CP), Tucker (TK) and Tensor-train (TT) decompositions. For example, consider a Tensor-train decomposition on K, the kernel can be factorized and stored as KS  RS×Rs , KH  RRs×H×R, KW  RR×W ×Rt and KT  RRt×T , which only requires (SRs + HRsR + W RtR + T Rt) parameters as illustrated in Figure 4d. The decomposition is rigorously defined element-wisely as

Rs-1 R-1 Rt-1

Ki,j,s,t =

KsS,rs KrHs,i,r KrW,j,rt KrTt,t

rs=0 r=0 rt=0

(3.2)

We defer the details of using CP and TK to Appendix G, although their tensor diagrams are illustrated in Figures 4b, 4c and 4d and their complexities are summarized in Tables 1 and 10.

4

Under review as a conference paper at ICLR 2019

U

X S
H

Y W

K

T
(a) Uncompressed

U S XY
HW Rs R Rt KS KH KW KT
T
(b) Compression via TT

U Y

S0 S1

X S2

HW

K20 R0 K1 R1 K2

KC

R2

T0 T1 T2

(c) Compression via r-TT (m = 3)

Figure 5: Convolutional layer diagram. Input U is passed through the layer kernel K. The forward propogation operation of an uncompressed layer, a plain tensor decomposition compressed layer and our reshaped tensor decomposition compressed layer are illustrated in (a), (b) and (c) respectively.

Motivation of Tensorization ­ Invariant Structures Consider a matrix M  RL2×L2 with M = a  b, a, b  RL2, where a = [c; · · · ; c] is periodic repetition of the same vector c  RL and b = [d01; d11; · · · ; dL-11] is a modulated version of another vector d  RL. Obviously, M as a rank-1 matrix can be represented by two length-L2 vectors a and b, resulting in a total of 2L2 parameters. However, if we reshape the matrix M into a 4-order tensor T  RL×L×L×L, it can be factorized by CP decomposition as T = 1  c  d  1 (1  RL), and represented by four length-L vectors, requiring only 4L parameters. We refer the process of reshaping an array into a higher-order tensor as tensorization, and the use of tensor decomposition following tensorization as reshaped tensor decomposition (RTD). Therefore, the example above demonstrates that RTD discovers additional invariant structures that baseline plain tensor decomposition (PD) fails to identify.

Reshaped Tensor Decomposition (RTD) Inspired by this intuition, we tensorize the convolutional kernel K into a higher-order tensor K  .H×W ×S0×···×Sm-1×T0×···×Tm-1 Correspondingly,
we define an equivalent tensorized convolutional layer to Equation 3.1, by further reshaping input U and output V into higher-order tensors U   RX×Y ×S0×···×Sm-1 and V   RX×Y ×T0×···×Tm-1 .

S0-1 Sm-1-1

Vx ,y,t0,··· ,tm-1 =

···

K U 
i,j,s0 ··· ,sm-1,t0,··· ,tm-1 i+dx,j+dy,s0,··· ,sm-1

s0=0

sm-1=0 i,j

(3.3)

Now we can compress the convolutional layer by factorizing the tensorized kernel K by tensor decompositions, and name the schemes using CP, Tucker and Tensor-train as reshaped CP (r-CP),
reshaped Tucker (r-TK) and reshaped Tensor-train (r-TT) respectively. For example, consider a r-TT decomposition on K, the tensorized kernel can now be stored in m+1 factors {K0, · · · , Km-1, KC } (a special example of m = 3 is illustrated in Figure 4h). The decomposition scheme is rigorously defined element-wisely as

R0-1

Rm-1 -1

K =
i,j,s0,··· ,sm-1,t0,··· ,tm-1

···

Ks00 ,r0 ,t0

K1
r0 ,s1 ,t1 ,r1

· · · KiC,j,rm-1

r0=1

rm-1 =0

(3.4)

where Kl

 RRl-1×Sl×Tl×Rl , l  [m] and KC

 RRm-1×H×W .

Assuming Sl

=

S

1 m

,

Tl

=

T

1 m

,

Rl

=

R, l



[m],

we

require

O(m(S

T

)

1 m

R2

+

HW R)

parameters.

We

defer

the

detailed

descriptions of r-CP and r-TK to Appendix I, but we illustrate their tensor diagrams in Figures 4f

and 4g and summarize their complexities in Tables 1 and 12.

Sequential Tuning Tensor decompositions provide weight estimates in the tensorized convolutional layers. However, decomposing higher order tensors is challenging and known methods are not guaranteed to converge to the minimum error decompositions (Hillar & Lim, 2013). Therefore fine tuning is needed to restore high performance. Analogous to Huang et al. (2017), our strategy of data reconstruction-based sequential tuning (Seq) sequentially fine-tunes the parameters layer by layer, using backpropagation to minimize the difference between the outputs from uncompressed layer V and the tensorized compressed layer V.

Computational Complexity As shown in Figure 5c, reshaped tensor decomposition maps a layer into multiple (and thus deeper) narrower layers, each of which has width Rl that is usually smaller

5

Under review as a conference paper at ICLR 2019

than the original width T . We design efficient algorithms for forward/backward propagations for

prediction/fine-tuning on these modified layers using tensor algebra. (1) Forward propagation:

computing the output V, given the input U  and kernel factors {Kl}lm=-01. tion: computing the derivative of loss function L with respect to (w.r.t.)

(2) Backward propagathe input L/U and

kernel factors {L/Kl}lm=-01, given the derivative w.r.t. the output L/V.

A naive forward and backward propagation mechanism is to explicitly reconstruct the original kernel K using the factors {Kl}lm=-01, which however makes propagations highly inefficient as shown in Appendix F, G and I. Alternatively, we propose a framework where both propagations are
evaluated efficiently without explicitly forming or computing the original kernel. The key idea is to interact the input U with each of the factors Kl individually. Taking r-TT as an example, we plug the decomposition 3.4 into 3.3, then the computation of V is reduced into m + 1 steps:

Rl-1-1 Sl-1

Uxl ,y,··· ,rl =

Kl-1
rl-1,sl ,tl,rl

U ,l-1
x,y,sl,··· ,sm-1,t0,··· ,tl-1,rl-1

l

= 1, · · ·

,m

rl-1=0 sl=0

Rm-1 -1

Vx,y,t0,··· ,tm-1 =

K Um m
rm-1,i,j i+dx,j+dy,t0,··· ,tm-1,rm-1

rm-1=0 i,j

(3.5) (3.6)

where Ul is the intermediate result after interacting with Kl-1, and U0 = U. Each step in 3.5

takes

O(max(S,

T

)1+

1 m

RX

Y

)

operations,

while

the

last

step

in

3.6

requires

O(HW T RXY

)

1.

Therefore,

the

time

complexity

for

the

forward

pass

is

O((m

max(S,

T

)1+

1 m

R

+

HW T )RXY

),

more efficient than uncompressed O(HW ST XY ) as R  S, T . Backpropagation is derived and

analyzed in Appendix I, and the analyses of other decomposition are in Appendix G and I, but we

summarize their computational complexities in Table 1, 10 and 12.

Parallel Computational Complexity Tensor algebra allows us to implement the propagations 3.5
and 3.6 in parallel given enough computational resources, further speeding up prediction. The parallel tine complexities of prediction 2 with our RTD implementation is displayed in Table 2. The
prediction time complexity of RTD outperforms the baseline PD, whereas the PD outperforms the original convolutional layers as R  N and m  3.

Decomp. original
CP TK TT
r-CP r-TK
r-TT

O(# of parameters)

k2N 2

(k2 + 2N )R

(k2R + 2N )R

2(kR + N )R

(k2

+

mN

2 m

)R

(k2R2m-1 + 2mN )R

(mN

2 m

R

+

k2

)R

O(# of forward ops.)

k2N 2D2

(k2 + 2N )RD2

(k2R + 2N )RD2

2(kR + N )RD2

(mN

1+

1 m

+ k2N )RD2

(k2R2m-1 + 2mN )RD2

(mN

1+

1 m

R

+

k2N

)RD2

O(# of backward ops.)

N 2D4

(D2 + 2N )RD2

(D2R + 2N )RD2

2(DR + N )RD2

(mN

1+

1 m

+

N D2)RD2

(R2m-1D2 + 2mN )RD2

(mN

1+

1 m

R

+

N

D2)RD2

Table 1: Number of parameters and operations required by a compressed convolutional layer by various types of tensor decompositions (for the special case of X = Y = X = Y  = D, S = T =
N , H = W = k and D  k). General settings are summarized in Tables 10 and 12.

Decomp. O(parallel complexity) Decomp. O(parallel complexity)

CP

N + k2 + R

r-CP

mN

1 m

+ k2R

TK

N + k2R + R

r-TK

2mN

1 m

+ k2Rm

TT

N + 2kR + R

r-TT

mN

1 m

R

+

k2R

Table 2: Parallel time complexity of forward pass using various types of tensor decompositions on

convolutional layers. The uncompressed parallel complexity of forward pass is O(k2N ).

1The optimal complexity of tensor algebra is NP complete in general Lam et al. (1997), therefore the complexity presented in this paper is the complexity of our implementation.
2Assuming adding n terms takes n time in parallel for memory efficiency, although it could be O(log n).
6

Under review as a conference paper at ICLR 2019

4 EXPERIMENTS

We evaluate our reshaped tensor decomposition method on the state-of-art networks for a set of benchmark datasets: we evaluate convolutional layer compression on ResNet-32 He et al. (2016) for CIFAR-10; we evaluate fully-connected layer compression on MNIST; and we evaluate the scalability of our compression method on ResNet-50 He et al. (2016) for ImageNet (2012) dataset. The baseline we compare against is the state-of-the-art low-rank approximation methods called plain tensor decomposition (PD), as other compression methods are complementary and can be used on top of our reshaped tensor decomposition (RTD) method. All types of tensor decomposition (CP, TK, and TT) in baseline PD will be evaluated and compared with corresponding types of tensor decomposition (r-CP, r-TK and r-TT) in our RTD method.
Our primary contribution is to introduce a new framework, reshaped tensor decomposition, that picks out additional invariance structure such as periodicity and modulation, which the low rank approximation baseline, plain tensor decomposition, fails to find. Now we demonstrate that our RTD maintains high accuracy even when the networks are highly compressed on CIFAR-10. We refer to traditional backpropogation-based tuning of the network as end-to-end (E2E) tuning, and to our proposed approach that trains each block individually as data reconstruction-based sequential (Seq) tunning.

Decomp.
SVD CP TK TT

Compression rate 5% 10% 20% 40%
83.09 87.27 89.58 90.85 84.02 86.93 88.75 88.75 83.57 86.00 88.03 89.35 77.44 82.92 84.13 86.64

Decomp.
r-TR r-CP r-TK r-TT

Compression rate 2% 5% 10% 20%
- 80.80 - 90.60 85.7 89.86 91.28 61.06 71.34 81.59 87.11 78.95 84.26 87.89 -

The reshaped tensor ring (r-TR) results are cited from Wang et al. (2018), and the accuracy of 80.8% is achieved by 6.67% compression rate.

Table 3: Percentage test accuracy of baseline PD with E2E tuning vs. our RTD with Seq tuning on CIFAR10. The uncompressed ResNet-32 achieves 93.2% accuracy with 0.46M parameters.

Our algorithm achieves 5% higher accuracy than baseline on ResNet-34 CIFAR10. As in Ta-

ble 3, using baseline CP decomposition with end-to-end tuning, ResNet-34 is compressed to 10% of

its original size, reducing the accuracy from 93.2% to 86.93%. Our reshaped tensor decomposition

using r-CP, paired with Seq tuning, increases the accuracy to 91.28% with the same 10% compres-

sion rate -- a performance loss of 2% with only 10% of the number of parameters. It achieves

further aggressive compression -- a performance loss of 6% with only 2% of the number of

parameters. We observe similar trends (higher compression and higher accuracy) for Tensor-train

decomposition. The structure of the Tucker decomposition (see section I) makes it less effective

with very high compression, since the "internal structure" of the network reduces to very low rank,

which may lose necessary information. Increasing the network size to 20% of the original provides

reasonable performance on CIFAR-10 for Tucker as well.
Table 3 shows that RTD with Seq tuning outperforms PD with end-to-end tuning. Now we address the following question: is one factor (Seq tuning or RTD) primarily re-

1 0.9 0.8 0.7 0.6

Seq-CP Seq-TT Seq-TK E2E-CP E2E-TK E2E-TT

sponsible for increased performance, or is the benefit due 0.5

to synergy between the two?

0.4

0.3

Test Error

0.2

Seq tuning, reshaped tensor decomposition, or both? 0.1

(1) We present the effect of different tuning methods on accuracy in Table 4. Other than at very high compres-

0 0 0.5 1 1.5 2 2.5 3 3.5
# Gradient Updates (×1011)

sion rate (5% column in Table 4), Seq tuning (Seq) consis- Figure 6: Convergence rate for Seq vs. tently outperforms end-to-end (E2E) tuning. In addition, E2E tuning on CIFAR10. Seq tuning is also much faster and leads to more stable

convergence compared to end-to-end tuning. Figure 6 plots the compression error over the number

of gradient updates for various tuning methods. (2) We present the effect of different compression

methods on accuracy in Table 5. Interestingly, if our RTD is used, the test accuracy is restored for

7

Under review as a conference paper at ICLR 2019

even very high compression ratios 3. These results confirm the existence of extra invariant structure in the parameter space of deep neural networks. Such invariant structure is picked up by our proposed aproach, tensorization combined with low rank approximation (i.e., our RTD), but not by low rank approximation itself (i.e., baseline PD). Therefore, our results show that RTD and Seq tuning are symbiotic, and both are necessary to simultaneously obtain a high accuracy and a high compression rate.

Decomp.
SVD CP TK TT

5% Seq E2E 74.04 83.09 83.19 84.02 80.11 83.57 80.77 77.44

Compression rate 10% 20% Seq E2E Seq E2E 85.28 87.27 89.74 89.58 88.50 86.93 90.72 88.75 86.75 86.00 89.55 88.03 87.08 82.92 89.14 84.13

40% Seq E2E 91.83 90.85 89.75 88.75 91.3 89.35 91.21 86.64

Table 4: Percentage accuracy of our Seq vs. baseline E2E tuning using PD on CIFAR10.

Decomp.
CP TK TT

Compression rate 5% 10% 83.19 88.50 80.11 86.73 80.77 87.08

Decomp.
r-CP r-TK r-TT

Compression rate 5% 10% 89.86 91.28 71.34 81.59 84.26 87.89

Table 5: Percentage accuracy of our RTD vs. baseline PD using Seq tuning on CIFAR10.

Scalability Finally, we show that our methods scale to state-of-the-art large networks, by evaluating performance on the ImageNet 2012 dataset on a 50-layer ResNet (uncompressed with 76.05% accuracy). Table 6 shows the accuracy of RTD (TT decomposition) with Seq tuning compared to plain tensor decomposition with E2E tuning and the uncompressed network, on ResNet-50 with 10% compression rate. Table 6 shows that Seq tuning of RTD is faster than the alternative. This is an important result because it empirically validates our hypotheses that (1) our RTD compression captures the invariance structure of the ResNet (with few redundancies) better and faster than the baseline PD compression, (2) data reconstruction Seq tuning is effective even on the largest networks and datasets, and (3) our proposed efficient RTD compression methods scale to the state-of-the-art neural networks.

# epochs 0.2 0.3 0.5 1.0 2.0

# samples 0.24M 0.36M 0.60M 1.20M 2.40M

Uncompressed # params. = 25M
4.22 6.23 9.01 17.3 30.8

TT (E2E) # params. = 2.5M
2.78 3.99 7.48 12.80 18.17

r-TT (Seq) # params. = 2.5M
44.35 46.98 49.92 52.59 54.00

Table 6: Convergence of percentage accuracies of uncompressed vs. PD (TT decomposition) vs. RTD (r-TT decomposition) achieving 10% compression rate for ResNet-50 ImageNet.

5 CONCLUSION AND PERSPECTIVES
We describe an efficient mechanism for compressing neural networks by tensorizing network layers. We implement tensorized decompositions to find approximations of the tensorized kernel, potentially preserving invariance structures missed by implementing decompositions on the original kernels. We extend vector/matrix operations to their higher order tensor counterparts, providing systematic notations and libraries for tensorization of neural networks and higher order tensor decompositions. As a future step, we will explore optimizing the parallel implementations of the tensor algebra.
3Note that Tucker remains an exception for aggressive compression due to the low rank internal structure that we previously discussed.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pp. 2654­2662, 2014.
Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2857­2865, 2015.
Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.
François Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.
Andrzej Cichocki, Namgil Lee, Ivan V Oseledets, Anh Huy Phan, Qibin Zhao, and D Mandic. Low-rank tensor networks for dimensionality reduction and large-scale optimization problems: Perspectives and challenges part 1. arXiv preprint arXiv:1609.00893, 2016.
Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in neural information processing systems, pp. 1269­1277, 2014.
Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate tensorization: compressing convolutional and fc layers alike. arXiv preprint arXiv:1611.03214, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630­645. Springer, 2016.
Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM (JACM), 60(6):45, 2013.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Furong Huang, Jordan Ash, John Langford, and Robert Schapire. Learning deep resnet blocks sequentially using boosting theory. arXiv preprint arXiv:1706.04964, 2017.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. arXiv preprint arXiv:1511.06530, 2015.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3): 455­500, 2009.
Jean Kossaifi, Aran Khanna, Zachary Lipton, Tommaso Furlanello, and Anima Anandkumar. Tensor contraction layers for parsimonious deep nets. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on, pp. 1940­1946. IEEE, 2017a.
Jean Kossaifi, Zachary C Lipton, Aran Khanna, Tommaso Furlanello, and Anima Anandkumar. Tensor regression networks. arXiv preprint arXiv:1707.08308, 2017b.
Chi-Chung Lam, P Sadayappan, and Rephael Wenger. On optimizing a class of multi-dimensional loops with reductions for parallel execution. Parallel Processing Letters, 7(2):157­168, 1997.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014.
9

Under review as a conference paper at ICLR 2019
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Michael Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks through ffts. arXiv preprint arXiv:1312.5851, 2013.
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. In Advances in Neural Information Processing Systems, pp. 442­450, 2015.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295­ 2317, 2011.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. In Advances in Neural Information Processing Systems, pp. 3088­3096, 2015.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In AAAI, volume 4, pp. 12, 2017.
Wenqi Wang, Yifan Sun, Brian Eriksson, Wenlin Wang, and Vaneet Aggarwal. Wide compression: Tensor ring nets. learning, 14(15):13­31, 2018.
Bichen Wu, Forrest N Iandola, Peter H Jin, and Kurt Keutzer. Squeezedet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving. In CVPR Workshops, pp. 446­454, 2017.
Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for video classification. arXiv preprint arXiv:1707.01786, 2017.
Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1476­1483, 2015.
Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and Jian Sun. Efficient and accurate approximations of nonlinear convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1984­1992, 2015.
10

Under review as a conference paper at ICLR 2019
Appendix: Exploiting Invariant Structures for Compression in Neural Networks
A RELATED WORKS
A recent survey Cheng et al. (2017) reviews state-of-the-art techniques for compressing neural networks, in which they group the methods into four categories: (1) low-rank factorization; (2) design of compact filters; (3) knowledge distillation; and 4) parameters pruning, quantization and encoding. Generally, our decomposition schemes fall into the category of low-rank factorization, but these schemes also naturally lead to novel designs of compact filters in the sublayers of the compressed network. On the other hand, our strategy of sequential tuning is an advanced scheme of knowledge distillation that transfers information from pre-trained teacher network to compressed student network block by block. Furthermore, our method is complementary to the techniques of parameters pruning, quantization and encoding, which can be applied on top of our method by further compressing the parameters in the sublayers returned by tensor decomposition.
· Low-rank Factorization. Low-rank approximation techniques have been used for a long time to reduce the number of parameters in both fully connected and convolutional layers. Pioneering papers propose to flatten/unfold the parameters in convolutional layer into matrices (a.k.a matricization), followed by (sparse) dictionary learning or matrix decomposition (Jaderberg et al., 2014; Denton et al., 2014; Zhang et al., 2015). Subsequently in Lebedev et al. (2014); Kim et al. (2015), the authors show that it is possible to compress the tensor of parameters directly by standard tensor decomposition (in particular CP or Tucker decomposition). The groundbreaking work (Novikov et al., 2015) demonstrates that the parameters in fully connected layer can be efficiently compressed by tensor decomposition by first reshaping the matrix of parameters into higher-order tensor, and the idea is later extended to compress LSTM and GRU layers in recurrent neural networks (Yang et al., 2017). Concurrent to Wang et al. (2018), our paper extends this basic idea to convolutional layer by exploiting the invariant structures among the filters. Different from Wang et al. (2018) that only focuses Tensor-ring decomposition, we investigates, analyzes and implements a boarder range of decomposition schemes, besides other benefits discussed below.
· Design of Compact Filters. These techniques reduce the number of parameters by imposing additional constraints on linear layers (fully connected or convolutional). For example, the matrix of parameters in fully connected layer is restricted to circular (Cheng et al., 2015), Toeplitz/Vandermonde/Cauchy (Sindhwani et al., 2015), or multiplication of special matrices (Yang et al., 2015). Historically, convolutional layer is proposed as a compact design of fully connected layer, where spatial connections are local (thus sparse) with repeated weights. Recent research further suggests to use more compact convolutional layers, such as 1 × 1 convolutional layer (Szegedy et al., 2017; Wu et al., 2017)(where each filter is simply a scalar), and depthwise convolutional layer (Chollet, 2016) (where connections between the feature maps are also sparse). In our paper, we show that the sublayers returned by our decomposition schemes are in fact 1 × 1 depthwise convolutional layers, combing advantages from both designs above.
· Knowledge Distillation. The algorithms of knowledge distillation aim to transfer information from a pre-trained teacher network to a smaller student network. In Ba & Caruana (2014); Hinton et al. (2015), the authors propose to train the student network supervised by the logits (the vector before softmax layer) of the teacher network. Romero et al. (2014) extends the idea to matching the outputs from both networks at each layer, up to an affine transformation. Our Seq tuning strategy is therefore similar to Romero et al. (2014), but we use identical mapping instead of affine transformation, and train the compressed network block by block.
· Pruning, Quantization and Encoding. Han et al. (2015) proposes a three-step pipeline to compress a pre-trained network, by (1) pruning uninformative connections, (2) quantizing the remaining weights and (3) encoding the discretized parameters. Since our decomposition schemes effectively transform one layer in the original network into the multiple sublayers, this pipeline can be applied by further compressing all sublayers. Therefore, our method is complementary (and can be used independently) to the techniques in this pipeline.
11

Under review as a conference paper at ICLR 2019

B SUPPLEMENTARY EXPERIMENTS

Convergence Rate Compared to end-to-end, an ancillary benefit of Seq tuning is much faster and leads to more stable convergence. Figure 6 plots compression error over number of gradient updates for various methods. (This experiment is for PD with 10% compression rate.) There are three salient points: first, Seq tuning has very high error in the beginning while the "early" blocks of the network are being tuned (and the rest of the network is left unchanged to tensor decomposition values). However, as the final block is tuned (around 2 × 1011 gradient updates) in the figure, the errors drop to nearly minimum immediately. In comparison, end-to-end tuning requires 50­100% more gradient updates to achieve stable performance. Finally, the result also shows that for each block, Seq tuning achieves convergence very quickly (and nearly monotonically), which results in the stair-step pattern since extra tuning of a block does not improve (or appreciably reduce) performance.

Performance on Fully-Connected Layers An extra advantage of reshaped tensor decomposition compression is that it can apply flexibly to fully-connected as well as convolutional layers of a neural network. Table 7 shows the results of applying reshaped tensor decomposition compression to various tensor decompositions on a variant of LeNet-5 network LeCun et al. (1998). The convolutional layers of the LeNet-5 network were not compressed, trained or updated in these experiments. The uncompressed network achieves 99.31% accuracy. Table 7 shows the fully-connected layers can be compressed to 0.2% losing only about 2% accuracy. In fact, compressing the dense layers to 1% of their original size reduce accuracy by less then 1%, demonstrating the extreme efficacy of reshaped tensor decomposition compression when applied to fully-connected neural network layers.

Method r-CP r-TK r-TT

Compression rate 0.2% 0.5% 1% 97.21 97.92 98.65 97.71 98.56 98.52 97.69 98.43 98.63

Table 7: Reshaped tensor decomposition combined with sequential for fully-connected layers on MNIST. The uncompressed network achieves 99.31% accuracy.

C NOTATIONS
Symbols: Lower case letters (e.g. v) are used to denote column vectors, while upper case letters (e.g. M) are used for matrices, and curled letters (e.g. T ) for multi-dimensional arrays (tensors). For a tensor T  RI0×···×Im-1 , we will refer to the number of indices as order, each individual index as mode and the length at one mode as dimension. Therefore, we will say that T  RI0×···×Im-1 is an m-order tensor which has dimension Ik at mode-k. Tensor operations are extensively used in this paper: The tensor (partial) outer product is denoted as , tensor convolution as , and finally × denotes either tensor contraction or tensor multiplication. Each of these operators will be equipped with subscript and superscript when used in practice, for example ×mn denotes mode(m, n) tensor contraction (defined in Appendix D). Furthermore, the symbol  is used to construct compound operations. For example, (  ) is a compound operator simultaneously performing tensor convolution and tensor partial outer product between two tensors.
Indexing: In this paragraph, we explain the usages of subscripts/superscripts for both multidimensional arrays and operators, and further introduce several functions that are used to alter the layout of multi-dimensional arrays.
· Nature indices start from 0, but reversed indices are used occasionally, which start from -1. Therefore the first entry of a vector v is v0, while the last one is v-1.
· For multi-dimensional arrays, the subscript is used to denote an entry or a subarray within an object, while superscript is to index among a sequence of arrays. For example, Mi,j denotes the entry at ith row and jth column of a matrix M, and M(k) is the kth matrix in a set of N matrices {M(0), M(1), · · · M(N-1)}. For operators, as we have seen, both subscript and superscript are used to denote the modes involved in the operation.
12

Under review as a conference paper at ICLR 2019

· The symbol colon ':' is used to slice a multi-dimensional array. For example, M:,k denotes the kth column of M, and T:,:,k denotes the kth frontal slice of a 3-order tensor T .

· Big-endian notation is adopted in conversion between multi-dimensional array and vectors.

Specifically, the function vec(·) flattens (a.k.a. vectorize) a tensor T  RI0×···×Im-1 into a vector

vR

m-1 l=0

Il

such

that

Ti0,··· ,im-1

=

v .im-1+im-2Im-1+···+i0I1···Im-1

· The function swapaxes(·) is used to permute ordering of the modes of a tensor as needed. For example, given two tensors U  RI×J×K and V  RK×J×I , the operation V = swapaxes(U ) convert the tensor U into V such that Vk,j,i = Ui,j,k.

· The function flipaxis(·, ·) flips a tensor along a given mode. For example, given a tensor U  RI×J×K and V = flipaxis(U , 0), the entries in V is defined as Vi,j,k = UI-1-i(mod I),j,k.

D TENSOR OPERATIONS

Operator
mode-(k, l) Tensor Contraction
mode-k Tensor Multiplication
mode-(k, l) Tensor Convolution
mode-(k, l) PartialOuter Product

Notation

Definition

T (0) = X ×lk Y

T (0)
i0,··· ,ik-1,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1
= X , Yi0,··· ,ik-1,:,ik+1,··· ,im-1 j0,··· ,jl-1,:,jl+1,··· ,jn-1
inner product of mode-k fiber of X

and mode-l fiber of Y

T (1)
i0,··· ,ik-1,r,ik+1,··· ,im-1

T (1) = X ×k M =

X , Mi0,··· ,ik-1,:,ik+1,··· ,im-1

:,r

inner product of mode-k fiber of X

and rth column of M

T (2) = X kl Y

T (2)
i0,··· ,ik-1,:,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

= X  Yi0,··· ,ik-1,:,ik+1,··· ,im-1

j0 ,··· ,jl-1,:,jl+1,··· ,jn-1

convolution of mode-k fiber of X

and mode-l fiber of Y

T (3) = X kl Y

T (3)
i0,··· ,ik-1,r,ik+1,··· ,im-1,j0,··· ,jn-1
= X Yi0,··· ,ik-1,r,ik+1,··· ,im-1 j0,··· ,jl-1,r,jl+1,··· ,jn-1 Hadamard product of mode-k fiber of X

and mode-l fiber of Y

Table 8: Summary of tensor operations. In this table, X  RI0×···×Im-1 , Y  RJ0×···×Jn-1 and matrix M  RIk×J . Mode-(k, l) tensor contraction and mode-(k, l) tensor partial-outer product are legal only if Ik = Jl. T (0) is an (m + n - 2)-order tensor, T (1) is an m-order tensor, T (2) is an (m + n - 1)-order tensor and T (3) is an (m + n - 1)-order tensor.

In this section, we introduce a number of tensor operations that serve as building blocks of tensorial neural networks. To begin with, we describe several basic tensor operations that are natural generalization to their vector/matrix counterparts. Despite their simplicity, these basic operations can be combined among themselves to construct complicated compound operators that are actually used in all designs. We will analyze their theoretical sequential time complexities in details, and point out the implementational concerns along the way. Although all these operations can in principle be implemented by parallel programs, the degree of parallelism depends on their particular software and hardware realizations. Therefore, we will use the sequential time complexity as a rough estimate of the computational expense in this paper.
Tensor contraction Given a m-order tensor T (0)  RI0×···×Im-1 and another n-order tensor T (1)  RJ0×···×Jn-1 , which share the same dimension at mode-k of T (0) and mode-l of T (1)( i.e. Ik = Jl), the mode-(k, l) contraction of T (0) and T (1), denoted as T T (0) ×kl T (1), returns a (m + n - 2)-order tensor T  R ,I0×···×Ik-1×Ik+1×···×Im-1×J0×···×Jl-1×Jl+1×···×Jn-1 whose

13

Under review as a conference paper at ICLR 2019

entries are computed as

Ti0 ,··· ,ik-1,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1
Ik -1
= T T(0) (1)
i0,··· ,ik-1,r,ik+1,··· ,im-1 j0,··· ,jl-1,r,jl+1,··· ,jn-1 r=0
= T , T(0) (1)
i0 ,··· ,ik-1,:,ik+1,··· ,im-1 j0,··· ,jl-1,:,jl+1,··· ,jn-1

(D.1a) (D.1b)

Notice that tensor contraction is a direct generalization of matrix multiplication to higher-order

tensor, and it reduces to matrix multiplication if both tensors are 2-order (and therefore matri-

ces). As each entry in T can be computed as inner product of two vectors, which requires

Ik = Jl multiplications, the total number of operations to evaluate a tensor contraction is there-

fore O((

m-1 u=0

Iu)(

n-1 v=0,v=l

Jv )),

taking

additions

into

account.

Notice that the analysis of time complexity only serves as a rough estimate for actual execution time, because we do not consider the factors of parallel computing and computer systems. In practice, (1) the modes that are not contracted over can be computed in parallel, and summations can be computed in logarithmic instead of linear time; (2) The spatial locality of the memory layout plays a key role in speeding up the computation of tensor operations. These arguments equally apply to all tensor operations in this paper, but we will not repeat them in the analyses for simplicity.

Tensor multiplication (Tensor product) Tensor multiplication (a.k.a. tensor product) is a special case of tensor contraction where the second operant is a matrix. Given a m-order tensor U  RI0×···×Im-1 and a matrix M  RIk×J , where the dimension of U at mode-k agrees with the
number of the rows in M, the mode-k tensor multiplication of U and M, denoted as V U ×k M, yields another m-order tensor V  R ,I0×···×Ik-1×J×Ik+1×···Im-1 whose entries are computed as

Vi0,··· ,ik-1,j,ik+1,··· ,im-1

Ik -1

= U Mi0,··· ,ik-1,r,ik+1,··· ,im-1 r,j
r=0

=

U , Mi0,··· ,ik-1,:,ik+1,··· ,im-1

:,j

(D.2a) (D.2b)

Following the convention of multi-linear algebra, the mode for J now substitutes the location origi-

nally for Ik (which is different from the definition of tensor contraction). Regardlessly, the number

of operations for tensor multiplication follows tensor contraction exactly, that is O((

m-1 u=0

Iu

)J

).

Tensor convolution Given a m-order tensor T (0)  RI0×I1×···×Im-1 and another n-order tensor T (1)  RJ0×J1×···×Jn-1 . The mode-(k, l) convolution of T (0) and T (1), denoted as T T (0) lk T (1), returns a (m + n - 1)-order tensor T  R .I0×···×Ik ×···×Im-1×J0×···×Jl-1×Jl+1×···×Jn-1 The entries of T can be computed using any convolution operation  that is defined for two vectors.

Ti0,··· ,ik-1,:,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

= T  T(0)
i0,··· ,ik-1,:,ik+1,··· ,im-1

(1) j0,··· ,jl-1,:,jl+1,··· ,jn-1

= T  T(1)
j0,··· ,jl-1,:,jl+1,··· ,jn-1

(0) i0,··· ,ik-1,:,ik+1,··· ,im-1

(D.3a) (D.3b)

Here we deliberately omit the exact definition of vector convolution , as it can be defined in multiple
forms depending on the user case (Interestingly, the "convolution" in convolutional layer indeed computes correlation instead of convolution). Correspondingly, the resulted dimension Ik at modek is determined by the chosen type of convolution. For example, the "convolution" in convolutional layer typically yields Ik = Ik (with same padding) or Ik = Ik - Jl + 1 (with valid padding). Notice that vector convolution itself is generally asymmetric , i.e. u  v = v  u (except for the case of circular convolution). For convenience, we can define its conjugate as  such that u  v = v  u.
With this notations, Equation D.3a can also be written as D.3b.

Generally speaking, Fast Fourier Transform (FFT) plays a critical role to lower the computational

complexities for all types of convolution. In the case of tensor convolution, the number of re-

quired operations without FFT is O((

m-1 u=0

Iu

)(

n-1 v=0

Jv

)),

while

FFT

is

able

to

reduce

it

to

14

Under review as a conference paper at ICLR 2019

O((

m-1 u=0,u=k

Iu)(

n-1 v=0,v=l

Jv

)

max(Ik

,

Jl)

log(max(Ik

,

Jl))).

That being said, FFT is not al-

ways necessary: if min(Ik, Jl) < log (max(Ik, Jl)) (which is typical in convolutional layers, where

Ik is the height/width of the feature maps and Jl is the side length of the square filters), computing

the convolution without FFT is actually faster. Furthermore, FFT can be difficult to implement (thus

not supported by popular software libraries) if convolution is fancily defined in neural networks (e.g.

dilated, atrous). Therefore, we will assume that tensor convolutions are computed without FFT in

subsequent sections unless otherwise noted.

Tensor outer product Given a m-order tensor T (0)  RI0×I1×···×Im-1 and another n-order

tensor T (1)  RJ0×J1×···×Jn-1 , the outer product of T (0) and T (1), denoted T

T (0) 

T (1), concatenates all the indices of T (0) and T (1), and returns a (m + n)-order tensor T 

RI0×···×Im-1×J0×···×Jn-1 whose entries are computed as

T = T Ti0 ,··· ,im-1,j0,··· ,jn-1

(0) (1) i0,··· ,im-1 j0,··· ,jn-1

(D.4)

It is not difficult to see that tensor outer product is a direct generalization for outer product for two

vectors M = u v = u v. Obviously, the number of operations to compute a tensor outer product

explicitly is O((

m-1 u=0

Iu

)(

n-1 v=0

Jv

)).

Tensor

outer

product

is

rarely

calculated

alone

in

practice

because it requires significant amounts of computational and memory resources.

Tensor partial outer product Tensor partial outer product is a variant of tensor outer product
defined above, which is widely used in conjunction with other operations. Given a m-order tensor T (0)  RI0×I1×···×Im-1 and another n-order tensor T (1)  RJ0×J1×···×Jn-1 , which share the same dimension at mode-k of T (0) and mode-l of T (1) (i.e. Ik = Jl), the mode-(k, l) partial outer product of T (0) and T (1), denoted as T T (0) lk T (1), returns a (m + n - 1)-order tensor T  R ,I0×···×Im-1×J0×···×Jl-1×Jl+1×···×Jn-1 whose entries are computed as

Ti0,··· ,ik-1,r,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1 = T T(0) (1)
i0,··· ,ik-1,r,ik+1,··· ,im-1 j0 ,··· ,jl-1,r,jl+1,··· ,jn-1
= T··(·0,)r,···  T··(·1,)r,···

(D.5a) (D.5b)

The operation bears the name "partial outer product" because it reduces to outer product

once we fix the indices at mode-k of T (0) and mode-l of T (1). Referring to the computa-

tional complexity of tensor outer product, the number of operations for each fixed index is

O((

m-1 u=0,u=k

Iu)(

n-1 v=0,v=l

Jv )),

therefore

the

total

time

complexity

for

the

tensor

partial

outer

product is O((

m-1 u=0

Iu

)(

n-1 v=0,v=l

Jv

)),

the

same

as

tensor

contraction.

Precautions of usage

· Similar to matrix multiplication, the operants in tensor operations are not commutative in general. For example, neither T (0) ×kl T (1) = T (1) ×lk T (0) nor T (0) ×lk T (1) = T (1) ×kl T (0) holds even if the dimensions at the specified modes happen to match.
· Different from matrix multiplication, the law of associative also fails in general. For example, (T (0) ×lk T (1)) ×qp T (2) = T (0) ×kl (T (1) ×qp T (2)), mainly because tensor operations can change the locations of modes in a tensor.
· However, both problems are not fundamental, and can be fixed by adjusting the superscripts and subscripts of the operators carefully (and further permute ordering of the modes in the result accordingly). For example, T (0) ×kl T (1) = swapaxes(T (1) ×lk T (0)) holds if swapaxes(·) is properly performed. Due to space limits, we can not develop general rules in this paper, and will derive such identities as needed. In general, the take away message is a simple statement: Given an expression that contains multiple tensor operations, these operations need to be evaluated from left to right unless a bracket is explicitly supplied.

Compound operations: As building blocks, the basic tensor operations defined above can further combined to construct compound operations that perform multiple operations on multiple tensors simultaneously. For simplicity, we illustrate their usage using two representative examples in this

15

Under review as a conference paper at ICLR 2019

section. More examples will arise naturally when we discuss the derivatives and backpropagation rules for compound operations in Appendix E.

· Simultaneous multi-operations between two tensors. For example, given two 3-order tensors T (0)  RR×X×S and T (1)  RR×H×S , we can define a compound operation 00  11  ×22 between T (0) and T (1), where mode-(0, 0) partial outer product, mode-(1, 1) convolution and mode-
(2, 2) contraction are performed simultaneously, which results in a 2-order tensor T of RR×X (it
is indeed a matrix, though denoted as a tensor). The entries of T T (0) 00  11  ×22 T (1) are computed as

S-1

Tr,: =

Tr(,0:,)s  Tr(,1:,)s

s=0

(D.6)

For commonly used vector convolution, it is not difficult to show that number of operations required to compute the result T is O (R max(X, H) log(max(X, H))S) with FFT and O(RXHS) without FFT, as each of the R vectors in T is computed with a sum of S vector convolutions.

· Simultaneous operations between a tensor and a set of multiple tensors. For example, given a 3-order tensor U  RR×X×S and a set of three tensors T (0)  RR×P , T (1)  RK×Q and T (2)  RS×T , we can define a compound operation on U as V U (00T (0) 01 T (1) ×02 T (2)), which performs mode-(0, 0) partial outer product with T (0), mode-(1, 0) convolution with T (1) and mode(2, 0) contraction with T (2) simultaneously. In this case, a 5-order tensor V  RR×X×P ×Q×T is
returned, with entries calculated as

S-1

Vr,:,p,q,t =

Tr(,0p)

s=0

Ur,:,s  T:,(q1)

Ts(,2t)

(D.7)

The analysis of time complexity of a compound operation with multiple tensors turns out to be a
non-trivial problem. To see this, let us first follow the naive way to evaluate the output according to the expression above: (1) each vector in the result Vr,:,p,q,t can be computed with a sum of S vector convolutions, which requires O(XHS) operations; (2) and with RP QT such vectors in the result V, the time complexity for the whole compound operation is therefore O(RXHP RST ).
However, it is obviously not the best strategy to perform these operations. In fact, the equations
can be equivalently rewritten as

Vr,:,p,q,t =

S-1
Ur,:,sTs(,2t)
s=0

 T:,(q1)

Tr(,0p)

(D.8)

If we follows the supplied brackets and break the evaluation into three steps, it is not difficult to verify that these steps take O(RXST ), O(RXHP T ) and O(RXHP T ) operations respectively, and result in a total time complexity of O(RXST + RXHP T + RXP QT ) for the compound operation, which is far lower than the one with the naive way.
Unfortunately, it is an NP-hard problem to determine the best order (with minimal number of operations) to evaluate a compound operation over multiple tensors, therefore in practice the order is either determined by exhaustive search (if there are only a few tensors) or follows a heuristic strategy (if the number of tensors is large).

The examples provided above are by no mean comprehensive, and in fact more complicated compound operations simultaneously perform multiple operations on multiple tensors can be defined, and we will see examples of them in the next section when we derive the backpropagation equations for the compound operations above. Generally, compound operations over multiple tensors are difficult to flatten into mathematical expressions without introducing tedious notations. Therefore, these operations are usually described by graphical representations, which are usually called tensor network in the physics literature (not to confuse with tensorial network in this paper). Interested readers are referred to the monograph Cichocki et al. (2016), which serves a comprehensive introduction to the application of tensor network in the field of machine learning.

16

Under review as a conference paper at ICLR 2019

E DERIVATIVES AND BACKPROPAGATION OF TENSOR OPERATIONS

All operations introduced in the last section, both basic and compound, are linear in their operants. Therefore, the derivatives of the result with respect to its inputs are in principle easy to calculate. In this section, we will explicitly derive the derivatives for all operations we have seen in Appendix D.
These derivatives can be further combined with classic chain rule to obtain the corresponding backpropagation equations (i.e. how gradient of the loss function propagates backward through tensor operations), which are the cornerstones of modern feed-forward neural networks. In the section, we show that these backpropagation equations can also be characterized by (compound) tensor operations, therefore their computational complexities can be analyzed similarly as in Appendix D.
Interestingly, the backpropagation equations associated with a tensor operation, though typically appear to be more involved, share the same asymptotic complexities as in the forward pass (with tensor convolution as an exception). This observation is extremely useful in the analyses of tensorial neural networks in Appendix G, H and I, which allows us to reuse the same number in the forward pass in the analysis of backward propagation.
In this section, we will assume for simplicity the loss function L is differentiable. However, all derivatives and backpropagation equations equally apply when L is only sub-differentiable (piecewise smooth). Also, we will focus on one step of backpropagation, therefore we assume the gradient of the loss function is known to us in prior.

Tensor contraction Recall the definition of tensor contraction in Equation D.1a, the partial derivatives of the result T with respect to its operants T (0), T (1) can be computed at the entries level:

Ti0,··· ,ik-1,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1 = T(0)
Ti0,··· ,ik-1,r,ik+1,··· ,im-1

(1) j0,··· ,jl-1,r,jl+1,··· ,jn-1

Ti0,··· ,ik-1,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1 = T(1)
Tj0,··· ,jl-1,r,jl+1,··· ,jn-1

(0) i0,··· ,ik-1,r,ik+1,··· ,im-1

(E.1a) (E.1b)

With classic chain rule, the derivatives of L with respect to T (0) and T (1) can be obtained through the derivative of Lwith respect to T .

L T (0)
i0,··· ,ik-1,r,ik+1,··· ,im-1

J0-1

Jl-1-1 Jl+1-1

Jn-1-1

= ···

···

j0 =0

jl-1=0 jl+1=0

jn-1 =0

T L Ti0,··· ,ik-1,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

(1) j0,··· ,jl-1,r,jl+1,··· ,jn-1

L T (1)
j0,··· ,jl-1,r,jl+1,··· ,jn-1

I0 -1

Ik-1-1 Ik+1-1

Im-1 -1

= ···

···

i0 =0

ik-1=0 ik+1=0

im-1 =0

T L Ti0,··· ,ik-1,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

(0) i0,··· ,ik-1,r,ik+1,··· ,im-1

Though tedious at entries level, it can be simplified with tensor notations in this paper.

(E.2a) (E.2b)

L T (0)

=

swapaxes

L T (1)

=

swapaxes

L T

×m0 -1  · · ·  ×lm-+1l-2  ×lm++1l-1  · · ·  ×nm-+1n-3

T (1)

L T

×00  · · ·  ×kk--11  ×kk+1  · · ·  ×mm--21

T (0)

(E.3a) (E.3b)

where swapaxes(·) is used to align the modes of outputs. Notice that the backpropagation equa-

tions are compound operations, even if the original operation is a basic one. It is not dif-

ficult to show that the number of operations required for both backpropagation equations are

O((

m-1 u=0

Iu

)(

n-1 v=0,v=l

Jv )),

which

are

exactly

the

same

as

in

the

forward

pass

in

Equation

D.1a.

The result should not surprise us however, since the tensor contraction is a direct generalization to

matrix multiplication (where backward propagation has exactly the same time complexity as the

matrix multiplication itself).

17

Under review as a conference paper at ICLR 2019

Tensor multiplication (Tensor product) As a special case of tensor contraction, the derivatives
and backpropagation equations for tensor multiplication can be obtained in the same manner. To begin with, the derivatives of V with respect to U and M can be computed from the definition in Equation D.2a.

Vi0,··· ,ik-1,j,ik+1,··· ,im-1 U = Mi0,··· ,ik-1,r,ik+1,··· ,im-1

r,j

Vi0,··· ,ik-1,j,ik+1,··· ,im-1 M = Ur,j

i0,··· ,ik-1,r,ik+1,··· ,im-1

(E.4a) (E.4b)

Subsequently, the derivatives of L with respect to U and M can be computed as with chain rule,

L J-1 L

V = U Mi0,··· ,ik-1,r,ik+1,··· ,im-1

j=0

i0,··· ,ik-1,j,ik+1,··· ,im-1

r,j

L

I0 -1

Ik-1-1 Ik+1-1

Im-1 -1

 Mr,j

= ···

···

i0 =0

ik-1=0 ik+1=0

im-1 =0

L Ui0,··· ,ik-1,r,ik+1,··· ,im-1 Vi0,··· ,ik-1,j,ik+1,··· ,im-1

(E.5a) (E.5b)

Again, the backpropagation equations above can be succinctly written in tensor notations.

L M

=

U

L V

=

L U

×k

M

×00  · · ·  ×kk--11  ×kk++11  · · ·  ×mm--11

L V

(E.6a) (E.6b)

where the time complexities for both equations are O((

m-1 u=0

Iu)J ),

which

is

identical

to

the

for-

ward pass in Equation D.2a (obviously since tensor multiplication is a special of tensor contraction).

Tensor convolution Recall in the definition of tensor convolution in Equation D.3a, we deliberately omit the exact definition of vector convolution for generality. For simplicity, we temporarily limit ourselves to the special case of circular convolution. In this case, tensor convolution can be concretely defined by either equation below:

Ti0,··· ,ik-1,:,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

= Cir T T(0)
i0 ,··· ,ik-1,:,ik+1,··· ,im-1

(1) j0,··· ,jl-1,:,jl+1,··· ,jn-1

Ti0,··· ,ik-1,:,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

= Cir T T(1)
j0 ,··· ,jl-1,:,jl+1,··· ,jn-1

(0) i0,··· ,ik-1,:,ik+1,··· ,im-1

(E.7a) (E.7b)

where Cir(·) returns a circular matrix of the input vector. Concretely, given a vector v  RI , the circular matrix Cir(v) is defined as Cir(v)i,j = vi-j(modI). Now, the derivatives of the result tensor T with respect to T (0) and T (1) can be obtained by matrix calculus.

Ti0,··· ,ik-1,:,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1 = Cir(0)
Ti0,··· ,ik-1,:,ik+1,··· ,im-1 Ti0,··· ,ik-1,:,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1
= Cir(1) Tj0,··· ,jl-1,:,jl+1,··· ,jn-1

T (1) 
j0,··· ,jl-1,:,jl+1,··· ,jn-1
T (0) 
i0,··· ,ik-1,:,ik+1,··· ,im-1

(E.8a) (E.8b)

18

Under review as a conference paper at ICLR 2019

Applying chain rule to the equations above, we arrive at two lengthy equations:

Cir Cir

L T (0)
i0,··· ,ik-1,:,ik+1,··· ,im-1

J0-1

Jl-1-1 Jl+1-1

Jn-1-1

= ···

···

j0 =0

jl-1=0 jl+1=0

jn-1 =0

L(1)  T Tj0,··· ,jl-1,:,jl+1,··· ,jn-1

i0,··· ,ik-1,:,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

L T (1)
j0,··· ,jl-1,r,jl+1,··· ,jn-1

I0 -1

Ik-1-1 Ik+1-1

Im-1 -1

= ···

···

i0 =0

ik-1=0 ik+1=0

im-1 =0

L(0)  T Ti0,··· ,ik-1,:,ik+1,··· ,im-1

i0,··· ,ik-1,:,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

With notations of tensor operations, they can be greatly simplified as

L T (0)

=

swapaxes

L T

×m0 · · · ×lm-+1l-1  lk 

×lm++1l-2  · · ·  ×nm-+1n-2 flipaxis(T (1), l)

L T (1)

=

swapaxes

L T

×00  · · ·  ×kk--11  kk

×kk++11  · · ·  ×mm--11 flipaxis(T (0), k)

(E.9a) (E.9b) (E.10a) (E.10b)

Although these backpropagation equations are derived for the special case of circular convolution, they hold for general convolution if we replace kl by its corresponding adjoint operator (lk) (and use the unflipped versions of T (0) and T (1)). With adjoint of convolution, Equations E.10a and
E.10b can be rewritten as

L T (0)

=

swapaxes

L T

×m0 · · · ×lm-+1l-1 (kl )  ×lm++1l-2 · · · ×nm-+1n-2

T (1)

(E.11a)

L T (1)

=

swapaxes

L T

×00  · · ·  ×kk--11  (kk)  ×kk++11  · · ·  ×mm--11

T (0)

(E.11b)

where the exact form of the adjoint operator (lk) depends on the original definition of vector convolution. Generally, the trick to start with circular convolution and generalize to general cases is very useful to derive backpropagation equations for operations that convolution plays a part.

Despite varieties in the definitions of tensor convolution, the analyses of their time complexities

of backpropagation equations are identical, since the numbers of operations only differ by a con-

stant for different definitions (therefore asymptotically the same). With FFT, the number of op-

erations for these two backpropagation equations are O((

m-1 u=0,u=k

Iu)(

n-1 v=0,v=l

Jv )

max(Ik ,

Jl)

log(max(Ik , Jl))) and O((

m-1 u=0,u=k

Iu

)(

n-1 v=0,v=l

Jv) max(Ik , Ik) log(max(Ik , Ik))),

and

with-

out FFT O((

m-1 u=0,u=k

Iu)(

n-1 v=0,v=l

Jv)Ik Jl)

and

O((

m-1 u=0,u=k

Iu)(

n-1 v=0,v=l

Jv)Ik Ik)

respec-

tively. Different from other operations, the time complexities for forward and backward passes

are different (with circular convolution as an exception). This asymmetry can be utilized in neural networks (whereIk  Ik  Jl) to accelerate backpropagation with FFT.

Tensor outer product The derivatives of T with respect to T (0) and T (1) can be directly observed from the definition of tensor outer product in Equation D.4.

Ti0,··· ,im-1,j0,··· ,jn-1 Ti(00,·)·· ,im-1

=

T (1)
j0,··· ,jn-1

Ti0,··· ,im-1,j0,··· ,jn-1 Tj(01,·)·· ,jn-1

=

T (0)
i0,··· ,im-1

(E.12a) (E.12b)

19

Under review as a conference paper at ICLR 2019

Similar to all previously discussed operations, we first derive the backpropagation equations for T (0) and T (1) at the entries level using standard chain rule.

L Ti(00,·)·· ,im-1

=

J0-1
·
j0 =0

·

·

Jn-1-1 jn-1 =0

L Ti0,··· ,im-1,j0,··· ,jn-1

T (1)
j0 ,··· ,jn-1

L Tj(01,)··· ,jn-1

I0 -1

Im-1 -1

= ···

L

i0 =0

 Tim-1 =0

i0,··· ,im-1,j0,··· ,jn-1

T (0)
i0,··· ,im-1

which can then be converted to tensor notations in this paper:

(E.13a) (E.13b)

L T (0)

=

L T

×m0  · · ·  ×nm-+1n-1

T (1)

L T (1)

=

L T

×00  · · ·  ×mm--11 T (0)

The number of operations required for both equations are O((

m-1 u=0,u=k

Iu)(

again identical to one in the forward pass in Equation D.4.

(E.14a)

(E.14b)

n-1 v=0

Jv

)),

which

are

Tensor partial outer product Finally, the derivatives of T with respect to T (0) and T (1) can be obtained from the definition of tensor partial outer product in Equation D.5a.

Ti0,··· ,ik-1,r,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1 = T(0)
Ti0,··· ,ik-1,r,ik+1,··· ,im-1

(1) j0,··· ,jl-1,r,jl+1,··· ,jn-1

(E.15a)

Ti0,··· ,ik-1,r,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1 = T(1)
Tj0,··· ,jl-1,r,jl+1,··· ,jn-1

(0) i0,··· ,ik-1,r,ik+1,··· ,im-1

(E.15b)

Again with chain rule, the backpropagation equations for T (0) and T (1) at the entries level are

L T (0)
i0,··· ,ik-1,r,ik+1,··· ,im-1

J0-1

Jl-1-1 Jl+1-1

Jn-1-1

= ···

···

j0 =0

jl-1=0 jl+1=0

jn-1 =0

T L Ti0,··· ,ik-1,r,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

(1) j0,··· ,jl-1,r,jl+1,··· ,jn-1

(E.16a)

L T (1)
j0,··· ,jl-1,r,jl+1,··· ,jn-1

I0 -1

Ik-1-1 Ik+1-1

Im-1 -1

= ···

···

i0 =0

ik-1=0 ik+1=0

im-1 =0

T L Ti0,··· ,ik-1,r,ik+1,··· ,im-1,j0,··· ,jl-1,jl+1,··· ,jn-1

(0) i0,··· ,ik-1,r,ik+1,··· ,im-1

(E.16b)

Though the backpropagation equations above appear very similar to the ones for tensor contraction

in Equations E.1a and E.1b, written in tensor notations, they are almost the same as the ones for tensor convolution in Equations E.10a and E.10b, except that (lk)'s are now replaced by kl .

L T (0)

=

swapaxes

L T

×m0  · · ·  ×lm-+1l-1  lk  ×lm++1l-2  · · ·  ×nm-+1n-2

T (1)

(E.17a)

L T (1)

=

swapaxes

L T

×00  · · ·  ×kk--11  kk  ×kk++11  · · ·  ×mm--11

T (0)

(E.17b)

It is not difficult to recognize the time complexity for the two equations above are O((

m-1 u=0

Iu)

(

n-1 v=0,v=l

Jv

)),

which

are

identical

to

the

ones

in

forward

pass

in

Equation

D.5a.

Compound operations Up to this point, we have developed the derivatives and backpropagation equations for all basic operations. In this part, we will continue to show similar techniques above equally apply to compound operations, though slightly more involved, and derive the backpropagation equations for the examples we used in Appendix D. Though these equations are not immediately useful in later sections, the techniques to derive them are useful for all other compound operations. Furthermore, these induced equations, which are more complicated than their original definitions, serve as complementary examples of compound operations to the ones in the last section.

20

Under review as a conference paper at ICLR 2019

· Simultaneous multi-operations between two tensors. In Appendix D, we introduced a compound operation (00  11  ×22) on two tensors T (0)  RR×X×S and T (1)  RR×H×S , which returns a tensor T  RR×X . Here, we recap its definitions as follows:

T T (0) 00  11  ×22 T (1)
S-1
Tr,: Tr(,0:,)s  Tr(,1:,)s
s=0

(E.18a) (E.18b)

To ease the derivation, we use the trick to start with circular convolution: directly apply the chain rule, the backpropagation equations at entries level are obtained as follows:

L  Tr(,0:,)s

= Cir

L  Tr(,1:,)s

= Cir

Tr(,1:,)s Tr(,0:,)s

 L Tr,:
 L Tr,:

(E.19a) (E.19b)

Now we convert the equations above to tensor notations, and replace the circular convolutions with their adjoints to obtain general backpropagation rules:

L T (0)

=

L T

L T (1)

=

L T

00  11 00  11

flipaxis(T

(1))

=

L T

flipaxis(T

(0))

=

L T

00  (11) 00  (11)

T (1) T (0)

(E.20a) (E.20b)

For simplicity, we assume FFT is not used to accelerate the backpropagation equations. In this case, the derivatives with respect to T (0) and T (1) can be computed in O(RHXST ) and O(RXXST ) operations respectively. Again, the time complexities of forward and backward
passes are not the same when a (compound) tensor operation contains convolution.

· Simultaneous operations between a tensor and a set of multiple tensors. Another compound operation presented in Appendix D is defined between a tensor U  RR×X×S and a set of tensors T (0)  RR×P , T (1)  RH×Q and T (2)  RS×T , which returns a tensor V  RR×X×P ×Q×T .
Again, we recap its definitions in the following:

V U 00T (0) 10 T (1) ×02 T (2)

(E.21a)

Vr,:,p,q,t

S-1
Tr(,0p)
s=0

Ur,:,s  T:,(q1)

Ts(,2t)

(E.21b)

In order to derive the backpropagation rule for the core tensor U, we follow the standard procedure to (1) first obtain its entries level representation, and (2) explicitly convert it to tensor notations subsequently. Concretely, the backpropagation equation in both formats are displayed as follows:

L  Ur,:,s

=

P -1 Q-1 T -1
Tr(,0p)
p=0 q=0 t=0

Cir

T:,(q1)

 L  Vr,:,p,q,t

Ts(,2t)

(E.22a)

L U

=

L V

00  ×12 T (0) (01)  ×13 T (1) ×02  ×14 T (2)

(E.22b)

Notice that the equation above is indeed simultaneous multi-operations between a tensor and
a set of multiple tensors, which combines two types of "basic" compound operations introduced in Appendix D. In principle, we can obtain backpropagation equations for {T (0), T (1), T (2)} in the same manner. However, there is a simpler way to derive them by rewriting the definition as:

V = swapaxes U 10T (1) ×20 T (2) 00 T (0) = swapaxes U (0) 00 T (0)

(E.23a)

= swapaxes U 00T (0) ×20 T (2) 10 T (1) = swapaxes U (1) 11 T (1)

(E.23b)

V = swapaxes U 00T (0) 10 T (1) ×02 T (2) = swapaxes U (2) ×22 T (2)

(E.23c)

21

Under review as a conference paper at ICLR 2019

where U (0), U (1) and U (2) are short-hand notations for U (10T (1) ×02 T (2)), U (00T (0) ×20 T (2)) and U (00T (0) 10 T (1)). With these notations, we are able to reduce these complex expressions to basic ones, by which we can reuse the backpropagation rules derived in this section:

L T (0)

=

L V

=

L V

L T (1)

=

L V

=

L V

L T (2)



=

L V

00  ×11  ×23  ×34 U (0) 00  ×11  ×23  ×34 U 01T (1) ×02 T (2) ×00  (11)  ×22  ×34 U (1) ×00  (11)  ×22  ×43 U 00T (0) ×20 T (2) ×00  ×11  ×23  ×34 U (2)

(E.24a) (E.24b)

=

L V

×00  ×11  ×23  ×43

U 00T (0) 01 T (1)

(E.24c)

The complicity of tensor operation culminates at this point: the equations above are examples of simultaneous multi-operations on multiple tensors, which we omitted in the discussion in Appendix D due to their complexity. Although the expressions themselves suggest particular orderings to evaluate the compound operations, they are merely the traces of the techniques used in deriving them. It is completely reasonable to reorganize the equations such that they can be computed with more efficient strategies: for instance, one can verify that the following set of equations is actually equivalent to the one above:

L T (0)

=

L V

L T (1)

=

L V

L T (2)



=

L V

(10)  ×31 T (1) ×14 T (2) 00  ×11  ×32 U 00  ×21 T (0) ×14 T (2) ×00  (11)  ×23 U 00  ×12 T (0) (01)  ×13 T (1) ×00  ×11 U

(E.25a) (E.25b) (E.25c)

As discussed in Appendix D, the problem to find the optimal order to evaluate a compound operation over multiple tensors is NP-hard in general and usually we need to resort to heuristics to obtain a reasonably efficient algorithm. Indeed, one can verify that the second set of equations is more efficient than the first one. For this example, interested readers are encouraged to find the most efficient way by combinatoric search.

F TENSOR DECOMPOSITIONS
Tensor decompositions are natural extensions of matrix factorizations for multi-dimensional arrays. In this section, we will review three commonly used tensor decompositions, namely CANDECOMP/PARAFAC (CP) decomposition, Tucker (TK) decomposition and Tensor-train (TT) decomposition. CP and Tucker decompositions are classic methods to factorize a tensor and are thoroughly reviewed in an early survey Kolda & Bader (2009), while Tensor-train decomposition is recently proposed in Oseledets (2011), in which the authors show that it has superior numerical stability and scalability than the classic ones. More advanced schemes of tensor decompositions (known as tensor networks) are reviewed in Cichocki et al. (2016).
For each of these decompositions, we will present their forms both at the entries level and in tensor notations introduced in Appendix D. When tensor decompositions are used in neural networks, a natural question to ask is how the backpropagation algorithm adapts to the decomposition schemes, i.e. how the gradient of the original tensor backpropagates to its factors. In this section, we will follow the standard procedure in Appendix E to derive the corresponding backpropagation equation for each tensor decomposition. Different from previous works (Novikov et al., 2015; Kossaifi et al., 2017b) that use matrix calculus following matricization, we present the backpropagation equations directly in tensor notations, which makes our presentation concise and easy to analyze. As we will see in the analyses, backpropagation equations through the original tensor to its factors are

22

Under review as a conference paper at ICLR 2019

computationally expensive for all decomposition schemes, therefore it is preferable to avoid explicit computation of these equations in practice.

CP decomposition CP decomposition is a direct generalization of singular value decomposition
(SVD) which decomposes a tensor into additions of rank-1 tensors (outer product of multiple vectors). Specifically, given an m-order tensor T  RI0×I1×···×Im-1 , CP decomposition factorizes it into m factor matrices {M(0)}ml=-01, where M(l)  RR×Il , l  [m], where R is called the canonical rank of the CP decomposition, which is allowed to be larger than the Il's.

Ti0,··· ,im-1 T

R-1
M(r0,i)0 · · · M(rm,im--11)
r=0
R-1
M(r0,:)  · · ·  M(rm,: -1)
r=0

(F.1a)

= 1 ×00 M(0) 00 · · · 00 M(m-1)

(F.1b)

where 1  RR is an all-ones vector of length R. With CP decomposition, T can be represented with

only (

m-1 l=0

Il)R

entries

instead

of

(

m-1 l=0

Il)

as

in

the

original

tensor.

Now we proceed to derive the backpropagation rules for CP decomposition, i.e. the equations relating L/T to {L/M(l)}ml=-01. In order to avoid deriving these equations from the entries level, we first isolate the factor of interest and rewrite the definition of CP decomposition as:

T = swapaxes M(0) · · · 00 M(l-1) 00 M(l+1) · · · 00 M(m-1) ×00 M(l) = swapaxes A(l) ×00 M(l)

(F.2)

where we treat the first term as a constant tensor A(l). Once we reduce the compound operation to a basic one, we can simply refer to the rule derived in Appendix E, which gives us

L  M(l)

=

L T

=

L T

×10 · · ·  ×ll-1  ×ll++11 · · · ×mm--11 ×01 · · ·  ×ll-1  ×ll++11 · · · ×mm--11

A(l)

M(0) 00 · · · 00 M(l-1) 00 M(l+1) 00 · · · 00 M(m-1)

(F.3)

The number of operations to compute one such equation both is O((

m-1 l=0

Il)R),

therefore

a

total

number of O(m(

m-1 l=0

Il)R)

is

required

for

all

m

equations.

Therefore,

evaluating

these

equations

are computationally expensive (which takes O(mR) order as many operations as the size (

m-1 l=0

Il

)

of the original tensor T ), and should be avoided whenever possible.

Tucker decomposition Tucker decomposition provides more general factorization than CP decomposition. Given an m-order tensor T  RI0×I1×···×Im-1, Tucker decomposition factors it into m factor matrices {M(l)}ml=-01, where M(l)  RRl×Il , l  [m] and an additional m-order core tensor C  RR0×R1×···×Rm-1 , where the Tucker ranks Rl's are required to be smaller or equal than
the dimensions at their corresponding modes, i.e. Rl  Il, l  [m].

Ti0,··· ,im-1

R0-1

Rm-1 -1

···

Cr0,...,rm-1

M(r00),i0

·

·

·

M(m-1)
rm-1,im-1

r0=0

rm-1 =0

(F.4a)

T C ×0M(0) ×1 M(1) · · · ×m-1 M(m-1)

(F.4b)

Notice that when R0 = · · · = Rm-1 = R and C is a super-diagonal tensor with all super-diagonal entries to be ones (a.k.a. identity tensor), Tucker decomposition reduces to CP decomposition, and

therefore CP decomposition is a special case of Tucker decomposition. With Tucker decomposition,

a tensor is approximately by (

m-1 l=0

Rl

+

m-1 l=0

IlRl)

entries.

23

Under review as a conference paper at ICLR 2019

The backpropagation equations relating L/T to L/C and {L/M(l)}ml=-01 can be derived similarly as in CP decomposition. First, we derive the equation for C at the entries level:

L Cr0,··· ,rm-1

=

I0 -1 i0 =0

·

·

·

Im-1 -1 im-1 =0

L Ti0,··· ,im-1

M(r00),i0

·

·

·

M(m-1)
rm-1 ,im-1

(F.5)

The equation above, written in tensor notations, reveals an expression in "reversed" Tucker form:

L C

=

L T

×0(M(0)) · · · ×m-1 (M(m-1))

(F.6)

Although the number of operations to evaluate the equation depends on the particular order of ten-

sor multiplications between L/T and {M(l))}ml=-01, the time complexity can be bounded by

O((

m-1 l=0

Rl)(

m-1 l=0

Il

)),

where the

worst case

is

achieved when

Rl

=

Il, l



[m].

Regarding

the backpropagation equations for the factors {M(l)}ml=-01, we again isolate the factor of interest and

rewrite the definition of Tucker decomposition as:

T = C ×0M(0) · · · ×l-1 M(l-1) ×l+1 M(l+1) · · · ×m-1 M(m-1) ×l M(l)

(F.7)

= A(l) ×l M(l)

where the first term is abbreviated as a tensor A(l). Subsequently, we apply the standard backpropagation rule of tensor multiplication in Appendix E and obtain the following equation:

L  M(l)



=

L T

=

L T

×00  · · ·  ×ll--11  ×ll++11  · · ·  ×mm--11 A(l) ×0 (M(0)) · · · ×l-1 (M(l-1)) ×l+1 (M(l+1))

· · · ×m-1 (M(m-1)) ×00  · · ·  ×ll--11  ×ll++11  · · ·  ×mm--11 C

(F.8)

where the second expression is equivalent to the first one, but requires fewer operations. Though the

exact number of operations depends on the order of tensor multiplications, it can be (again) bounded

by O((

m-1 l=0

Rl)(

m-1 l=0

Il)).

Therefore,

the

total

time

complexity

for

all

(m

+

1)

equations

(m

for

{M(l)}ml=-01 and one for C) is bounded by O((m + 1)(

m-1 l=0

Rl

)(

m-1 l=0

Il)),

which

is

also

highly

inefficient and should be avoided in practice.

Tensor-train decomposition Tensor-train decomposition factorizes a m-order tensor into m interconnected low-order core tensors {T (l)}ml=-01, where T (l)  RRl×Il×Rl+1 , l = 1, · · · , m - 2 with T (0)  RI0×R0 , and T (m-1)  RRm-1×Im-1 such that

Ti0 ,...,im-1 T

R0 -1

Rm-2 -1

···

Ti(00,r)0

Tr(01,)i1,r1

·

·

·

T (m-1)
rm-2 ,im-1

r0 =1

rm-2 =1

T (0) ×0-1 T (1) ×-0 1 · · · ×-0 1 T (m-1)

(F.9a) (F.9b)

where the Rl's are known as Tensor-train ranks, which controls the tradeoff between the number of

parameters and accuracy of representation. With Tensor-train decomposition, a tensor is represented

by (R0I0 +

m-2 l=1

Rl Il Rl+1

+

Rm-1Im-1)

entries.

The backpropagation equations are derived following the paper Novikov et al. (2015), although
we reformat them in tensor notations. To begin with, we introduce two sets of auxiliary tensors {P (l)}ml=-01 and {Q(l)}ml=-01 as follows:

P (l) = T (0) ×-0 1 · · · ×-0 1 T (l) = P (l-1) ×0-1 T (l), l = 1, · · · , m - 1 Q(l) = T (l) ×-0 1 · · · ×-0 1 T (m-1) = T (l) ×0-1 Q(l+1), l = 0, · · · , m - 2

(F.10a) (F.10b)

with corner cases as P(0) = T (0) and Q(m-1) = T (m-1). Note that both {P(l)}ml=-01 and {Q(l)}ml=-01 can be computed using dynamic programming (DP) using the recursive definitions

24

Under review as a conference paper at ICLR 2019

above. With these auxiliary tensors, the definition of Tensor-train decomposition in Equation F.9b can be rewritten as:

T = P (l-1) ×0-1 T (l) ×0-1 Q(l+1), l = 0, m - 1 = T (0) ×-0 1 Q(1) = P (m-2) ×-0 1 T (m-1)

(F.11)

Applying the backpropagation rule for tensor contraction twice, the backpropagation equations can be obtained in tensor notations as:

L T (l)

= P (l-1)

×00  · · ·  ×ll--11

L T

×12  · · ·  ×mm--ll-1

Q(l+1), l = 0, m - 1

L T (0)

=

L T

×11  · · ·  ×mm--11

Q(1),

L T (m-1)

= P (m-2)

×00  · · ·  ×mm--22

L T

(F.12)

Neglecting the expense of precomputing the auxiliary tensors {P(l)}ml=-01 and {Q(l)}ml=-01, the time

complexity for the lth equation is O(RlRl+1(

m-1 l=0

Il)),

therefore

the

total

number

of

operations

for all m equations is O((R0 +

m-2 l=0

RlRl+1

+

Rm-1)(

m-1 l=0

Il)),

which

is

again

prohibitively

large for practical use.

Variants of standard decompositions In this paper, tensor decompositions are usually used in
flexible ways, i.e. we will not stick to the standard formats defined in the previous paragraphs.
Indeed, we consider tensor decomposition as a reverse mapping of tensor operations: given a tensor T and a set of operations, the corresponding tensor decomposition aims to recover the input factors {T (l)}ml=-01 such that the operations on these factors return a tensor approximately equal to the given one. In the following, we demonstrate some possibilities using examples:

· The ordering of the modes can be arbitrary. Therefore, CP decomposition of 3-order tensor

T  RI0×I1×I2 can be factorized as Ti0,i1,i2 =

R-1 r=0

Mi(00,)r

Mi(11,)r

Mi(22,)r

or

Ti0 ,i1 ,i2

=

R-1 r=0

Mr(0,i)0

M(r1,i)1

M(r2,i)2 ,

or even Ti0,i1,i2

=

R-1 r=0

Mi(00,)r Mr(1,i)1

M(i22,)r .

It is easy to observe

these decompositions are equivalent to each other if factor matrices are properly transposed.

· A tensor may be partially factorized over a subset of modes. For example, we can define a

partial Tucker decomposition which factors only the last two modes of a 4-order tensor T 

RI0×I1×I2×I3 into a core tensor C  RI0×I1×R2×R3 and two factor matrices M(2)  RR2×I2 ,

M(3)  RR3×I3 such that Ti0,i1,i2,i3 =

R2-1 r2=0

R3-1 r3=0

Ci0 ,i1 ,r2 ,r3

M(r22),i2

Mr(33),i3

or

alternatively

T = C ×2M(2) ×3 M(3) if written in our tensor notations.

· Multiple modes can be grouped into supermode and decomposed like a single mode. For ex-

ample, given a 6-order tensor T  RI0×I1×I2×J0×J1×J2 can be factorized into three factors

T (0)  RI0×J0×R0 , T (1)  RR0×I1×J1×R1 and T (2)  RR1×I2×J2 such that Ti0,j0,i1,j1,i2,j2 =

R2-1 r2=0

R3-1 r3=0

Ti(00,j)0 ,r0

T (1)
r0 ,i1 ,j1 ,r1

Tr(12,)i2,j2 ,

or

more

succinctly

as

T

= T (0) ×-0 1 T (1) ×-0 1 T (2).

where I0 and J0 are grouped into a supermode (I0, J0) and similarly for (I1, J1) and (I2, J2).

Decomp. original
CP TK TT

Notations
T
1 ×00 (M(0) 00 · · · 00 M(m-1)) C(×0M(0) · · · ×m-1 M(m-1))
T (0) ×-0 1 · · · ×-0 1 T (m-1)

O(# of params.)

I

mI

1 m

R

Rm

+

mI

1 m

R

mI

1 m

R2

O(# of backprop ops.) 0
mI R m2 I R mI R2

Table 9: Summary of tensor decompositions. In this table, we summarize three types of tensor

decompositions in tensor notations, and list their numbers of parameters and time complexities to

backpropagate the gradient of a tensor T  RI0×I1×···Im-1 to its m factors (and an additional core

tensor C for Tucker decomposition). For simplicity, we assume all dimensions Il's of T are equal,

and denote the size of T as the product of all dimensions I =

m-1 l=0

Il.

Furthermore,

we

assume

all ranks Rl's (in Tucker and Tensor-train decompositions) share the same number R.

25

Under review as a conference paper at ICLR 2019

G PLAIN TENSOR DECOMPOSITION S ON CONVOLUTIONAL LAYER
In this section, we will show how tensor decomposition is able to compress (and accelerate) the standard convolutional layer in neural networks. In order to achieve this, we first represent the operation of a standard convolutional layer in tensor notations. By factorizing the tensor of parameters (a.k.a. kernel) into multiple smaller factors, compression is achieved immediately.
As we discussed in Appendix F, learning the factors through the gradient of the original tensor of parameters is highly inefficient. In this section, we provide an alternative strategy that interacts the input with the factors individually, in which explicit reference to the original kernel is avoided. Therefore, our strategy also reduces the computational complexity along with compression.
For simplicity, we assume FFT is not used in computing convolutions, although we show in Appendix E that FFT can possibly speed up the backward pass Mathieu et al. (2013).

Standard convolutional layer In modern convolutional neural network (CNN), a standard convolutional layer is parameterized by a 4-order kernel K  RH×W ×S×T , where H and W are height
and width of the filters (which are typically equal), S and T are the number of input and output channels respectively. A convolutional layer maps a 3-order tensor U  RX×Y ×S to another 3order tensor V  RX×Y ×T , where X and Y are the height and width for the input feature map, while X and Y  are the ones for the output feature map, with the following equation:

S-1

Vx,y,t =

Ki,j,s,t Ui+dx,j+dy,s

s=0 i,j

(G.1)

where d is the stride of the convolution layer and the scopes of summations over i and j are determined by the boundary conditions. Notice that the number of parameters in a standard convolutional layer is HW ST and the number of operations needed to evaluate the output V is O(HW ST XY ). With tensor notations in this paper, a standard convolutional layer can be defined abstractly as

V = U 00  11  ×22 K

(G.2)

which states that the standard convolutional layer in fact performs a compound operation of two tensor convolutions and one tensor contraction simultaneously between the input tensor U and the kernel of parameters K. Following the standard procedure in Appendix E, we obtain both backprop-
agation equations in tensor notations as follows:

L U

=

L V

(00)  (11)  ×32

K,

L K

=U

(00)  (11)

L V

(G.3)

It is not difficult to verify that the numbers of operations to compute these two backpropagation equations are O(HW ST XY ) and O(XY ST XY ) respectively.

In the next few paragraphs, we will apply various decompositions in Appendix F as well as singular value decomposition (SVD) on the kernel K, and derive the steps to evaluate Equation G.1 that interact the input with the factors individually. Interestingly, these steps are themselves (non-standard)
convolutional layers, therefore tensor decomposition on the parameters is equivalent to decoupling
a layer in the original model into several sublayers in the compressed network, which can be imple-
mented efficiently using modern deep learning libraries. For simplicity, we assume in the analyses
of these decomposition schemes that the output feature maps have approximately the same size as the input ones, i.e. X  X, Y  Y .

SVD-convolutional layer Many researchers propose to compress a convolutional layer using singular value decomposition, under the name of dictionary learning (Jaderberg et al., 2014; Denton et al., 2014; Zhang et al., 2015). These methods differ in their matricization of the tensor of parameters K, i.e. how to group the four modes into two and flatten the kernel K into a matrix. By simple combinatorics, it is not difficult to show there are seven different types of matricization in total. Here, we only pick to present the one by Jaderberg et al. (2014), which groups filter height

26

Under review as a conference paper at ICLR 2019

and input channels as a supermode (H, S) and filter width and output channels (W, T ) as another.

R-1

Ki,j,s,t =

Ki(,0s),r Kj(1,r),t

r=0

K = swapaxes K(0) ×21 K(1)

(G.4a) (G.4b)

where K(0)  RH×S×R and K(1)  RW ×R×T are the two factor tensors. It is easy to see an SVDconvolutional layer has (HS + W T )R parameters in total (HSR in K(0) and W T R in K(1)). Now we plug the Equation G.4a into G.1, and break the evaluation of V into two steps such that only one
factor is involved at each step.

S-1

Ux(0,j)+dy,r =

Ki(,0s),r Ui+dx,j+dy,s

s=0 i

R-1

Vx,y,t =

Kj(,1r),t Ux(0,j)+dy,r

r=0 j

(G.5a) (G.5b)

where U (0)  RX×Y ×R is the intermediate result after the first step. This two-steps procedure requires only requires O((HSXY +W T XY )R) operations in the forward pass (O(HSRXY ) at the first step and O(W T RXY ) at the second). This number is smaller than O(HW ST XY ) as in the standard convolutional layer, since (HS + W T )R  HW ST implies (HSXY + W T XY )R  (HS + W T )RXY  HW ST XY . Therefore, SVD-convolutional layer also outperforms the standard one in efficiency. (Notice that if K is reconstructed explicitly, the forward pass requires at least O(HW ST XY ) operations.) In tensor notations, these steps can be written concisely as follows:

U (0) = U 00  ×21 K(0) V = U (0) 11  ×12 K(1)

(G.6a) (G.6b)

After decomposition, each operation is still a compound operation of tensor convolution and tensor contraction, and therefore itself a convolutional layer whose filters have size either H × 1 and 1 × W . Effectively, SVD-convolutional layer is in fact a concatenation of two convolutional layers without nonlinearity in between. Now we proceed to derive the corresponding backpropagation equations for these two steps following the procedure in Appendix E, which are presented in the following:

L U (0)

=

L V

L U

=

L U (0)

(10)  ×22 (00)  ×22

K(1) ,

L  K(1)

= U (0)

×00  (11)

L V

K(0) ,

L  K(0)

=U

(00)  ×11

L U (0)

(G.7a) (G.7b)

It is not hard to show the number of operations required to obtain the derivatives with respect to U and U (0) is O((HSXY + W T XY )R), and the one for the factors is O((XSXY + Y T XY )R).

CP-convolutional layer Both Lebedev et al. (2014); Denton et al. (2014) propose to decompose the kernel K using CP decomposition, differing at whether the height H and width W of the filters are grouped into a supermode. For simplicity, we follow the scheme in Denton et al. (2014):

R-1

Ki,j,s,t =

Ks(0,r) Ki(,1j),r Kr(2,t)

r=0

K = 1 ×02 K(1) 21 K(0) 20 K(2)

(G.8a) (G.8b)

where K(0)  RS×R, K(1)  RH×W ×R and K(2)  RR×T are three factor tensors, which contain (HW + S + T )R parameters in total. Again, plugging Equation G.8a into G.1 yields a three-steps

27

Under review as a conference paper at ICLR 2019

procedure to evaluate V:

S-1

Ui(+0)dx,j+dy,r =

Ks(0,r) Ui+dx,j+dy,s

s=0

(G.9a)

Ux(1,y),r =

Ki(,1j),r Ui(+0)dx,j+dy,r

i,j

(G.9b)

R-1

Vx,y,t =

Kr(2,t) Ux(1,y),r

(G.9c)

r=0

where U (0)  RX×Y ×R and U (1)  RX×Y ×R are two intermediate tensors. Written in tensor

notations, these equations are represented as:

U (0) = U ×2 K(0)

(G.10a)

U (1) = U (0) 00  11  22 K(1)

(G.10b)

V = U (1) ×2 K(2)

(G.10c)

After CP decomposition, the first and third steps are basic tensor multiplications on the input/intermediate tensor, which are usually named (weirdly) as 1 × 1 convolutional layers despite

that no convolution is involved at all, while the second step is a compound operation of two

tensor convolutions and one partial outer product, which is known as depth-wise convolutional layer (Chollet, 2016). The number of operations for these three steps are O(SRXY ), O(HW RXY ) and O(T RXY ) respectively, resulting in a time complexity of O((SXY + HW XY + T XY )R) for the forward pass, which is faster than the standard convolutional layer, since (HW + S + T )R  HW ST implies (SXY + HW XY + T XY )R  HW ST XY . Now we proceed to obtain their

backpropagation equations following the procedure in Appendix E:

L U (1)

=

L V

×2

(K(2) ) ,

L  K(2)

= U (1)

×00  ×11

L V

(G.11a)

L U (0)

=

L U (1)

(00)  (11)  22

K(1) ,

L  K(1)

=

L U (1)

(00)  (11)  22

U (1)

L U

=

L U (0)

×2 (K(0)),

L  K(0)

=U

×00  ×11

L U (0)

(G.11b)

The number of operations in all three steps to calculate the derivatives with respect to in-

put/intermediate tensors can be counted as O((SXY + HW XY  + T XY )R), while the one

for the factors as O((SXY + XY XY  + T XY )R).

Tucker-convolutional layer The use of Tucker decomposition to compress and accelerate convo-

lutional layers is proposed in Kim et al. (2015). Despite the name of Tucker decomposition, they in

fact suggest a partial Tucker decomposition, which only factorizes the modes over the numbers of

input/output filters and keeps the other two modes for filter height/width untouched.

Rs-1 Rt-1

Ki,j,s,t =

Ks(0,r)s Ki(,1j),rs,rt Kr(2t,)t

rs=0 rt=0

(G.12a)

K = K(1) ×2(K(0)) ×3 K(2)

(G.12b)

where K(0)  RS×Rs , K(1)  RH×W ×Rs×Rt and K(2)  RRt×T are three factor tensors, with a total of (SRs + HW RsRt + RtT ) parameters. All that follow are identical to the ones for SVD and CP layers. A three-steps forward pass procedure is obtained by plugging Equation G.12a in G.1.

S-1

Ui(+0)dx,j+dy,rs =

Ks(0,r)s Ui+dx,j+dy,s

s=0

(G.13a)

Rs -1

Ux(1,y),rt =

Ki(,1j),rs,rt Ui(+0)dx,j+dy,rs

rs=0 i,j

(G.13b)

Rt -1

Vx,y,t =

Kr(2t,)t Ux(1,y),rt

rt=0

(G.13c)

28

Under review as a conference paper at ICLR 2019

where U (0)  RX×Y ×Rs and U (1)  RX×Y ×Rt are two intermediate tensors. These three steps, with number of operations of O(RsSXY ), O(HW RsRtXY ) and O(RtT XY ) respectively, leads to a total time complexity of O(SRsXY + HW RsRtXY + RtT XY ) for the forward pass. Like CP and SVD convolutional layers, Tucker-convolutional layer is faster than the standard convolutional layer, since SRs + HW RsRt + RtT  HW ST implies SRsXY + HW RsRtXY + RtT XY   HW ST XY . These equations, again, can be concisely written in tensor notations:

U (0) = U ×2 K(0) U (1) = U (0) 00  11  ×22
V = U (1) ×2 K(2)

K(1)

(G.14a) (G.14b) (G.14c)

where the first and the third steps are two 1 × 1 convolutional layers, and the second step is itself a standard convolutional layer, which only differs from CP-convolutional layer at the second step. For completeness, we summarize all backpropagation equations in the following:

L U (0)

=

L U (1)

L U (1)

=

L V

×2 (K(2)),

L  K(2)

=

U (1)

×00  ×11

L V

(00)  (11)  ×23

K(1),

L  K(1)

=

L U (1)

(00)  (11)

L U

=

L U (0)

×2 (K(0)),

L  K(0)

=

U

×00  ×11

L U (0)

U (1)

(G.15a) (G.15b) (G.15c)

Referring to the CP-convolutional layer, the time complexity for the backward pass is obtained with slight modification: the number of operations for the input/intermediate tensors is O(SRsXY + HW RsRtXY  + RtT XY ), and the one for factors is O(SRsXY + XY HW XY  + RtT XY ).

Tensor-train-convolutional layer Lastly, we propose to apply Tensor-train decomposition to compress a convolutional layer. However, naive Tensor-train decomposition on the kernel K may give
inferior results (Garipov et al., 2016), and careful reordering of the modes is necessary. In this paper, we propose to reorder the modes as (input channels S, filter height H, filter width W , output channels T ), and decompose the kernel as

Rs-1 R-1 Rt-1

Ki,j,s,t =

Ks(0,r)s Kr(1s),i,r Kr(2,j),rt Kr(3t,)t

rs=0 r=0 rt=0

K = swapaxes K(0) ×-0 1 K(1) ×-0 1 K(2) ×-0 1 K(3)

(G.16a) (G.16b)

where K(0)  RS×Rs , K(1)  RRs×H×R, K(2)  RR×W ×Rt and K(3)  RRt×T are factors, which
require (SRs+HRsR+W RRt+RtT ). Once we plug the decomposition scheme in Equation G.16a into G.1, the evaluation of V is decoupled into four steps , with number of operations as O(RsSXY ), O(HRsRXY ), O(W RRtXY ) and O(RtT XY ) respectively, resulting in a total time complexity as O(RsSXY + HRsRXY + W RRT XY + RtT XY ) in the forward pass.

S-1

Ui(+0)dx,j+dy,rs =

Ks(0,r)s Ui+dx,j+dy,s

s=0

Rs -1

Ux(1,j)+dy,r =

Kr(1s),i,r Ui(+0)dx,j+dy,rs

rs=0 i

R-1

Ux(2,y),rt =

Kr(2,j),rt Ux(1,j)+dy,r

r=0 j

Rt-1

Vx,y,t =

Kr(3t,)t Ux(2,y),rt

rt=0

(G.17a) (G.17b) (G.17c) (G.17d)

29

Under review as a conference paper at ICLR 2019

where U (0)  RX×Y ×Rs , U (1)  RX×Y ×R and U (2)  RX×Y ×Rt are three intermediate tensors. In tensor notations, these equations can be rewritten as as follows:

U (0) = U ×2 K(0)
U (1) = U (0) (10  ×02) K(1) U (2) = U (1) (11  ×02) K(2)
V = U (2) ×2 K(3)

(G.18a) (G.18b) (G.18c) (G.18d)

Tensor-train-convolutional layer is concatenation of four sub-layers, where the first and the last ones are 1 × 1 convolutional layers, while the other two in between are convolutional layers with rectangular kernels. In fact, Tensor-train-convolutional layer can either be interpreted as (1) a Tucker-convolutional layer where the second sublayer is further compressed by a SVD, or (2) a SVD-convolutional layer where both factors are further decomposed again by SVD. Referring to the previous results, the corresponding backpropagation equations are easily derived as

L U (2)

=

L V

×2

(K(3) ) ,

L  K(3)

=

U (2)

×00  ×11

L V

L U (1)

=

L U (2)

(01)  ×22

K(2) ,

L  K(2)

= U (1)

×00  (11)

L U (2)

L U (0)

=

L U (1)

(00)  ×22

K(1) ,

L  K(2)

= U (0)

(00)  ×11

L U (1)

L U

=

L U (0)

×2

(K(0) ) ,

L  K(0)

=

U

×00  ×11

L U (0)

(G.19a) (G.19b) (G.19c) (G.19d)

Similar to all previous layers, the time complexities for input/intermediate tensors and factors can be calculated as O(SRsXY +HRsRXY +W RtT XY +RtT XY ) and O(SRsXY +XRsRXY + Y RtRXY  + RtT XY ) respectively.

Decomp. original
SVD CP
TK
TT

O(# of params.) O(# of forward ops.)
HW ST HW ST XY
(HS + W T )R HSXY + W T XY )R
(HW + S + T )R (SXY + HW XY + T XY )R
(HW RsRt+ SRs + RtT ) (HW RsRtXY + SRsXY + RtT XY ) (SRs + HRsR+ W RtR + RtT ) (SRsXY + HRsRXY + W RtRXY + RtT XY )

O(# of backward ops. for inputs)
O(# of backward ops. for params.) HW ST XY  XY ST XY 
(HSXY + W T XY )R (XSXY + Y T XY )R (SXY + HW XY  + T XY )R (SXY + XY XY  + T XY )R
(HW RsRtXY + SRsXY + RtT XY )
(XY RsRtXY + SRsXY + RtT XY ) (SRsXY + HRsRXY + W RtT XY  + RtT XY )
(SRsXY + XRsRXY + Y RtRXY  + RtT XY )

Table 10: Summary of plain tensor decomposition on convolutional layer. We list the number
of parameters and the number of operations required by forward/backward passes for various plain
tensor decomposition on convolutional layer. For reference, a standard convolutional layer maps a set of S feature maps with height X and width Y , to another set of T feature maps with height X and width Y . All filters in the convolutional layer share the same height H and width W .

H RESHAPED TENSOR DECOMPOSITION S ON DENSE LAYER
The operation of dense layer (a.k.a. fully connected layer) in neural network can be simply characterized by a matrix-vector multiplication, which maps a vector u  RS to another vector v  RT ,
30

Under review as a conference paper at ICLR 2019

where S and T are the number of units for the input and output respectively.

S-1
vt = Ks,tus
s=0
v=Ku

(H.1a) (H.1b)

It is easy to see that a dense layer is parameterized by a matrix K with ST parameters, and evaluating the output v requires O(ST ) operations. With a matrix at hand, the simplest compression is via singular value decomposition (SVD), which decomposes K into multiplication of two matrices K = P Q, where P  RS×R, Q  RR×T with R  min(S, T ). With SVD decomposition, the number of parameters is reduced from ST to ((S +T )R) and time complexity from O(ST ) to O((S +T )R).

Inspired by the intuition that invariant structures can be exploited by tensor decompositions in

Section 3, we tensorize the matrix K into a tensor K  RS0×···×Sm-1×T0×···×Tm-1 such that

S=

m-1 l=0

Si,

T

=

m-1 l=0

Tl

and vec(K)

=

vec(K).

Correspondingly, we reshape the in-

put/output u, v into U  RS0×···×Sm-1 , V  RT0×···×Tm-1 such that vec(U ) = u, vec(V) = v and

present an (uncompressed) tensorized dense layer as follows:

S0-1

Sm-1 -1

Vt0,··· ,tm-1 =

···

K Us0,··· ,sm-1,t0,··· ,tm-1 s0,··· ,sm-1

s0 =0

sm-1 =0

V = U ×00  · · ·  ×mm--11 K

(H.2a) (H.2b)

Therefore, a tensorized dense layer, parameterized by a 2m-order tensor K, maps an m-order tensor U to another m-order tensor V. It is straightforward to observe that the tensorized dense layer is mathematically equivalent is to the dense layer in Equation H.1a. Correspondingly, its backpropaga-
tion equations can be obtained by simply reshaping the ones for standard dense layer:

L u

=

K

L v

,

L K

=u

L v



=

u



L v

L U

=

K

×m0 · · · ×m2m--11

L V

,

L K

=U



L V

(H.3) (H.4)

In the section, we will compress the tensorized dense layer by decomposing the kernel K into mul-

tiple smaller factors. As we will see, the schemes of tensor decompositions used in this section are

not as straightforward as in Appendix F and G, and our principles in their designs are analyzed at

the end of this section. Again, learning the the factors through the gradient of the original tensor is

extremely costly, therefore a multi-steps procedure to compute the output by interacting the input

with the factors individually is desirable. For simplicity of analyses, we will assume for the rest of

the

paper

that

S

and

T

are

factored

evenly,

that

is

Sl



S

1 m

,

Tl



T

1 m

,

l



[m]

and

all

ranks

are

equal to a single number R.

r-CP-dense layer Obviously, the simplest way to factorize K is to perform naive CP decomposition over all 2m modes without grouping any supermode. However, such naive decomposition
leads to significant loss of information, as we discuss at the end of this section. In this paper, we
instead propose to factor the kernel K by grouping (Sl, Tl)'s as supermodes. Concretely, the tensor of parameters K is decomposed as:

R-1

K =s0,··· ,sm-1,t0,··· ,tm-1

Kr(0,s)0 ,t0

·

·

·

K(m-1)
r,sm-1 ,tm-1

r=0

K = swapaxes 1 ×00 (K(0) 00 · · · 00 K(m-1)

(H.5a) (H.5b)

where K(l)  RR×Sl×Tl , l  [m] are m factors and R is the canonical rank that controls the

tradeoff between the number of parameters and the fidelity of representation. Therefore, the total

number of parameters of a r-CP-dense layer is approximately

m-1 l=0

SlTlR



m(ST

)

1 m

R,

which

is

significantly smaller than ST given that R is reasonably small. The next step to derive the sequential

procedure mirrors the ones in all schemes in Appendix G, by plugging the Equation H.5a into H.2a,

31

Under review as a conference paper at ICLR 2019

we arrive at a multi-steps procedure for the forward pass.

Ur(,0s)0,··· ,sm-1 = Us0,··· ,sm-1

Sl -1

U (l+1)
r,sl+1,··· ,sm-1,t0,··· ,tl

=

K U(l) (l)
r,sl,tl r,sl,··· ,sm-1,t0,··· ,tl-1

sl =0

(H.6a) (H.6b)

R-1

Vt0,t1,··· ,tm-1 =

Ur(,mt0),··· ,tm-1

r=0

(H.6c)

where the m intermediate results U (l)  R ,R×Sl×···Sm-1×T0×···×Tl-1 l  [m] are (m+1)-order ten-

sors, and the (l + 1)th step above (i.e. the equation interacting U (l) with K(l)) requires O((

l k=0

Sk)

(

m-1 k=l

Tk)R)

operations.

The

number

can

be

bounded

by

O(max(S,

T

)1+

1 m

R),

and equality is

achieved when Sl

=

Tl

=

S1 m

=

T

1 m

.

Since the first and last steps require O(SR) and O(T R) that

are negligible compared to the other m steps, therefore the total number of operations is bounded by

O(m

max(S,

T

)1+

1 m

R).

These

steps

in

tensor

notations

are

presented

as

follows:

U (0) = 1  U

(H.7a)

U (l+1) = U (l) 00  ×11 K(l) V = 1 ×00 U (m)
Following the procedure in Appendix E, their backpropagation equations are obtained as:

(H.7b) (H.7c)

L U (l)

= K(l)

00  ×2-1

L U (l+1)

(H.8a)

L  K(l)

= U (l)

00  ×12  · · ·  ×mm-1

L U (l+1)

(H.8b)

L U (m)

=

1



L V

,

L U

= 1 ×00

L U (0)

As we discussed in Appendix E, if a compound operation does not contain convolution, the time

complexity of backpropagation is identical to the forward pass. Therefore, we claim the number of

operations

required

for

backward

pass

is

also

bounded

by

O(m

max(S,

T

)1+

1 m

R).

r-Tucker-dense layer The application of TK decomposition is rather straightforward, which factors the tensor of parameters K exactly the same as in Appendix F.

R0s -1

Rms -1-1 Rt0-1

Rtm-1 -1

K =s0,··· ,sm-1,t0,··· ,tm-1

···

···

r0s =0

rms -1=0 r0t =0

rmt -1=0

P · · · P C Q · · · Q(0)
s0 ,r0s

(m-1)

(0)

sm-1,rms -1 r0s,··· ,rms -1,r0t ,··· ,rmt -1 r0t ,t0

(m-1) rmt -1,tm-1

K = C ×0(P(0)) · · · ×m-1 (P(m-1)) ×m Q(0) · · · ×2m-1 Q(m-1)

(H.9a) (H.9b)

where P(l)  RSl×Rsl , l  [m] are named as input factors, C  RR0s×···×Rsm-1×Rt0×···×Rmt -1 as core factor, and lastly Q(l)  RRlt×Tl , l  [m] as output factors. Similar to Tucker-convolutional
layer, the procedure to evaluate the result V can be broken into three steps, by plugging Equa-

tion H.9a in H.2a.

S0-1

Sm-1 -1

U =(0)
r0s,··· ,rms -1

···

P(s00),r0s

·

·

·

P U(m-1)
sm-1,rms -1 s0,···

,sm-1

s0 =0

sm-1 =0

(H.10a)

Rs0 -1

Rms -1-1

U = · · · C U(1)
r0t ,··· ,rmt -1

r0s =0

rms -1=0

r0s,··· ,rms -1,r0t ,···rmt -1

(0) r0s,··· ,rms -1

(H.10b)

R0t -1

Rmt -1-1

Vt0,··· ,tm-1 =

···

Qr(00t),t0

·

·

·

Q U(m-1)

(1)

rmt -1,tm-1 r0t ,··· ,rmt -1

r0t =0

rmt -1=0

(H.10c)

32

Under review as a conference paper at ICLR 2019

where the first and last steps are compound operations between a tensor and a set of multiple ten-

sors, while the middle step is a multi-operations between two tensors. Under the assumptions that

Sl



S

1 m

,

Tl



T

1 m

,

Rls

=

Rlt

=

R, l



[m],

the

order

to

contract

the

factors

in

the

first

and

last steps makes no difference, therefore we assume the order follows the indices without loss of

generality. With this strategy, the contraction with the lth factor takes O((

l k=0

Rk )(

m-1 k=l

Sk

))

operations,

which

is

roughly

O(Rl+1S

m-l m

)

and

can

be

further

bounded

by

O(SR)

since

R



S

1 m

by the definition of Tucker decomposition: therefore, the time complexity to contract all m factors

is at most O(mSR). Likewise, the number of operations for the last step can also be bounded by O(mT R). Lastly, it is easy to see the middle step needs O(R2m) operations, therefore leads to a total time complexity of O(m(S + T )R + R2m) for the three-step procedure. In tensor notations,

these equations can be concisely written as

U (0) = U ×0P(0) · · · ×m-1 P(m-1)

(H.11a)

U (1) = U (0) ×00  · · ·  ×mm--11 C V = U (1) ×0Q(0) · · · ×m-1 Q(m-1)

(H.11b) (H.11c)

Though compound in nature, the procedure to derive their backpropagation rules are pretty straight-

forward: notice the equations for the first and last steps have the exactly the same form as standard

Tucker decomposition in Appendix F. Therefore, we can simply modify the variable names therein

to obtain the backpropagation equations for these two steps.

L U (1)

=

L V

×0(Q(0)) · · · ×m-1 (Q(m-1))

L  Q(l)



=

L V

×00  · · ·  ×ll--11  ×ll++11  · · ·  ×mm--11

(H.12a)

U (1) ×0Q(0) · · · ×l-1 Q(l-1) ×l+1 Q(l+1) · · · ×m-1 Q(m-1)

(H.12b)

L U

=

L U (0)

×0(P(0)) · · · ×m-1 (P(m-1))

L  P(l)



=

L U (0)

×00  · · ·  ×ll--11  ×ll++11  · · ·  ×mm--11

(H.12c)

U ×0P(0) · · · ×l-1 P(l-1) ×l+1 P(l+1) · · · ×m-1 P(m-1)

(H.12d)

The step in the middle is itself a tensorized layer defined in Equation H.2b, therefore its backpropa-

gation rules can be obtained by renaming the variable in Equations H.4.

L U (0)

=

C

×m0  · · ·  ×m2m--11

L C

=

U (0)



L U (1)

L U (1)

(H.13a) (H.13b)

Despite their technical complicity, we can resort to conclusion that the complexities for forward

and backward passes are the same for operations without convolution, and claim that the number of operations required for the backpropagation equations above is bounded by O(m(S + T )R + R2m).

r-Tensor-train-dense layer The layer presented in this part follows closely the pioneering work in compressing network using tensor decompositions (Novikov et al., 2015), except that we replace the backpropagation algorithm in the original paper (as discussed in Appendix F) with a multistep procedure similar to all other layers in this paper. With the replacement, the efficiency of the backward is greatly improved compared to original design. Similar to r-CP-dense layer, we will group (Sl, Tl)'s as supermodes and decomposed the kernel K by Tensor-train decomposition following the order of their indices:

R0-1

Rm-2 -1

K =s0,··· ,sm-1,t0,··· ,tm-1

···

Ks(00),t0

,r0

K(1)
r0 ,s1 ,t1 ,r1

·

·

·

K(m-1)
rm-2 ,sm-1,tm-1

r0=0

rm-2 =0

(H.14a)

K = swapaxes K(0) ×-0 1 K(1) ×-0 1 · · · ×-0 1 K(m-1)

(H.14b)

33

Under review as a conference paper at ICLR 2019

where the factor tensors are K(l)  RRl-1×Sl×Tl×Rl , l = 1, · · · , m - 2, with two corner cases

K(0)  RS0×T0×R0 and K(m-1)  R .Rm-2×Sm-1×Tm-1 The number of parameters in its lth factor

K(l)

is

approximately

(S

T

)

1 m

R2

,

therefore

the

layer

has

O(m(S

T

)

1 m

R2

)

parameters

in

total.

For

aestheticism, we add singleton mode R-1 = 1 to U , K(0) such that U  RS0×···×Sm-1×R-1 , K(0)  RR-1×S0×T0×R0 , and rename U as U (0). Now insert the Equation H.14a into H.2a and expand

accordingly, we obtain an (m + 1)-steps procedure to evaluate the output V:

Rl-1-1 Sl-1

U =(l+1)
sl+1,··· ,sm-1,t0,··· ,tl,rl

K U(l) (l)
rl-1,sl,tl,rl sl,··· ,sm-1,t0,··· ,tl-1,rl-1

rl-1=0 sl=0

(H.15)

where the last result U(m) is equal to the final output V, and the other m tensors U(l)'s are interme-

diate results. Similar to r-CP-dense layer, the number of operations at the lth step is O((

l k=0

Sk)

(

m-1 k=l

Tl)Rl-1Rl)

and

can

be

bounded

by

O(max(S,

T

)1+

1 m

R2),

therefore

the

time

complexity

for

all

m-steps

is

bounded

by

O(m

max(S,

T

)1+

1 m

R2).

These

steps

very

simple

in

tensor

notations.

U (l+1) = U (l) ×10  ×-0 1 K(l)

(H.16)

Observe that the operations in the forward pass are entirely tensor contractions, therefore their backpropagation equations are easily derived following the procedure in Appendix E.

L U (l)

= K(l)

×1-2  ×2-1

L U (l+1)

L  K(l)

=

swapaxes

U (l) ×01  · · ·  ×mm--12

L U (l+1)

(H.17a) (H.17b)

In the analysis of the backward pass, we can again take advantage of the argument that the forward

and backward passes share the same number of operations. Therefore, we claim the time complexity

for

backpropagation

is

bounded

by

O(m

max(S,

T

)1+

1 m

R2).

Relation to tensor contraction layer: In Kossaifi et al. (2017a), the authors propose a novel ten-
sor contraction layer, which takes a tensor of arbitrary order as input and return a tensor of the same order. Formally, a tensor contraction layer, parameterized by a set of m matrices {M(l)})ml=-01, with M(l)  RSl×Tl , l  [m], maps a m-order tensor U  RS0×···×Sm-1 to another m-order tensor V  RT0×···Tm-1 such that

S0-1

Sm-1 -1

Vt0,··· ,tm-1 =

···

Us0,··· ,sm-1

M(s00),t0

·

·

·

M(m-1)
sm-1 ,tm-1

s0 =0

sm-1 =0

V = U ×0M(0) · · · ×m-1 M(m-1)

(H.18a) (H.18b)

It is not difficult to observe that the tensor contraction layer is in fact special case of r-CP-dense layer

where

the

kernel

is

restricted

to

rank-1, that

is

Ks0,··· ,sm-1,t0,··· ,tm-1

=

Ms(00),t0

·

·

·

M(m-1)
sm-1 ,tm-1

,

or

equivalently K = M(0)  · · ·  M(m-1) in our tensor notations.

Relation to tensor regression layer: Along with the tensor contraction layer, tensor regression
layer is also proposed in Kossaifi et al. (2017b), which takes a tensor of arbitrary order as input and maps it to a scalar. Formally, given an m-order tensor U  RS0×···×Sm-1, it is reduced to a scalar v by contracting of all the modes with another tensor of the same size K  RS0×···×Sm-1.

S0-1

Sm-1 -1

v = ···

U Ks0,··· ,sm-1 s0,··· ,sm-1

s0 =0

sm-1 =0

v = U ×00  ×11  · · ·  ×mm--11 K

(H.19a) (H.19b)

where the tensor K is stored in Tucker-format as in Equation F.4a. Therefore, the tensor regression layer is effectively parameterized by a set of matrices {M(l)}ml=-01, M(l)  RSl×Rl , l  [m], with

34

Under review as a conference paper at ICLR 2019

an additional core tensor C  RR0×···×Rm-1. Therefore the definition of tensor regression layer in Equation H.19a can also be rephrased as

S0 -1

Sm-1 -1

v = ···

Us0,··· ,sm-1

Ms(00),r0

·

·

·

M(m-1)
sm-1 ,rm-1

Cr0,··· ,rm-1

s0=0

sm-1 =0

= U ×0M(0) · · · ×m-1 M(m-1) ×00  · · ·  ×mm--11 C

(H.20a) (H.20b)

Now we are able to observe that the tensor regression layer is indeed a special case of r-Tuckerdense layer where the input factors P(l) = M(l), l  [m], while the output factors Q(l)'s are simply scalar 1's with Tl = 1, l  [m].

Comments on the designs: As we can observe, the design of r-Tucker-dense is different from the other two layers using CP and Tensor-train decompositions, in which (Sl, Tl)'s are first grouped into supermodes before factorization. Indeed, it is a major drawback in the design of r-Tucker-dense layer: notice that the first intermediate result U(0) obtained in the forward pass is a tensor of size R0s×R1s · · ·×Rms -1, which becomes very tiny if the kernel K is aggressively compressed. Therefore, the size of the intermediate tensor poses an "information bottleneck", causing significant loss during the forward pass, which is verified by our experimental results in Section app:experiments. Therefore, the use of r-Tucker-dense layer is not recommended when we expect excessive compression rate. On the other hand, by grouping (Sl, Tl)'s as supermodes in r-CP-dense layer and r-Tensor-traindense layer, all intermediate tensors U(l)'s have similar size as the input, therefore the bottleneck in r-Tucker-dense layer is completely avoided.
But why do we not group (Sl, Tl)'s together in the design of Tucker-dense layer at the beginning? In theory, we are for sure able to factorize the kernel as

R0-1

Rm-1 -1

K = ···

Cr0,··· ,rm-1

Kr(00),s0,t0

·

·

·

K(m-1)
rm-1 ,sm-1

,tm-1

r0=1

rm-1 =0

(H.21)

However, the contractions among the input and the factors become problematic: (1) interacting the input with the factors K(l)'s yields an intermediate tensor of size T0 × · · · × Tm-1 × R0 × · · · × Rm-1, which is too large to fit into memory; (2) while reconstructing the kernel K from K(l)'s and subsequently invoking the Equation H.2a will make the time complexity for backward pass intractable as we discussed in Appendix F. Therefore, we have to abandon this attempting design, in order to maintain a reasonable time complexity.
As a compensation for the possible loss, the current design of Tucker-dense layer actually has one benefit over the other two layers: the numbers of operations for the backward pass remains at the same order as the number of parameters, while the number of operations required by r-CP-dense and r-Tensor-train-dense layers are orders higher. As a result, r-Tucker-dense layer is much faster than r-CP-dense and r-Tensor-train-dense layers at the same compression rate. Therefore, r-Tucker-dense layer is more desirable if we value speed over accuracy and the compression rate is not too high.

Decomp. original
SVD
r-CP r-TK r-TT

O(# of params.)

ST

(S + T )R

m(S

T

)

1 m

R

m(S

1 m

+T

1 m

)R

+

R2m

m(S

T

)

1 m

R2

O(# of forward/backprop ops.)

ST

(S + T )R

m

max(S,

T

)1+

1 m

R

m(S + T )R + R2m

m

max(S,

T

)1+

1 m

R2

Table 11: Summary of reshaped tensor decomposition on dense layer. In this table, we list

the numbers of parameters and time complexities of forward/backward passes required by various

reshaped tensor decompositions on dense layer. For simplicity, we assume that the number of input

units

S

and

outputs

units

T

are

factorized

evenly,

i.e.

Sl

=

S

1 m

,

Tl

=

T

1 m

,

l



[m]

and

all

ranks

(in r-TK and r-TT) share the same number R, i.e. Rl = R, l  [m].

35

Under review as a conference paper at ICLR 2019

I RESHAPED TENSOR DECOMPOSITION S ON CONVOLUTIONAL LAYER
In Appendix H, we tensorize the parameters into higher-order tensor in order to exploit their invariance structures. It is tempting to extend the same idea to convolutional layer such that similar structures can be discovered. In the section, we propose several additional convolutional layers based on the same technique as in Appendix H: the input and output tensors are folded as U  RX×Y ×S0×···×Sm-1 and V  RX×Y ×T0×···×Tm-1 , while the tensor of parameters are reshaped into K  R .H×W ×S0×···×Sm-1×T0×···×Tm-1 Similar to the tensorized dense layer in Equation H.2a, we define a (uncompressed) tensorized convolutional layer equivalent to Equation G.1:

S0 -1

Sm-1 -1

Vx,y,t0,··· ,tm-1 =

···

K Ui,j,s0··· ,sm-1,t0,··· ,tm-1 i+dx,j+dy,s0,··· ,sm-1

s0=0

sm-1=0 i,j

V = U 00  11  ×22  · · ·  ×mm++11 K

(I.1a) (I.1b)

Their corresponding backpropagation equations are then easily obtained by reshaping the ones for standard convolutional layer in Equation G.3.

L U

=

K

L K

=

U

(00)  (11)  ×m2 +2 · · · ×2mm++11

(00)  (11)

L V

L V

(I.2a) (I.2b)

What follows are almost replications of Appendix G and H: applying tensor decompositions in Appendix F to the kernel K and derive the corresponding multi-steps procedures. As we shall expect, all layers in this section mimic their counterparts in Appendix H (in fact they will reduce to counterpart layers when original layer is 1 × 1 convolutional layer). Therefore in this section, we will borrow results from last section whenever possible and only emphasize their differences.

r-CP-convolutional layer In this part, CP decomposition is used in a similar way as in r-CP-
dense layer, which decomposes the kernel K grouping (Sl, Tl)'s as supermodes, and (H, W ) as an additional supermode. Specifically, the tensor K takes the form as

R-1

K =i,j,s0,··· ,sm-1,t0,··· ,tm-1

Kr(0,s)0,t0 · · · Kr(m,sm--1)1,tm-1 Kr(m,i,)j

r=0

K = swapaxes 1 ×00 K(0) 00 · · · 00 K(m)

(I.3a) (I.3b)

where K(l)  RR×Sl×Tl , l  [m] and K(m)  RR×H×W are (m + 1) factors. Notice that Equa-

tion I.3a only differs from Equation H.5a in r-CP-dense layer by an additional factor K(m), therefore

has

HWR

more

parameters

and

reaches

O(m(ST

)1 m

R

+

HW R)

in

total.

Accordingly,

the

multi-

steps procedure to evaluate the output V now has one extra step at the end, and the (m + 2)-steps

algorithm is presented at the entries level as follows:

Ur(,0i)+dx,j+dy,s0,··· ,sm-1 = Ui+dx,j+dy,s0,··· ,sm-1

Sl -1

Ur(,li++1d)x,j+dy,sl+1,··· ,sm-1,t0,··· ,tl =

Kr(l,)sl ,tl

U (l)
r,i+dx,j+dy,sl,··· ,sm-1,t0,··· ,tl-1

sl =0

R-1

Vx,y,t0,··· ,tm-1 =

Kr(m,i,)j Ur(,mi+)dx,j+dy,t0,··· ,tm-1

r=0 i,j

(I.4a) (I.4b) (I.4c)

where U (l)  RR×Sl×···×Sm+1×T0×···×Tl-1 , l  [m] are m intermediate tensors. Notice that the order to interact the (m + 1) factors is arbitrary, that is the convolutional factor U(m) can be convoluted over at any step during the forward pass. In this paper, we place the convolutional factor U(m)
to the end simply for implementational convenience: it is not difficult to recognize that the last step is a 3D-convolutional layer with R input feature volumes and one output feature volume, if we treat the number of feature maps T as depth of the feature volumes. The time complexity in the forward

36

Under review as a conference paper at ICLR 2019

pass are easily obtained through the results of r-CP-dense layer: compared to r-CP-dense layer, each

of the existing m steps will be scaled by a factor of XY , while the additional last step requires

O(HW T RXY ) operations. Therefore, the total number of operations for r-CP-convolutional layer

is

O(m

max(S,

T

)1+

1 m

RX

Y

+ HW T RXY ).

In

tensor

notations, these

steps

are

rephrased

as:

U (0) = 1  U

(I.5a)

U (l+1) = U (l) 00  ×13 K(l)

(I.5b)

V = U (m) ×00  11  22 K(m)

(I.5c)

The backpropagation equations are also very similar to their r-CP-dense layer counterparts. For

completeness, we list all of them in the following:

L U (m)

= K(m)

L  K(m)

= U (m)

(10)  (12)

L V

(01)  (21) ×32  · · ·  ×mm++21

L V

(I.6a) (I.6b)

L U (l)

=

swapaxes

K(l) 00  ×2-1

L U (l+1)

(I.6c)

L  K(l)

= U (l)

00  11  22  ×43  · · ·  ×mm++32

L U

=

1

×00

L U (0)

L U (l+1)

(I.6d) (I.6e)

In the analyses of these backpropagation equations, it is again convenient to make connections to their r-CP-dense layer counterparts: the time complexities for the first m steps are scaled by XY , while the last step of 3D-convolutional layer requires O(HW T RXY ) operations for the derivative with respect to U (m), and O(XY T RXY ) for the gradient of K(m).

r-Tucker-convolutional layer Incorporating the features from both Tucker-convolutional layer in
Appendix G and r-Tucker-dense layer in Appendix H, we propose to apply partial Tucker decomposition on the tensorized kernel K over all modes except the filter height H and width W . Concretely, the tensorized kernel K is factorized as:

R0s -1

Rsm-1-1 Rt0-1

Rtm-1 -1

K =i,j,s0,··· ,sm-1,t0,··· ,tm-1

···

···

r0s =0

rms -1=0 r0t =0

rmt -1=0

P · · · P C Q · · · Q(0)
s0 ,r0s

(m-1)

(0)

sm-1,rms -1 i,j,r0s,··· ,rms -1,r0t ,··· ,rmt -1 r0t ,t0

(m-1) rmt -1,tm-1

(I.7a)

K = C ×2(P(0)) · · · ×m+1 (P(m-1)) ×m+2 Q(0) · · · ×2m+1 Q(m-1)

(I.7b)

where P(l)  RSl×Rsl , l  [m], C  RH×W ×R0s×···×Rms -1×Rt0×···×Rmt -1 and Q(l)  RRlt×Tl , l  [m] are again named as input factors, core factor and output factors respectively. The reason that (Sl, Tl)'s are not grouped into supermodes follows exactly the same arguments as in Appendix H. Compared to r-Tensor-train-dense layer, the only difference is that the core tensor now has two extra
modes for filter height and width, and therefore the number of parameters is magnified by a factor of HW . Similar to Tucker-convolutional and r-Tucker-dense layers, the procedure to evaluate V can be sequentialized into three steps:

S0-1

Sm-1 -1

U (0)
i+dx,j+dy,r0s,··· ,rms -1

=

···

Ps(00),r0s

·

·

·

P U(m-1)
sm-1,rms -1 i+dx,j+dy,s0,··· ,sm-1

s0 =0

sm-1 =0

R0s -1

Rms -1-1

U = · · · C U(1)
x,y,r0t ,··· ,rmt -1

r0s=0

rms -1=0 i,j

i,j,r0s,··· ,rms -1,r0t ,···rmt -1

(0) i+dx,j+dy,r0s,··· ,rms -1

R0t -1

Rmt -1-1

Vx,y,t0,··· ,tm-1 =

···

Q(r00t),t0

·

·

·

Q U(m-1)

(1)

rmt -1,tm-1 x,y,r0t ,···

,rmt -1

r0t =0

rmt -1=0

(I.8a) (I.8b) (I.8c)

37

Under review as a conference paper at ICLR 2019

where U (0)  RX×Y ×Rs0×···×Rms -1 and U (1)  RX×Y ×Rt0×···×Rtm-1 are two intermediate tensors.

Referring to r-Tucker-dense layer, the number of operations of the first step is scaled up by XY , the

number for the middle step by HW XY and the last step by XY . Therefore, the time complexity

for

the

three-steps

process

is

O(mS

1 m

XY

+ HW R2mXY

+

mT

1 m

XY

).

Subsequently, we can

rewrite these steps concisely in tensor notations.

U (0) = U ×2P(0) · · · ×m+1 P(m-1) U (1) = U (0) 00  11 ×22  · · ·  ×mm--11 C
V = U (1) ×2Q(0) · · · ×m+1 Q(m-1)

(I.9a) (I.9b) (I.9c)

In principle, the backpropagation rules for the sequential steps can be derived almost identically as in the r-Tucker-dense layer. For reference, we list all equations for the first and last steps as follows:

L U (1)

=

L V

×2(Q(0)) · · · ×m+1 (Q(m-1))

L  Q(l)



=

L V

×00  · · ·  ×ll++11  ×ll++33  · · ·  ×mm++11

(I.10a)

U (1) ×2Q(0) · · · ×l+1 Q(l-1) ×l+3 Q(l+1) · · · ×m+1 Q(m-1)

(I.10b)

L U

=

L U (0)

×2(P(0)) · · · ×m+1 (P(m-1))

L  P(l)



=

L U (0)

×00  · · ·  ×ll++11  ×ll++33  · · ·  ×mm++11

(I.10c)

U ×2P(0) · · · ×l+1 P(l-1) ×l+3 P(l+1) · · · ×m+1 P(m-1)

(I.10d)

Notice that the middle step itself is a tensorized convolutional layer defined in Equation I.1b, therefore its backpropagation equations are exactly the same as the ones in Equations I.2a and I.2b.

L U (0)

=

C

(00)  (11)  ×m2 +2  · · ·  ×2mm++11

L C

=

U (0)

(00)  (11)

L U (1)

L U (1)

(I.11a) (I.11b)

The analyses for the backpropagation equations mimic the ones in the forward pass, again by com-

parison against the ones in r-Tucker-dense layer: the time complexity to obtain the derivatives in

the first step is magnified by XY , while the ones for the middle step and the last step are scaled by HW XY  and XY  respectively. Therefore, the total number of operations for the derivatives with

respect to

input/intermediate results

is

O(mS

1 m

RX

Y

+ HW R2mXY 

+

mT

1 m

RX

Y

),

and the

number for

the

factors is

O(mS

1 m

RX

Y

+ XY R2mXY 

+ mT

1 m

RX

Y

).

r-Tensor-train-convolutional layer Following the r-CP-convolutional layer, we propose to apply
Tensor-train decomposition to the tensorized kernel K by grouping (Sl, Tl)'s and filter height/width (H, W ) as supermodes. In Tensor-train decomposition, these supermodes are ordered by their indices, with the extra supermode (H, W ) appended to the end. Concretely, the tensorized kernel K
is decomposed as:

R0-1

Rm-1 -1

K =i,j,s0 ,··· ,sm-1,t0,··· ,tm-1

···

Ks(00),t0,r0 Kr(10),s1,t1,r1

·

·

·

K(m)
rm-1

,i,j

r0=0

rm-1 =0

K = swapaxes K(0) ×0-1 K(1) ×-0 1 · · · ×0-1 K(m)

(I.12a) (I.12b)

where K(0)  RS0×T0×R0 , K(l)  RRl-1×Sl×Tl×Rl and K(m)  RRm-1×H×W are (m + 1)

factor tensors. Compared to r-Tensor-train-dense layer, the r-Tensor-train-convolutional layer has

an additional factor K(m) that contains RHW parameters, which leads to a total number of

O((m(ST

)1 m

+HW

)R).

For

conciseness,

we

follow

the

preprocessing

steps

to

add

singleton

mode

38

Under review as a conference paper at ICLR 2019

R-1 = 1 to U and K(0) such that U  RX×Y ×S0×···×Sm-1×R-1 and K(0)  RR-1×S0×T0×R0 and rename U as U(0). As we shall expect, the multi-steps procedure to evaluate V now has (m + 1)
steps, with the last step as a 3D-convolutional layer:

Rl-1-1 Sl-1

Ui(+l)dx,j+dy,··· ,rl =

K U(l) (l)
rl-1,sl,tl,rl i+dx,j+dy,sl,··· ,sm-1,t0,··· ,tl-1,rl-1

rl-1=0 sl=0

(I.13a)

Rm-1 -1

Vx,y,t0,··· ,tm-1 =

K U(m) (m)
rm-1,i,j i+dx,j+dy,t0,··· ,tm-1,rm-1

rm-1=0 i,j

(I.13b)

where U (l)  RX×Y ×Sl×···×Sm-1×T0×···×Tl-1×Rl-1 , l  [m] are the intermediate results. The

number

of

operations

for

the

first

m

steps

is

O(m

max(S,

T

)1+

1 m

RX

Y

)

by

comparing

against

the corresponding steps in r-Tensor-train-dense layer, and the last step of 3D-convolutional layer

requires O(HW RT XY ) operations in the forward pass. In tensor notations, these steps are nicely

represented as:

U (l) = U (l) ×12  ×-0 1 K(l)

(I.14a)

V = U (m) 01  12  ×0-1 K(m)

(I.14b)

The backpropagation rules are easily obtained by modifying the ones in r-Tensor-train-dense layer.

For completeness, we list all backpropagation equations in the following:

L U (m)

=

L V

(01)  (12)

K(m)

L  K(m)

=

L V

(00)  (11)  ×22 · · · ×mm++11

U (m)

L U (l)

=

swapaxes

K(l) ×2-2  ×3-1

L U (l+1)

(I.15a) (I.15b) (I.15c)

L  K(l)

=

swapaxes

U (l) ×00  ×11  ×32 · · · ×mm+1

L U (l+1)

(I.15d)

The analyses of the backward steps again follow by building connections with r-Tensor-train-dense layer: each of the first m steps is scaled by XY , while the last step requires O(HW T RXY ) for
U (m) and O(XY T RXY ) for K(m).

Decomp. original
r-CP
r-TK
r-TT

O(# of params)
O(# of forward ops.)
HW ST HW ST XY
#ParamsCP + HW R #OpsCPXY + HW T RXY
(HW #ParamsTKC + #ParamsTKI + #ParamsTKO
(HW #OpsTKC XY + #OpsTKI X Y + #OpsTKO X Y )
#ParamsTT + HW R #OpsTTXY + HW T RXY

O(# of back ops. for inputs)
O(# of back ops. for params.) HW ST XY  XY ST XY 
#OpsCPXY + HW T RXY  #OpsCPXY + XY T RXY 
(HW #OpsTKC XY + #OpsTKI X Y + #OpsTKO X Y )
(XY #OpsTKC XY + #OpsTKI X Y + #OpsTKO X Y )
#OpsTTXY + HW T RXY  #OpsTTXY + XY T RXY 

In order to compare against reshaped tensor decompositions on dense layer in Table 11, we denote the numbers for the dense layers as:

1
#ParamsCP = m(ST ) m R

#OpsCP

=

m

max(S,

T

)1+

1 m

1
#ParamsTKI = mS m R

#OpsTKI = mSR

#ParamsTK = L #ParamsTKL
#OpsTK = L #OpsTKL #ParamsTKC = R2m #OpsTKC = R2m

#ParamsTT

=

1
m(ST ) m

R2

#OpsTT

=

m

max(S,

T

)1+

1 m

R2

1
#ParamsTKO = mT m R

#OpsTKO = mT R

Table 12: Summary of reshaped tensor decomposition on convolutional layer. In this table, we

list the number of parameters and time complexities required by various reshaped tensor decompo-

sition s on convolutional layer. Recall that a convolutional layer, composed with ST filters of size

H × W , maps a set of S feature maps of size X × Y to another set of T feature maps of size X × Y .

For simplicity, we assume the numbers of input/output feature maps S, T are factorized evenly, i.e.

Sl

=

S

1 m

,

Tl

=

T

1 m

,

l



[m],

and

all

ranks

are

equal

to

R,

i.e

Rl

=

R,

l



[m].

39

