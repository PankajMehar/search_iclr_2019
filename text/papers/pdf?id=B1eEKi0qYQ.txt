Under review as a conference paper at ICLR 2019
INTERACTIVE PARALLEL EXPLORATION FOR REINFORCEMENT LEARNING IN CONTINUOUS ACTION SPACES
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, a new interactive parallel learning scheme is proposed to enhance the performance of off-policy continuous-action reinforcement learning. In the proposed interactive parallel learning scheme, multiple identical learners with their own value-functions and policies sharing a common experience replay buffer search a good policy in collaboration with the guidance of the best policy information in the previous search interval. The information of the best policy parameter of the previous search interval is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search space by the multiple learners. The guidance by the previous best policy and the enlarged search space by the proposed interactive parallel learning scheme enables faster and better policy search in the policy parameter space. A working algorithm is constructed by applying the proposed interactive parallel learning scheme to the TD3 algorithm, and numerical results show that the constructed new algorithm outperforms most of the current state-of-the-art reinforcement learning algorithms for continuous action control.
1 INTRODUCTION
Reinforcement learning (RL) for continuous action control is an active research field. In RL, an agent learns a policy through interaction with the environment to maximize the cumulative reward. One of the key issues in RL is the trade-off between exploitation and exploration. Exploitation is to make a best decision based on the already collected information, whereas exploration is to collect more new information about the environment. The balance between the two is important for good RL algorithms. For example, DQN (Mnih et al. (2015)) balances exploitation and exploration by taking actions based on the -greedy approach. Deep deterministic policy gradient (DDPG) (Lillicrap et al. (2015)) and twin delayed deep deterministic (TD3) (Fujimoto et al. (2018)) policy gradient algorithms promote exploration by adding Ornstein-Uhlenbeck noise and Gaussian noise to the best decision action, respectively. Soft actor-critic (SAC) (Haarnoja et al. (2018)) performs balancing by using a maximum entropy objective. However, most of the previous works focus on exploration to obtain unobserved states or actions.
In this paper, we consider exploration in the policy parameter space by using parallel identical learners for the same environment. By having multiple identical learners for the same environment, we can have increased search capability for a better policy. Parallelism in learning is common in distributed RL (Nair et al. (2015), Mnih et al. (2016), Horgan et al. (2018)), and is shown to be effective in sample generation efficiency and diverse experience. In this paper, we apply parallelism to RL setting to enhance the learning performance.
The proposed algorithm is intended for any off-policy RL algorithms and is composed of a chief, N environment copies of the same environment, and N identical learners with a shared common experience replay buffer and a common base algorithm, as shown in Fig. 1. Each learner has its own value function(s) and policy, and trains its own policy by interacting with its own environment copy with some additional interaction with the chief, as shown in Fig. 1. At each time step, each learner performs an action to its environment copy by using its own policy, stores its experience to the shared common experience replay buffer. Then, each learner updates its value function parameter
1

Under review as a conference paper at ICLR 2019
and policy parameter by drawing mini-batches from the shared common replay buffer by minimizing its own value loss function and policy loss function, respectively.
One way to parallel learning under the above setup is to implement N fully independent parallel learning without interaction among the learners until the end of time steps and to choose the policy from the learner with the maximum accumulated reward at the end for future use. However, this independent method ignores possible benefit from mutual interaction among the learners during training. In order to harness the benefit of mutual interaction among the learners in parallel learning, we exploit the information from the best learner among all learners periodically during training. Suppose that the value parameter and the policy parameter of each learner are initialized and learning is performed as described above for M time steps. At the end of M time steps, we can determine who is the best learner based on the average of the most recent Er episodic rewards for each learner. Then, the policy parameter information of the best learner can be used to enhance the learning of other learners for the next M time steps. This information can help learners stuck in local pitfalls escape from the pitfalls and guide other learners for better direction.
One simple way to exploit this best policy parameter information is that we reset the policy parameter of each learner with the policy parameter of the best learner at the beginning of the next M time steps, make each learner perform learning from this initial point in the policy parameter space for the next M time steps, select the best learner again at the end of the next M time steps, and repeat this procedure every M time steps. However, this method has the problem that the search area covered by all N searching policies is very narrow around the previous best policy point. To overcome such drawback, instead of resetting the policy parameter with the best policy parameter every M time steps, we here propose using the policy parameter information of the best learner in a soft manner to enhance the performance of the overall parallel learning. In the proposed scheme, the shared best policy information is used only to guide the policies of other learners. The policy of each learner is updated by improving the performance around a certain distance from the shared guiding policy. The chief periodically determines the best policy among the policies of all learners and distributes the best policy parameter to all learners so that the learners search for better policies around the previous best policy. The chief also enforces that the N searching policies are spread in the policy parameter space with a given distance from the previous best policy point so that the search area in the policy space by all N learners maintains a wide area and does not collapse into a narrow region.
The proposed interactive parallel exploration (IPE) learning method can be applied to any off-policy RL algorithms and the implementation is easy. Furthermore, this proposed method can be extended directly to distributed RL systems. We apply our IPE scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm, which is a state-of-the-art off-policy algorithm, as our base algorithm, and the new algorithm is named the IPE-TD3 algorithm. Numerical result shows that the IPE-TD3 algorithm outperforms TD3 in both the speed of convergence and the final steady-state performance.
2 BACKGROUND
2.1 PARALLEL LEARNING FOR A COMMON ENVIRONMENT
The considered parallel learning setting consists of the environment E and N policies {1, · · · , N }. The environment E is described as a Markov decision process (MDP), defined by the tuple S, A, T , r , where S is the state space, A is the action space, T : S × A × S  [0, 1] is the state transition probability, and r : S × A  R is the reward function. There exist N copies {E1, · · · , EN } of the environment E, i.e., E1 = · · · = EN = E, and the N environment copies may have different random initial seeds. The policy i interacts with its corresponding environment copy Ei and builds up its trajectory {(sit, ati, rti), t = 1, 2, · · · } for each i = 1, · · · , N . At time step t, the environment copy Ei has a state sit  S. The policy i interacts with the environment copy Ei by taking an action ait according to i given the current state sit. Then, the environment copy Ei yields the reward rti = r(sti, ait) and makes transition to the next state sti+1 according to T .
In this paper, in order to account for the actual amount of interaction with the environment, we define environment steps as the total number of interactions by all N parallel policies with all N environment copies. Suppose that all N policies generate their trajectories simultaneously in parallel, and
2

Under review as a conference paper at ICLR 2019

Learners
Env. 
  

   


Episodic Reward Distance Info. Parameter 


Parameters  

Update  

Chief

Replay Buffer 

Parameters  

Figure 1: The overall structure of the proposed IPE scheme

suppose that M time steps have elapsed. Then, although the number of elapsed time steps is M , the number of environment steps is N M .

2.2 THE TWIN DELAYED DEEP DETERMINISTIC POLICY GRADIENT ALGORITHM

Although the proposed IPE method can be applied to any off-policy RL algorithms, we here consider

the TD3 algorithm (Fujimoto et al. (2018)) as our base algorithm in IPE. The TD3 algorithm is a

current state-of-the-art off-policy algorithm and is a variant of the deep deterministic policy gradient

(DDPG) algorithm (Lillicrap et al. (2015)). The TD3 algorithm tries to resolve two problems in

typical actor-critic algorithms: 1) overestimation bias and 2) high variance on the approximation

of the Q-function. In order to reduce the bias, the TD3 considers two Q-functions and uses the

minimum of the two Q-function values to compute the target value, while in order to reduce the

variance on the gradient, the policy is updated less frequently than the Q-functions. Specifically, let

Q1 , Q2 and  are two current Q-functions and the current deterministic policy, respectively, and

let are

Q1 , Q2 and initialized by

 the

are the target networks of Q1 , Q2 and , respectively. same networks as the current networks. At time step t, the

The TD3

target networks algorithm takes

an action at with exploration noise : at = (st) + , where  is zero-mean Gaussian noise with variance 2, i.e.,   N (0, 2). Then, the environment returns reward rt and the state is switched to st+1. The TD3 algorithm stores the experience (st, at, rt, st+1) at the experience replay buffer D. After storing the experience, the Q-function parameters 1 and 2 are updated by gradient descent
of the following loss functions:

L(j ) = E^(s,a,r,s)D (y - Qj (s, a))2 , j = 1, 2

(1)

where E^(s,a,r,s)D denotes the sample expectation with an uniform random mini-batch of size B drawn from the replay buffer D, and the target value y is given by

y

=

r

+



min
j=1,2

Qj (s,

 (s)

+

),

  clip(N (0, ~2), -c, c).

(2)

Here, for the computation of the target value, the minimum of the two target Q-functions is used to

reduce the bias. (Note that even in TD3 some kind of parallelism is used.) The procedure of action

taking and gradient descent for 1 and 2 are repeated for d times (d = 2), and then the policy and target networks are updated. The policy parameter  is updated by gradient descent by minimizing

the loss function for :

L() = -E^sD [Q1 (s, (s))] ,

(3)

and the target network parameters j and  are updated as

j  (1 -  )j +  j   (1 -  ) +  .

(4)

The networks are trained until the number of time steps reaches a predefined maximum.

3

Under review as a conference paper at ICLR 2019

3 INTERACTIVE PARALLEL POLICY EXPLORATION

We now present the proposed IPE scheme with the parallel environment learning setting described in Section 2.1, and the overall structure is described in Fig. 1.

We have N identical parallel learners with a shared common experience replay buffer D, and all N
identical learners employ a common base algorithm, which can be any off-policy RL algorithm. The execution is in parallel. The i-th learner has its own environment Ei, which is a copy of the common environment E, and has its own value function, e.g., Q-function, parameter i and policy parameter i. The i-th learner interacts with the environment copy Ei with some additional interaction with the chief, as shown in Fig. 1. At each time step, the i-th learner performs an action ati to its environment copy Ei by using its own policy i , stores its experience (sti, ait, rti, sti+1) to the shared common experience replay buffer D, for all i = 1, 2, · · · , N . Note that one time step corresponds to N
environment steps. Then, at each time step, each learner updates its value function parameter and policy parameter for N times by drawing N mini-batches of size B from the shared common replay buffer D by minimizing its own value loss function and policy loss function, respectively. The N time updates for each learner for each time step is to exploit the samples provided by other N - 1
learners stored in the shared common replay buffer.

In order to harness the benefit of mutual interaction among the learners in parallel learning, we ex-
ploit the information from the best learner periodically during training. Suppose that the Q-function
parameter and the policy parameter of each learner are initialized and learning is performed as described above for M time steps. At the end of M time steps, we determine who is the best learner based on the average of the most recent Er episodic rewards for each learner. Let the index of the best learner be b. Then, the policy parameter information b of the best learner can be used to enhance the learning of other learners for the next M time steps. Here, we propose using the information of b in a soft manner to enhance the performance of the overall parallel learning. That is, during the next M time steps, whereas we set the loss function L~(i) for the Q-function to be the same as the loss L(i) of the base algorithm, we set the loss function L~(i) for the policy parameter i of the i-th learner as the following augmented version:

L~(i) = L(i) + 1{i=b}E^sD D(i , b )

(5)

where L(i) is the policy loss function of the base algorithm, 1{·} denotes the indicator function, (> 0) is a weighting factor, D(, ) be some distance measure between two policies  and , and E^sD denotes the sample expectation based on mini-batch drawn randomly from the experience replay buffer D. The augmented loss function L~(i) in (5) is composed of two terms L(i) and 1{i=b}E^sD D(i, b) . For the non-best learners in the previous M time steps, the gradient of L~(i) is the mixture of two directions: one is to maximize the return by itself and the other is to
follow the previously best learner's policy. Thus, the second term in the right-hand side (RHS) of
(5) guides non-best learners towards a good direction in addition to each leaner's self search.

3.1 DESIGN OF THE WEIGHTING FACTOR 

In (5), the weighting factor  is common to all N learners and should be determined judiciously to

balance between improving its performance by each learner itself and going towards the previous

best policy among the N learners. We adopt an adaptive method to determine the value of  as

follows:

=

  2 if D^ best > min{D^ old, dsearch} × 1.5   /2 if D^ best < min{D^ old, dsearch}/1.5

(6)

where D^best is the estimated distance between i and b averaged over all N -1 non-best learners, and D^old is the estimated distance between i and the i-th learner's policy oild at the end of the previous M time steps averaged over all N - 1 non-best learners, given by

D^ best

=

N

1 -

1

E^sD D(i , b ) ,

iI -b

D^ old

=

N

1 -

1

E^ sD D(i , oi ld )

iI -b

(7)

Here, I-b = {1, . . . , N } \ {b}, and dsearch and  are predetermined parameters.

4

Under review as a conference paper at ICLR 2019

This adaptation method is similar to that used in proximal policy optimization (PPO)

(Schulman et al. (2017)). The update of  is done every M time steps and the updated  is used

for the next M time steps. First, suppose that we do not have the first term D^old in the minimum of the condition in (6). Then, when the estimated average distance D^best from the best policy to the remaining policies is smaller than dsearch/1.5, the parameter  is decreased by half. Hence,

the movement in the gradient direction of the second term in the RHS of (5) is diminished and the

independent movement into the optimization direction for L(i) becomes more important. So, each

policy gradually diverges from the previous best policy serving as the reference point. On the other

hand, when D^best is larger than dsearch · 1.5, the parameter  increases by factor two and the movement towards the previous best policy becomes more important. As time steps elapse,  is settled

down so that D^best is around dsearch. Hence, the proposed IPE scheme searches a wide area with

rough radius dsearch around the previous best policy in the policy parameter space, as shown in Fig.

2(a). Furthermore, with the first term D^old in the minimum of the condition in (6), we can control

the speed of tracking the best policy.

With the first term, if

D^ best D^ old

>

, i.e., the distance from i

to

b is larger than  times the distance from i to iold , then this means that the speed of tracking

the best policy is slow. Hence, we increase  by factor two. Otherwise, we decrease  by half.

When the search for the current M time steps is finished, the new best policy is selected and a new

search for a wide area around the new best policy is performed, as illustrated in Fig. 2(b).

1

dsearch

2

b

3 N
···

(a) (b)
Figure 2: (a) the conceptual search coverage in the policy parameter space by parallel learners (the proper individual search area by each learner may be larger than that in the figure) and (b) an illustration of search comparison: single search (blue) versus the proposed interactive parallel search guided by the best polity at each search interval (pink - best policies during search)

Now, the overall procedure for the proposed IPE scheme is explained with the diagram in Fig. 1. The value function parameter and policy parameter of each learner are initialized. The chief distributes the parameter  and the reference policy parameter b, which is the best policy over the previous M time steps, to all learners. At each time step, each learner interacts with its own environment copy by taking its action and receiving the reward and the next state, and stores its experience to the shared common replay buffer D. Then, the i-th learner updates its value function parameter i by minimizing its own value loss function L~(i) which is the same as that of the base algorithm, and updates the policy parameter i by minimizing the augmented loss function L~(i) in (5) for N times by drawing N mini-batches from the shared common replay buffer D. Whenever an episode ends for a learner, the learner reports the episodic reward to the chief. The i-th learner reports E^sD D(i , b ) to the chief for computation of D^best in (7). At every M time steps, the chief updates  according to (6), determines the best policy over the most recent M time steps based on the collected episodic rewards from each learner, obtains the policy parameter b from the determined best learner, and distributes the new  and the new best policy parameter b to all N learners. This procedure repeats until the time steps reaches the predefined maximum. When the parallel learning based IPE reaches a steady state, we can choose any of the N learners' policies and use the chosen policy for the environment E in future, since it is expected that at the steady-state the performance of all N policies is more or less similar due to their distance property.
5

Under review as a conference paper at ICLR 2019

Algorithm 1 The Interactive Parallel Exploration TD3 (IPE-TD3) Algorithm
Require: N : number of learners, T : maximum time steps, M : the best-policy update period, B: size of mini-batch, d: update interval for policy and target networks.
1: Initialize 1 = · · · = N = b randomly. 2: Initialize  = 1, t = 0 3: while t < T do 4: t  t + 1 (one time step) 5: for i = 1, 2, · · · , N in parallel do 6: Take an action ati = i sit + ,   N (0, 2) to environment copy Ei 7: Store experience (sit, ati, rti, sit+1) to the shared common experience replay D 8: end for 9: for i = 1, 2, · · · , N in parallel do 10: for k = 1, 2, · · · , N do 11: Sample a mini-batch B = {(stl , atl , rtl , stl+1)}l=1,...,B from D 12: Update ji , j = 1, 2, by gradient descent for minimizing L(ji ) in (9) with B 13: if k = 0(mod d) then 14: Update i by gradient descent for minimizing L~(i) in (10) with B 15: Update the target networks: (ji )  (1 -  )(ji ) +  ji , (i)  (1 -  )(i) +  i 16: end if
17: end for
18: end for
19: if t = 0(mod M ) then 20: Select the best policy b 21: Adapt  with (6)
22: end if
23: end while

3.2 THE IPE-TD3 ALGORITHM

We apply the proposed IPE scheme with the TD3 algorithm as the base algorithm and the constructed

algorithm is named the IPE-TD3 algorithm. With TD3 as the base algorithm, each learner has its

own parameters and (i) which

1i , are

2i , the

and i for its parameters of

two the

Q-functions and policy. Furthermore, it has (1i ), (2i ), corresponding target networks. For the distance measure

between two policies, we use the mean square difference given by

D((s), ~(s))

=

1 2

(s) - ~(s)

2 2

.

(8)

For the i-th learner, as in TD3, the parameters ji , j = 1, 2 are updated every time step by minimizing

L~(ji ) = E^ (s,a,r,s)D (y - Qji (s, a))2

(9)

where y = r +  minj=1,2 Q(ji) (s, (i) (s) + ),   clip(N (0, ~2), -c, c). The parameter i is updated every d time steps by minimizing the following augmented loss function:

L~(i) = E^sD

-Q1i

(s,

i

(s))

+

1{i=b}

 2

i (s) - b (s)

2 2

.

(10)

With these loss functions, all procedure is the same as the general IPE procedure described previously. The pseudocode of the IPE-TD3 algorithm is shown in the Algorithm 1.

4 EXPERIMENTS
We evaluated the performance of the proposed IPE-TD3 and current state-of-the-art on-policy and off-policy baseline algorithms on several MuJoCo enrivonments (Todorov et al. (2012)). The baseline algorithms are Proximal Policy Optimization (PPO) (Schulman et al. (2017)), Soft Actor-Critic (SAC) (Haarnoja et al. (2018)), and TD3 (Fujimoto et al. (2018)).

6

Under review as a conference paper at ICLR 2019
4.1 PARAMETER SETTING
All parameters we used for evaluation are the same as those in the original papers (Schulman et al. (2017); Haarnoja et al. (2018); Fujimoto et al. (2018)).
TD3 The networks for two Q-functions and the policy have 2 hidden layers. The first and second layers have sizes 400 and 300, respectively. The non-linearity function of the hidden layers is ReLU, and the activation functions of the last layers of the Q-functions and the policy are linear and hyperbolic tangent, respectively. We used the Adam optimizer with learning rate 10-3, discount factor  = 0.99, target smoothing factor  = 5 × 10-3, the period d = 2 for updating the policy. The experience replay buffer size is infinite (this means we use all experiences to update parameters), and the mini-batch size B is 100. The standard deviation for exploration noise  and target noise ~ are 0.1 and 0.2, respectively, and the noise clipping factor c is 0.5.
IPE-TD3 In addition to the parameters for TD3, we used N = 4 learners, the period M = 250 of updating the best policy and , the number of recent episodes Er = 10 for determining the best policy b . The parameters dsearch and  for the exploration range are 0.8 and 2, respectively.
SAC The networks for the state-value function, two Q-functions, and the policy have 2 hidden layers of size 256. The activation functions for the hidden layers and the last layers are ReLU and linear, respectively. We used the Adam optimizer with learning rate 3 × 10-4, discount factor  = 0.99, and target smoothing factor  = 5 × 10-3. The algorithm is trained by random mini-batches of size B = 256 from the experience replay buffer of the maximum size 106. Reward scale for updating Q-functions is 5 for all environments.
PPO The networks of the state-value function and the policy have 2 hidden layers of size 64. The activation functions for the hidden layers and the last layer are hyperbolic tangent and linear, respectively. We used the Adam optimizer with initial learning rate 3 × 10-4, discount factor  = 0.99, and GAE parameter  = 0.95. The horizon is 2048 which is the number of samples generated by the same policy at one iteration. The policy and value networks are updated 10 × 2048/64 times using random mini-batches of size 64 from the collected 2048 experiences generated by the same policy. The clipping range of importance sampling ratio is 0.2.
4.2 COMPARISON TO BASELINES
To have sample-wise fair comparison among the baseline and IPE-TD3 algorithms, we obtain the performance with respect to environment steps (not time steps), which is defined as the total number of interactions with the environment by the agent. This comparison makes sense because the performance at the same environment steps means that all algorithms use the same number of samples obtained from the environment. The performance is obtained through the evaluation method that is similar to those in (Haarnoja et al. (2018); Fujimoto et al. (2018)). Evaluation of the policies are conducted every Reval environment steps with Reval = 2048, 4000, and 1000 for PPO, IPE-TD3, and the other baseline algorithms, respectively. At each evaluation instant, the agent (or learner) fixes its policy as the one at the evaluation instant, and interacts with the same environment separate for the evaluation purpose with the fixed policy to obtain 10 episodic rewards. The average of these 10 episodic rewards is the performance at the evaluation instant. In the case of IPE-TD3, each of the N learners fixes its policy as the one at the evaluation instant, and interacts with the environment with the fixed policy to obtain 10 episodic rewards. First, the 10 episodic rewards are averaged for each learner and the 10-episode-average rewards of the N learners are averaged again to yield the performance at that evaluation instant. We performed this operation for five different random seeds, and the mean and variance of the learning curve are obtained from these five simulations. The policies used for evaluation are stochastic for PPO and deterministic for the others.
Fig. 3 shows the learning curves over one million environment steps for several MuJoCo tasks: Hopper-v1, Walker2d-v1, HalfCheetah-v1, and Ant-v1. First, it is observed that the performance of TD3 here is similar to that in the original TD3 paper (Fujimoto et al. (2018)), and the performance of other baseline algorithms is also similar to that in the original papers (Schulman et al. (2017); Haarnoja et al. (2018)). It is seen that the IPE-TD3 algorithm outperforms the state-of-the-art RL algorithms in terms of both the speed of convergence with respect to environment steps and the final steady-state performance (except in Walker2d-v1, the initial convergence is a bit slower than TD3.) Especially, in the cases of Hopper-v1 and Ant-v1, TD3 has large variance and this means that the
7

Under review as a conference paper at ICLR 2019

(a) Hopper-v1

(b) Walker2d-v1

(c) HalfCheetah-v1

(d) Ant-v1

Figure 3: Performance for PPO (brown), SAC (red), TD3 (green), and IPE-TD3 (proposed method, blue) on MuJoCo tasks.

performance of TD3 is quite dependent on the initial condition of the environment and it is not easy for TD3 to escape out of a bad local point in certain environments. However, it is seen that IPE-TD3 yields much less variance as compared to TD3. This implies that the wide area search by IPE in the policy parameter space helps the learners escape out of bad local pitfalls. Indeed, it is seen that the wide area search around the previous best policy point in the policy parameter space by IPE yields faster and better policy search.
5 CONCLUSION
In this paper, we have proposed a new interactive parallel learning scheme, IPE, to enhance the performance of off-policy RL systems. In the proposed IPE scheme, multiple identical learners with their own value-functions and policies sharing a common experience replay buffer search a good policy with the guidance of the best policy information in the previous search interval. The information of the best policy parameter of the previous search interval is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search space by the multiple learners. The guidance by the previous best policy and the enlarged search space by IPE enables faster and better policy search in the policy parameter space. The IPE-TD3 devised by applying the proposed IPE scheme to TD3 outperforms most of the current state-of-the-art continuous-action RL algorithms.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning, pp. 1587­1596, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning, pp. 1861­1870, 2018.
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized experience replay. In International Conference on Learning Representations, 2018.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning, pp. 1928­ 1937, 2016.
Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, et al. Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012.
9

