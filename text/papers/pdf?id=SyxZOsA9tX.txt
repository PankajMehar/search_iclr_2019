ACCELERATED VALUE ITERATION VIA ANDERSON MIXING
Anonymous authors Paper under double-blind review
ABSTRACT
Accelerating reinforcement learning methods is an important and challenging topic. We introduce the Anderson acceleration technique into the value iteration and develop an accelerated value iteration algorithm Anderson Accelerated Value Iteration (A2VI). We further apply our method to Deep Q-learning algorithm and propose Deep Anderson Accelerated Q-learning (DA2Q) algorithm. Our approach can be viewed as an approximation of the policy evaluation by interpolating on historical data. A2VI is more efficient than classical modified policy iteration methods. We provide a theoretical analysis of our algorithm and conduct experiments on both toy problems and Atari games. Both the theoretical and empirical results demonstrate the effectiveness of our algorithm.
1 INTRODUCTION
In reinforcement learning (Sutton & Barto, 1998), an agent seeks for the optimal policy in a specific sequential decision problem. Several algorithms have been proposed over the course of time, including the famous Q-learning (Watkins & Dayan, 1992), SARSA (Rummery & Niranjan, 1994; Sutton & Barto, 1998), and policy gradient methods (Sutton et al., 2000). In complicated decision problems where tabular representations are intractable, function approximations are usually used for estimating state-action values (Kaelbling et al., 1996; Sutton & Barto, 1998; Sutton et al., 2000). Inspired by the success of deep learning, Deep Q-Learning (DQN) (Mnih et al., 2013) and its variants (Bellemare et al., 2017; Schaul et al., 2015; Van Hasselt et al., 2016; Wang et al., 2015) utilize a deep neural network as the value approximator, which has successfully solved end-to-end decision problems such as Atari2000.
The value iteration (VI) and policy iteration (PI) (Puterman, 2014) are the most classical methods for value updating. The main difference between them is that PI evaluate the current policy accurately during the iteration while VI does not. Thanks to the accurate evaluation of the current policy, policy iteration uses significantly less policy improvement steps to converge to the optimal value. Although PI has a faster convergence rate than VI, most of the existing methods employ a rather slow value iteration procedure, because thoroughly evaluating a policy is costly or even intractable under complex environments. To retain the fast convergence property of policy iteration while reducing its computation overhead, researchers have proposed several modifications to the original policy iteration (Alla et al., 2015; Puterman, 2014). The modified policy iteration (MPI) method (Puterman, 2014) tries to deal with this problem by approximating the solution to policy evaluation via the Neumann expansion of an inverse matrix. However, this approximation requires extra iterative steps, which is still computationally inefficient for complex decision problems.
Interpolation methods have been widely used in first order optimization problems (Bubeck et al., 2015; Scieur et al., 2016; 2017; Xie et al., 2018). These methods extract information from historical data and are proven to converge faster than vanilla gradient methods. However, the interpolation method is not widely applied in reinforcement learning. The most recent work related to interpolation is the averaged-DQN (Anschel et al., 2016), which calculates the average Q-value over the history and demonstrated that such an operation is effective for variance reduction.
1

Acceleration in value iteration and policy iteration has attracted researchers' great attention. Classical methods for accelerating value iteration include Gauss-Seidel value iteration (Puterman, 2014) and Jacobi value iteration (JAC) (Puterman, 2014). More recently, Alla et al. (2015) proposed an acceleration method that switches between a coarse-mesh value iteration and a fine-mesh policy iteration during different stages. Laurini et al. (2016) performed a Jacobi-like acceleration method on dynamic programming problems. In a recent work (Laurini et al., 2017), the value iteration procedure is accelerated by only updating a part of the values. None of the previous methods have proposed acceleration methods with an application of interpolation.
In this paper, to solve the policy evaluation problem more efficiently, we propose an alternative algorithm based on multi-step interpolation. Explicitly, the solution to the policy evaluation problem is approximately represented by a weighted combination of historical values, whose weights are adaptively updated by an optimization procedure. To reduce the computational complexity, we resort to the Anderson mixing method (Anderson, 1965; Walker & Ni, 2011; Toth & Kelley, 2015) to do the approximation with only a short length of history. Our approach fits the gap between value iteration and policy iteration, ending in an updating rule without adding much extra computational complexity to the original value iteration procedure. We also extend this approach to deep reinforcement learning problems.
The remainder of this paper is organized as follows. In Section 2, we introduce the foundations of reinforcement learning and present typical value updating algorithms. In Section 3, we derive the Anderson accelerated methods. In Section 4, we give a theoretical analysis of the convergence of our method. In Section 5, we test our method in different environments and empirically show the effectiveness of it. Finally, we conclude our work in Section 6.

2 PRELIMINARIES

In this paper we mainly consider a finite-state and finite-action scenario in reinforcement learning. In this case,

an Markov Decision Process (MDP) system is defined by a 5-tuple (S, A, P, r, ), where S is a finite state

space, A is a finite action space, P  R(|S|×|A|)×|S| is the collection of state-to-state transition probabilities, r  R|S|×|A| is the reward matrix,  is the discount factor. A policy   A|S| is a vector of actions at each state. The transition matrix P  R|S|×|S| and reward vector r  R|S| under policy  are defined as P(i, j) = P ((i, (i)), j), r(i) = r(i, (i)). We further define the value v  R|S| and the Q-value q  R|S|×|A| under a given MDP and policy, where each element of v and q is defined as



v (s) = Es0=s,st+1P(st,...)

 t r (st ),

t=0



q(s, a) = r(s, a) + Es1P ((s,a),...),st+1P(st,...)

 t r (st ).

t=1

We can verify that q = r + P v. We define q~  R|S| by q~ (i) = q(i, ~(i)), and say a vector to be the maximum among a set if each entry of it is bigger than that of the other vectors. The values satisfy the

Bellman equation:

v = (v) = r + Pv.

TNhoeteptohlaict yvsa=tisfiaregsmthaexBeqllmiasncoalplteidmtahlietyopetqiumaatilopnolicy, whose value or Q-value is denoted as v or q. v = (v) = max(r + Pv).


Therefore, finding the optimal policy is equivalent to finding the fixed point of the operator (v).

2

2.1 FIXED POINT ITERATION METHODS

Value iteration (VI) is the most widely used and best-understood algorithm for solving Markov decision problems. It solves the fixed point problem by iterating the following steps repeatedly,
v(t+1) = (v(t)) = max(r + Pv(t)).

An alternative solution is policy iteration (PI), which maintains both the value v(t) and the policy (t) during each iteration. The procedure alternatively iterates the following two steps:

· Policy evaluation: Find a v(t) such that

v(t) = (t) (v(t)) = r(t) + P(t) v(t), which can be directly computed by

(1)

v(t) = (I - P(t) )-1r(t) .

(2)

· Policy improvement: Improve the current policy by

(t+1) = argmax(r + Pv(t)).


Theoretical analysis has shown that VI enjoys a -linear convergence rate (i.e., v(t) - v    v(t-1) -

v ), while PI converges much faster with

v(t) - v

K

v(t-1) - v

2 

(Puterman,

2014).

Both

VI and PI are model-based, because the greedy policy cannot be determined when r and P are unknown. The

VI under q-notation is well-known as Q-learning (Watkins & Dayan, 1992). We will analyze our method

under v-notation, but our analysis also works under the corresponding q-notation.

The main difference between VI and PI is whether the current policy is fully evaluated. Though PI converges
faster than VI, this advantage diminishes under several settings. In most cases, we can only access an oracle
that returns the reward and next state given the current state and selected action. Under such a setting,
value iteration can be finished by estimating (v) through sampling. But the policy evaluation step based on equation (2) becomes intractable because it is quite time-consuming to compute (I - P(t) )-1. The modified policy iteration method (Puterman, 2014) partially solves the problem by setting vt  ((t) )mt (v(t-1)) where mt is a (possibly large) integer related to t. However, this method requires to evaluate a series of values ((t) )i(v(t-1)) for i = 1, 2, . . . , mt, which is computationally inefficient.

3 ANDERSON ACCELERATED VALUE ITERATION

Based on the observation that full policy evaluation accelerates convergence, we propose an approximate policy evaluation method. The method aims to approximately solve the policy evaluation problem, circumventing the matrix inversion and iterative procedures mentioned above.

We first utilize the linearity of equation (1), defining B(v) = (v) - v and converting the prob-

lem into an equivalent form of solving the equation B(v) = 0. Suppose we have obtained a set of

values B(v1), B(v2), . . . , B(vk) with respect to v1, v2, . . . , vk. Consider to find a set of weights

 = (1, 2, . . . , k)T , subject to

k i=1

i

=

1,

which

satisfies

that

k
iB(vi) = 0.
i=1

3

Then the combination v~ =

k i=1

ivi

will

satisfy

the

following

relationship:

k
B(v~) = r + Pv~ - v~ = i(r + Pvi - vi)

i=1

k
= iB(vi) = 0.

i=1

(3) (4)

This relation implies v~ can be viewed as an approximate solution to equation (1) provided the sampling
estimations are accurate enough. However, this step needs to keep track of the previous values and recompute  on all of them. To reduce the huge memory usage and computation, we choose vi from the recent history, i.e., vi = v(t-i), i = 1, 2, . . . , k, and replace B(t) (v(t-i)) with the previously computed values B(t-i) (v(t-i)). This modification is based on the observation that the recent successive policies do not change sharply and therefore B(t-i) (v(t-i))  B(t) (v(t-i)). This modification approximately solves the policy evaluation problem without model estimation or extra function evaluations.

Another critical issue is that we cannot guarantee the existence of  given that k is small, because the

dimension of B(v) is usually much higher than k. Inspired by the Anderson acceleration technique (Anderson, 1965; Ortega & Rheinboldt, 1970; Walker & Ni, 2011), we instead look for a combination of

{B(t-i) (v(t-i))}ki=1,

(t) = argmin B(t) ,

(5)



where B(t) = (B(t-1) (v(t-1)), B(t-2) (v(t-2)), . . . , B(t-k) (v(t-k))),  = {|1T  = 1},  is an extra constraint on the values attainable by . Typically,  can be chosen from the following forms:

· Total space, tot = Rk; · Boxing constraint, box = {| - m1    m1}; · Convex combination constraint, cvx = {|0    1}; · Extrapolation constraint, exp = {|1  1, i  0, i = 2, 3, . . . , k}.

When the 2 norm is used and  = tot, the solution can be written explicitly as (t) =

[(B(t)) B(t)]-11/1 [(B(t)) B(t)]-11. Note that if we simply set v(t) =

k i=1

i(t)v(t-i),

the

values

will always locate in the subspace expanded by historical values v(t-1), v(t-2), . . . , v(t-k). When the so-

lution to equation (1) does not lie in such a subspace, there is no hope for convergence with application of

such updating rule directly. To jump out of the subspace, we perform an extra value iteration step to this

combination. Then we will get the updated value,

k

v(t) = max r + P


i(t)v(t-i)

i=1

.

3.1 THE ALGORITHM
Based on our previous discussion, we present the k-step Anderson Accelerated Value Iteration (A2VI) in Algorithm 1. In the first k steps, the value is updated according to the original VI. Otherwise, we perform an interpolation procedure, where the weights are attained from solving the problem (5). The original value iteration algorithm can be viewed as a special case of our algorithm with k = 1.
Both Anderson Acceleration (AA) and A2VI have the same spirit of interpolating on historical data. However, A2VI does not straightforwardly apply AA to the Bellman optimality equation. Note that AA has the updating

4

Algorithm 1 Anderson Accelerated Value Iteration (A2VI)

Input: v(0), P, r, , k, T

1: for t = 1, 2, . . . , T do

2:

B(t-1) (v(t-1))

=

max 

(r

+

 P v(t-1) )

-

v(t-1)

3: if t < k then

4: v(t) = max(r + Pv(t-1)) 

5: else

6: Calculate (1(t), 2(t), . . . , k(t)) by solving the optimization problem (5)

7:

v(t) = max r + P 

k i=1

i(t) v (t-i)

8: end if

9: end for 10: (T ) = argmax(r + Pv(T ))
11: return v(T ), (T )

rule vt = iB(vt-i), while A2VI exchange the order of the operator sum and B(·) due to the motivation from equation (3). This exchange puts the nonsmooth operator max out of the affine combination, simplifying the theoretical analysis.
We present a geometric explanation on the iterative steps of VI, PI, A2VI under 1-dimensional case in Figure 1. In value iteration, v(t) is attained by making a vertical line at (v(t-1), 0), finding its intersection with the function line at (v(t-1), B(v(t-1))), then drawing a line with slope -1 through (v(t-1), B(v(t-1))) and finding its intersection with the horizon axis at (v(t), 0); In policy iteration, v(t) is attained by first getting (v(t-1), B(v(t-1))) in the same way as value iteration, then calculating the tangent line through (v(t-1), B(v(t-1))) and finding its intersection with the horizon axis. In Anderson accelerated value iteration, each step is first performed in a similar style to policy iteration except that the tangent line is replaced with a secant line. Then a value iteration step is performed to get v(t).
From the figure, we can see that VI only utilizes the current value of the Bellman residual, while PI is similar to Newton's method, utilizing the gradient information to achieve a faster convergence rate. Our method serves as an intermediate between them, each step of which is composed of an ordinary value iteration step and a secant step. Both PI and A2VI converge to the fixed point in a smaller number of steps than VI. Compared with PI, A2VI is more practical because it approximates the tangent line by a secant line, which circumvents the costly model estimation step.

Value iteration

Policy iteration

Anderson accelerated value iteration

Bv Bv Bv

r1 + ( P1 I)v

r2 + ( P2 I)v r3 + ( P3 I)v

v1

v2 vv3 v4 v5

v*

r1 + ( P1 I)v
r2 + ( P2 I)v r3 + ( P3 I)v
v1 v v2 v3 v4 = v *

Bv1 r1 + ( P1 I)v

Bv2 r2 + ( P2 I)v

Bv3 r3 + ( P3 I)v

v1

v2 v

v3 v4 v *

Figure 1: Geometric interpolation of VI, PI and A2VI.

3.2 EXTENSION TO MODEL-FREE LEARNING ALGORITHM We can rewrite our algorithm under q-notation, and get the Anderson Accelerated Q-Learning (A2Q) Algorithm shown in the appendix. Combined with the technique of deep learning, our method can be applied
5

to end-to-end decision problems, resulting in the Deep Anderson Accelerated Q-Learning (DA2Q) Algorithm (Algorithm 2).

Algorithm 2 Deep Anderson Accelerated Q-learning (DA2Q)

Input: M, N, T, , b, B, , , K, C

1: Initialize replay memory D to capacity N , initialize Q-value function Q with random weights 

2: -k = , 1 = 1, k = 0 for k = 2, ..., K

3: s = 0

4: for episode = 1, 2, . . . , M do

5: Initialize s1  (s)

6: for t = 1, 2, . . . , T do

7: With probability  select a random action at, otherwise select at = arg maxa Q(st, a; )

8: Execute action at, observe reward rt and state st+1 9: Store transition (st, at, rt, st+1) in D 10: Sample a random minibatch of transitions {(sj, aj, rj, sj)}bj=1 from D 11: for j = 1, 2, . . . , b do

 

rj

for terminal state





12: yj =

K

 rj +  max

 

a

kQ(sj, a; -k) for non-terminal state

k=1

13: end for

14:

L()

=

1 b

jb=1(yj - Q(sj , aj ; ))2

15:



=



-



L 

16: s = s + 1

17: if s mod C = 0 then

18: Assign -k = -(k-1) for k = K, K - 1, . . . , 2. Assign -1 = . 19: if s  K(C - 1) then

20: Sample a random minibatch of transitions {(sj, aj, rj, sj)}jB=1 from D 21: for j = 1, 2, . . . , B do

22: for k = 1, 2, . . . , K do

rj - Q(sj , aj ; -k)

for terminal state

23:

dkj =

rj

+



max a

Q(sj ,

a;

-k )

-

Q(sj ,

aj ;

-k )

for non-terminal state

24: end for

25: end for

26: end if

27:

(1, 2, . . . , K ) = argmin(1,2,...,K )

jB=1 (

K k=1

kdkj )2

s.t.

K k=1

k

=

1

28: end if

29: end for

30: end for

4 THEORETICAL ANALYSIS
We first analyze of the local convergence of the A2VI algorithm under boxing constraint. Our result shows that in a small neighborhood of the optimal value, our algorithm enjoys an exponential convergence rate. Theorem 1. For any MDP with a unique optimal policy, there exists some  > 0, such that for any initial value v(0)  U(v) = {v| v - v   }, the A2VI algorithm under boxing constraint maintains the following properties:
(i) (v(t)) - v(t)    (v(t-1)) - v(t-1) , t = 1, 2, . . .;
6

(ii)

v(t) - v





t 1-

(v(0)) - v(0)

, t = 1, 2, . . ..

Generally, it is difficult to obtain the global convergence rate of A2VI, since the operation max is nonsmooth.

To guarantee the convergence, we introduce a rejection step to the original algorithm. We say v is monotonic

improving if (v)  v, and denote the set of such values as VB. We propose the A2VI algorithm with the rejection step, which only differs with Algorithm 1 at line 6. After calculating (t), we test whether the affine

combination

k i=1

i(t)vt-i

lies

in

VB .

If

the

answer

is

negative,

the

interpolation

step

will

be

replaced

with

an ordinary value iteration step. We put the pseudocode of A2VI with the rejection step in Appendix. With

this modification, we can have the following convergence properties.

Theorem 2. For the A2VI algorithm with the rejection step with  = cvx, if v(0)  VB, then we have

v(t)  VB, (v(t)) - v(t)   (v(t-1)) - v(t-1) , t = 1, 2, . . .

Theorem 3. For the A2VI algorithm with  = exp, if v0  0 and v(0)  VB, then we have

(a) Monotone improving values, v(t-1)  v(t)  v, v(t)  VB, t = 1, 2, . . .

(b) -linear convergence rate,

v - v(t)    v - v(t-1) .

5 EXPERIMENTS
To validate the effectiveness of our method, we conduct several experiments.

5.1 EXPERIMENTS ON TOY MODELS
We first test our method on three toy models. The first model is a randomly generated MDP with |S| = 100 and |A| = 50. The transition probabilities of the MDP are generated from a uniform distribution on [0, 1], and the rewards are generated from a standard normal distribution. The second model is the N -Chain problem with N = 100, where a reward of 0.1 is given at state 0 and a reward of 1 is given at state N . At each state, the agent can either choose to move forward or backward, and will move to the selected direction with probability 0.9 and to the opposite direction with probability 0.1. The last model is a 20 × 20 Gridworld model, where a reward of 1 is given at state (20, 20). At each state, the agent can choose one of the 4 directions and will move to that direction with probability 0.7, or move to one of the other directions with probability 0.1 for each. We perform the standard value iteration, policy iteration and Anderson accelerated value iteration with/without the rejection step on these models. In our experiment, each policy iteration step is approximately solved by the modified policy iteration method with 100 inner iterations. To compare our method with the averaged updating scheme (Anschel et al., 2016), we further construct and compare our algorithm with the averaged value iteration. The value of vt - v w.r.t. step t is shown in Figure 2, where the results are averaged from 30 independent experiments.
From the results we can see that the policy iteration converges fastest for all of the three models, however, since each of its steps includes 100 inner iterations, the actual computation cost is very high. Among value iteration methods, the Anderson accelerated value iteration converges fastest. The acceleration effect is remarkable in randomly generated MDPs, but A2VI slows down at the first few steps in the latter two experiments. However, adding a rejection step solves the problem and attains a faster convergence rate. Another observation is that in the toy model case, the averaged value iteration cannot be used for acceleration.

7

Distance to Optimal Value Distance to Optimal Value Distance to Optimal Value

104 102 100 10 2 10 4 10 6 10 8 10 10 0

Random MDP

104

102

100

10 2

10 4

10 6

10 8

100 200 300 400 500 10 10 0 Steps

N-Chain

104

102

100

10 2

10 4

10 6

10 8

200 400 600 800 1000 10 10 0 Steps

Gridworld
A2VI-5 A2VI-5, Rej AVE-5 VI PI 100 200 300 400 500 Steps

Figure 2: Experiment results on several toy models.

5.2 EXPERIMENTS ON ATARI GAMES WITH DEEP LEARNING BASED TECHNIQUES

To figure out the performance of our method on complex environments, we apply our method to Atari games from Gym (Brockman et al., 2016), which is a Python API to Arcade Learning Environment (Bellemare et al., 2013). We compare DA2Q with DQN (Mnih et al., 2013) and Averaged-DQN (Anschel et al., 2016). Details of the experiment settings are given in Appendix D.
As Figure 3 points out, our algorithm DA2Q obtains a significant improvement over both the original DQN algorithm and the Averaged DQN algorithm. When compared with other interpolation method such as Averaged-DQN, the overall performance of our method also tends to be stabler, always being superior than other methods among all of the three environments, while the performance of Averaged-DQN varies a lot.

Score Score Score

3500 Alien 3000 2500 2000 1500 1000 DA2Q, K=2
AVE, K=2 DQN 0 5 10 15 Ti2m0e 25 30 35 40

Amidar 1200
1000
800
600
400
200 DA2Q, K=2 AVE, K=2 DQN
0 5 10 15 Ti2m0e 25 30 35 40

100 Boxing
80
60
40
20 DA2Q, K=2 AVE, K=2 DQN
00.0 2.5 5.0 7.5 T1i0m.0e 12.5 15.0 17.5 20.0

Figure 3: Training Performance on Atari games, score is smoothed with 250 windows while the shaded area is the 0.25 standard deviation.

Compared with DQN, the extra computational cost is actually low, since the  is updated only once every C steps, which only involves an inversion on a very small-size matrix (k × k). The k target values are computed parallelly in the TensorFlow (Abadi et al., 2016), which cost the same time as in DQN. Moreover, the extra runtime can be ignored when compared with the costly back propagations and interaction with environments.

6 CONCLUSION
We have proposed the Anderson accelerated value iteration method, which is a novel acceleration approach for reinforcement learning. We have proved the convergence property of our method under certain conditions. Our algorithm empirically achieves a superior performance on toy models and several Atari games. Despite the success of our algorithm, several questions remain open. The convergence analysis for the general case is lacking, and we only provide convergence guarantees but do not give a theoretical analysis of the acceleration effect of A2VI, which we leave for future work.

8

REFERENCES
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale machine learning. In OSDI, volume 16, pp. 265­283, 2016.
Alessandro Alla, Maurizio Falcone, and Dante Kalise. An efficient policy iteration algorithm for dynamic programming equations. SIAM Journal on Scientific Computing, 37(1):A181­A200, 2015.
Donald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM (JACM), 12 (4):547­560, 1965.
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning. arXiv preprint arXiv:1611.01929, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253­279, 2013.
Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. arXiv preprint arXiv:1707.06887, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends R in Machine Learning, 8(3-4):231­357, 2015.
Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237­285, 1996.
Mattia Laurini, Piero Micelli, Luca Consolini, and Marco Locatelli. A jacobi-like acceleration for dynamic programming. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pp. 7371­7376. IEEE, 2016.
Mattia Laurini, Luca Consolini, and Marco Locatelli. A consensus approach to dynamic programming. IFAC-PapersOnLine, 50(1):8435­8440, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
James M Ortega and Werner C Rheinboldt. Iterative solution of nonlinear equations in several variables, volume 30. Siam, 1970.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. University of Cambridge, Department of Engineering, 1994.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.
Damien Scieur, Alexandre d'Aspremont, and Francis Bach. Regularized nonlinear acceleration. In Advances In Neural Information Processing Systems, pp. 712­720, 2016.
9

Damien Scieur, Francis Bach, and Alexandre d'Aspremont. Nonlinear acceleration of stochastic algorithms. In Advances in Neural Information Processing Systems, pp. 3982­3991, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000.
Alex Toth and CT Kelley. Convergence analysis for anderson acceleration. SIAM Journal on Numerical Analysis, 53(2):805­819, 2015.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In AAAI, volume 16, pp. 2094­2100, 2016.
Homer F Walker and Peng Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on Numerical Analysis, 49(4):1715­1735, 2011.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992. Guangzeng Xie, Yitan Wang, Shuchang Zhou, and Zhihua Zhang. Interpolatron: Interpolation or extrapolation
schemes to accelerate optimization for deep neural networks. arXiv preprint arXiv:1805.06753, 2018.
10

APPENDIX

A. A2Q Here is the pseudocode of A2Q, which is A2VI in q-notation.

Algorithm 3 Anderson Accelerated Q-Learning (A2Q)

Input: q0, P, r, , k, T

1: for t = 1, 2, . . . , T do 2: B(t-1) (qt-1) = vec(r) + P max qt-1 - vec(qt-1) 3: if t < k then 4: vec(qt) = vec(r) + P max qt-1
5: else

6: Calculate (1, 2, . . . , k) by solving the optimization problem (5)

7:

vec(qt) = vec(r) + P max(

k i=1

i

qt-i

)

8: end if

9: end for 10: T = arg max qT 11: return qT , T

B. A2VI WITH THE REJECTION STEP
Here is the pseudocode of A2VI with the rejection step, the only difference between this algorithm with algorithm 1 is before the interpolation step, we first check whether the affine combination is in VB. If the answer is negative, then this interpolation step is replaced with an ordinary value iteration step.

Algorithm 4 Anderson Accelerated Value Iteration with the Rejection Step

Input: v(0), P, r, , k, T

1: for t = 1, 2, · · · , T do

2:

B(t-1) (v(t-1))

=

max


(r

+

 P v(t-1) )

-

v(t-1)

3: if t < k then

4: v(t) = max(r + Pv(t-1))


5: else

6: Calculate (1(t), 2(t), · · · , k(t)) by solving the optimization problem (5)

7:

v~ =

k i=1

i(t)v(t-i)

8:

if

max


(r

+

 P v~)



v~

then

9: v(t) = max (r + Pv~)


10: else

11: 1(t) = 1, i(t) = 0 for i = 1

12:

v(t) = max


r + Pv(t-1)

13: end if

14: end if

15: end for

16: (T ) = argmax(r + Pv(T )) 17: return v(T ), (T )

11

C. PROOFS
Lemma 1. For any MDP whose optimal policy is unique, there exists a  > 0 and a policy  such that for any v  U(v) = {v| v - v   },
(v) = r + P v

Proof Because the optimal policy is unique, for any nonoptimal policy , for any state s such that (s) = (s) we have that [ (v)]s > [(v)]s, [·]s means executing operations on state s. Denote A() = {s|(s) = (s)}.

Suppose the optimal policy is , then there exists  such that

min min
= sA()

[(v) - (v)]s

>



>

0,

since

the

optimal

policy

is

unique

and

the

state

space

and

the

action

space

are

finite.

We

choose



=

 3

,

then

for any policy  and any v  U(v) we have

(v) - (v)  =

P(v - v)



v - v



 3

Then for any policy , for any state s  A() , we have that

[ (v) - (v)]s = [( (v) -  (v)) + ( (v) - (v)) + ((v) - (v))]s   -  (v) -  (v)  - (v) - (v)  -  -  33  = 3

which means  does not choose the optimal action in state s. Therefore, if  selects the optimal action in every state s  S, then we must have s = s, s  S, which implies   arg max (v), i.e., (v) = r + P v. Lemma 2. For any given MDP with optimal value v and any value v, we always have
(1 - ) v - v   (v) - v   (1 + ) v - v .

Proof

v - v  = v - (v) + (v) - (v)   v - (v)  + (v) - (v)   v - (v)  +  v - v 
 (1 - ) v - v   (v) - v  v - v  = v - (v) + (v) - (v)   v - (v)  - (v) - (v)   v - (v)  -  v - v 
 (1 + ) v - v   (v) - v 

12

Proof of Theorem 1 From lemma 1 we know there exists a policy  and a ~ > 0 such that the optimal Bellman operator is a linear function on U~(v). We now set  sufficiently small such that

km(1 + ) 1-

v(0) - v



<

km(1 + ) 1- 

<

~.

The result is trivial for the first k - 1 steps, which are performed exactly by standard value iteration. When t > k, we prove the result by induction. Suppose the conclusion is correct for previous steps, then we have

k

i(t)v(t-i) - v  

k

|i(t)| v(t-i) - v  

k

|i(t)|

1

1 -



B(v(t-i)) 

i=1 i=1

i=1



k

|i(t)

|

1

1 -



B(v(0))





km 1-

B(v(0)) 

i=1



km(1 + 1-

)

v(0) - v

<

km(1 + 1-

)



<

~

It follows that

v(t) - v  = 

k
( i(t)vt-i) - (v)   
i=1
k
i(t)v(t-i) - v  < ~
i=1

k
i(t)v(t-i) - v 
i=1

Therefore,

k i=1

i(t)v(t-i)



U~(v), v(t)



U~(v),

which

implies

kk

( i(t)v(t-i)) = r + P

i(t)v(t-i), (v(t)) = r + P v(t).

i=1 i=1

Then we can get

B(vt) = r + (P - I)v(t)

k

= r + (P - I)(r + P

i(t)v(t-i))

i=1

k
= i(t) r + (P - I)(r + P v(t-i))
i=1

k
= i(t)P
i=1

r + P v(t-i) - v(t-i)

k

= P

i(t)B(v(t-i))

i=1

13

Taking norm on both side of the equation and utilizing the definition of i(t), i = 1, 2, · · · , k, we get

k

(v(t)) - v(t)    P 

i(t)B(v(t-i))    B(v(t-1))  =  (v(t-1)) - v(t-1) .

i=1

Therefore, we justify property (i). Property (ii) then follows directly from Lemma 2. Lemma 3. For any MDP, suppose the values u and v satisfy u  v, then
(u)  (v).

Proof As stated in section 2, u  v means that u(s)  v(s) for any state s. Suppose ~ = arg max r + Pv, therefore for any s we have that
(u)(s)  ~ (u)(s)  ~ (v)(s) = (v)(s).

Proof of Theorem 2 On the one hand,

k

(v(t)) - v(t) = max(r + Pv(t)) - max(r + P

i(t)v(t-i))

i=1

k

 r(t) + P(t) v(t) - r(t) - P(t)

i(t)v(t-i)

i=1

kk

= P(t) (max(r + P

i(t)v(t-i)) -

i(t)v(t-i))

i=1 i=1

k

 P(t)

i(t)(max(r + Pv(t-i)) - v(t-i))

i=1

k

= P(t)

i(t)B(v(t-i))

i=1

On the other hand, we denote ~ = argmax(r + P

k i=1

i(t)v(t-i)),

then

k

v(t) - (v(t)) = max(r + P


i(t)v(t-i))

-

max(r


+

 P v(t) )

i=1

(k)

 r~ + P~

i(t)v(t-i) - r~ - P~ v(t)

i=1

kk

= P~ ( i(t)v(t-i) - max(r + P

i(t)v(t-i)))

i=1 i=1

k
= -P~ B( i(t)v(t-i))
i=1

14

The above two results shows

kk

P~ B( i(t)v(t-i))  B(v(t))  P(t)

i(t)B(v(t-i)).

i=1 i=1

Now, consider the rejection step. First we show that if v  VB then (v)  VB. If v  VB, then (v)  v. With Lemma 3, we have that ((v))  (v), i.e. (v)  VB.

Next we show that if v(i)  VB for i < t, then v(t)  VB. According to the rejection algorithm, we have

v(t) = (

k i=1

i(t)

v(t-i)

).

If (

k i=1

i(t)v(t-i))



k i=1

i(t)v(t-i),

then

k i=1

i(t)v(t-i)



VB

and

v(t)  VB. If (

k i=1

i(t)

v(t-i)

)

<

k i=1

i(t)

v(t-i)

,

then

v(t)

=

(v(t-1))

due

to

the

rejection

step.

Since v(t-1)  VB, we have that v(t)  VB. Therefore, we also have that B(

k i=1

i(k)

v(t-i)

)



0.

We

have that

(v(t)) - vt

= B(v(t))   P(t)

k
i(t)B(v(t-i))

i=1

  B(v(t-1)) =  (v(t-1)) - v(t-1)

The second inequality is due to the definition of i(t), i = 1, 2, ..., k.

Proof of Theorem 3 We prove the conclusion by induction. It is evident that the conclusion holds for the first k - 1 steps. Suppose the conclusion holds for the first t - 1 steps, then

k

v(t) = max(r + P

i(t)v(t-i))

i=1

k
 r~ + P~ ( i(t)v(t-i))
i=1

 r~ + P~ v(t-1)

 v(t-1),

where ~ = arg max r + Pv(t-1). The second inequality comes from the extrapolation restriction. The Third inequality is due to that if v  VB then (v)  VB, which is shown in Theorem 2.

As shown in Theorem 2, we have that v(t)  VB and

k i=1

i(t)v(t-i)



VB .

Therefore,

v(t)



v.

k

v - v(t) = v - max(r + P

i(t)v(t-i))

i=1

k

 v - (r + P

i(t)v(t-i))

i=1

k

= P

i(t)(v - v(t-i))

i=1

 P (v - v(t-1))

15

Taking the infinite norm on both sides, we get v - v(t)    P  v - v(t-1)    v - v(t-1) ,
which completes the proof. D. EXPERIMENT DETAILS D.1 MODEL ARCHITECTURE AND HYPER-PARAMETERS For our experiments, we used the DQN(Mnih et al., 2013) architecture, where the Q-value network is composed of 3 convolutional layers, 1 fully connected layer, and 1 output fully connected layer. Each layer except the final layer is followed with a rectified linear activation(ReLU). The first convolutional layer use 32 8 × 8 filters with stride 4, the second has 64 4 × 4 filters with stride 2, and the third convolutional layer has 64 3 × 3 filters with stride 1. The fully connected layer consists of 512 units and the final layer outputs a single value for each action. We used the Adam optimizer with learning rate 0.0001 and = 0.0015. The discount was set to  = 0.99. Training is done over 20M or 40M frames. We updated the target networks every 10000 steps. The size of experience replay buffer is 100000 tuples, where 32 mini batches were sampled every 4 steps to update the network. The exploration policy is -greedy policy with fixed  = 0.01. D.2 PREPROCESSING OF ENVIRONMENTS We preprocess the environment in the same way as the original DQN paper (Mnih et al., 2013) does. We utilize the action repeat technique, i.e., each action is repeated for the next four consecutive frames. The frames are firstly grey-scaled and then rescaled to the size of 84 × 84 pixels. Each state is represented by a concatenation of 4 consecutive frames. We fix all positive rewards to be 1 and all negative rewards to be -1, leaving 0 rewards unchanged. Transitions associated with the loss of a life are considered terminal.
16

