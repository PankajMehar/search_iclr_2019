Under review as a conference paper at ICLR 2019
MAJOR-MINOR LSTMS FOR WORD-LEVEL LANGUAGE MODEL
Anonymous authors Paper under double-blind review
ABSTRACT
As a widely-accepted evaluation criterion, complexity has attracted more and more attention in the design of language models. The parameter count is a proxy for complexity, which is often reported and compared in research papers. In general, more parameters means better model performance, but higher complexity. Therefore, reconciling the contradiction between the complexity and the model performance is necessary. In this paper, we propose a simple method to make use of model parameters more effectively, so that the LSTM-based language models can reach better results without the cost of increasing parameters. The method constructs another small-scale LSTM with a part of parameters originally belonging to the vanilla LSTM in each layer, whose output can assist the next layer in processing the output of the vanilla LSTM. We name these two LSTMs MajorMinor LSTMs. In experiments, we demonstrate the language model with MajorMinor LSTMs surpasses the existing state-of-the-art model on Penn Treebank and WikiText-2 with fewer parameters.
1 INTRODUCTION
Language model (LM) is a foundational component of natural language processing (NLP) tasks, which estimates the probability distribution of a sequence of words (w0, ..., wn) by modeling the probability of the next word (wi) given preceding words (w0, ..., wi-1), i.e.
N
P (w0, ..., wn) = P (w0) P (wi|w0, ..., wi-1)
i=1
Language model plays an important role of systems for machine translation Koehn (2009), speech recognition (Yu & Deng, 2014), learning token embeddings (Botha & Blunsom, 2014; Press & Wolf, 2016), natural language generation (Radford et al., 2017; Merity et al., 2017) and text classification (Howard & Ruder, 2018).
For word-level language model, the vanilla multi-layer Long Short Term Memory (LSTM) networks have been demonstrated to achieve state-of-the-art performance (Yang et al., 2017; Merity et al., 2017). As one of the successful variants of Recurrent Neural Network (RNN), LSTM (Hochreiter & Schmidhuber, 1997) can not only process word sequences of any length, but also alleviate the issue of gradient vanishing effectively.
Since the probability distribution to be predicted is complex, current LSTM-based language models often utilize large-scale LSTMs with multiple layers to enhance generalization capability of the language model, which lead to a huge amount of parameters. In fact, we have found through a lot of experiments that when the scale of a certain LSTM layer is large enough, the improvement of overall model performance will reduce as the LSTM output dimension increases, even the change within a certain range will not cause obvious difference of the model result. But for these largescale LSTM layers, a small increase in their output size requires a large number of parameters to generate. This means that in order to achieve model effect as good as possible, there will inevitably be a part of parameters in the LSTMs with little contribution. Therefore, how to effectively improve the utilization of model parameters is a challenge for language model.
For the sake of clarity, we call the LSTM in each layer of the vanilla LSTM-based language model Major LSTM. In this paper, we propose to reduce the dimension of Major LSTM appropriately, and
1

Under review as a conference paper at ICLR 2019
set up a Minor LSTM with the saved parameters, under the premise of not impairing the performance of the Major LSTM significantly. The Minor LSTM generates a set of auxiliary features, which has capability to assist the next layer in processing the output of the current Major LSTM. Since the scale of the Minor LSTM is much smaller than the Major LSTM, the parameters building the Minor LSTM do not exceed the saved parameters. The architecture of the layer we proposed is called Major-Minor LSTMs (MMLSTMs), and for convenience the language model consisting of MMLSTMs is called Major-Minor Language Model (MMLM). We will detailed introduce the architecture of MMLSTMs and explain the rationality of the Minor LSTM later. We integrate some existing methods (e.g. regularization, optimization, Mixture of Softmax) into our MMLM, and evaluate it on standard language modeling benchmarks. The experimental results show that the MMLM surpasses existing state-of-the-art language model on both Penn Treebank and WikiText-2 datasets with fewer parameters.
2 RELATED WORK
Language modeling has gone through significant development from conventional N-gram language models (Kneser & Ney, 1995; Chen & Goodman, 1999) to neural language models (Bengio et al., 2003; Mikolov et al., 2010; Jozefowicz et al., 2016) in these decades. These classical N-gram models suffer from data sparsity, which makes it difficult to represent large contexts and thus, long-range dependencies. The LSTM-based language models effectively alleviate the problem of long-range dependency, so their performance greatly exceed other neural network language models. In addition, these LSTM-based language models also use a large amount of parameters for better generalization. Zaremba et al. (2014) applied dropout to the non-recurrent connections in LSTM language models with medium and large sizes, and achieved the best results with the large one at that time. Although, the large model was only slightly better than the medium model in the final performance, its parameter count is three times that of the latter. Gal & Ghahramani (2016) used the large LSTM language model with a dropout variation to surpass the result of Zaremba et al. (2014). But there was still no effective way to reduce the model complexity. Inan et al. (2016) introduced a novel theoretical framework leadingto tying together the input embedding and the output projection matrices, greatly reducing the number of trainable parameters. Merity et al. (2017) proposed a weight-dropped LSTM, which used DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, they introduced a variant of averaged stochastic gradient method named NT-AvSGD, which was a successful improvement in the language model optimization method. Zolna et al. (2017) proposed to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their predictions. Yang et al. (2017) identified the Softmax bottleneck by formulating language modeling as a matrix factorization problem, and proposed a method called Mixture of Softmax to address it. However, for these language models using multi-layer LSTMs, in order to improve the model performance, they mainly leveraged the regularization method to reduce the over-fitting of the models under the same parameter counts. In contrast, our method is more straightforward. We assist the Major LSTM directly y constructing the Minor LSTM without increasing the cost of parameters. In addition, our model can be combined with existing regularization methods to produce better results.
3 MAJOR-MINOR LSTMS (MMLSTMS) FOR LANGUAGE MODEL
3.1 THE VANILLA LSTM-BASED LANGUAGE MODEL
A vanilla LSTM-based language model can be simply divided into three components: the input of word embeddings, the model of LSTMs, and the Softmax layer.
2

Under review as a conference paper at ICLR 2019

The mathematical formulation of the LSTM is as follow:
ij =  (Wi [hj-1; vj ] + bj ) fj =  (Wf [hj-1; vj ] + bf ) oj =  (Wo [hj-1; vj ] + bo) gj = tanh (Wr [hj-1; vj] + br)
cj = ij gj + fj cj-1 hj = oj tanh (cj)

(1) (2) (3) (4) (5) (6)

Where [Wi, Wf , Wo, Wr] , [bi, bf , bo, br] are weight matrices and bias vectors. vj is the jth input vector, hj is the current exposed hidden state, cj is the memory cell state, and is element-wise multiplication. The embedding of word sequence is fed into LSTM layers, and the output of the last
LSTM layer is used in predicting the probabilities distribution.

The Softmax layer calculates the probabilities of each word in the word sequence:

eWs ·xj yj = 1T · eWs·xj

(7)

Where [Ws] is weight matrix, xj is the output of the last LSTM layer corresponding to jthword, 1T = (1, ..., 1).

3.2 MAJOR-MINOR LSTMS FOR LANGUAGE MODEL

Major LSTM

Minor LSTM

Figure 1: The architecture of MMLSTMs layer. (v1, ..., vn) and (e1, ..., en) denote the output of the previous layer and the word embedding of the original word sequence respectively. (a1, ..., an) and (c1, ..., cn) represent the output of the Major LSTM and the Minor LSTM.
As we discussed before, the size of LSTMs should be large enough so that the language model has sufficient learning capability. However, the promotion of the model performance will become unnoticeable when the parameter count reachs a certain level. For this reason, instead of keeping increasing the parameters of each layer of Major LSTM for a slight performance improvement, it is better to build a Minor LSTM with a low cost to generate an auxiliary feature vector, which assists the next layer in handling the output of the Major LSTM.
The architecture of a Major-Minor LSTMs layer is shown as Figure 1. The inputs of the Major and Minor LSTMs are the output of the previous layer (v1, ..., vn) and the original embedding of word sequence (e1, ..., en), respectively. And the size of the Major LSTM output vector (a1, ..., an) is much larger than Minor LSTM (c1, ..., cn). For facilitating adjustment, we only set the size of entire Major-Minor LSTMs layer, and adjust the proportions of the Major and Minor LSTMs.
3

Under review as a conference paper at ICLR 2019

The reason why we do not use the same input of the Major LSTM as the input of the Minor LSTM is three-fold. First, according to our hypothesis, the Major LSTM has enough capability to process the output of the previous layer, so that there is no need to process it with the Minor LSTM. Second, if we consider the layers before the current layer as a whole, the output of the current layer can be written as the function of the original embedding of the word sequence. So the output of the previous layer is only an intermediate result for generating the current layer output. The original embeddings, by contrast, can generate the auxiliary features needed by the current layer more directly. Third, as in Merity et al. (2017), the size of embedding is smaller than that size of hidden state, so that ease the issue of overfitting of the word embeddings. Therefore, compared with the output of the previous LSTM layer, using the original embedding as the input of the Minor LSTM can save more parameters.

3.3 THE RATIONALITY OF THE MINOR LSTM

In MMLM, the output of Major-Minor LSTMs layers will flow into the next MMLSTM layer or the last Softmax layer. But from the Formula (1-7) we can infer that whether the next layer is LSTM or Softmax layer, it can be simplified to the combination of different components in a form of nonlinear affine transformation:

yt = f W1 · xtMajor + U1 · st-1 + b1 + W2 · xMt inor

(8)

where xMt ajor xtMinor denotes the tth output of the Major (Minor) LSTM in the previous layer, st-1 denotes the (t - 1)th hidden state in LSTM or 0 in the Softmax layer, [W1, W2, U1, b1] are parameters of the component. f denotes the nonlinear function of the component, which is sigmoid or tanh function in LSTM and softmax function in Softmax layer. The Formula (8) indicates that we can understand W2 · xtMinor as an adaptive bias vector, which adjusts the values of the component yt for different input word sequence. As a result, the auxiliary feature vector xMt inor has the capability to assist the next layer in processing the output of the current Major LSTM.
But for general LSTM-based language models (only have Major LSTMs), we can expand the vector
xMt ajor to xtMajor, xtMajor by increasing the dimension of the Major LSTM. In this way, the
W2 · xMt inor in Formula (8) is replaced by W2 · xMt ajor , and the form of the Formula (8) does not have obvious changes. But since the xMt ajor and the xtMajor are generated by the same LSTM, xMt ajor cannot be freely and independently transformed into an auxiliary feature vector for xMt ajor like xtMinor generated by another LSTM. Concretely, if we simplify the Major LSTM to a vanilla RNN, the xtMajor and the xMt ajor can be calculated as:

xMt ajor = f W · zt + U · xMt ajor, xMt ajor + b xtMajor = f W · zt + U · xMt ajor, xMt ajor + b

(9) (10)

Where f is the nonlinear function, zt is the input corresponding to the tth word. [W, b] and [W , b ] are parameters corresponding to xtMajor and xMt ajor respectively, and they do not interfere with each other. But because of the appearance of xMt ajor, xMt ajor , the generation
of xtMajor xtMajor becomes a recursive procedure, which is controlled by both [W, U, b] and
[W , U , b ]. Any change in each parameter will affect every output dimension of the LSTM, so it is difficult to split an independent portion of its output which can be freely transformed as the auxiliary feature vector of the other part.
For MMLSTMs, the auxiliary feature vector from the Minor LSTM can generate an adaptive bias vector to adjust the calculation of current Major LSTM output. By contrast, splitting a part of output in a single LSTM as the auxiliary feature vector will bring a heavier parameter burden to the LSTM.

4

Under review as a conference paper at ICLR 2019

Hyper-parameter
Learning rate MMLSTMs layer size Ratio of Major LSTM Embedding V-dropout Hidden state V-dropout Non-monotone interval

PTB
20 [1080, 1080, 620] [90%, 90%, 60%]
0.5 0.25 10

WT2
20 [1200, 1200, 650] [90%, 90%, 60%]
0.55 0.2 10

Table 1: Hyper-parameters used for AWD-MMLSTMs-MoS. V-dropout abbreviates variational dropout (Gal & Ghahramani, 2016). See Merity et al. (2017)for more detailed descriptions.

Hyper-parameter
Batch size Learning rate()
  bptt

PTB
150 0.0024 0.0025
0.07 7

WT2
130 0.00198 0.0023 0.0245
7

Table 2: Hyper-parameters used for dynamic evaluation of AWD-MMLSTMs-MoS. See Krause et al. (2017) for more detailed descriptions.

4 EXPERIMENT
4.1 EXPERIMENT DETAILS
We evaluate our MMLM on a preprocessed version of the Penn Treebank (PTB) (Marcus et al., 1993) and the WikiText-2 (WT2) dataset (Merity et al., 2016), and use perplexity as the primary metric. In order to directly verify that our method can further heighten the effect of LSTM-based language model on the basis of some existing techniques, we apply MMLSTMs in AWD-LSTM­MoS architecture proposed by Yang et al. (2017), named AWD-MMLSTMs-MoS. We finetune the hyperparameters in Yang et al. (2017) based on the validation performance while controlling the model size, and list the hyper-parameters we modified in Table 1. For facilitating the adjustment of the regularization methods on Major and Minor LSTMs, we tie up their embedding and hidden state Variational dropout values. Regarding the dimensional proportion of the Major and Minor LSTMs, we divide each layer into 10 parts, and search various combinations heuristically and manually. It was found that the dimensions of the Major LSTMs on three layers accounted for [90%, 90%, 60%] respectively, and this model can achieve the best results on both datasets. We also apply dynamic evaluation Krause et al. (2017) to further improve our model performance, whose hyper-parameters are listed in Table 2.
4.2 MAIN RESULTS
We present the perplexity results from both our models (AWD-MMLSTMs-MoS) and other competitive models in Table 3 and 4 for PTB and WT2 respectively. We improve the state-of-the-art results on both datasets, 46.81 on PTB and 40.15 on WT2. In conclusion, obtaining our model requires three stages: training, fine-tuning, and dynamic estimation. Our model surpasses AWD-LSTM-MoS in all stages, which fully proves the effectiveness of our method.
4.3 STUDY OF DIMENSIONAL CHANGES ON DIFFERENT LSTM LAYERS
The idea of Major-Minor LSTMs layer is based on our observation that reducing the dimensions of Major LSTMs in each layer properly does not significantly affect the performance of the Major LSTMs. So we build a small-scale Minor LSTM to extract a set of auxiliary features to assist the next layer in processing the output of the current Major LSTM. In this subsection, we study the influence of dimensional changes in different LSTM layers on the vanilla LSTM-based language
5

Under review as a conference paper at ICLR 2019

Model
Mikolov & Zweig (2012) ­ RNN-LDA + KN-5 + cache Zaremba et al. (2014) - LSTM (large)
Gal & Ghahramani (2016) ­ Variational LSTM (MC) Kim et al. (2016) - CharCNN
Merity et al. (2016) ­ Pointer Sentinel-LSTM Grave et al. (2016) - LSTM + continuous cache pointer Inan et al. (2016) ­ Tied Variational LSTM + augmented loss
Zilly et al. (2016)­ Variational RHN Zoph & Le (2016) ­ NAS Cell
Melis et al. (2017) ­ 2-layer skip connection LSTM Merity et al. (2017) ­ AWD-LSTM w/o finetune Merity et al. (2017) ­ AWD-LSTM
Yang et al. (2017) ­ AWD-LSTM-MoS w/o finetune Yang et al. (2017) ­ AWD-LSTM-MoS
Ours-AWD-MMLSTMs-MoS w/o finetune Ours-AWD-MMLSTMs-MoS
Merity et al. (2017) ­ AWD-LSTM + continuous cache pointer* Krause et al. (2017) ­ AWD-LSTM + dynamic evaluation*
Yang et al. (2017) ­ AWD-LSTM-MoS + dynamic evaluation* Ours-AWD-MMLSTMs-MoS + dynamic evaluation*

#Param
9M 66M 66M 19M 21M
51M 23M 54M 24M 24M 24M 21.5M 21.5M 21.3M 21.3M 24M 24M 21.5M 21.3M

Validation
82.2
72.4 71.1 67.9 60.9 60.7 60.0 58.08 56.54 56.52 55.42 53.9 51.6 48.33 47.31

Test
92.0 78.4 73.4 78.9 70.9 72.1 68.5 65.4 62.4 58.3 58.8 57.3 55.97 54.44 54.51 53.46 52.8 51.1 47.69 46.81

Table 3: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained from Merity et al. (2017) and Yang et al. (2017). * indicates using dynamic evaluation.

Model
Inan et al. (2016) - Variational LSTM (tied) + augmented loss Grave et al. (2016) - LSTM + continuous cache pointer Melis et al. (2017) - 2-layer skip connection LSTM (tied) Merity et al. (2017) ­ AWD-LSTM w/o finetune Merity et al. (2017) ­ AWD-LSTM Yang et al. (2017) ­ AWD-LSTM-MoS w/o finetune Yang et al. (2017) ­ AWD-LSTM-MoS Ours-AWD-MMLSTM-MoS w/o finetune Ours-AWD-MMLSTM-MoS
Merity et al. (2017) ­ AWD-LSTM + continuous cache pointer* Krause et al. (2017) ­ AWD-LSTM + dynamic evaluation*
Yang et al. (2017) ­ AWD-LSTM-MoS + dynamical evaluation* Ours-AWD-MMLSTMs-MoS + dynamical evaluation*

#Param
28M -
24M 33M 33M 35M 35M 32.3M 32.3M 33M 33M 35M 32.3M

Validation
91.5 -
69.1 69.1 68.6 66.01 63.88 64.3 63.11 53.8 46.4 42.41 41.91

Test
87.0 68.9 65.9 66.0 65.8 63.33 61.45 61.77 60.51 52.0 44.3 40.68 40.15

Table 4: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and Yang et al. (2017). * indicates using dynamic evaluation.

model orderly. We use AWD-LSTM 1 and AWD-LSTM-MoS as experimental objects, and change the LSTM dimension of different layers, then record the perplexities of the language model on PTB and WT2 datasets. Except for the dimensions of the LSTM layer, the other hyper-parameters are the same as in Merity et al. (2017) and Yang et al. (2017). Moreover, we exclude the finetuning and dynamic evaluation steps and the experiments of AWD-LSTM-MoS on WT2 for saving computational resources and avoiding distractive factors. The results are reported on both Figure 2 and Figure 3.

1The third LSTM layer of AWD-LSTM is connected to the Softmax layer, whose input size is fixed after tying the word vectors and word classifier. So we only experiment over the first two LSTM layers.
6

Under review as a conference paper at ICLR 2019

(a) AWD-LSTM on PTB

(b) AWD-LSTM on WT2

(c) MoS on PTB

Figure 2: The line graphs of models with changed LSTM layers dimensions perplexities on PTB and WT2. The solid line and the dotted line denote the perplexity variation trend of the first LSTM and the second LSTM layer respectively. And the dimensional interval of these three line graphs is 200.

(a) AWD-LSTM on PTB

(b) AWD-LSTM on WT2

(c) MoS on PTB

Figure 3: The perplexity bands of models with changed LSTM layers dimensions on PTB and WT2. The standard dimension is denoted as c, and the dimension range of perplexity bands in (a) (b) and (c) is from 0.6c to 1.2c. In every perplexity band, the red box is used to mark the region within the range of [-0.25, 0.25] compared with the perplexity at dimension c (standard perplexity).

In order to obtain the overall trend of perplexities with dimensional changes in each LSTM layer, we firstly use three line graphs2 to observe in a large dimension range from 200 to 1200 at intervals of 200. From these line graphs we can clearly see that with the increase of the dimension, the perplexity keeps decreasing, but the rate of decline is getting slower and slower. This means that the contribution of newly added parameters to the improvement of model performance decreases, so it is not worthwhile to strengthen the final effect by setting larger dimensions of LSTM layers. Moreover, we can also find that the dimensional change of the second LSTM layer has more influence than the first LSTM layer on the total model performance.
On the other hand, for intuitively verifying that the dimensional changes in a certain range for each LSTM layer will not cause obvious fluctuation in perplexity, we take the dimensions of each LSTM layer in Merity et al. (2017) and Yang et al. (2017) as standard, then draw the perplexity bands of models with dimensions from 0.6c to 1.2c in each layers at intervals of 0.1c( 100). In each perplexity band, we mark the part whose perplexity differs from the standard perplexity within the range of [-0.25, 0.25] with a red box. We find the dimensions of 0.9c in the most layers are included in red boxes, and the red box of the third LSTM layer in AWD-LSTM-MoS includes the dimension of 0.6c. This demonstrates that we can greatly reduce the useless parameters and maintain the model performance by setting the dimension ratio of the Major LSTMs to 90% and 60% in the first two and the last MMLSTMs layers.
4.4 STUDY OF MINOR LSTM INPUT
In Section 3.2 we explained the reason why we use the original embeddings as the input of the Minor LSTMs instead of the output of the previous layer. To compare the influence of the two types inputs on the final results, we conduct experiments on PTB and WT2, while limiting the amount of parameters. We construct two AWD-MMLSTMs-MoS with the two types input of Minor LSTMs, and denote them as MMLM-1 (embedding input) and MMLM-2. The dimension of the first two LSTM layers are set to 1150 for MMLM-2, so that the total parameters of the two models
2Since the dimension of the third LSTM of AWD-LSTM-MoS is much smaller than the first two layers, it is not suitable to draw their dimensional changes in the same coordinate system, so we only plot the dimensional changes of the first two LSTM layers in the line graph.
7

Under review as a conference paper at ICLR 2019

Model
MMLM-1 MMLM-2

#Param 21.3M 21.7M

PTB
Val 56.52 57.95

Test 54.51 55.93

#Param 32.3M 32.5M

WT2
Val 64.30 65.43

Test 61.77 62.76

Table 5: The perplexities of MMLM-1 and MMLM-2 over PTB and WT2 datasets.

are roughly equal. The other hyper-parameters of MMLM-1 and MMLM-2 are the same, and we still remove the fine-tuning and dynamic evaluation steps. The results are shown in Table 5. The MMLM-1 surpasses MMLM-2 on both datasets steadily, which demonstrates using the embedding of input word sequence can better generate auxiliary features for Major-LSTM.

Model
MMLM-w/o-Minor-1 MMLM-w/o-Minor-2 MMLM-w/o-Minor-3
MMLM

AD 1030 1030 520
-

PTB
#Param 21.5M 21.5M 21.5M 21.3M

Val 57.3 57.14 57.84 56.52

Test 55.10 54.93 55.74 54.51

AD 1150 1130 550
-

WT2
#Param 32.5M 32.6M 32.6M 32.3M

Val 65.34 65.07 65.66 64.3

Test 62.73 62.43 63.02 61.77

Table 6: Ablation study on PTB and WT2 datasets without fine-tuning or dynamical evaluation. AD ( Adjusted Dimension) means adjusted dimension of MMLSTM layer whose Minor LSTM is removed, and the columns with the title Val and Test denote the perplexities on the validation sets and test sets respectively.

4.5 ABLATION STUDY
Through the above experiments, we can reach the conclusion that using MMLSTMs layers can control the amount of parameters effectively and improve the model performance. To further study the contribution of the Minor LSTM in each MMLSTMs layer, we conduct an ablation study on both PTB and WT2. We remove the Minor LSTM from each MMLSTMs layer by layer, and denote the model with removed layer as MMLM-w/o-Minor-i (i = 1, 2, 3). The dimension of the layer removed the Minor LSTM will be adjusted to keep total parameters consistent, and the other hyper-parameters are the same to ensure a single variable contrast experiment. We also exclude the fine-tuning and dynamic evaluation steps, and the results are reported in Table 6.
Comparing with our original three-layer MMLM, we can conclude that the Minor LSTM of the third layer has the greatest effect on the final performance of the model, followed by the first Minor LSTM and finally the second. In addition, this experiment demonstrates once again that MMLSTMs layer can not only reduce the amount of parameters in each layer, but also benefit the model performance.
5 CONCLUSIONS
We observed that when the scale of LSTM is large enough, the benefits to the vanilla LSTM-based language model of increasing the dimension of LSTM are very limited, while leading to a lot of parameter waste. In this paper, we studied the phenomenon systematically, then proposed to reduce the dimension of the Major LSTM in the original LSTM layer appropriately, and construct a smallscale Minor LSTM to extract a set of auxiliary features, so that the next layer can process the output of the Major LSTM better. In experiments, we verified the MMLM can control the total parameter counts effectively, and further improve the performance of the language model based on the existing methods such as regularization, optimization and Mixture of Softmax. Moreover, our method improved the current state-of-the-art results on standard benchmarks with less amount of parameters.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137­1155, 2003.
Jan Botha and Phil Blunsom. Compositional morphology for word representations and language modelling. In International Conference on Machine Learning, pp. 1899­1907, 2014.
Stanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling. Computer Speech & Language, 13(4):359­394, 1999.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pp. 1019­1027, 2016.
Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Jeremy Howard and Sebastian Ruder. Fine-tuned language models for text classification. CoRR, abs/1801.06146, 2018. URL http://arxiv.org/abs/1801.06146.
Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In AAAI, pp. 2741­2749, 2016.
Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In icassp, volume 1, pp. 181e4, 1995.
Philipp Koehn. Statistical machine translation. Cambridge University Press, 2009.
Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence models. arXiv preprint arXiv:1709.07432, 2017.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313­330, 1993.
Ga´bor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589, 2017.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182, 2017.
Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model. SLT, 12(234-239):8, 2012.
Toma´s Mikolov, Martin Karafia´t, Luka´s Burget, Jan C ernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.
Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.
9

Under review as a conference paper at ICLR 2019 Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax
bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017. Dong Yu and Li Deng. Automatic Speech Recognition: A Deep Learning Approach. Springer, 2014. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329, 2014. Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn´ik, and Ju¨rgen Schmidhuber. Recurrent
highway networks. arXiv preprint arXiv:1607.03474, 2016. Konrad Zolna, Devansh Arpit, Dendi Suhubdy, and Yoshua Bengio. Fraternal dropout. arXiv
preprint arXiv:1711.00066, 2017. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
10

