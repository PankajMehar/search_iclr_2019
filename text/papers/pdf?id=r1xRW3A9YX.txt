Under review as a conference paper at ICLR 2019
RIEMANNIAN TRANSE: MULTI-RELATIONAL GRAPH EMBEDDING IN NONEUCLIDEAN SPACE
Anonymous authors Paper under double-blind review
ABSTRACT
Multi-relational graph embedding has wide applications for social network analysis, recommendation systems, and knowledge base completion. The problem is to obtain embeddings that are beneficial for tasks with low-dimensional parameters. This paper proposes a novel framework, called Riemannian TransE for multirelational graph embedding. Our method realizes embedding in non-Euclidean space, where the loss function is based on the distance. Thus, our model can use a non-Euclidean space that has good compatibility with the data, and achieves good performance with low-dimensional parameters. An evaluation on real knowledge base data shows that with an appropriate choice of manifold, our method achieves comparable accuracy in graph completion with low-dimensional parameters.
1 INTRODUCTION
1.1 BACKGROUND
Multi-relational graphs, such as social networks and knowledge bases, have a variety of applications, and embedding methods for these graphs are particularly important for these applications. For instance, multi-relational graph embedding has been applied to social network analysis KrohnGrimberghe et al. (2012) and knowledge base completion Bordes et al. (2013). A multi-relational graph consists of entities V, a set R of relation types, and a collection of real data triples, where each triple (h, r, t)  V × R × V represents some relation r  R between a head entity h  V and a tail entity t  V. Embedding a multi-relational graph refers to a map from the entity and the relation set to some space. Mathematical operations in this space enable many tasks, including clustering of entities and completion, prediction, or denoising of triples. Indeed, completion tasks for knowledge bases attract considerable attention, because knowledge bases are known to be far from complete, as discussed in West et al. (2014) Krompaß et al. (2015). Multi-relational graph embedding can help its completion and improve the performance of applications that use the graph. This is the reason why much work focuses on multi-relational graph embedding. Figure 1 shows an example of a multi-relational graph and a completion task.
In multi-relational graph embedding, reducing the number of parameters is an important problem in the era of big data. Many parameters are needed with tensor-factorization-based methods, such as Bayesian clustered tensor factorization (BCTF) Sutskever et al. (2009), RESCAL Nickel et al. (2011), and a neural tensor network (NTN) Socher et al. (2013). Thus, TransE Bordes et al. (2013) was proposed to reduce the number of parameters, to overcome this problem and avoid evaluating dense matrices or tensors. In TransE, each entity is mapped to a point in Euclidean space and each relation is no more than a vector addition, rather than a matrix operation. The successors to TransE, TransH Wang et al. (2014) and TransD Ji et al. (2016), also use only a small number of parameters. Some methods succeeded in reducing parameters using diagonal matrices instead of dense matrices: e.g. DISTMULT Yang et al. (2015), ComplEx Trouillon et al. (2016), HolE (through the Fourier transform) Nickel et al. (2016), and ANALOGY Liu et al. (2017).
Whereas these methods use distances or inner products in Euclidean space, recent work has shown that using non-Euclidean space can further reduce the number of parameters. One typical example of this is Poincare´ Embedding Nickel & Kiela (2017) for hierarchical data, where a hyperbolic space
1

Under review as a conference paper at ICLR 2019
is used as a space for embedding. Here, the tree structure of hierarchical data has good compatibility with the exponential growth of hyperbolic space. As a result, Poincare´ embedding achieved good graph completion accuracy, even in low dimensionality. The success of the Poincare´ embedding suggests that the appropriate choice of a manifold (i.e., space) can retain low dimensionality. Since Poincare´ embedding, several methods have been proposed for single-relational graph embedding in non-Euclidean space (e.g. Ganea et al. (2018b) Nickel & Kiela (2018)) and shown good results. However, these methods are limited to single-relational graph embedding, and an extension to multi-relational graph embedding is needed. This background forms the motivation for our work, which aims at reducing parameters using non-Euclidean space in multi-relational graph embedding without significant loss of performance (e.g., in graph completion tasks).

Figure 1: Multi-relational graph and its completion. There are five entities and two kinds of relation (hypernym and synonym). Graph completion refers to answering questions such as "is mammal a hypernym of cannis?"

Figure 2: Tangent space and exponential map. The key idea of this paper is using the exponential map as an alternative of vector addition.

1.2 CONTRIBUTIONS
We propose a novel method for multi-relation graph embedding using a non-Euclidean manifold. Our method, called Riemannian TransE, can be regarded as an extension of TransE Bordes et al. (2013) in a non-Euclidean manifold. Riemannian TransE realizes embedding in non-Euclidean space, where the loss function is based on the distance. The entities are thus mapped to a Riemannian manifold and we can solve tasks (e.g. graph completion) by using the distance. Since Riemannian TransE can use a non-Euclidean space that has good compatibility with the graph (e.g. hyperbolic space to a hierarchical data), it achieves good performance with low-dimensional parameters. Riemannian TransE further exploits the advantages of TransE: that is, there is no need to evaluate the matrix or tensors. Our extension is not trivial because we cannot define vector addition in a Riemannian manifold, and this is essential in TransE. To realize our extension, we replace the vector addition operation in TransE by an exponential map -- a move along a geodesic. Numerical experiments on graph completion tasks show that with an appropriate choice of manifold, our method can improve the performance of multi-relational graph embedding with few parameters.

2 RELATED WORK

2.1 MULTI-RELATIONAL GRAPH EMBEDDING

Let V and R denote the entities and relations in a multi-relational graph, and let T  V × R × V
denote the triples in the graph. Multi-relational graph embedding refers to a pair of maps from V
and R into Me and Mr, respectively. Particularly, learning multi-relational graph embedding refers to obtaining an appropriate pair of maps v  pv (v  V, pv  Me) and r  wr (r  R, wr  Mr) from the triples T . In this paper, we call pv the planet of entity v, wr the launcher of relation r, and Me and Mr the planet manifold and launcher manifold, respectively. The quality of embedding is measured through a score function f : (Me × Me) × Mr  R, which is designed by each
method. Embedding is learned such that the value score function f (ph, pt; wr) will be low when ph, pt; wr  T and high when ph, pt; wr / T . For specific loss functions designed from the score
function, see Subsection 2.3. We interpret the score function of multi-relational graph embedding as
dissimilarity in a manifold, which we call a satellite manifold Ms. We rewrite the score function f in multi-relational graph embedding using two maps H , T : Me × Mr  Ms and the dissimilarity measure function D : Ms × Ms  R as follows:

f (ph, pt; wr) := D shH;r, sTt;r , where shH;r = H (ph; wr) , stT;r = T (pt; wr).

(1)

2

Under review as a conference paper at ICLR 2019

We call H and T the head and tail launch map, respectively, and call svH;r and sTv;r the head and tail satellite of entity v (or of planet pv) with respect to relation r.

Table 1: Score Functions:  denotes conjugate transpose. F denotes the discrete Fourier Transform. The interpretation here of HolE is given by Liu et al. (2017) and Hayashi & Shimbo (2017).

Model

Planets Launchers

Head satellites shH;r Tail satellites stT;r

Dissimilarity # parameters

TransE Bordes et al. (2013) TransH Wang et al. (2014)
TransR Lin et al. (2015)
TransD Ji et al. (2016)

pv  RD wr  RD
pv  RD (wr , wrpr)  RD × RD
pv  RD (W r , wr )  RD×D~ × RD~
(pv , pvpr )  RD/2 × RD/2 (wr , wprr)  RD~ × RD~

ph + wr  RD pt  RD

I - wprrwrpr I - wprrwrpr

ph + wr  RD pt  RD

W r ph + wr  RD~ W r pt  RD~

I + wprrphpr I + wprrppt r

ph + wr  RD~ pt  RD~

sTt;r - shH;r D |V| + D |R|
stT;r - sHh;r D |V| + 2D |R|
sTt;r - sHh;r D |V| + DD~ + D~ |R|
stT;r - shH;r D |V| + 2D~ |R|

RESCAL Nickel et al. (2011)
DISTMULT Yang et al. (2015)
ComplEx Trouillon et al. (2016)
HolE Nickel et al. (2016)
ANALOGY Liu et al. (2017)

pv  RD W  RD×D
pv  RD wr  RD
pv  CD/2 wr  CD/2
pv  RD wr  RD
pCv , pvR  CD/4 × RD/2 wrC, wRr  CD/4 × RD/2

W r ph  RD pt  RD

diag {wr } ph  RD pt  RD

diag {wr } ph  CD/2 pt  CD/2

F (ph)  CD diag {F (wr )} F (pt)  CD

diag

{wr } ph



C

3 4

D

pt



C

3 4

D

sHh;r stT;r D |V| + D2 |R|
sHh;r sTt;r D |V| + D |R|
Re sHh;r sTt;r D |V| + D |R|
Re shH;r stT;r D |V| + D |R|
Re shH;r sTt;r D |V| + D |R|

A simple example of this is TransE Bordes et al. (2013), where all of the planets, satellites, and launchers share the same Euclidean space, i.e. Me = Ms = Mr = RD, the launch maps are given by vector addition as H (p; w) = p + w and T (p; w) = p, and the distance in a norm space-- i.e. the norm of the difference--is used as a dissimilarity measure i.e. D sH, sT = sT - sH (the L1 or L2 norm is often used in practice). See Figure 3 (left). As Nguyen (2017) suggested, one can associate the idea of representing relations as vector additions with the fact that we can find a relation through a substraction operator in Word2Vec Mikolov et al. (2013). That is, we can find relations such as pFrance - pParis  pItaly - pRome in Word2Vec. As explained above, TransE is based on the distance between satellites, and each satellite is given by simple vector addition. Regardless of this simplicity, the performance of TransE has been exemplified in review papers Nickel et al. (2016) Nguyen (2017). Indeed, the addition operation in a linear space is essential in the launcher map, and hence TransE can easily be extended to a Lie group, which is a manifold equipped with an addition operator, as suggested in Ebisu & Ichise (2017). Some methods, such as TransH Wang et al. (2014), TransR Lin et al. (2015), and TransD Ji et al. (2016), also use a norm in linear space as a dissimilarity measure, integrating a linear map into a latent space.
Another simple example is RESCAL Nickel et al. (2011), which uses the inner product as a dissimilarity measure function. In RESCAL, the launcher of relation r is a matrix W  Mr = RD×D, the launch maps are given by a linear map, i.e. H (p; (W , w)) = W p and T (p; (W , w)) = p, and
the dissimilarity measure is the (negative) inner product D sH, sT = - sH sT. Other methods are also based on the inner product dissimilarity: e.g., DISTMULT Yang et al. (2015), ComplEx Trouillon et al. (2016), HolE (through the Fourier transform) Nickel et al. (2016), and ANALOGY Liu et al. (2017). Table 2.1 shows score functions of these methods.
Whereas some methods are based on a neural network (e.g., the neural tensor network Socher et al. (2013) and ConvE Dettmers et al. (2017)), their score function consists of linear operations and element-wise nonlinear functions.

3

Under review as a conference paper at ICLR 2019

2.2 GRAPH EMBEDDING IN NON-EUCLIDEAN SPACE
Graph embedding using non-Euclidean space has attracted considerable attention, recently. Specifically, embedding methods using hyperbolic space have achieved outstanding results Nickel & Kiela (2017) Ganea et al. (2018b) Nickel & Kiela (2018). With these methods, each node in the graph is mapped to a point in hyperbolic space and the dissimilarity is measured by a distance function in the space. Although these methods exploit the advantages of non-Euclidean space, specifically those of a negative curvature space, they focus on single- rather than multi-relational graph embedding.
By contrast, TransE has been extended to an embedding method in a Lie group--that is, a manifold with the structure of a group Ebisu & Ichise (2017). As such, the regularization problem in TransE is avoided by using torus, which can be regarded as a Lie group. Although this extension to TransE deals with multi-relational embedding, it cannot be applied to all manifolds. This is because not all manifolds have the structure of a Lie group. Indeed, we cannot regard a hyperbolic space (if D = 1) or a sphere (if D = 1, 3) as a Lie group.

2.3 LOSS FUNCTION

We can simply design a loss function on the basis of the negative log likelihood of a Bernoulli model as follows:

L {pv}vV , {wr}rR := -

log ( (f (ph, pt; wr))) -

log (1 -  (f (ph , pt ; wr ))) ,

(h,r,t)T

(h ,r ,t )T c

(2)

where T c := (V × R × V)\T and  : R  [0, 1] is a sigmoid function. However, this loss function needs evaluation of the score function for all negative triplets (V × R × V) \ T . To avoid this, most
methods (e.g., TransE) use the following margin-based loss function:

L {pv}vV , {wr}rR :=

[ + f (ph, pt; wr) - f (ph , pt ; wr)]+ ,

(h,h ,r,t ,t)Q

(3)

where Q is the set of the triples with its corrupted head and tail. That is,

Q := {(h, h , r, t , t)  V × V × R × V × V | [(h, r, t)  T ]  [(h = h)  (t = t)]} , (4)

where   R0 is the margin hyperparameter, and [·]+ denotes the negative value clipping--i.e. for all x  R, [x]+ := max(x, 0). We use this loss function throughout this paper.

3 RIEMANNIAN TRANSE

3.1 ADVANTAGES OF RIEMANNIAN MANIFOLD
We focus on Riemannian manifolds for several reasons. One reason is that some manifolds have completely different properties from those of Euclidean space. Specifically, the difference in curvature or topology can greatly influence embedding performance. For example, hyperbolic space has constant negative curvature and its space grows exponentially. Indeed, the circumference with radius R is given by 2 sinh R( 2 exp R) in a hyperbolic plane. As suggested by Nickel & Kiela (2017), this property works well when embedding hierarchical data such as a complete binary tree. In fact, Poincare´ embedding offers excellent performance with link prediction tasks in low dimensionality. Note that the circumference with radius R is given by 2R along a Euclidean plane, and this, at least, is unsuitable for embedding a complete binary tree or typical hierarchical data, where the number of nodes grows exponentially. For this reason, high performance in multi-relational graph embedding is also expected even with low dimensionality. A second reason is implementability. Riemannian manifolds are locally homeomorphic to Euclidean space, which allows us to express them by numerical vectors thorough its coordinate system.

3.2 DIFFICULTIES IN RIEMANNIAN MANIFOLDS
As explained above, a score function on the basis of the distance function defined in a non-Euclidean manifold has advantages for graph embedding. However, the extension of TransE is not trivial.

4

Under review as a conference paper at ICLR 2019
Whereas satellites are given by vector addition in TransE, a vector is not generally defined in a nonEuclidean manifold (see Supplementary Material). Therefore, we replace vector addition in TransE with an exponential map, which is defined as a move along a vector in a Riemannian manifold. This is our key idea, and we introduce the exponential map and related notions in Riemannian manifolds in the following subsection.
3.3 RIEMANNIAN MANIFOLDS AND OPERATIONS Preliminaries and Notation Let (M, g) be a Riemannian manifold with metric g. We denote the tangent and cotangent space of M on p by TpM and TpM, respectively, and we denote the collection of all smooth vector fields on M by X (M). Let f : M  R be a C function, and its gradient vector gradf (x) of f at x  M as the tangent vector in TxM that satisfies g (gradf (x) , v) = vf for all v  TxM. Let : TpM  TpM and : TpM  TpM denote the index raising and lowering (i.e. identification of tangent and cotangent vectors through the metric), respectively. Note that one is the inverse map of the other. Let  : X (M) × X (M) (X, Y )  X Y  X (M) denote the Levi­Civita connection, the unique metric-preserving torsion-free affine connection.
Geodesic and Exponential Map Now, we define geodesics and the exponential map as extensions of the straight lines and vector addition, respectively. A smooth curve  : (- , )  M is a geodesic when   = 0 on curve , where  is the differential of curve . Geodesics are generalizations of straight lines, in the sense that they are constant speed curves that are locally distance-minimizing. We define the exponential map Expp, which moves point p  M towards a vector by the magnitude of the vector. In this sense, the exponential map is regarded as an extension of vector addition in a Riemannian manifold. Figure 2 shows an intuitive example of an exponential map on a sphere. Let v (v  TpM) denote the geodesic that satisfies v (0) = v. The exponential map Expp : TpM  M is given by Expp(v) := v (1). We define the logarithmic map Logp : M  TpM as the inverse of the exponential map. Note that the exponential map is not always bijective, and we have to limit the domain of the exponential and logarithmic map appropriately, while some manifolds, such as Euclidean and hyperbolic space, do not suffer from this problem.
Figure 3: Difference between TransE and Riemannian TransE. In these examples, the number |V| of entities is three (1, 2, 3) and the number |R| of relations is two (red and orange), with triples (1, orange, 2) and (1, red, 3). Hence, these models learn that the orange head satellite of Entity 1 is close to the orange tail satellite of Entity 2 and the red head satellite of Entity 1 is close to the red tail satellite of Entity 3. In addition, the distance of the other pair of satellites should be long in the representation learned by each method. The figure on the left shows the original formulation of TransE, where the satellites are given by vector addition. In other words, the satellites are given by a move towards a point at infinity from the planet. The center figure shows an alternative formulation of TransE, which is equivalent to the original TransE. Here, the tail satellites are launched and the head satellites are fixed in the red relation. In Riemannian TransE in the figure on the right, the vector additions are replaced by a move towards a (finite) point.
5

Under review as a conference paper at ICLR 2019

Figure 4: Relation of the sign for . If is positive (e.g. the orange relation), the relation runs from low (e.g. Entity 2 and 3) to high hierarchy (e.g. Entity 1), and vice versa (e.g. the red relation).

3.4 FORMULATION OF RIEMANNIAN TRANSE

Our aim is the extension of TransE in a general Riemannian manifold. Thus, we design a loss
function on the basis of the distance between satellites in a Riemannian manifold. The problem here
is how to define the position of the satellites. As explained above, we cannot define a vector on
a manifold in non-Euclidean space. However, we can interpret a move along a vector as a move
towards a point at infinity. We obtain the Riemannian TransE as an extension of TransE, replacing the relation vectors wr  RD in TransE by pairs ( r, pr)  R × M of a scalar value and point, indicating the length and destination of the satellites' move, respectively. We call pr the attraction point of relation r. In other words, we use a fixed manifold Me = M for entity embedding and use direct product manifold Mr = R × M for relation embedding. However, the extension still has arbitrariness. For instance, we could launch the tail satellite instead of the head satellite in
TransE; in other words, the following launching map also gives us a score function equivalent to that of the original TransE: H (p; (W , w)) = W p and T (p; (W , w)) = W p - w. On the other hand, the score function depends on whether we move the head or tail satellites in our case, where
the attraction points are not at infinity. With hierarchical data, an entity at a higher hierarchy has
many related entities in a lower hierarchy. Therefore, it is best to always launch the satellites of
"children," the entities in a lower hierarchy, toward their parent. Hence, we move the head satellites
when r > 0 and fix the tail satellites, and vice versa when r < 0; specifically, we move the head satellites by length  = [ r]+ and move the tail satellites by length  = [- r]+. Thus, bottom-totop relation cases correspond to r > 0, and top-to-bottom relation cases correspond to r < 0, as shown in Figure 4. Another problem pertains to launching the satellites near the attraction point. If
 >  (pr, pv), the naive rule causes overrun. In this case, we simply clip the move and set the satellite in the place of pr.

We turn now to the score function of Riemannian TransE. The score function f : (M × M) × (R, M)  R in Riemannian TransE is given as follows:

f (ph, pt; ( r, pr)) := 

shH;r, sTt;r

,

where

sHh;r

:= H

(ph; (

r, pr)) :=
[

m (ph) ,
r ]+,pr

stT;r 

:= T

(pt; (

r, pr)) :=
[-

m (pt) ,
r ]+,pr

(5)

where transform m : M  M denotes a move, defined as follows:
,p

m
,p

(q)

:=

Expp

[ (q, p) - ]+

Logp (q) Logp (q) p

.

(6)

Here, note that m (q) is on the geodesic that passes through p and q. Figure 3 (right) shows the
,p
Riemannian TransE model. If M = RD and the attraction points are at infinity, the score function is equivalent to that of TransE (without the sphere constraint). Although the exponential map and
logarithmic map in closed form are required to implement Riemannian TransE, we can obtain them when the manifold M is a sphere SD (positive curvature), Euclidean space RD (zero curvature),

6

Under review as a conference paper at ICLR 2019

and hyperbolic space HD (negative curvature), or a direct product of them. These are practically sufficient. Also note that the computation costs of these maps are O(D), which is small enough.

3.5 OPTIMIZATION

In typical cases, the number of entities is very large. Therefore, stochastic gradient methods are effective for optimization. Although we can directly apply stochastic gradient methods of Euclidean space or the natural gradient method Amari (1998), Riemannian gradient methods (e.g. Zhang & Sra (2016) Zhang et al. (2016)) work better for non-Euclidean embedding Enokida et al. (2018). In this paper, we use the following simple (projected) stochastic (Riemannian) (sub-) gradient methods Zhang & Sra (2016):

( +1)  Exp() -~ ( ) ,

(7)

where ()  M|V| × (R × M)|R| denotes the parameter in the  -th step,   R0 is the learning rate, and ~ ()  T() M|V| × (R × M)|R| is a stochastic gradient that satisfies
E ~ () = gradL () = dL () . Recall that denotes index raising. Specifically, we use the following stochastic loss function based on the mini-batch method:

L~ () :=

[ + f (ph, pt; wr) - f (ph , pt ; wr)]+ ,

(h,h ,r,t ,t)Q

(8)

where the stochastic quintet set Q()  Q is a set of uniform-distributed random variables on Q.  srH (ph) , sTr (pt) . We obtain a stochastic gradient as follows:

~ () = dL~ ()

 =


L~ ()

d,

~ () =

~ ()

(9)

where  is a local coordinate representation of . We obtain ~ () easily using an automatic differentiation framework. Algorithm 1 shows the learning algorithm for Riemannian TransE. In the experiments, we applied norm clipping such that the norm of a stochastic gradient is smaller than 1.

Algorithm 1 Learning Riemannian TransE

for  = 1, 2, . . . do

Sample Q() from uniform distribution on Q.

~ () 




(h,h ,r,t ,t)Q [ + f (ph, pt; wr ) - f (ph , pt ; wr )]+

( +1)  Exp( ) -( )~ ( ) end for

return ()

Table 2: Triple classification performance. Bold: Top 1, Italic: Top 3.

Dataset

WN11

FB13

Dim.

8 16 32 64 128

8 16 32 64

Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

64.74 68.51 82.07 80.73 72.66 60.94 67.55 62.34 77.55 52.58 53.43 60.36 61.05 62.63 53.01 63.60

66.51 72.88 83.11 81.37 73.99 64.63 66.18 62.78 75.44 53.13 54.61 57.65 61.01 62.47 53.03 59.24

67.78 74.70 82.99 77.12 75.27 63.20 64.07 63.33 70.03 55.30 55.76 56.85 58.97 57.91 51.19 58.55

67.92 75.83 83.13 69.05 76.69 61.92 63.23 63.19 65.46 53.10 63.32 56.62 57.11 56.02 52.62 57.51

67.87 77.03 83.30 63.42 77.04 58.46 61.51 63.45 63.75 55.80 61.59 57.62 55.85 53.47 53.09 57.00

80.05 78.42 64.45 71.26 81.84 67.60 76.44 61.51 71.07 57.43 55.02 74.28 64.54 70.07 58.12 66.38

78.06 77.06 63.38 71.34 80.03 68.29 80.22 58.04 75.25 52.41 56.68 70.17 65.04 72.36 62.13 66.18

77.53 77.39 64.69 71.23 75.44 68.86 77.24 63.06 76.89 51.65 53.69 67.88 63.32 71.11 61.35 64.54

84.65 77.74 70.07 73.03 76.99 75.68 76.01 60.31 78.32 51.87 56.28 65.90 59.77 67.36 60.74 59.48

128
84.67 78.53 69.74 74.83 77.52 74.92 75.59 58.14 80.32 52.38 56.02 63.20 54.76 64.49 54.61 55.26

7

Under review as a conference paper at ICLR 2019
4 EXPERIMENTS
Evaluation Tasks We evaluated the performance of our method for a triple classification task Socher et al. (2013) on real knowledge base datasets. We also report the results of link prediction tasks Bordes et al. (2013), in the Supplementary Materials. The triple classification task involved predicting whether a triple in the test data is correct. We label a triple positive when f (ph, pt; ( r, pr)) > r, and vice versa. Here, r  R0 denotes the threshold for each relation r, which is determined by the accuracy of the validation set. We evaluated the accuracy of classification with the FB13 and WN11 datasets Socher et al. (2013).
Manifolds in Riemannian TransE To evaluate the dependency of performance for Riemannian TransE, we compared Riemannian TransE using the following five kinds of manifolds: Euclidean space RD (Euclidean TransE), hyperbolic space HD (Hyperbolic TransE), a sphere SD (Spherical TransE), the direct product H4 × H4 × · · · × H4 of hyperbolic space (PHyperbolic TransE), and the direct product S4 × S4 × · · · × S4 of a sphere (PSpherical TransE).
Baselines and Implementation We compared our method with the following baselines: RESCAL Nickel et al. (2011), TransE Bordes et al. (2013), TransH Wang et al. (2014), TransR Lin et al. (2015), TransD Ji et al. (2016), TorusE Ebisu & Ichise (2017), RESCAL Nickel et al. (2011), DISTMULT Yang et al. (2015), HolE Nickel et al. (2016), ComplEx Trouillon et al. (2016) and Analogy Liu et al. (2017). We used implementations of these methods on the basis of OpenKE http://openke.thunlp.org/static/index.html, and we used the evaluation scripts there. Note that we compensated for some missing constraints (for example, in TransR and TransD) and regularizers (for example, in DISTMULT and Analogy) in OpenKE. We also found that omitting the constraint of the entity planets onto the sphere in TransE gave much better results in our setting, so we also provide these unconstrained results (UnconstraintTransE). We determined the hyperparameters by following each paper. For details, see the Supplementary Materials.
Results Table 2 shows the results for the triple classification task in each dimensionality. In WN11, the sphere-based Riemannian TransEs achieved good accuracy. The accuracy did not degrade dramatically even with low dimensionality. On the other hand, in FB13, the hyperbolic-space-based Riemannian TransEs was more accurate than other methods. Moreover for each dimensionality, these results with the proposed Riemannian TransE were at least comparable to those of the baselines. The accuracy of Euclidean-space-based methods (e.g. the original TransE, and Euclidean TransE) are between that of the sphere-based Riemannian TransEs and that of the hyperbolic-spacebased Riemannian TransEs in most cases. Note that these results are compatible with the curvature of each space (i.e. Sphere: positive, Euclidean space: 0, a hyperbolic space: negative). That is, positive curvature spaces were good in WN11 and negative curvature spaces were good FB15k, and zero curvature spaces were between them. Note that non-Euclidean methods are not always the best. In the Supplementary Materials, we also report the triple classification task results in FB15k and those of the link prediction task in WN18 and FB15k Bordes et al. (2013), where Riemannian TransE did not always outperform the baseline methods. These results show that Riemannian TransE can attain good accuracy with small dimensionality provided that an appropriate manifold is selected.
5 CONCLUSION AND FUTURE WORK
We proposed Riemannian TransE, a novel framework for multi-relational graph embedding, by extending TransE to a Riemannian TransE. Numerical experiments showed that Riemannian TransE outperforms baseline methods in low dimensionality, although its performance depends significantly on the choice of manifold. Hence, future research shall clarify which manifolds work well with particular kinds of data, and develop a methodology for choosing the appropriate manifold. This is important work not only for graph completion tasks but also for furthering our understanding of the global characteristics of a graph. In other words, observing which manifold is effective can help us to understand the global "behavior" of a graph. Other important work involves using "subspaces" in non-Euclidean space. Although the notion of a subspace in a non-Euclidean manifold is nontrivial, it may be that our method offers advantages over TransH and TransD, which exploit linear subspaces.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane´, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vie´gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural Comput., 10(2):251­276, February 1998. ISSN 0899-7667. doi: 10.1162/089976698300017746. URL http://dx. doi.org/10.1162/089976698300017746.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems, pp. 2787­2795, 2013.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. arXiv preprint arXiv:1707.01476, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Takuma Ebisu and Ryutaro Ichise. Toruse: Knowledge graph embedding on a lie group. arXiv preprint arXiv:1711.05435, 2017.
Yosuke Enokida, Atsushi Suzuki, and Kenji Yamanishi. Stable geodesic update on hyperbolic space and its application to poincare embeddings. arXiv preprint arXiv:1805.10487, 2018.
Octavian-Eugen Ganea, Gary Be´cigneul, and Thomas Hofmann. Hyperbolic entailment cones for learning hierarchical embeddings. arXiv preprint arXiv:1804.01882, 2018a.
Octavian-Eugen Ganea, Gary Be´cigneul, and Thomas Hofmann. Hyperbolic neural networks. arXiv preprint arXiv:1805.09112, 2018b.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010.
Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex embeddings for link prediction. arXiv preprint arXiv:1702.05563, 2017.
Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Knowledge graph completion with adaptive sparse transfer matrix. In AAAI, pp. 985­991, 2016.
Artus Krohn-Grimberghe, Lucas Drumond, Christoph Freudenthaler, and Lars Schmidt-Thieme. Multi-relational matrix factorization using bayesian personalized ranking for social network data. In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining, WSDM '12, pp. 173­182, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-0747-5. doi: 10. 1145/2124295.2124317. URL http://doi.acm.org/10.1145/2124295.2124317.
Denis Krompaß, Stephan Baier, and Volker Tresp. Type-constrained representation learning in knowledge graphs. In International Semantic Web Conference, pp. 640­655. Springer, 2015.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In AAAI, volume 15, pp. 2181­2187, 2015.
Hanxiao Liu, Yuexin Wu, and Yiming Yang. Analogical inference for multi-relational embeddings. In International Conference on Machine Learning, pp. 2168­2178, 2017.
9

Under review as a conference paper at ICLR 2019

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Dis-

tributed representations of words and phrases and their compositionality.

In

C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger

(eds.), Advances in Neural Information Processing Systems 26, pp. 3111­3119.

Curran Associates, Inc., 2013.

URL http://papers.nips.cc/paper/

5021-distributed-representations-of-words-and-phrases-and-their-compositionality.

pdf.

Dat Quoc Nguyen. An overview of embedding models of entities and relationships for knowledge base completion. arXiv preprint arXiv:1703.08098, 2017.

Maximilian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. arXiv preprint arXiv:1806.03417, 2018.

Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In ICML, volume 11, pp. 809­816, 2011.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11­33, 2016.

Maximillian Nickel and Douwe Kiela. Poincare´ embeddings for learning hierarchical representations. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6341­6350. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7213-poincare-embeddings-for-learning-hierarchical-representations. pdf.

Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In Advances in neural information processing systems, pp. 926­934, 2013.

Ilya Sutskever, Joshua B Tenenbaum, and Ruslan R Salakhutdinov. Modelling relational data using bayesian clustered tensor factorization. In Advances in neural information processing systems, pp. 1821­1828, 2009.

Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pp. 57­66, 2015.

The´o Trouillon, Johannes Welbl, Sebastian Riedel, E´ ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International Conference on Machine Learning, pp. 2071­2080, 2016.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In AAAI, volume 14, pp. 1112­1119, 2014.

Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. Knowledge base completion via search-based question answering. In Proceedings of the 23rd international conference on World wide web, pp. 515­526. ACM, 2014.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of the International Conference on Learning Representations, 2015.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.

Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir (eds.), 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 1617­1638, Columbia University, New York, New York, USA, 23­26 Jun 2016. PMLR. URL http://proceedings. mlr.press/v49/zhang16b.html.

10

Under review as a conference paper at ICLR 2019
Hongyi Zhang, Sashank J. Reddi, and Suvrit Sra. Riemannian svrg: Fast stochastic optimization on riemannian manifolds. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 4592­ 4600. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/ 6515-riemannian-svrg-fast-stochastic-optimization-on-riemannian-manifolds. pdf.
A RIEMANNIAN MANIFOLDS AND VECTORS
We discuss the reason why we cannot define a vector on a manifold. We can define a parallel transform along a geodesic. This parallel transform maps a tangent vector in a tangent space to one in another. At one glance, it seems that we can define a vector using the parallel transform. However, a parallel transform is not determined only by the origin and destination but depends on the path i.e. the geodesic. Figure 5 shows an example on a sphere, where two ways to map a vector from a tangent space to another are shown and these two give different maps. As this figure shows we cannot obtain a well-defined vector on more than two points.

Figure 5: Parallel transforms in a sphere S2. This figure shows two ways to transform vector v  TpS2 to TrS2. We denote the parallel transform from along segment pq by qp : TpS2  TqS2. The red vector on TrS2 denotes the vector obtained by the direct transform along segment pr. The blue vector TrS2 denotes the vector obtained by the transform via q. As this figure shows we cannot
obtain a well-defined vector on more than two points.

B EXAMPLES OF RIEMANNIAN MANIFOLDS

We introduces some Riemannian manifolds useful in applications, and the formula of the expo-

nential map and logarithmic map in these manifolds. The closed form of exponential map and

logarithmic map enables implementation of Riemannian TransE in these manifolds. In the follow-

ing,

we

omit

symbols

 x

and

d

x

of

the

basis

in

a

tangent

and

cotangent

space,

respectively,

for

notation simplicity. Moreover, we give the composition of the exponential map and index raising

and that of the index lowering and logarithmic map instead of the exponential map and logarithmic

map themselves. This is because we use a cotangent vector rather than a tangent vector in a prac-

tical

implementation

and

map

from/to

cotangent

space

is

more

useful

(Recall

that




L~ is not the

coordinate of a tangent but the coordinate of a cotangent vector).

B.1 EUCLIDEAN SPACE

In a D-dimensional Euclidean Space, the exponential map (with the index raising) Expp  : Tp RD  RD is given by

Expp  () = p + .

(10)

Apparently, the logarithmic map (with the index lowering)  Logp : RD  Tp RD is given by

 Logp (q) = q - p.

(11)

11

Under review as a conference paper at ICLR 2019

B.2 SPHERE

A D-dimensional (unit) sphere is given by point set SD := p  R(D+1) p p = 1 , and the
cotangent space Tp SD on p  SD is identified with   R(D+1) p  = 0 . The distance  (p, q) between two points p  SD and q  SD is given as follows:

 (p, q) = arccos p q ,

(12)

where arccos : [-1, 1]  [0, ] denote arc-cosine function. The exponential map (with the index raising) Expp  : TpSD  SD is given by

Expp  () = cos   p + sinc   ,

where sinc denotes the cardinal sine function defined as follows:

sincx =

sin x x

if x = 0

1 if x = 0.

The logarithmic map (with the index lowering)  Logp : SD  Tp SD is given by

arccos p q

 Logp (q) =

q- p q p .

1 - (p q)2

(13) (14) (15)

Note

that

in

optimization,

we

need

the

projection

of

the

differential

~

=




L ()|=p

of

the

loss

function L to cotangent vector  given by:

 = ~ - p ~ p.

(16)

B.3 HYPERBOLIC SPACE
In this subsection, we introduces models of a hyperbolic space, which are mathematically equivalent to each other, but have practically different aspects. There are many models of a hyperbolic space. We introduce two of them: the hyperboloid model and Poincare´ disk model.

B.3.1 HYPERBOLOID MODEL

Some formulae here are also given and used in Nickel & Kiela (2018). Let GM denote diagonal matrix
-1 

GM

:=

 



1 ...

 



R(D+1)×(D+1)



(17)

1

Let ·, · M : R(D+1) × R(D+1)  R denote the Minkowski inner product defined by

 p0 

 q0 

D p1 q1

p, q M := p

GMq

= -p0q0 + pdqd,
d=1

for

p

=

 



...

 

,

q



=

 



...

 

.



pD qD

(18)

In the hyperboloid model, a (canonical) hyperbolic space is given by point set HD := p  RD+1 p, p M = -1, p0 > 0 . The tangent space TpHD on p  HD is identified with   RD+1 p,  = 0 , and the metric gp : TpHD × TpHD  R in the tangent space is
given by gp (u, v) = u, v M. Hence, the cotangent space TpHD on p  HD is identified with
  RD+1 p  = 0 , and the metric gp : Tp HD × TpHD  R in the cotangent space is given
by gp (, ) = ,  M. Note that   TpHD is identified with  = G-M1  TpHD. The
distance  (p, q) between two points p  HD and q  HD is given as follows:

 (p, q) = arcosh (- p, q M) ,

(19)

12

Under review as a conference paper at ICLR 2019

where, arcosh : [1, )  [0, ) denotes the area hyperbolic cosine function, i.e. the inverse
fucntion of the hyperbolic cosine function. The exponential map (with the index raising) Expp  : TpHD  HD is given by

Expp  () = cosh

,  M p + sinhc

,  M G-M1.

(20)

where sinhc denotes the hyperbolic sine cardinal function defined as follows:

sinhc x =

sinh x x

if x = 0

1 if x = 0.

(21)

The logarithmic map (with the index lowering)  Logp : HD  TpHD is given by

 Logp

arcosh (- (q) = GM

p, q

M) (q +

p, q

2 M

-1

p, q

M p) .

(22)

Note

that

in

optimization,

we

need

the

projection

of

the

differential

~

=




L ()|=p

of

the

loss

function L to cotangent vector  given by:

 = ~ + GM

p, G-M1~

p.
M

(23)

B.3.2 POINCARE´ DISK MODEL

In the Poincare´ disk model, the D-dimensional hyperbolic space is given by the unit open hyper-ball DD := p  RD p p < 1 . The Poincare´ disk model and the hyperboloid model are derived
from each other by the following map:

 p0 

 p1 

HD

p

=

p1



 

...

 



1 1 + p0

p2



 

...

 



DD

pD pD

 p1 

1 - µp

DD

p

=

p2



 

...

 



1 µp

  

p1 ...

 



HD ,



pD pD

(24)

where µp :=

1-p
2

p.

2

The metric is given by gp (u, v) =

2
1-p p

u v. The distance  (p, q) between two points

p  HD and q  HD is given as follows:

 (p, q) = arcosh (1 + M ) ,

(25)

where

2 (q - p) (q - p)

M := (1 - p

p) (1 - q

. q)

The exponential map (with index raising) Expp  : TpDD  DD is given by

Expp 



µp sech µ2p  - 1

µp2 tanhc µp2  

() = 1 -

p+

,







(26) (27)

where

 := µpsech µ2p  + 1 - µp + µp p  tanhc µ2p .

(28)

13

Under review as a conference paper at ICLR 2019

The logarithmic map (with index lowering)  Logp : DD  Tp DD is given by

 Logp

(q) =

arcosh(1 + M ) µp µq(q - p) - M p M M +2

dx.

(29)

These formulae can be obtained by the coordinate transformation and can be interpreted as a mod-

ification useful in

of an

eaxuitsotminagtifcordmifufelareenstiuacthioanssoynsteesmin, bGecaanuesaeesteaclh.(2x0,1t8aan).hcInaxd,daintidonar,ctohsehs(x1e+fxo)r,maunldaethaerier

derivatives do not diverge when x  0.

C DETAILS OF EXPERIMENTS
C.1 EVALUATION TASKS
We evaluated the performance of our method in the link prediction Bordes et al. (2013) and task and the triple classification task Socher et al. (2013) on real knowledge base data sets.

C.1.1 LINK PREDICTION TASK

In the link prediction task, we predict the head or the tail entity given the relation type and the other
entity. We evaluate the ranking of each correct test triple (h, r, t) in the corrupted triples. We corrupt
each triple as follows. In our setting, either its head or tail is replaced by one of the possible head
or entity, respectively. In addition, we applied "filtered" setting proposed by Bordes et al. (2013), where the correct triples, that is, the triples T in the original multi-relational graph are excluded. Thus, the corrupted triples are given by (h , r, t) h  Vh  (h , r, t) / T (head corruption) or
{(h, r, t ) | t  Vt  (h, r, t ) / T } (tail corruption). where Vrh and Vrt denote the possible heads and tails in relation r, given as follows:

Vrh := {h  V | t : (h, r, t)  T } , Vrt := {t  V | h : (h, r, t)  T } .
As evaluation metrics, we use the following:

(30)

Mean rank (MR) the mean rank of the correct test triples. The value of this metric is always equal to or greater than 1, and the lower, the better.
Hits @ n (@n) the propotion of correct triples ranked in the top n predictions (n = 1, 3, 10). The value ranges from 0 to 1, and the higher, the better.
Mean reciprocal rank (MRR) the mean of the reciprocal rank of the correct test triples. The value ranges from 0 to 1, and the higher, the better.

C.1.2 TRIPLE CLASSIFICATION TASK
In triple classification tasks, we predict whether a triple in the test data is correct or not. The classification is simply based on the score function i.e. we label a triple positive when f (ph, pt; ( r, pr)) > r, and the other way around. Here, r  R0 denotes the threshold for each relation r, which is determined by the accuracy in the validation set.

C.2 DATASETS
In link prediction tasks, we used WN18 and FB15k Bordes et al. (2013) datasets, and WN11 and FB13 datasets Socher et al. (2013). In triple classification tasks, we used WN11 and FB13 datasets, as well as FB15k. Note that WN18 and FB15k are originally used for link prediction tasks, whereas WN11 and FB13 are originally used for triple classification tasks. Also note that WN18 cannot be used for the triple classification task because WN18 does not have test negative data. Table 3 shows the number of the entities, relations, and triples in each dataset.

14

Under review as a conference paper at ICLR 2019

Table 3: Statistics of the experimental datasets

Dataset

|V| |R|

# triples train valid

test

WN18 FB15k WN11 FB13

40943 14951 38696 70543

18 1345
11 13

141442 483142 112581 316232

5000 50000 2609 5908

5000 59071 10544 23733

Manifolds in Riemannian TransE To evaluate the dependency of performance of Riemannian TransE, we compared Riemannian TransE using the following five kinds of manifolds: Euclidean space RD (Euclidean TransE), hyperbolic spaces HD (HyperbolicTransE), spheres SD (SphericalTransE), the direct product H4 × H4 × · · · × H4 of hyperbolic spaces (PHyperbolicTransE), and the direct product S4 × S4 × · · · × S4 of spheres (PSphericalTransE).
C.3 BASELINES AND IMPLEMENTATION
We compared our method with baselines. As baselines, we used RESCAL Nickel et al. (2011), TransE Bordes et al. (2013), TransH Wang et al. (2014), TransR Lin et al. (2015), TransD Ji et al. (2016), DISTMULT Yang et al. (2015), HolE Nickel et al. (2016) and ComplEx Trouillon et al. (2016). We used implementations of the baselines in OpenKE http://openke.thunlp. org/static/index.html, a Python library of knowledge base embedding based on Tensorflow Abadi et al. (2015), and moreover, we implemented some lacked constraints (for example, in TransR, TransD) and regularizers (for example, in DistMult, Analogy) in OpenKE. We also found that omitting the constraint of the entity planets onto sphere in TransE gives much better results in our setting, and this is why we also show the result without the constraint (UnconstraintTransE). We also implemented Riemannian TransEs as derivations of the base class of OpenKE.
We set the dimensionality of the entity manifold as D = 8, 16, 32, 64, 128. Although we also have to determine the dimensionality of the projected space in TransR and TransD, we let them be equal to D. Due to limitation of the computational costs, we fixed the batch size in baselines and Riemannian TransEs such that that the training data are split to 100 batches. We also fixed the number of epochs to 1000. Note that in the first 100 epochs in Riemannian TransEs, we fixed the launchers. Also note that we applied norm clipping such that the norm of a stochastic gradient in the tangent space is smaller than 1. We did not use "bern" setting introduced in Wang et al. (2014), where the ratio between head and tail corruption is not fixed to one to one; in other words, we replaced head and tail with equal probability.
Other than the dimensionality and batch sizes, we used hyperparameters such as learning rate  and margin paremeter  of baselines used in each paper. Note that some methods only reports link prediction tasks, and reports hyperparameters for WN18 and FB15k and do not reports ones for WN11 and FB13. Some methods do not mention settings of hyperparameters, and in these cases, we used the default parameters in OpenKE. In these cases, we used hyperparameters of WN18 and FB15k also for WN11 and FB13, respectively. Note that the parameters of TorusE is supposed to be used with very high dimensionality, and the hyperparameters are designed for high dimensionality settings. In Riemannian TransEs, we simply followed the hyperparameters in TransE.
We used the Xavier initializer Glorot & Bengio (2010) as an initializer. When we have to use the points on a sphere (in the original TransE and Spherical TransEs), we projected the points generated by the initialization onto the sphere. We found that choice of an initializer has significant effect on embedding performance, and the Xavier initializer achieves very good performance.
We selected optimizers in baselines following each paper. Note that while using ADADELTA (Zeiler, 2012) is also proposed in TransD, we used SGD in TransD. In Riemannian TransEs, we used we simply followed the hyperparameters in TransE. Table 4 shows the hyperparameters and optimization method for each method.
15

Under review as a conference paper at ICLR 2019

Table 4: Hyperparameters and optimizers: SGD denotes the stochastic gradient descent method (in a Euclidean space). SRGD denotes the stochastic Riemannian gradient descent method Zhang & Sra (2016) with gradient clipping. Adagrad is proposed by Duchi et al. (2011).

Method Optimizer

Learning rate 

Margin 

WN18 FB15k WN11 FB13 WN18 FB15k WN11 FB13

RiemannianTransEs TransE TransH TransR TransD TorusE
RESCAL DistMult ComplEx
HolE Analogy

SRGD SGD SGD SGD SGD SGD
Adagrad Adagrad Adagrad Adagrad Adagrad

0.01 0.01 0.01 0.01 0.01 0.0005
0.1 0.1 0.5 0.1 0.1

0.01 0.01 0.005 0.005 0.01 0.001
0.1 0.1 0.5 0.1 0.1

0.01 0.01 0.001 0.001 0.01 0.0005
0.1 0.1 0.5 0.1 0.1

0.01 0.01 0.005 0.005 0.01 0.001
0.1 0.1 0.5 0.1 0.1

2.0 2.0 1.0 4.0 1.0 2000.0 1.0 1.0 1.0 1.0 1.0

1.0 1.0 0.5 1.0 1.0 500.0 1.0 1.0 1.0 1.0 1.0

2.0 2.0 2.0 4.0 1.0 2000.0 1.0 1.0 1.0 1.0 1.0

1.0 1.0 0.25 2.0 1.0 500.0 1.0 1.0 1.0 1.0 1.0

C.4 RESULTS
Table 5 shows the results of triple classification tasks in FB15k. In FB15k, the baselines such as TransH, ComplEx and Analogy attained good accuracies and the Riemannian TransEs did not outperform the baselines. Table 6, Table 7, and Table 8 shows hit@10, mean rank, and mean reciprocal rank score of link prediction tasks, respectively. As in triple classification tasks, the sphere-based Riemannian TransEs achieved good accuracy in WN11, whereas the hyperbolic-space-based Riemannian TransEs was more accurate than other methods in FB13. The Riemannian TransEs did not outperform the baselines in WN18 and FB15k. This tendency is apparent in MR score. The distance-based methods such as TransE, TransH and Riemannian TransEs tend to attain good scores in MR and the inner-product-based methods such as DistMult, ComplEx and Analogy tend to attain good scores in MRR and hit@10.
C.5 ADDITIONAL DISCUSSION
Why do these baselines attain good results in WN18 and FB15k but bad results in WN11 and FB13? One reason may simply be that WN18 and FB15k datasets have good compatibility with zero curvature spaces i.e. Euclidean space. This is supported by the results of Euclidean TransE. A possible second reason is the redundancy of FB15k. Whereas some "easy" relations are excluded from FB15k Bordes et al. (2013), it still contain many reversible triples, as noted by Toutanova & Chen (2015). By contrast, these are removed in WN11 and FB13. Recall that projection-based methods such as TransH, TransR and TransD, and inner-product-based methods such as ComplEx and DISTMULT can exploit a linear subspace. When a dataset has apparent clusters inside which one relation is easily recovered from the others, we can allocate each cluster to a subspace and separate subspaces from one another. This separation is easily realized by setting some elements in the launchers to zero in these methods. Indeed, the TransE without the sphere constraint attains good accuracies in WN11 and FB13.
Differences between criteria are also interesting phenomena. Note that MRR and hit@10 is generous for heavy mistakes. It is possible that inner-product-based methods earn good scores in trivial relations, but further intensive investigation is needed.

16

Under review as a conference paper at ICLR 2019

Table 5: Triple classification performance. Bold: Top 1, Italic: Top 3.

Dataset

FB15K

Dim.

8 16 32 64 128

Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

75.46 76.78 68.43 74.38 79.46 74.02 78.05 56.17 78.10 69.85 56.44 77.66 77.13 78.72 68.87 76.41

76.86 81.40 68.36 79.73 83.31 78.72 81.72 56.09 82.74 75.09 60.11 81.36 82.89 85.66 73.61 83.87

77.34 85.89 70.12 84.31 87.22 81.05 84.42 56.15 85.83 77.65 63.12 84.08 88.19 89.22 78.37 88.23

77.73 89.33 68.51 88.39 90.11 80.50 85.45 56.10 87.33 78.01 66.17 83.71 89.64 90.37 83.80 89.75

77.87 91.13 70.28 90.31 91.52 76.67 84.74 56.22 87.82 75.53 71.87 81.10 89.90 89.75 86.12 90.13

17

Under review as a conference paper at ICLR 2019

Table 6: hit@10 in link prediction task. Bold: Top 1, Italic: Top 3.

WN18 (hit@10) / dim

8 16 32 64 128

Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

21.73 29.18 32.16 30.13 38.37 12.35 37.36 19.45 42.11 00.89 03.23 14.91 15.69 19.29 10.54 12.77

31.88 75.03 31.38 55.56 75.05 17.80 66.23 30.04 76.36 01.32 10.64 39.31 63.18 73.79 08.64 70.67

38.09 85.15 33.63 85.82 84.84 19.56 86.34 31.62 88.34 04.95 23.65 74.80 93.95 93.99 15.00 94.20

41.55 86.52 34.80 92.83 86.50 17.00 88.82 36.52 92.41 18.57 66.14 81.98 94.09 94.20 28.59 94.22

40.93 87.56 36.18 93.22 87.17 11.19 86.94 36.85 90.72 43.19 92.14 77.26 94.08 93.88 81.44 94.33

FB15K (hit@10) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
44.45 43.17 34.92 40.17 45.52 39.10 44.18 18.85 42.99 31.49 20.79 44.50 38.58 39.05 35.50 38.16

16
48.70 51.67 34.98 49.38 53.82 47.01 51.86 19.64 52.06 40.40 25.35 51.16 45.69 49.87 39.75 47.01

32
50.85 61.38 37.60 59.45 64.15 53.82 60.42 20.29 61.21 47.44 31.13 56.21 58.86 62.92 45.13 58.85

64
51.94 71.86 38.47 70.48 74.91 55.85 67.09 19.71 70.48 50.41 37.71 58.22 74.09 79.96 55.86 74.50

128
52.37 79.35 39.98 77.88 81.11 51.86 69.80 19.51 75.29 49.34 44.01 53.92 83.37 81.59 63.45 83.49

WN11 (hit@10) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
08.97 08.74 11.29 11.86 10.47 03.28 10.32 07.24 16.80 00.77 00.83 03.61 02.32 02.78 04.02 03.92

16
13.41 14.48 11.65 17.61 14.65 05.71 09.75 09.28 18.25 01.17 01.42 02.94 03.62 04.21 03.12 03.04

32
15.23 17.93 12.07 17.69 17.80 06.45 09.80 10.49 13.17 01.72 02.60 03.12 03.60 03.03 30.55 03.22

64
16.43 19.42 11.07 12.18 19.87 05.14 09.62 10.85 08.80 01.10 05.31 03.19 02.85 02.35 01.27 03.12

128
15.69 20.62 13.41 07.98 20.57 04.34 08.38 10.05 07.38 02.07 04.81 03.21 02.46 01.61 01.30 02.50

FB13 (hit@10) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
31.11 34.05 25.12 32.60 31.85 21.57 32.64 16.66 18.28 14.29 15.31 33.58 23.54 26.26 27.64 23.27

16
35.18 37.44 28.69 33.81 33.91 27.39 34.39 17.22 22.50 12.80 13.31 32.18 22.36 27.89 30.74 23.29

32
37.20 39.15 29.73 33.00 36.36 23.37 32.21 16.63 26.82 13.21 15.39 28.88 21.71 28.11 30.24 22.58

64
38.38 39.90 42.87 33.29 38.43 29.01 31.86 17.49 28.94 13.65 17.51 26.46 19.84 27.13 26.03 19.46

128
39.33 40.79 45.47 33.55 39.49 31.40 32.04 17.63 29.84 13.78 18.23 23.48 17.88 23.99 18.05 17.77

18

Under review as a conference paper at ICLR 2019

Table 7: MR in link prediction task. Bold: Top 1, Italic: Top 3.

WN18 (MR) / dim

8 16 32 64 128

Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

1899.9 0437.9 0653.6 0763.6 0225.4 6493.7 0258.9 2949.0 0307.0 3095.2 4785.2 0501.7 0444.9 0411.5 3755.3 0592.5

1388.5 0174.5 0578.5 0336.9 0137.5 5836.3 0200.9 2398.0 0258.4 3539.9 4450.0 0382.6 0267.2 0259.4 2374.6 0212.0

1012.3 0152.8 0527.7 0113.3 0134.0 6563.7 0225.5 2369.5 0295.2 1803.2 4093.0 0325.0 0277.4 0267.5 1116.2 0269.2

0807.3 0125.9 0536.7 0175.8 0120.5 8268.8 0227.6 2212.3 0318.2 0638.4 0878.5 0360.4 0270.2 0303.7 0900.3 0297.6

0839.4 0104.4 0517.8 0235.4 0112.7 7798.4 0257.4 2257.4 0317.1 0299.0 0233.1 0354.2 0289.1 0355.7 0615.1 0285.5

FB15K (MR) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
0150.6 0136.2 0191.9 0136.1 0098.7 0136.1 0103.1 0397.2 0101.7 0178.7 0355.6 0103.2 0127.8 0114.6 0226.0 0129.6

16
0135.1 0087.2 0204.4 0090.7 0067.0 0091.8 0070.1 0400.6 0064.4 0121.2 0304.6 0069.6 0079.8 0062.2 0174.5 0073.3

32
0126.0 0053.5 0185.6 0056.1 0041.5 0071.3 0051.7 0399.7 0044.8 0086.1 0302.7 0052.0 0043.5 0036.9 0127.0 0043.0

64
0121.9 0034.3 0189.8 0036.2 0029.8 0069.9 0045.9 0395.6 0037.7 0070.9 0302.7 0051.4 0032.7 0031.7 0076.4 0032.5

128
0120.0 0027.0 0171.7 0029.0 0024.9 0090.5 0050.1 0391.9 0038.5 0074.2 0219.6 0066.8 0033.4 0036.7 0054.8 0032.2

WN11 (MR) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
5248.9 4420.0 1856.2 2059.3 3466.7 7615.2 3538.6 5389.1 2669.1 7291.8 6883.8 5395.7 5320.1 5036.6 6681.5 4802.2

16
4974.3 3603.0 1697.9 2030.0 3220.4 7642.1 3791.9 5329.2 2985.7 6386.8 6437.3 5855.6 5369.7 4916.1 6775.8 5682.7

32
4853.2 3115.9 1689.5 2827.6 3103.8 7369.9 4349.4 5275.6 3952.4 6040.7 6501.1 5983.3 5790.2 5791.5 6342.5 5882.0

64
4771.8 2920.1 1609.3 4128.2 2772.1 7852.5 4618.2 5273.0 4718.7 5924.2 5077.1 5997.4 6079.6 6211.7 6777.8 6106.2

128
4824.7 2638.3 1614.0 5067.7 2606.1 7872.7 5145.5 5335.9 5380.7 5330.1 5321.8 6003.5 6312.5 6576.1 6462.2 6150.7

FB13 (MR) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
3970.3 2889.3 4563.6 2304.7 2072.1 5993.4 5143.0 5972.7 4460.7 4474.6 5844.6 2356.9 2202.8 2131.2 4695.4 2313.2

16
3391.0 2148.5 3997.7 1718.1 1899.7 6341.6 5508.6 6081.8 4230.5 5272.6 6643.1 2303.4 2400.3 2041.4 3866.8 2265.1

32
3304.6 1835.7 3844.3 1797.9 1792.8 6935.9 6631.5 6028.5 4386.7 3913.0 6040.2 2467.6 2332.0 2261.2 2966.3 2271.7

64
3214.8 1690.3 1869.3 2238.4 1675.4 5406.0 7918.1 5709.4 4465.0 2654.0 5893.8 2744.5 2452.5 2457.6 3332.8 2469.5

128
3215.6 1588.8 1813.7 2636.6 1607.5 5462.7 7639.0 5468.1 4633.8 2063.4 5893.2 3236.0 2777.8 2923.1 3647.3 2664.6

19

Under review as a conference paper at ICLR 2019

Table 8: MRR in link prediction task. Bold: Top 1, Italic: Top 3.

WN18 (MRR) / dim

8 16 32 64 128

Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

11.07 13.77 16.42 15.49 17.87 06.82 18.73 11.72 22.45 00.58 01.52 08.58 08.14 09.72 07.27 06.83

16.55 36.26 16.22 29.82 35.64 09.70 38.72 21.54 39.07 00.65 05.38 20.78 34.52 43.12 06.02 41.48

19.59 42.57 19.12 49.00 42.01 11.05 54.88 23.25 49.83 02.31 13.30 46.32 76.38 80.42 08.20 81.11

21.00 44.18 17.81 56.07 44.15 09.35 55.44 28.56 57.85 08.19 39.37 61.98 83.72 92.25 17.00 93.22

20.49 44.94 19.04 55.92 44.65 06.45 53.00 29.22 58.10 18.27 55.03 60.63 83.67 92.90 58.33 93.99

FB15K (MRR) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
26.60 25.81 20.05 23.55 26.65 22.48 26.53 10.66 24.68 18.27 12.00 27.84 23.30 22.93 26.14 23.29

16
29.92 31.14 21.17 29.33 32.36 27.91 31.76 11.16 30.25 23.60 14.89 32.19 27.45 29.24 28.44 28.06

32
31.79 38.23 22.82 36.31 40.24 32.56 37.28 11.40 36.62 27.59 19.07 35.06 36.07 37.77 33.31 35.81

64
32.74 47.36 24.09 46.20 49.91 33.98 42.26 11.04 44.21 29.71 23.65 36.19 47.29 51.84 41.68 47.22

128
32.94 55.43 24.83 54.63 57.24 30.79 45.18 11.43 50.79 28.73 27.36 33.14 60.70 60.47 46.73 61.53

WN11 (MRR) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
04.57 04.42 05.67 06.16 05.35 01.66 05.55 03.66 10.22 00.49 00.55 01.93 01.16 01.33 03.79 01.91

16
06.86 07.05 06.03 08.86 07.22 02.96 05.31 04.84 09.88 00.73 00.85 01.56 01.92 02.22 02.92 01.58

32
07.71 08.72 06.38 08.56 08.56 03.31 05.44 05.98 06.61 01.03 01.50 01.81 01.93 01.65 30.57 01.77

64
08.18 09.48 05.95 06.37 09.36 02.67 05.22 06.30 04.51 00.64 02.82 01.88 01.58 01.28 00.83 01.67

128
07.87 09.65 07.02 04.37 09.50 02.31 04.42 05.62 03.82 01.05 02.52 01.91 01.36 00.84 00.71 01.32

FB13 (MRR) / dim
Hyperbolic TransE PHyperbolic TransE
Spherical TransE PSpherical TransE Euclidean TransE
TransE TransE (unconstraint)
TorusE TransH TransR TransD RESCAL DistMult ComplEx
HolE Analogy

8
20.60 20.83 14.35 18.77 22.84 13.71 22.12 11.37 12.73 12.48 12.44 23.91 13.89 14.65 23.12 14.99

16
22.23 21.53 16.24 18.60 24.08 16.62 24.58 09.35 14.54 07.58 10.24 21.69 14.61 17.61 27.43 15.51

32
24.51 23.52 16.94 18.97 22.54 16.48 22.85 12.98 17.22 08.13 08.81 17.44 14.39 17.78 24.33 14.93

64
27.36 24.29 30.67 19.64 23.48 21.66 22.86 11.61 19.76 09.06 10.55 15.68 13.16 16.41 21.11 13.02

128
28.61 25.29 33.05 21.81 24.05 23.07 22.90 09.58 21.05 09.42 11.36 13.86 11.56 14.09 12.46 11.67

20

