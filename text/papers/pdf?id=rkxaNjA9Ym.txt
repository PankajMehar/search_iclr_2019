Under review as a conference paper at ICLR 2019
PER-TENSOR FIXED-POINT QUANTIZATION OF THE BACK-PROPAGATION ALGORITHM
Anonymous authors Paper under double-blind review
ABSTRACT
The high computational and parameter complexity of neural networks makes their training very slow and difficult to deploy on energy and storage-constrained computing systems. Many network complexity reduction techniques have been proposed including fixed-point implementation. However, a systematic approach for designing full fixed-point training and inference of deep neural networks remains elusive. We describe a precision assignment methodology for neural network training in which all network parameters, i.e., activations and weights in the feedforward path, gradients and weight accumulators in the feedback path, are assigned close to minimal precision. The precision assignment is derived analytically and enables tracking the convergence behavior of the full precision training, known to converge a priori. Thus, our work leads to a systematic methodology of determining suitable precision for fixed-point training. The near optimality (minimality) of the resulting precision assignment is validated empirically for four networks on the CIFAR-10, CIFAR-100, and SVHN datasets. The complexity reduction arising from our approach is compared with other fixed-point neural network designs.
1 INTRODUCTION
Though deep neural networks (DNNs) have established themselves as powerful predictive models achieving human-level accuracy on many machine learning tasks (He et al., 2016), their excellent performance has been achieved at the expense of a very high computational and parameter complexity. For instance, AlexNet (Krizhevsky et al., 2012) requires over 800 × 106 multiply-accumulates (MACs) per image and has 60 million parameters, while Deepface (Taigman et al., 2014) requires over 500 × 106 MACs/image and involves more than 120 million parameters. DNNs' enormous computational and parameter complexity leads to high energy consumption (Chen et al., 2017), makes their training via the stochastic gradient descent (SGD) algorithm very slow often requiring hours and days (Goyal et al., 2017), and inhibits their deployment on energy and resource-constrained platforms such as mobile devices and autonomous agents.
A fundamental problem contributing to the high computational and parameter complexity of DNNs is their realization using 32-b floating-point (FL) arithmetic in GPUs and CPUs. Reduced-precision representations such as quantized FL (QFL) and fixed-point (FX) have been employed in various combinations to both training and inference. Many employ FX during inference but train in FL, e.g., fully binarized neural networks (Hubara et al., 2016) use 1-b FX in the forward inference path but the network is trained in 32-b FL. Similarly, Gupta et al. (2015) employs 16-b FX for all tensors except for the internal accumulators which use 32-b FL, and 3-level QFL gradients were employed (Wen et al., 2017; Alistarh et al., 2017) to accelerate training in a distributed setting. Note that while QFL reduces storage and communication costs, it does not reduce the computational complexity as the arithmetic remains in 32-b FL.
Thus, none of the previous works address the fundamental problem of realizing true fixed-point DNN training, i.e., an SGD algorithm in which all parameters/variables and all computations are implemented in FX with minimum precision required to guarantee the network's inference/prediction accuracy and training convergence. The reasons for this gap are numerous including: 1) quantization errors propagate to the network output thereby directly affecting its accuracy (Lin et al., 2016); 2) precision requirements of different variables in a network are interdependent and involve hard-toquantify trade-offs (Sakr et al., 2017); 3) proper quantization requires the knowledge of the dynamic range which may not be available (Pascanu et al., 2013); and 4) quantization errors may accumulate during training and can lead to stability issues (Gupta et al., 2015).
Our work makes a major advance in closing this gap by proposing a systematic methodology to obtain close-to-minimum per-layer precision requirements of an FX network that guarantees statistical
1

Under review as a conference paper at ICLR 2019

Input batch

  ()

+1 +1

Step 1: Forward Propagation

Step 3: Update

 Weight  update

: quantization  to  bits

( ) ( )

()()

() ()

Accumulator update

( ) ( )

()()  

(+1) (+1)

Cost

function



(true label)

Step 2: Back Propagation

Figure 1: Problem setup: FX training at layer l of a DNN showing the quantized tensors and the associated precision configuration Cl = (BWl , BAl , BGl(W ) , BGl(+A1) , BWl(acc) ).

similarity with full precision training. In particular, we jointly address the challenges of quantization noise, inter-layer and intra-layer precision trade-offs, dynamic range, and stability. As in (Sakr et al., 2017), we do assume that a fully-trained baseline FL network exists and one can observe its learning behavior. This one assumption is not too restrictive as state-of-the-art neural networks such as AlexNet (Krizhevsky et al., 2012), VGG-Net (Simonyan and Zisserman, 2014), and ResNet (He et al., 2016) to name a few, are already trained in FL. In practice, this assumption can be accounted for via a warm-up FL training on a small held-out portion of the dataset (Dwork et al., 2015).

Applying our methodology to three benchmarks reveals several lessons. First and foremost, our work shows that it is possible to FX quantize all variables including back-propagated gradients even though their dynamic range is unknown (Köster et al., 2017). Second, we find that the per-layer weight precision requirements decrease from the input to the output while those of the activation gradients and weight accumulators increase. Furthermore, the precision requirements for residual networks are found to be uniform across layers. Finally, hyper-precision reduction techniques such as weight and activation binarization (Hubara et al., 2016) or gradient ternarization (Wen et al., 2017) are not as efficient as our methodology since these do not address the fundamental problem of realizing true fixed-point DNN training.

We demonstrate FX training on three deep learning benchmarks (CIFAR-10, CIFAR-100, SVHN) achieving high fidelity to our FL baseline in that we observe no loss of accuracy higher then 0.56% in all of our experiments. Our precision assignment is further shown to be within 1-b per-tensor of the minimum. We show that our precision assignment methodology reduces representational, computational, and communication costs of training by up to 6×, 8×, and 4×, respectively, compared to the FL baseline and related works.

2 PROBLEM SETUP, NOTATION, AND METRICS

We consider a L-layer DNN deployed on a M -class classification task using the setup in Figure 1. We denote the precision configuration as the L × 5 matrix C = (BWl , BAl , BGl(W ) , BGl(+A1) , BWl(acc) )Ll=1 whose lth row consists of the precision (in bits) of weight Wl (BWl ), activation Al (BAl ), weight gradient Gl(W ) (BG(lW) ), activation gradient Gl(+A1) (BGl(+A1) ), and internal weight accumulator Wl(acc) (BWl(acc) ) tensors at layer l. This DNN quantization setup is summarized in Appendix A.
2.1 FIXED-POINT CONSTRAINTS & DEFINITIONS

We present definitions/constraints related to fixed-point arithmetic based on the design of fixed-point adaptive filters and signal processing systems (Parhi, 2007):

· A signed fixed-point scalar a with precision BA and binary representation RA =

(a0, a1, . . . , aBA-1)  {0, 1}BA is equal to: a = rA -a0 +

BA -1 i=1

2-iai

, where rA is the

predetermined dynamic range (PDR) of a. The PDR is constrained to be a constant power of 2 to

minimize hardware overhead.

· An unsigned fixed-point scalar a with precision BA and binary representation RA =

(a0, a1, . . . , aBA-1)  {0, 1}BA is equal to: a = rA

BA -1 i=0

2-iai.

· A fixed-point scalar a is called normalized if rA = 1.

2

Under review as a conference paper at ICLR 2019

·

The precision BA is determined as: BA

= log2

rA A

+ 1, where A is the quantization step size

which is the value of the least significant bit (LSB).

· An additive model for quantization is assumed: a = a~ + qa, where a is the fixed-point number

obtained by quantizing the floating-point scalar a~, qa is a random variable uniformly distributed

on the interval

-

A 2

,

A 2

,

and

the

quantization

noise

variance

is

V

ar(qa)

=

A2 12

.

The

notion

of

quantization noise is most useful when there is limited knowledge of the distribution of a~.

·

The relative quantization bias A

is the offset:

A

=

|A -µA | µA

,

where

the

first

unbiased

quanti-

zation level µA = E a~ a~  I1

and I1 =

A 2

,

3A 2

. The notion of quantization bias is useful

when there is some knowledge of the distribution of a~.

· The reflected quantization noise variance from a tensor T to a scalar  = f (T ), for an arbitrary

function

f (),

is

:

VT 

=

ET



T2 12

,

where

T

is

the

quantization

step

of

T

and

ET 

is

the

quantization noise gain from T to .

· The clipping rate T of a tensor T is the probability: T = Pr ({|t|  rT : t  T }), where rT is

the PDR of T .

2.2 COMPLEXITY METRICS

We use a set of metrics inspired by those introduced by Sakr et al. (2017) which have also been used by Wu et al. (2018a). These metrics are algorithmic in nature which makes them easily reproducible.

· Representational Cost for weights (CW ) and activations (CA):

CW =

L l=1

|Wl|

B + B + BWl G(lW ) Wl(acc)

& CA =

L l=1

|Al

|

B + BAl Gl(+A1)

,

which equals the total number of bits needed to represent the weights, weight gradients, and

internal weight accumulators (CW ), and those for activations and activation gradients (CA). 1

· Computational Cost of training: CM =

L l=1

|Al+1|

Dl

BWl BAl + B BWl G(l+A1) + B BAl G(l+A1)

,

where Dl is the dimensionality of the dot product needed to compute one output activation at layer

l. This cost is a measure of the number of 1-b full adders (FAs) utilized for all multiplications in

one back-prop iteration. 2

· Communication Cost: CC =

L l=1

|Wl|

BGl(W

)

,

which

represents

cost

of

communicating

weight

gradients in a distributed setting (Wen et al., 2017; Alistarh et al., 2017).

3 PRECISION ASSIGNMENT METHODOLOGY AND ANALYSIS

We aim to obtain a minimal or close-to-minimal precision configuration Co of a FX network such that the mismatch probability pm = Pr{Y^fl = Y^fx} between its predicted label (Y^fx) and that of an associated FL network (Y^fl) is bounded, and the convergence behavior of the two networks is similar.
Hence, we require that: (1) all quantization noise sources in the forward path contribute identically to the mismatch budget pm (Sakr et al., 2017), (2) the gradients be properly clipped in order to limit the dynamic range (Pascanu et al., 2013), (3) accumulation of quantization noise bias in the weight updates be limited (Gupta et al., 2015), (4) the quantization noise in activation gradients be limited as these are back-propagated to calculate the weight gradients, and (5) the precision of weight accumulators should be set so as to avoid premature stoppage convergence (Goel and Shanbhag, 1998). The above insights can be formally described via the following five quantization criteria.
Criterion 1. Equalizing Feedforward Quantization Noise (EFQN) Criterion. The reflected quantization noise variances onto the mismatch probability pm from all feedforward weights ({VWlpm }lL=1) and activations ({VAlpm }Ll=1) should be equal:
VW1pm = . . . = VWLpm = VA1pm = . . . = VALpm
Criterion 2. Gradient Clipping (GC) Criterion. The clipping rates of weight ({G(lW) }lL=1) and activation ({G(l+A1) }lL=1) gradients should be less than a maximum value 0:
Gl(W ) < 0 & Gl(+A1) < 0 for l = 1 . . . L.
1We use the notation |T | to denote the number of elements in tensor T . Unquantized tensors are assumed to have a 32-b FL representation, which is the single-precision in a GPU.
2 When considering 32-b FL multiplications, we ignore the cost of exponent addition thereby favoring the FL (conventional) implementation. Boundary effects (in convolutions) are neglected.

3

Under review as a conference paper at ICLR 2019

Criterion 3. Relative Quantization Bias (RQB) Criterion. The relative quantization bias of weight gradients ({Gl(W) }Ll=1) should be less than a maximum value 0:

G(lW) < 0 for l = 1 . . . L.

Criterion 4. Back-propagated Quantization Noise (BQN) Criterion. The reflected quantization noise variance V ,G(l+A1)l i.e., the total sum of element-wise variances of G(lW ) reflected from quantizing G(l+A1) , should be less than VGl(W )l :

V  VG(l+A1) l

Gl(W )l

for l = 1 . . . L.

where l is the total sum of element-wise variances of G(lW ).
Criterion 5. Accumulator Stopping (AS) Criterion. The quantization noise of the internal accumulator should be zero, equivalently:

V = 0Wl(acc)(lacc) for l = 1 . . . L
where VWl(acc)(lacc) is the reflected quantization noise variance from Wl(acc) to (lacc), its total sum of element-wise variances.

The following claim ensures the satisfiability of the above criteria. This leads to closed form expressions for the precision requirements we are seeking and completes our methodology. The validity of the claim is proved in Appendix B.
Claim 1. Satisfiability of Quantization Criteria. The five quantization criteria (EFQN, GC, RQB, BQN, AS) are satisfied if:
· The precisions BWl and BAl are set as follows:

BWl = rnd log2

EWl pm E (min)

+ B(min) & BAl = rnd log2

EAl pm E (min)

+ B(min) (1)

for l = 1 . . . L, where rnd() denotes the rounding operation, EWlpm and EAlpm are the weight and activation quantization noise gains at layer l, respectively, B(min) is a reference minimum
precision, and E(min) = min {EWlpm }Ll=1 , {EAlpm }lL=1 . · The weight and activation gradients PDRs are lower bounded as follows:

rG(lW )  2G(m(lWax))

&

r  4Gl(+A1)

(max) Gl(+A1)

for l = 1 . . . L

(2)

where

(max)
Gl(W )

and

(max)
Gl(+A1)

are

the

largest

recorded

estimates

of

the

weight

and

activation

gradients

standard deviations Gl(W) and  ,Gl(+A1) respectively.

· The weight and activation gradients quantization step sizes are upper bounded as follows:

Gl(W )

<

(min)
G(lW )
4

&  <G(l+A1)

G(lW )

 G(lW ) 1/4



(max)
G(l+A1) Gl(W )

Gl(+A1)

for l = 1 . . . L

(3)

where

(min)
G(lW )

is

the

smallest

recorded

estimate

of

Gl(W )

and

(max)
G(l+A1) G(lW )

is

the

largest

singular

value of the square-Jacobian (Jacobian matrix with squared entries) of G(lW ) with respect to Gl(+A1).

· The accumulator PDR and step size satisfy:

r  2Wl(acc)

-BWl

&

 <  Wl(acc)

(min) Gl(W )

for l = 1 . . . L

where (min) is the smallest value of the learning rate used during training.

(4)

4

Under review as a conference paper at ICLR 2019
Practical considerations: Note that one of the 2L feedforward precisions will equal B(min). The formulas to compute the quantization noise gains are given in Appendix B and require only one forward-backward pass on an estimation set. We would like the EFQN criterion to hold upon convergence; hence, (1) is computed using the converged model from the FL baseline. For backward signals, setting the values of PDR and LSB is sufficient to determine the precision as explained in Section 2.1. As per Claim 1, estimates of the second order statistics, e.g., Gl(W) and  ,Gl(+A1) of the gradient tensors, are required. These are obtained via tensor spatial averaging, so that one estimate per tensor is required, and updated in a moving window fashion, as is done for normalization parameters in BatchNorm (Ioffe and Szegedy, 2015). Furthermore, it might seem that computing the Jacobian in (3) is a difficult task; however, the values of its elements are already computed by the back-prop algorithm, requiring no additional computations (see Appendix B). Thus, the Jacobians (at different layers) are also estimated during training. Due to the typical very large size of modern neural networks, we average the Jacobians spatially, i.e., the activations are aggregated across channels and mini-batches while weights are aggregated across filters. This is again inspired by the work on Batch Normalization (Ioffe and Szegedy, 2015) and makes the probed Jacobians much smaller.
4 NUMERICAL RESULTS
We conduct numerical simulations in order to illustrate the validity of the predicted precision configuration Co and investigate its minimality and benefits. We employ three deep learning benchmarking datasets: CIFAR-10, CIFAR-100 (Krizhevsky and Hinton, 2009), and SVHN (Netzer et al., 2011). All experiments were done using a Pascal P100 NVIDIA GPU. We train the following networks:
· CIFAR-10 ConvNet: a 9-layer convolutional neural network trained on the CIFAR-10 dataset described as 2 × (64C3) - M P 2 - 2 × (128C3) - M P 2 - 2 × (256C3) - 2 × (512F C) - 10 where C3 denotes 3 × 3 convolutions, M P 2 denotes 2 × 2 max pooling operation, and F C denotes fully connected layers.
· SVHN ConvNet: the same network as the CIFAR-10 ConvNet, but trained on the SVHN dataset. · CIFAR-10 ResNet: a wide deep residual network (Zagoruyko and Komodakis, 2016) with ResNet-
20 architecture but having 8 times as many channels per layer compared to (He et al., 2016). · CIFAR-100 ResNet: same network as CIFAR-10 ResNet save for the last layer to match the
number of classes (100) in CIFAR-100.
A step by step description of the application of our method to the above four networks is provided in Appendix C. We hope the inclusion of these steps would: (1) clarify any ambiguity the reader may have from the previous section and (2) facilitate the reproduction of our results.
4.1 PRECISION CONFIGURATION Co & CONVERGENCE
The precision configuration Co, with target pm  1%, 0  5%, and 0  1%, via our proposed method is depicted in Figure 2 for each of the four networks considered. We observe that Co is dependent on the network type. Indeed, the precisions of the two ConvNets follow similar trends as do those the two ResNets. Furthermore, the following observations are made for the ConvNets:
· weight precision BWl decreases as depth increases. This is consistent with the observation that weight perturbations in the earlier layers are the most destructive (Raghu et al., 2017).
· the precisions of activation gradients (BGl(A) ) and internal weight accumulators (BWl(acc) ) increases as depth increases which we interpret as follows: (1) the back-propagation of gradients is the dual of the forward-propagation of activations, and (2) accumulators store the most information as their precision is the highest.
· the precisions of the weight gradients (BG(lW) ) and activations (BAl ) are relatively constant across layers.
Interestingly, for ResNets, the precision is mostly uniform across the layers. Furthermore, the gap between BWl(acc) and the other precisions is not as pronounced as in the case of ConvNets. This suggests that information is spread equally among all signals which we speculate is due to the shortcut connections preventing the shattering of information (Balduzzi et al., 2017).
FX training curves in Figure 3 indicate that Co leads to convergence and consistently track FL curves with close fidelity. This validates our analysis and justifies the choice of Co.
5

Under review as a conference paper at ICLR 2019

# of bits

CIFAR-10 ConvNet
Co = (10.1, 5.1, 9.0, 7.1, 16.3) 1221111111118976543210601231094875 1 2 3 L4ayer5Dept6h 7 8 9
(a)
CIFAR-10 ResNet
Co = (13.5, 6.4, 11.6, 10.2, 13.9)
18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1
0 1 2 3 4 5 6 7 8 L9a1y0e1r1D12e1p3 t1h4 15 16 17 18 19 20 21 22
(c)

#of bits

# of bits

SVHN ConvNet
Co = (8.3, 4.6, 9.2, 8.4, 17.8) 21121212111111345678901245678390012231 1 2 3 L4ayer5Dept6h 7 8 9
(b)
CIFAR-100 ResNet
Co = (13.7, 6.1, 12.3, 10.3, 14.1)
18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1
0 1 2 3 4 5 6 7 8 L9a1y0e1r1D12e1p3 t1h4 15 16 17 18 19 20 21 22
(d)

# of bits

Figure 2: The predicted precision configurations Co for the CIFAR-10 ConvNet (a), SVHN ConvNet (b),

CIFAR-10 ResNet (c), and CIFAR-100 ResNet (d). For each network, the 5-tuple C^o represents the average

number of bits per tensor type. For the ResNets, layer depths 21 and 22 correspond to the strided convolutions in

the shortcut connections of residual blocks 4 and 7, respectively. Activation gradients go from layer 2 to L + 1

and are "shifted to the left" in order to be aligned with the other tensors.

CIFAR-10 ConvNet

SVHN ConvNet

4 × 10 2

FL FL

C C100 o 2 × 10 1

o

C C+1 +1

C C1 10 1

1 3 × 10 2

Test Error (solid)

Test Error (solid) Cross-Entropy Loss (dotted)

Cross-Entropy Loss (dotted)

10 1 0
100 10 1

10 1
25 50 75 Ep1o00ch 125 150 175 200
(a) CIFAR-10 ResNet
FL CCC+o 11 10 1

0
100 10 1

20 40 Epoch 60 80 100 2 × 10 2
(b) CIFAR-100 ResNet
4 × 10 1
FL CCC+o 11
3 × 10 1

Test Error (solid)

Test Error (solid) Cross-Entropy Loss (dotted)

Cross-Entropy Loss (dotted)

10 2

6 × 10 2

10 2

0 25 50 75 Ep1o00ch 125 150 175 200
(c)

0 25 50 75 Ep1o00ch 125 150 175 200
(d)

Figure 3: Convergence curves for the CIFAR-10 ConvNet (a), SVHN ConvNet (b), CIFAR-10 ResNet (c), and CIFAR-100 ResNet (d) including FL training as well as FX training with precision configurations Co, C1, and C-1.

4.2 NEAR MINIMALITY OF Co
To determine that Co is a close-to-minimal precision assignment, we compare it with: (a) C+1 = Co + 1L×5, and (b) C-1 = Co - 1L×5, where 1L×5 is an L × 5 matrix with each entry equal to 13, i.e., we perturb Co by 1-b in either direction. Figure 3 also contains the convergence curves for the two new configurations. As shown, C-1 always results in a noticeable gap compared to Co for both

3PDRs are unchanged across configurations, except for rWl(acc) as per (4).

6

Under review as a conference paper at ICLR 2019

Table 1: Complexity (CW , CA, CM , and CC ) and accuracy (test error) for the floating-point (FL), fixed-point (FX) with precision configuration Co, binarized network (BN), stochastic quantization (SQ), and TernGrad (TG) training schemes.

CW CA CM CC Test (106b) (106b) (109FA) (106b) Error

FL FX (Co)
BN SQ TG

148 56.5 100 78.8 102

CIFAR-10 ConvNet 9.3 94.4 49 1.7 11.9 14 4.7 2.8 49 1.7 11.9 14 9.3 94.4 3.1

12.02% 12.58% 18.50% 11.32% 12.49%

FL FX (Co)
BN SQ TG

1784 726 1208 1062 1227

CIFAR-10 ResNet 96 4319 596 25 785 216 50 128 596 25 785 216 96 4319 37.3

7.42% 7.51% 7.24% 7.42% 7.94%

CW CA CM CC Test (106b) (106b) (109FA) (106b) Error

SVHN ConvNet 148 9.3 94.4 49 2.43% 54.3 1.9 10.5 14 2.58% 100 4.7 2.8 49 3.60% 76.3 1.9 10.5 14 2.73% 102 9.3 94.4 3.1 3.65%

1789 750 1211 1081 1230

CIFAR-100 ResNet 97 4319 597 25 776 216 50 128 597 25 776 216 97 4319 37.3

28.06% 27.43% 29.35% 28.03% 30.62%

the loss function (except for the CIFAR-10 ResNet) and the test error. Furthermore, C+1 offers no observable improvements over Co (except for the test error of CIFAR-10 ConvNet). These results support our contention that Co is close-to-minimal in that increasing the precision above Co leads to diminishing returns while reducing precision below Co leads to a noticeable degradation in accuracy.
4.3 COMPLEXITY VS. ACCURACY

We would like to quantify the reduction in training cost and expense in terms of accuracy resulting from our proposed method and compare them with those of other methods. Importantly, for a fair comparison, the same network architecture and training procedure are used. We report CW , CA, CM , CC, and test error, for each of the four networks considered for the following training methods:
· baseline FL training and FX training using Co, · binarized network (BN) training, where feedforward weights and activations are binary (constrained
to ±1) while gradients and accumulators are in floating-point and activation gradients are backpropagated via the straight through estimator (Bengio et al., 2013) as was done in (Hubara et al., 2016), · fixed-point training with stochastic quantization (SQ). As was done in (Gupta et al., 2015), we quantize feedforward weights and activations as well as all gradients, but accumulators are kept in floating-point. The precision configuration (excluding accumulators) is inherited from Co (hence we determine exactly how much stochastic quantization helps), · training with ternarized gradients (TG) as was done in TernGrad (Wen et al., 2017). All computations are done in floating-point but weight gradients are ternarized according to the instantaneous tensor spatial standard deviations {-2.5, 0, 2.5} as was suggested by Wen et al. (2017). To compute costs, we assume all weight gradients use two bits although they are not really fixed-point and do require computation of 32-b floating-point scalars for every tensor.

The comparison is presented in Table 1. The first observation is a massive complexity reduction compared to FL. For instance, for the CIFAR-10 ConvNet, the complexity reduction is 2.6× (148/56.5), 5.5× (9.3/1.7), 7.9× (94.4/11.9), and 3.5× (49/14) for CW , CA, CM , and CC , respectively. Similar trends are observed for the other four networks. Such complexity reduction comes at the expense of no more than 0.56% increase in test error. For the CIFAR-100 network, the accuracy when training in fixed-point is even better than that of the baseline.
The representational and communication costs of BN is significantly greater than that of FX because the gradients and accumulators are kept in full precision, which masks the benefits of binarizing feedforward tensors. However, benefits are noticeable when considering the computational cost which is lowest as binarization eliminates multiplications. Furthermore, binarization causes a severe accuracy drop for the ConvNets but surprisingly not for the ResNets. We speculate that this is due to the high dimensional geometry of ResNets (Anderson and Berg, 2017).
As for SQ, since Co was inherited, all costs are identical to FX, save for CW which is larger due to full precision accumulators. Furthermore, SQ has a positive effect only on the CIFAR-10 ConvNet where it clearly acted as a regularizer.

7

Under review as a conference paper at ICLR 2019
TG does not provide complexity reductions in terms of representational and computational costs which is expected as it only compresses weight gradients. Additionally, the resulting accuracy is slightly worse than that of all other considered schemes, including FX. Naturally, it has the lowest communication cost as weight gradients are quantized to just 2-b.
5 DISCUSSION
5.1 RELATED WORKS
Many works have addressed the general problem of reduced precision/complexity deep learning.
Reducing the complexity of inference (forward path): several research efforts have addressed the problem of realizing a DNN's inference path in FX. For instance, the works in (Lin et al., 2016; Sakr et al., 2017) address the problem of precision assignment. While Lin et al. (2016) proposed a non-uniform precision assignment using the signal-to-quantization-noise ratio (SQNR) metric, Sakr et al. (2017) analytically quantified the trade-off between activation and weight precisions while providing minimal precision requirements of the inference path computations that bounds the probability pm of a mismatch between predicted labels of the FX and its FL counterpart. An orthogonal approach which can be applied on top of quantization is pruning (Han et al., 2015). While significant inference efficiency can be achieved, this approach incurs a substantial training overhead. A subset of the FX training problem was addressed in binary weighted neural networks (Courbariaux et al., 2015; Rastegari et al., 2016) and fully binarized neural networks (Hubara et al., 2016), where direct training of neural networks with pre-determined precisions in the inference path was explored with the feedback path computations being done in 32-b FL.
Reducing the complexity of training (backward path): finite-precision training was explored in (Gupta et al., 2015) which employed stochastic quantization in order to counter quantization bias accumulation in the weight updates. This was done by quantizing all tensors to 16-b FX, except for the internal accumulators which were stored in a 32-b floating-point format. An important distinction our work makes is the circumvention of the overhead of implementing stochastic quantization (Hubara et al., 2016). Similarly, DoReFa-Net (Zhou et al., 2016) stores internal weight representations in 32-b FL, but quantizes the remaining tensors more aggressively. Thus arises the need to re-scale and re-compute in floating-point format, which our work avoids. Finally, Köster et al. (2017) suggests a new number format ­ Flexpoint ­ and were able to train neural networks using slightly 16-b per tensor element, with 5 shared exponent bits and a per-tensor dynamic range tracking algorithm. Such tracking causes a hardware overhead bypassed by our work since the arithmetic is purely FX. Augmenting Flexpoint with stochastic quantization effectively results in WAGE (Wu et al., 2018b), and enables integer quantization of each tensor.
As seen above, none of the prior works address the problem of predicting precision requirements of all training signals. Furthermore, the choice of precision is made in an ad-hoc manner. In contrast, we propose a systematic methodology to determine close-to-minimal precision requirements for FX-only training of deep neural networks.
5.2 CONCLUSION
In this paper, we have presented a study of precision requirements in a typical back-propagation based training procedure of neural networks. Using a set of quantization criteria, we have presented a precision assignment methodology for which FX training is made statistically similar to the FL baseline, known to converge a priori. We realized FX training of four networks on the CIFAR-10, CIFAR-100, and SVHN datasets and quantified the associated complexity reduction gains in terms costs of training. We also showed that our precision assignment is nearly minimal.
The presented work relies on the statistics of all tensors being quantized during training. This necessitates an initial baseline run in floating-point which can be costly. An open problem is to predict a suitable precision configuration by only observing the data statistics and the network architecture. We speculate that a similar study to that of the effects of initialization (Glorot and Bengio, 2010) on convergence could be one way to address this challenge. An even greater extension would be to completely suppress data and network dependence so that universal, yet nearly minimal precision configurations can be determined. Such extension would be of immense value to system designers.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. (2017). Qsgd: Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1707­1718.
Anderson, A. G. and Berg, C. P. (2017). The high-dimensional geometry of binary neural networks. arXiv preprint arXiv:1705.07199.
Balduzzi, D., Frean, M., Leary, L., Lewis, J. P., Ma, K. W.-D., and McWilliams, B. (2017). The shattered gradients problem: If resnets are the answer, then what is the question? In Proceedings of the 34th International Conference on Machine Learning, pages 342­350.
Bengio, Y., Léonard, N., and Courville, A. (2013). Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432.
Chen, Y.-H., Krishna, T., Emer, J. S., and Sze, V. (2017). Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. IEEE Journal of Solid-State Circuits, 52(1):127­ 138.
Courbariaux, M., Bengio, Y., and David, J.-P. (2015). Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems, pages 3123­3131.
Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., and Roth, A. (2015). The reusable holdout: Preserving validity in adaptive data analysis. Science, 349(6248):636­638.
Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 249­256.
Goel, M. and Shanbhag, N. (1998). Finite-precision analysis of the pipelined strength-reduced adaptive filter. Signal Processing, IEEE Transactions on, 46(6):1763­1769.
Goodfellow, I. J. et al. (2013). Maxout networks. ICML (3), 28:1319­1327.
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. (2017). Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677.
Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. (2015). Deep learning with limited numerical precision. In Proceedings of The 32nd International Conference on Machine Learning, pages 1737­1746.
Han, S., Mao, H., and Dally, W. J. (2015). Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149.
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770­778.
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. (2016). Binarized neural networks. In Advances in Neural Information Processing Systems, pages 4107­4115.
Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448­ 456.
Köster, U., Webb, T., Wang, X., Nassar, M., Bansal, A. K., Constable, W., Elibol, O., Hall, S., Hornof, L., Khosrowshahi, A., et al. (2017). Flexpoint: An adaptive numerical format for efficient training of deep neural networks. In Advances in Neural Information Processing Systems, pages 1740­1750.
Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097­1105.
9

Under review as a conference paper at ICLR 2019
Lin, D., Talathi, S., and Annapureddy, S. (2016). Fixed point quantization of deep convolutional networks. In Proceedings of The 33rd International Conference on Machine Learning, pages 2849­2858.
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5.
Parhi, K. (2007). VLSI Digital Signal Processing Systems: Design and Implementation. John Wiley & Sons.
Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pages 1310­1318.
Raghu, M. et al. (2017). On the expressive power of deep neural networks. In Proceedings of the 34th International Conference on Machine Learning, pages 2847­2854.
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. (2016). XNOR-Net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pages 525­542. Springer.
Sakr, C., Kim, Y., and Shanbhag, N. (2017). Analytical guarantees on numerical precision of deep neural networks. In Proceedings of the 34th International Conference on Machine Learning, pages 3007­3016.
Simonyan, K. and Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.
Srivastava, N. et al. (2014). Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929­1958.
Taigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014). Deepface: Closing the gap to human-level performance in face verification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1701­1708.
Wen, W., Xu, C., Yan, F., Wu, C., Wang, Y., Chen, Y., and Li, H. (2017). Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pages 1508­1518.
Wu, J., Wang, Y., Wu, Z., Wang, Z., Veeraraghavan, A., and Lin, Y. (2018a). Deep k-means: Retraining and parameter sharing with harder cluster assignments for compressing deep convolutions. In International Conference on Machine Learning, pages 5359­5368.
Wu, S., Li, G., Chen, F., and Shi, L. (2018b). Training and inference with integers in deep neural networks. arXiv preprint arXiv:1802.04680.
Zagoruyko, S. and Komodakis, N. (2016). Wide residual networks. arXiv preprint arXiv:1605.07146. Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y. (2016). DoReFa-Net: Training low bitwidth
convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160.
10

Under review as a conference paper at ICLR 2019

Supplementary Material

A SUMMARY OF QUANTIZATION SETUP

The quantization setup depicted in Figure 1 is summarized as follows:
· Feedforward computation at layer l:
Al+1 = fl(Al, Wl)
where fl() is the function implemented at layer l, Al (Al+1) is the activation tensor at layer l (l + 1) quantized to a normalized unsigned fixed-point format with precision BAl (BAl+1 ), and Wl is the weight tensor at layer l quantized to a normalized signed fixed-point format with precision BWl . We further assume the use of a ReLU-like activation function with a clipping level of 2 and a max-norm constraint on the weights which are clipped between [-1, 1] at every iteration. · Back-propagation of activation gradients at layer l:
G(lA) = gl(A)(Wl, Gl(+A1) )
where gl()(A) is the function that back-propagates the activation gradients at layer l, Gl(A) (Gl(+A1)) is the activation gradient tensor at layer l (l + 1) quantized to a signed fixed-point format with precision BGl(A) (BGl(+A1) ). · Back-propagation of weight gradient tensor G(lW ) at layer l:
G(lW ) = gl(W )(Al, Gl(+A1) )
where gl(W )() is the function that back-propagates the weight gradients at layer l, and G(lW ) is quantized to a signed fixed-point format with precision BG(lW) . · Internal weight accumulator update at layer l:
Wl(acc) = U (Wl(acc), Gl(W ), )
where U () is the update function,  is the learning rate, and Wl(acc) is the internal weight accumulator tensor at layer l quantized to signed fixed-point with precision B .Wl(acc) Note that, for the next iteration, Wl is directly obtained from Wl(acc) via quantization to BWl bits.
B PROOF OF CLAIM 1

The validity of Claim 1 is derived from the following five lemmas. Note that each lemma addresses the satisfiability of one of the five quantization criteria presented in the main text and corresponds to part of Claim 1.
Lemma 1. The EFQN criterion holds if the precisions BWl and BAl are set as follows:

BWl = rnd log2

EWl pm E (min)

+ B(min) & BAl = rnd log2

EAl pm E (min)

+ B(min)

for l = 1 . . . L, where rnd() denotes the rounding operation, B(min) is a reference minimum precision, and E(min) is given by:

E(min) = min {EWlpm }Ll=1 , {EAlpm }Ll=1 .

(5)

Proof. By definition of the reflected quantization noise variance, the EFQN, by definition, is satisfied if:

2W1 12

EW1 pm

=

...

=

2WL 12

EWL pm

=

A2 1 12

EA1 pm

=

...

=

A2 L 12

EAL

pm

,

11

Under review as a conference paper at ICLR 2019

where the quantization noise gains are given by:

 (Zi-ZY^fl ) 2

 (Zi-ZY^fl ) 2

M w



EWl pm

=

E 

 i=1

wWl
2|Zi - ZY^fl |2

M a

   

&



EAl pm

=

E 

 i=1

aAl
2|Zi - ZY^fl |2

   

i=Y^f l

i=Y^f l

(6)

for l = 1 . . . L, where {Zi}iM=1 are the soft outputs and ZY^fl is the soft output corresponding to Y^fl. The expressions for these quantization gains are obtained by linearly expanding (across layers) those used in (Sakr et al., 2017). Note that a second order upper bound is used as a surrogate expression for pm.
From the definition of quantization step size, the above is equivalent to:

2-2BW1 EW1pm = . . . = 2-2BWL EWLpm = 2-2BA1 EA1pm = . . . = 2-2BAL EALpm . Let E(min) be as defined in (5):

E(min) = min {EWlpm }lL=1 , {EAlpm }lL=1 .

We can divide each term by E(min):

2-2BW1 EW1pm = . . . = 2-2BWL EWLpm = 2-2BA1 EA1pm = . . . = 2-2BAL EALpm

E (min)

E (min)

E (min)

E (min)

where each term is positive, so that we can take square roots and logarithms such that:

BW1 - log2

EW1 pm E (min)

= . . . = BWL - log2

EWL pm E (min)

=BA1 - log2

EA1 pm E (min)

= . . . = BAL - log2

EAL pm E (min)

Thus we equate all of the above to a reference precision B(min) yielding:

BWl = log2

EWl pm E (min)

+ B(min) & BAl = log2

EAl pm E (min)

+ B(min)

for l = 1 . . . L. Note that because E(min) is the least quantization noise gain, it is equal to one of the above quantization noise gains so that the corresponding precision actually equates B(min). As precisions must be integer valued, each of B(min), {BWl }Ll=1, and {BAl }Ll=1 have to be integers, and thus a rounding operation is to be applied on all logarithm terms. Doing so results in (1) from Lemma
1 which completes this proof.

Lemma 2. The GC criterion holds for 0 = 5% provided the weight and activation gradients pre-defined dynamic ranges (PDRs) are lower bounded as follows:

rGl(W )  2G(m(lWax))

&

r  4Gl(+A1)

(max) G(l+A1)

for l = 1 . . . L

where

(max)
G(lW )

and

(max)
G(l+A1)

are

the

largest

ever

recorded

estimates

of

the

weight

and

activation

gradients standard deviations Gl(W) and  ,Gl(+A1) respectively.

Proof. Let us consider the case of weight gradients. The GC criterion, by definition requires:
Gl(W ) = Pr |g|  rG(lW ) : g  Gl(W ) < 0.05 Typically, weight gradients are obtained by computing the derivatives of a loss function with respect to a mini-batch. By linearity of derivatives, weight gradients are themselves averages of instantaneous

12

Under review as a conference paper at ICLR 2019

derivatives and are hence expected to follow a Gaussian distribution by application of the Central Limit Theorem. Furthermore, the gradient mean was estimated during baseline training and was found to oscillate around zero.

Thus

Gl(W ) = 2Q

rGl(W ) Gl(W )

where we used the fact that a Gaussian distribution is symmetric and Q() is the elementary Q-function,

which is a decreasing function. Thus, in the worst case, we have:



Gl(W )



2Q



rG(lW ) (max)
G(lW )



.

Hence, for a PDR as suggested by the lower bound in (2):

rG(lW )  2G(m(lWax)) in Lemma 2, we obtain the upper bound:

G(lW)  2Q(2) = 0.044 < 0.05 which means the GC criterion holds and completes the proof.

For activation gradients, the same reasoning applies, but the choice of a larger PDR in (2):

r  4G(l+A1)

(max) G(l+A1)

than for weight gradients is due to the fact that the true dynamic range of the activation gradients is

larger than the value indicated by the second moment. This stems from the use of activation functions

such as ReLU which make the activation gradients sparse. We also recommend increasing the PDR

even more when using regularizers that sparsify gradients such as Dropout (Srivastava et al., 2014) or

Maxout (Goodfellow et al., 2013).

Lemma 3. The RQB criterion holds for 0 = 1% provided the weight gradient quantization step size is upper bounded as follows:

G(lW )

<

(min)
G(lW )
4

for l = 1 . . . L

where

(min)
G(lW )

is

the

smallest

ever

recorded

estimate

of

Gl(W ) .

Proof. For the Gaussian distributed (see proof of Lemma 2) weight gradient at layer l, the true mean conditioned on the first non-zero quantization region is given by:

3Gl(W )

µ = Q -xQexp - 2dxGl(W)

2
Gl(W ) 2
G(lW ) 2G(lW )

x2 2G2 (lW )
3Gl(W ) 2G(lW )

2 G(lW )

=  Qexp - - QGl(W )

G2 l(W ) 8G2 (lW )
G(lW ) 2Gl(W )

- exp

- 92Gl(W )
8G2 (lW )

3G(lW ) 2G(lW )

 2

,

where G(lW )

is the standard deviation of G(lW ).

By substituting G(lW)

=

Gl(W ) 4

into the above

expression of µGl(W) and plugging in the definition of relative quantization bias, we obtain:

Gl(W ) =

 - µGl(W )

G(lW )

µG(lW )

= 0.4% < 1%.

13

Under review as a conference paper at ICLR 2019

Hence, this choice of the quantization step satisfies the RQB. In order to ensure the RQB holds

throughout

training,

(min)
Gl(W )

is

used

in

Lemma

3.

This

completes

the

proof.

Lemma 4. The BQN criterion holds provided the activation gradient quantization step size is upper bounded as follows:

 <Gl(+A1)

Gl(W )

 Gl(W ) 1/4



(max)
Gl(+A1) G(lW )

G(l+A1)

for l = 1 . . . L

where

(max)
Gl(+A1) G(lW )

,

the

largest

singular

value

of

the

square-Jacobian

(Jacobian

matrix

with

squared

entries) of Gl(W ) with respect to G(l+A1) .

Proof. Let us unroll Gl(W ) and G(l+A1) to vectors of size Gl(W ) and Gl(+A1) , respectively. The element-

wise

quantization

noise

variance

of

each

weight

gradient

is

2Gl(W ) 12

.

Therefore

we

have:

VG(W )l =

Gl(W )

2
Gl(W

)

.

12

The reflected quantization noise variance from an activation gradient ga  Gl(+A1) onto a weight

gradient gw  G(lW ) is

gw

 ,2

2 Gl(+A1)

ga 12

where cross products of quantization noise are neglected (Sakr et al., 2017). Hence, the reflected quantization noise variance element-wise from Gl(+A1) onto G(lW ) is given by:

2
12 J 1 ,Gl(+A1)

G(l+A1) Gl(W )

Gl(+A1)

where JGl(+A1)G(lW) is the square-Jacobian of Gl(W ) with respect to G(l+A1) and 1 denotes the all one vector with size denoted by its subscript. Hence, we have:

V = 12G(l+A1) l

2 G(l+A1)

J 1Gl(+A1) G(lW ) G(l+A1)

T
1 G(lW )



2
Gl(A)

12

J 1Gl(+A1) Gl(W ) Gl(+A1)

1 Gl(W )

(W )  G 12 J 1l

2 G(l+A1)

G(l+A1) G(lW )

Gl(+A1)

 (max)
Gl(+A1) Gl(W )

Gl(+A1)

Gl(W )

2
 ,G(l+A1) 12

where we used the Cauchy-Schwarz inequality and the spectral norm of a matrix. Next we set this upper bound on VG(l+A1)l to be less than the value of VG(lW)l determined above. This condition, by definition, is enough to satisfy the BQN criterion. Rearranging terms yields Lemma 4 which
completes the proof.

In the main text, it was menitoned that each entry in the Jacobian matrix above is already computed by

the back-propagation algorithm. We now explain how. Let us denote the instantaneous loss function

2

being minimized by . Note that each entry of JG(l+A1)Gl(W) is of the form

gw  ga(0)

where

gw

=

 w

14

Under review as a conference paper at ICLR 2019

with

w



Wl

and

ga(0)

=

  a(0)

with

a(0)



Al+1.

The back-propagation algorithm

computes

gw

using the chain rule as follows:

  a(i)

gw = w =

a(i) w .

a(i) Al+1

In

particular,

note

that

ga(0)

appears

only

once

in

the

summation

above

and

is

multiplied

by

.a(0)
w

Thus

gw  ga(0)

=

.a(0)
w

This

establishes

that

each

entry

of

the

Jacobian

matrix

is

already

computed

via

the back-propagation algorithm.

Lemma 5. The AS criterion holds provided the accumulator PDR and quantization step size satisfy:

r  2Wl(acc)

-BWl

&

 <  Wl(acc)

(min) Gl(W )

for l = 1 . . . L

where (min) is the smallest value of the learning rate used during training.

Proof. The lower bound on the PDR of the weight accumulator, given by

r  2Wl(acc)

-BWl

for l = 1 . . . L, ensures that updates are able to cross over the feedforward weight quantization

threshold so that it can be updated. Additionally, the lower bound on the quantization step size, given

by

 <  Wl(acc)

(min) Gl(W )

for l = 1 . . . L, simply ensures that the internal weight accumulator overlaps with the least significant

part of the representation of the weight gradient multiplied by the learning rate. Thus, the quantization

noise of the internal accumulator is zero, or equivalently,

V = 0Wl(acc)l(acc) for l = 1 . . . L
which, by definition, is enough for the AS criterion to hold. Note that this criterion applies to the Vanilla-SGD learning rule (which was used in our experiments). Future work includes extending this criterion to other learning rules such as momentum and ADAM.

C ILLUSTRATION OF METHODOLOGY USAGE

We illustrate a step by step description of the application of our precision assignment methodology to the four networks we report results on.
C.1 CIFAR-10 CONVNET
Feedforward Precisions: The first step in our methodology consists of setting the feedforward precisions BWl and BAl . As per Claim 1, this requires using (1). To do so, it is first needed to compute the quantization noise gains using (6). Using the converged weights from the baseline run we obtain:

Layer Index l
EWl pm EAl pm
Layer Index l
EWl pm EAl pm

1 1.52E+06 5.51E+04
6 5.61E+05 7.49E+02

2 1.24E+06 3.27E+02
7 5.97E+04 6.32E+02

3 4.21E+06 5.15E+02
8 3.23E+04 2.37E+02

4 3.57E+06 6.60E+02
9 8.66E+03 9.47E+01

5 2.35E+06 7.78E+02

And therefore, E(min) = 94.7 and the feedforward precisions should be set according to (1) as follows:

Layer Index l
BWl BAl Layer Index l
BWl BAl

1 7+B(min) 5+B(min)
6 6+B(min) 1+B(min)

2 7+B(min) 1+B(min)
7 5+B(min) 1+B(min)

3 8+B(min) 1+B(min)
8 4+B(min) 1+B(min)

4 8+B(min) 1+B(min)
9 3+B(min) 0+B(min)

5 7+B(min) 2+B(min)

15

Under review as a conference paper at ICLR 2019

The value of B(min) is swept and pm i evaluated on the validation set. It is found that the smallest value of B(min) resulting in pm < 1% is equal to 4 bits. Hence the feedforward precisions are set as
follows and as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9 BWl 11 11 12 12 11 10 9 8 7 BAl 9 5 5 5 6 5 5 5 4
Gradient Precisions: The second step of the methodology is to determine the precisions of weight BGl(W) and activation BGl(+A1) gradients. As per Claim 1, an important statistic is the spatial variance of the gradient tensors. We estimate these variances via moving window averages, where at each iteration, the running variance estimate ^2 is updated using the instantaneous variance ~2 as follows:
^2  (1 - )^2 + ~2
where  is the running average factor, chosen to be 0.1. The running variance estimate of each gradient tensor is dumped every epoch. Using the maximum recorded estimate and (2) we compute the PDRs of the gradient tensors (as a reminder, the PDR is forced to be a power of 2):

Layer Index l
rGl(W ) rG(l+A1)
Layer Index l
rGl(W ) rG(l+A1)

1 5.00E-01 4.88E-04
6 3.13E-02 1.56E-02

2 1.25E-01 9.77E-04
7 3.13E-02 7.81E-03

3 1.25E-01 9.77E-04
8 1.56E-02 7.81E-03

4 1.25E-01 1.95E-03
9 1.25E-01 3.13E-02

5 6.25E-02
7.81E-03

Furthermore, using the minimum recorded estimates of the weight gradient spatial variances and (3) we compute the values of the quantization step sizes of the weight tensors:

Layer Index l Gl(W )
Layer Index l G(lW )

1 3.91E-03
6 2.44E-04

2 1.95E-03
7 2.44E-04

3 9.77E-04
8 1.22E-04

4 9.77E-04
9 4.88E-04

5 4.88E-04

Hence the weight gradients precisions BGl(W) are set as follows and as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9

BG(lW )

9 9 9 9 9 9 9 9 10

In order to compute the activation gradients precisions, (3) dictates that we need the values of largest singular values of the of the square-Jacobians of G(lW ) with respect to G(l+A1) for l = 1 . . . L. The square Jacobians matrices are estimated in a moving window fashion as for the variances above. However, instead of updating a matrix every iteration, the updates are done every first batch of every epoch. The following are the maximum recorded singluar values:

Layer Index l (max)
G(l+A1) Gl(W )
Layer Index l (max)
G(l+A1) G(lW )

1 1.44E+02
6 9.08E+00

2 2.37E+02
7 1.37E+01

3 4.28E+02
8 1.26E+01

4 2.03E+02
9 3.51E+00

5 4.20E+01

Using the above values and (3) we obtain the values of the quantization step sizes for the activation gradients:

Layer Index l G(l+A1)
Layer Index l G(l+A1)

1 6.10E-05
6 1.53E-05

2 3.05E-05
7 1.53E-05

3 7.63E-06
8 7.63E-06

4 1.53E-05
9 6.10E-05

5 1.53E-05

16

Under review as a conference paper at ICLR 2019

Hence the activation gradients precisions BGl(+A1) are set as follows and as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9

BGl(+A1)

5 8 9 9 11 12 11 11 11

Internal Weight Accumulators Precisions: By application of (4), we use the above results to obtain the internal weight accumulator precisions. The only additional information needed is the value of the smallest learning rate value used in the training, which in our case is 0.0001. We obtain the following precisions which are illustrated in Figure 2

Layer Index l 1 2 3 4 5 6 7 8 9

BWl(Acc)

13 15 14 14 16 18 19 21 20

C.2 SVHN CONVNET
Feedforward Precisions: The quantization noise gains are used to obtain values for the precisions as a function of B(min) as summarized below:

Layer Index l EWl pm BWl EAl pm BAl
Layer Index l EWl pm BWl EAl pm BAl

1 3.07E+03 6+B(min) 7.58E+02 5+B(min)
6 1.25E+03 6+B(min) 8.18E+00 2+B(min)

2 4.50E+02 5+B(min) 2.86E+00 1+B(min)
7 7.91E+01 4+B(min) 1.78E+01 3+B(min)

3 1.54E+03 6+B(min) 7.09E+00 2+B(min)
8 1.20E+01 2+B(min) 1.14E+00 1+B(min)

4 1.79E+03 6+B(min) 2.55E+00 1+B(min)
9 9.13E+00 2+B(min) 3.90E-01 0+B(min)

5 6.01E+03 7+B(min) 8.33E+00 2+B(min)

The value of B(min) is again swept, and it is found that the pm < 1% for B(min) = 3. The feedforward precisions are therefore set as follows and as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9
BWl 9 8 9 9 10 9 7 5 5 BAl 8 4 5 4 6 6 7 4 3

Gradient Precisions: The spatial variance of the gradient tensors is used to determine the PDRs and the quantization step sizes of weight gradients. The singular values of the square-Jacobians are needed to determine the quantization step sizes of activation gradients. They were computed as follows:

Layer Index l
rG(lW ) G(lW ) rG(l+A1) (max)
G(l+A1) Gl(W )
G(l+A1)
Layer Index l
rG(lW ) Gl(W ) rG(l+A1) (max)
G(l+A1) G(lW )
G(l+A1)

1 6.25E-02 2.44E-04 4.88E-04
5.13E+00
3.05E-05
6 7.81E-03 3.05E-05 1.56E-02
2.20E+01
1.91E-06

2 1.56E-02 6.10E-05 4.88E-04
1.48E+02
9.54E-07
7 3.91E-03 1.53E-05 3.91E-03
9.58E+00
9.54E-07

3 1.56E-02 6.10E-05 9.77E-04
3.25E+02
9.54E-07
8 3.91E-03 7.63E-06 3.91E-03
1.78E+00
9.54E-07

4 1.56E-02 6.10E-05 1.95E-03
1.37E+02
9.54E-07
9 1.56E-02 6.10E-05 3.13E-02
1.71E+00
7.63E-06

5 1.56E-02 6.10E-05 3.91E-03
8.84E+01
1.91E-06

Hence the gradients precisions are set as follows and as illustrated in Figure 2:

17

Under review as a conference paper at ICLR 2019

Layer Index l 1 2 3 4 5 6 7 8 9

BGl(W ) BGl(+A1)

9 9 9 9 9 9 9 10 9 5 10 11 12 12 14 13 13 13

Internal Weight Accumulators Precisions: The smallest learning rate value for this network is 0.001 which results in the following precisions for the internal weight accumulators as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9

BWl(Acc)

14 17 16 16 15 17 20 23 20

C.3 CIFAR-10 RESNET

Feedforward Precisions: The quantization noise gains are used to obtain values for the precisions as a function of B(min) as summarized below:

Layer Index l EWl pm BWl EAl pm BAl
Layer Index l EWl pm BWl EAl pm BAl
Layer Index l EWl pm BWl EAl pm BAl
Layer Index l EWl pm BWl EAl pm BAl

1 2.41E+03 11+B(min) 7.32E-01 5+B(min)
7 1.47E+03 10+B(min) 7.70E-02 3+B(min)
13 7.25E+03 11+B(min) 1.13E-01 4+B(min)
19 1.41E+03 9+B(min) 4.80E-03 1+B(min)

2 9.80E+02 10+B(min) 5.15E-01 5+B(min)
8 2.15E+03 11+B(min) 8.39E-02 3+B(min)
14 2.99E+03 11+B(min) 8.51E-02 3+B(min)
20 1.30E+03 7+B(min) 7.82E-04 0+B(min)

3 1.22E+03 10+B(min) 1.29E-01 4+B(min)
9 2.74E+03 11+B(min) 6.38E-02 3+B(min)
15 2.86E+03 11+B(min) 6.57E-02 3+B(min)
21 1.08E+02 11+B(min)

4 1.62E+03 10+B(min) 1.12E-01 4+B(min)
10 4.96E+03 11+B(min) 1.92E-01 4+B(min)
16 3.00E+03 11+B(min) 1.29E-01 4+B(min)
22 8.31E+00 11+B(min)

5 1.52E+03 10+B(min) 7.31E-02 3+B(min)
11 4.23E+03 11+B(min) 1.54E-01 4+B(min)
17 5.02E+03 10+B(min) 6.51E-02 3+B(min)

6 3.05E+03 11+B(min) 8.98E-02 3+B(min)
12 4.20E+03 12+B(min) 1.33E-01 4+B(min)
18 4.34E+03 10+B(min) 2.16E-02 2+B(min)

Note that for weights, layer depths 21 and 22 correspond to the strided convolutions in the shortcut connections of residual blocks 4 and 7, respectively. The value of B(min) is again swept, and it is found that the pm < 1% for B(min) = 3. The feedforward precisions are therefore set as follows and as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9 10 11 BWl 14 13 13 13 13 14 13 14 14 14 14 BAl 8 8 7 7 6 6 6 6 6 7 7
Layer Index l 12 13 14 15 16 17 18 19 20 21 22 BWl 15 14 14 14 14 13 13 12 10 14 14 BAl 7 7 6 6 7 6 5 4 3
Gradient Precisions: The spatial variance of the gradient tensors is used to determine the PDRs and the quantization step sizes of weight gradients. The singular values of the square-Jacobians are needed to determine the quantization step sizes of activation gradients. They were computed as follows:

18

Under review as a conference paper at ICLR 2019

Layer Index l
rG(lW ) G(lW ) rG(l+A1) (max)
G(l+A1) Gl(W )
G(l+A1)
Layer Index l
rG(lW ) G(lW ) rG(l+A1) (max)
G(l+A1) G(lW )
Gl(+A1)
Layer Index l
rGl(W ) G(lW ) rG(l+A1) (max)
G(l+A1) Gl(W )
G(l+A1)
Layer Index l
rGl(W ) Gl(W ) rG(l+A1) (max)
G(l+A1) G(lW )
Gl(+A1)

1 2.50E-01 2.44E-04 4.88E-04
8.07E+02
7.63E-06
7 3.13E-02 3.05E-05 2.44E-04
4.94E+03
2.38E-07
13 3.13E-02 3.05E-05 2.44E-04
8.07E+02
9.54E-07
19 1.56E-02 7.63E-06 2.44E-04
3.01E+02
5.96E-08

2 6.25E-02 3.05E-05 2.44E-04
2.84E+03
4.77E-07
8 3.13E-02 3.05E-05 2.44E-04
1.22E+03
4.77E-07
14 3.13E-02 3.05E-05 4.88E-04
1.93E+02
9.54E-07
20 1.56E-02 7.63E-06 6.25E-02
2.32E+01
3.81E-06

3 6.25E-02 3.05E-05 2.44E-04
2.84E+03
4.77E-07
9 3.13E-02 3.05E-05 4.88E-04
1.22E+03
4.77E-07
15 1.56E-02 1.53E-05 4.88E-04
1.93E+02
9.54E-07
21 1.56E-02 1.91E-06

4 3.13E-02 3.05E-05 2.44E-04
5.43E+03
2.38E-07
10 3.13E-02 3.05E-05 4.88E-04
1.08E+03
4.77E-07
16 1.56E-02 1.53E-05 4.88E-04
2.98E+02
4.77E-07
22 2.50E-01 3.05E-05

5 3.13E-02 3.05E-05 2.44E-04
5.43E+03
2.38E-07
11 3.13E-02 3.05E-05 2.44E-04
1.08E+03
4.77E-07
17 1.56E-02 1.53E-05 2.44E-04
2.98E+02
2.38E-07

6 3.13E-02 3.05E-05 2.44E-04
4.94E+03
2.38E-07
12 3.13E-02 3.05E-05 2.44E-04
8.07E+02
9.54E-07
18 1.56E-02 1.53E-05 2.44E-04
3.01E+02
2.38E-07

Hence the gradients precisions are set as follows and as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9 10 11

BG(lW ) BG(l+A1)

11 12 12 11 11 11 11 11 11 11 11 7 10 10 11 11 11 11 10 11 11 10

Layer Index l 12 13 14 15 16 17 18 19 20 21 22

BGl(W ) BG(l+A1)

11 11 11 11 11 11 11 12 12 14 14 9 9 10 10 11 11 11 13 15

Internal Weight Accumulators Precisions: The smallest learning rate value for this network is 0.001 which results in the following precisions for the internal weight accumulators as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9 10 11

BWl(Acc)

9 13 13 13 13 12 13 12 12 12 12

Layer Index l 12 13 14 15 16 17 18 19 20 21 22

BWl(Acc)

11 12 13 13 13 15 15 18 16 12 13

19

Under review as a conference paper at ICLR 2019

C.4 CIFAR-100 RESNET
Feedforward Precisions: The quantization noise gains are used to obtain values for the precisions as a function of B(min) as summarized below:

Layer Index l EWl pm BWl EAl pm BAl
Layer Index l EWl pm BWl EAl pm BAl
Layer Index l EWl pm BWl EAl pm BAl
Layer Index l EWl pm BWl EAl pm BAl

1 2.32E+03 10+B(min) 1.42E+00 5+B(min)
7 3.03E+03 10+B(min) 7.52E-02 3+B(min)
13 7.67E+03 11+B(min) 1.54E-01 3+B(min)
19 8.35E+02 9+B(min) 2.01E-02 2+B(min)

2 8.23E+02 9+B(min) 7.84E-01 4+B(min)
8 5.80E+03 11+B(min) 8.70E-02 3+B(min)
14 1.40E+04 11+B(min) 1.09E-01 3+B(min)
20 2.30E+01 7+B(min) 1.80E-03 0+B(min)

3 1.18E+03 10+B(min) 2.52E-01 4+B(min)
9 7.29E+03 11+B(min) 1.38E-01 3+B(min)
15 1.13E+04 11+B(min) 1.93E-01 3+B(min)
21 6.78E+03 11+B(min)

4 1.28E+03 10+B(min) 1.46E-01 3+B(min)
10 9.20E+03 11+B(min) 2.49E-01 4+B(min)
16 1.09E+04 11+B(min) 2.36E-01 4+B(min)
22 6.03E+03 11+B(min)

5 1.70E+03 10+B(min) 7.68E-02 3+B(min)
11 9.81E+03 11+B(min) 2.11E-01 3+B(min)
17 5.35E+03 11+B(min) 1.27E-01 3+B(min)

6 2.78E+03 10+B(min) 7.40E-02 3+B(min)
12 1.41E+04 11+B(min) 1.51E-01 3+B(min)
18 3.97E+03 11+B(min) 3.01E-02 2+B(min)

The value of B(min) is again swept, and it is found that the pm < 1% for B(min) = 3. The feedforward precisions are therefore set as follows and as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9 10 11 BWl 13 12 13 13 13 13 13 14 14 14 14 BAl 8 7 7 6 6 6 6 6 6 7 6
Layer Index l 12 13 14 15 16 17 18 19 20 21 22 BWl 14 14 14 14 14 14 14 12 10 14 14 BAl 6 6 6 6 7 6 5 5 3
Gradient Precisions: The spatial variance of the gradient tensors is used to determine the PDRs and the quantization step sizes of weight gradients. The singular values of the square-Jacobians are needed to determine the quantization step sizes of activation gradients. They were computed as follows:

20

Under review as a conference paper at ICLR 2019

Layer Index l
rGl(W ) Gl(W ) rG(l+A1) (max)
G(l+A1) Gl(W )
G(l+A1)
Layer Index l
rGl(W ) G(lW ) rG(l+A1) (max)
G(l+A1) Gl(W )
G(l+A1)
Layer Index l
rG(lW ) Gl(W ) rG(l+A1) (max)
G(l+A1) Gl(W )
Gl(+A1)
Layer Index l
rG(lW ) G(lW ) rG(l+A1) (max)
G(l+A1) Gl(W )
G(l+A1)

1 5.00E-01 6.10E-05 2.44E-04
6.46E+02
9.54E-07
7 3.13E-02 1.53E-05 6.10E-05
5.11E+03
5.96E-08
13 3.13E-02 1.53E-05 1.22E-04
6.37E+02
2.38E-07
19 1.56E-02 3.81E-06 6.10E-05
2.80E+02
5.96E-08

2 6.25E-02 1.53E-05 1.22E-04
1.86E+03
1.19E-07
8 3.13E-02 1.53E-05 1.22E-04
1.05E+03
1.19E-07
14 3.13E-02 7.63E-06 1.22E-04
2.31E+02
2.38E-07
20 6.25E-02 7.63E-06 7.81E-03
7.81E+01
2.38E-07

3 6.25E-02 1.53E-05 6.10E-05
1.86E+03
1.19E-07
9 3.13E-02 1.53E-05 1.22E-04
1.05E+03
1.19E-07
15 1.56E-02 7.63E-06 2.44E-04
2.31E+02
2.38E-07
21 3.13E-02 1.53E-05

4 3.13E-02 1.53E-05 6.10E-05
3.54E+03
1.19E-07
10 3.13E-02 1.53E-05 1.22E-04
8.23E+02
2.38E-07
16 3.13E-02 7.63E-06 1.22E-04
2.79E+02
1.19E-07
22 1.56E-02 7.63E-06

5 6.25E-02 1.53E-05 6.10E-05
3.54E+03
1.19E-07
11 3.13E-02 1.53E-05 1.22E-04
8.23E+02
2.38E-07
17 1.56E-02 7.63E-06 6.10E-05
2.79E+02
1.19E-07

6 3.13E-02 1.53E-05 6.10E-05
5.11E+03
5.96E-08
12 3.13E-02 1.53E-05 1.22E-04
6.37E+02
2.38E-07
18 1.56E-02 7.63E-06 6.10E-05
2.80E+02
1.19E-07

Hence the gradients precisions are set as follows and as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9 10 11

BG(lW ) BG(l+A1)

14 13 13 12 13 12 12 12 12 12 12 9 11 10 10 10 11 11 11 11 10 10

Layer Index l 12 13 14 15 16 17 18 19 20 21 22

BG(lW ) BG(l+A1)

12 12 13 12 13 12 12 13 14 12 12 10 10 10 11 11 10 10 11 16

Internal Weight Accumulators Precisions: The smallest learning rate value for this network is 0.001 which results in the following precisions for the internal weight accumulators as illustrated in Figure 2:

Layer Index l 1 2 3 4 5 6 7 8 9 10 11

BWl(Acc)

12 15 14 14 14 14 14 13 13 13 13

Layer Index l 12 13 14 15 16 17 18 19 20 21 22

BWl(Acc)

13 13 14 14 14 14 14 17 18 13 14

21

