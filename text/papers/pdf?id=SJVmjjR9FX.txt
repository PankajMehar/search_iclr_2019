Under review as a conference paper at ICLR 2019
VARIATIONAL BAYESIAN PHYLOGENETIC INFERENCE
Anonymous authors Paper under double-blind review
ABSTRACT
Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates. In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis. We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions. We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models. We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference. Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling. Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.
1 INTRODUCTION
Bayesian phylogenetic inference is an essential tool in modern evolutionary biology. Given an alignment of nucleotide or amino acid sequences and appropriate prior distributions, Bayesian methods provide principled ways to assess the phylogenetic uncertainty by positing and approximating a posterior distribution on phylogenetic trees (Huelsenbeck et al., 2001). In addition to uncertainty quantification, Bayesian methods enable integrating out tree uncertainty in order to get more confident estimates of parameters of interest, such as factors in the transmission of Ebolavirus (Dudas et al., 2017). Bayesian methods also allow complex substitution models (Lartillot & Philippe, 2004), which are important in elucidating deep phylogenetic relationships (Feuda et al., 2017).
Ever since its introduction to the phylogenetic community in the 1990s, Bayesian phylogenetic inference has been dominated by random-walk Markov chain Monte Carlo (MCMC) approaches (Yang & Rannala, 1997; Mau et al., 1999; Huelsenbeck & Ronquist, 2001). However, this approach is fundamentally limited by the complexities of tree space. A typical MCMC method for phylogenetic inference involves two steps in each iteration: first, a new tree is proposed by randomly perturbing the current tree, and second, the tree is accepted or rejected according to the Metropolis-Hastings acceptance probability. Any such random walk algorithm faces obstacles in the phylogenetic case, in which the high-posterior trees are a tiny fraction of the combinatorially exploding number of trees. Thus, major modifications of trees are likely to be rejected, restricting MCMC tree movement to local modifications that may have difficulty moving between multiple peaks in the posterior distribution (Whidden & Matsen IV, 2015). Although recent MCMC methods for distributions on Euclidean space use intelligent proposal mechanisms such as Hamiltonian Monte Carlo (Neal, 2011), it is not straightforward to extend such algorithms to the composite structure of tree space, which includes both tree topology (discrete object) and branch lengths (continuous positive vector) (Dinh et al., 2017).
Variational inference (VI) is an alternative approximate inference method for Bayesian analysis which is gaining in popularity (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017). Unlike MCMC methods that sample from the posterior, VI seeks the best candidate from a family of tractable distributions that minimizes a statistical distance measure to the target posterior, usually
1

Under review as a conference paper at ICLR 2019

the Kullback-Leibler (KL) divergence. By reformulating the inference problem into an optimization problem, VI tends to be faster and easier to scale to large data (via stochastic gradient descent) (Blei et al., 2017). However, VI can also introduce a large bias if the variational distribution is insufficiently flexible. The success of variational methods, therefore, largely relies on the existence of appropriate tractable variational distributions and efficient training procedures.
To our knowledge, there have been no previous variational formulations of Bayesian phylogenetic inference. This has been due to the lack of an appropriate family of distributions on phylogenetic trees that can be used as an approximating distribution. However the prospects for variational inference have changed recently with the introduction of subsplit Bayesian networks (SBNs) (Zhang & Matsen IV, 2018), which provide a family of flexible distributions on tree topologies (i.e. trees without branch lengths). SBNs build on previous work (Ho¨hna & Drummond, 2012; Larget, 2013), but in contrast to these previous efforts, SBNs are sufficiently flexible for real Bayesian phylogenetic posteriors (Zhang & Matsen IV, 2018).
In this paper, we develop a general variational inference framework for Bayesian phylogenetics. We show that SBNs, when combined with appropriate approximations for the branch length distribution, can provide flexible variational approximations over the joint latent space of phylogenetic trees with branch lengths. We use recently-proposed unbiased gradient estimators for the discrete and continuous components separately to enable efficient stochastic gradient ascent. We also leverage the similarity of local structures among trees to reduce the complexity of the variational parameterization for the branch length distributions and provide an extension to better capture the between-tree variation. Finally, we demonstrate the effectiveness and efficiency of our methods on both synthetic data and a benchmark of challenging real data Bayesian phylogenetic inference problems.

2 BACKGROUND

Phylogenetic Posterior A phylogenetic tree is described by a tree topology  and associated nonnegative branch lengths q. The tree topology  is used to model the evolutionary diversification of the species. It is a bifurcating tree with N leaves, each of which has a label corresponding to one of the observed species. The internal nodes of  represent the unobserved characters (e.g. DNA bases) of the ancestral species. A continuous-time Markov model is often used to describe the transition probabilities of the characters along the branches of the tree. A Bayesian approach to phylogenetic analysis requires a likelihood model for sequence evolution through a phylogenetic tree, and prior distributions on the tree topologies and the branch lengths. Let Y = {Y1, Y2, . . . , YM }  N×M be the observed sequences (with characters in ) of length M over N species. The probability of each site observation Yi is defined as the marginal distribution over the leaves

p(Yi|, q) = (ai )

Paui avi (quv )

ai (u,v)E( )

where  is the root node (or any internal node if the tree is unrooted and the Markov model is time reversible), ai ranges over all extensions of Yi to the internal nodes with aiu being the assigned character of node u, E( ) denotes the set of edges of  , Pij(t) denotes the transition probability from character i to character j across an edge of length t and  is the stationary distribution of the
Markov model. Assuming different sites are identically distributed and evolve independently, the
likelihood of observing the sequence set Y has the form

M

p(Y |, q) =

(ai )

Paui avi (quv )

i=1 ai

(u,v)E( )

(1)

The phylogenetic likelihood in equation 1 can be evaluated efficiently through the pruning algorithm (Felsenstein, 2003), also known as the sum-product algorithm in probabilistic graphical models (Strimmer & Moulton, 2000; Koller & Friedman, 2009; Ho¨hna et al., 2014). Given a proper prior distribution with density p(, q) imposed on the tree topologies and the branch lengths, the posterior p(, q|Y ) is proportional to the joint density

p(, q|Y ) = p(Y |, q)p(, q)  p(Y |, q)p(, q) p(Y )

where p(Y ) is the intractable normalizing constant.

2

Under review as a conference paper at ICLR 2019

Species A: ATGAAC · · · Species B: ATGCAC · · · Species C: ATGCAT · · · Species D: ATGCAT · · ·

AA

A

B BC B

ABC

C

C

D
DD

DD

S4 S2 S5

S1

AA
A
B AB B B
C CD C C

S3 S6 S7

D
DD

Figure 1: A simple subsplit Bayesian network for a leaf set that contains 4 species. Left: A leaf

label set X of 4 species, each label corresponds to a DNA sequence. Middle (left): Examples of

phylogenetic trees (rooted) that are hypothesized to model the evolutionary history of the species.

Middle (right): The corresponding SBN assignments for the trees. For ease of illustration, subsplit

(W, Z) is represented as

W Z

in the graph.

The dashed gray subgraphs represent deterministic split-

ting processes and are used purely to complement the networks such that the overall network has a

fixed structure. Right: The (fixed) SBN for these examples.

Subsplit Bayesian Networks We now review subsplit Bayesian networks (Zhang & Matsen IV, 2018), powerful probabilistic models that can provide a rich family of flexible distributions on tree topologies. Let X be the set of leaf labels. We call a nonempty subset of X a clade. Let be a total order on clades (e.g., lexicographical order). A subsplit (W, Z) of a clade X is an ordered pair of disjoint subclades of X such that W  Z = X and W Z. A subsplit Bayesian network BX on a leaf set X of size N is a Bayesian network whose nodes take on subsplit values or singleton clade values that represent the local topological structure of trees (Figure 1). Since any rooted tree has a unique subsplit decomposition that follows its branching (splitting) process, SBNs by design provide distributions over the entire tree space. The Bayesian network formulation of SBNs enjoys many benefits: i) flexibility. The expressiveness of SBNs is freely adjustable by changing the dependency structures between nodes, allowing for a wide range of flexible distributions; ii) normality. SBN-induced distributions are all naturally normalized if the associated conditional probability tables (CPTs) are consistent, which is a common property of Bayesian networks. Given the subsplit decomposition of a rooted tree  = {s1, s2, . . .}, where s1 is the root subsplit and {si}i>1 are other local subsplits, the SBN tree probability takes the following form

psbn(T =  ) = p(S1 = s1) p(Si = si|Si = si )
i>1
where i is the index set of the parents of Si. The SBN framework also generalizes to unrooted trees, which are the most common tree type in phylogenetics. Concretely, unrooted trees can be viewed as rooted trees with unobserved roots. Marginalizing out the unobserved root node S1, we have the SBN probability estimates for unrooted trees

psbn(T u =  ) =

p(S1 = s1) p(Si = si|Si = si )

s1 

i>1

where  means all root subsplits that are compatible with  . Conditional subsplit probabilities for the same parent-child subsplit pairs are shared across the network to reduce model complexity and encourage generalization. Prior to this work, SBNs were introduced as an extension of previous work (Ho¨hna & Drummond, 2012; Larget, 2013) to further improve the posterior estimation of phylogenetic trees based on MCMC samples (Zhang & Matsen IV, 2018).

3 VARIATIONAL PHYLOGENETIC INFERENCE VIA SBNS
The flexible and tractable tree topology distributions provided by SBNs also serve as an essential building block to perform variational inference (Jordan et al., 1999) for phylogenetics. Suppose that we have a family of approximate distributions Q( ) (e.g., SBNs) over phylogenetic tree topologies, where  denotes the corresponding variational parameters (e.g., CPTs for SBNs). For each tree  , we posit another family of densities Q(q| ) over the branch lengths, where  is the branch length

3

Under review as a conference paper at ICLR 2019

variational parameters. We then combine these distributions and use the product

Q,(, q) = Q( )Q(q| )

as our variational approximation. Inference now amounts to finding the member of this family that minimizes the Kullback-Leibler (KL) divergence to the exact posterior,

,  = arg min DKL (Q,(, q) p(, q|Y ))
,

(2)

which is equivalent to maximizing the evidence lower bound (ELBO),

L(, ) = EQ,(,q) log

p(Y |, q)p(, q) Q( )Q(q| )

 log p(Y ).

As the ELBO is based on a single-sample estimate of the evidence, it heavily penalizes samples that fail to explain the observed sequences. As a result, the variational approximation tends to cover only the high-probability areas of the true posterior. This effect can be minimized by averaging over K > 1 samples when estimating the evidence (Burda et al., 2016; Mnih & Rezende, 2016), which leads to tighter lower bounds

LK (, ) = EQ,( 1:K , q1:K ) log

1 K p(Y | i, qi)p( i, qi)

K
i=1

Q( i)Q(qi| i)

 log p(Y )

(3)

where Q,( 1:K , q1:K ) 

K i=1

Q,( i,

qi);

the

tightness

of

the

lower

bounds

improves

as

the

number of samples K increases (Burda et al., 2016). We will use multi-sample lower bounds in the

sequel and refer to them as lower bounds for short.

3.1 VARIATIONAL PARAMETERIZATION

The CPTs in SBNs are, in general, associated with all possible parent-child subsplit pairs. Therefore, a full parameterization requires a combinatorially increasing number of parameters. In practice, however, we can find a sufficiently large subsplit support of CPTs (i.e. where the associated conditional probabilities are allowed to be nonzero) that covers favorable subsplit pairs from trees in the high-probability areas of the true posterior. Compared to estimating the true posterior probability of the trees that often involves an expensive long MCMC run, locating the high-probability areas or finding favorable subsplit pairs could be much cheaper and can be done via either initial MCMC runs, fast bootstrap methods or heuristic search based on compatibility or a distance measure induced by the input sequences. In this paper, we will mostly focus on the variational approach and assume the support of CPTs is available, although in our experiments we find that a simple bootstrap-based approach does provide a reasonable CPT support estimate for real data. We leave the development of more sophisticated methods for finding the support of CPTs to future work.

Now denote the set of root subsplits in the support as Sr and the set of parent-child subsplit pairs in the support as Sch|pa. The CPTs are defined according to the following equations

p(S1 = s1) =

exp(s1 ) , srSr exp(sr )

p(Si = s|Si = t) =

exp(s|t) sS·|t exp(s|t)

where S·|t denotes the set of child subsplits for parent subsplit t.
In phylogenetic trees, the branch lengths are all non-negative. We, therefore, use the Log-normal distribution Lognormal(µ, 2) as our variational approximation. A naive parameterization of Q(q| ) then is to assign a set of µ(e,  ), (e,  ) for each edge e on each tree  . Although most less likely trees are excluded given the subsplit support, this simple approach still requires a large number of parameters especially when the high-probability areas of the posterior are diffuse. Like in SBNs, we can reduce the number of parameters by exploiting the similarity of local structures among trees and this time we use a split-based parameterization. A split is a bipartition (X1, X2) of the leaf labels X (i.e. X1  X2 = X , X1  X2 = ) and each edge e of a phylogenetic tree  represents a split, the bipartition that consists of the leaf labels from both side of the edge. Note that a split can be viewed as a root subsplit. We then assign µ(·, ·), (·, ·) for each split (·, ·) in Sr. We denote the corresponding split of edge e of tree  as e/ .

4

Under review as a conference paper at ICLR 2019

A Simple Independent Approximation Given a phylogenetic tree  , we use a simple model that
assumes the branch lengths for the edges of the tree are independently distributed. The approximate density Q(q| ), therefore, has the form

Q(q| ) =

pLognormal (qe | µ(e,  ), (e,  )) , µ(e,  ) = eµ/ , (e,  ) = e/

eE( )

(4)

where pLognormal(·|µ, ) is the probability density function of Lognormal(µ, 2).

Capturing Between-Tree Branch Length Variation The above approximation equation 4 implicitly assumes that the branch lengths in different trees have the same distribution if they correspond to the same split, which fails to account for between-tree variation. To capture the between-tree variation, one can use a more sophisticated parameterization that allows other tree-dependent terms for the variational parameters µ and . For example, besides the split, there is an additional local structure associated with each edge defined as follows:
Definition 1 (primary subsplit pair) Let e be an edge of a phylogenetic tree  which represents a split e/ = (W, Z). Assume that at least one of W or Z, say W , contains more than one leaf label and denote its subsplit as (W1, W2). We call the parent-child subsplit pair (W1, W2)|(W, Z) a primary subsplit pair.

W
1
Z
2

Z1 W2

e
WZ µ(W, Z)

µ (W1, W2|W, Z) 

+



µ(Z
1

,

Z
2

|W,

Z

)

µ(e,  )
Figure 2: Branch length parameterization using primary subsplit pairs, which is the sum of parameters for a split and its neighboring subsplits. Edge e represents a split (W, Z). Parameterization for the variance is the same as for the mean.

We assign additional parameters for each primary subsplit pair. Denoting the primary subsplit pair(s) of edge e in tree  as e// , we then simply sum all variational parameters associated with e to form the mean and variance parameters for the corresponding branch length (Figure 2):

µ(e,  ) = eµ/ +

sµ, (e,  ) = e/ +

s .

se//

se//

This modifies the density in equation 4 by adding contributions from primary subsplit pairs and hence allows for more flexible between-tree approximations.

3.2 STOCHASTIC GRADIENT ESTIMATORS AND THE VBPI ALGORITHM
In practice, the lower bound is usually maximized via stochastic gradient ascent (SGA). However, the naive stochastic gradient estimator obtained by differentiating the lower bound has very large variance and is impractical for our purpose. Fortunately, various variance reduction techniques have been introduced in recent years including the control variate (Paisley et al., 2012; Ranganath et al., 2014; Mnih & Gregor, 2014; Mnih & Rezende, 2016) for general latent variables and the reparameterization trick (Kingma & Welling, 2014) for continuous latent variables. In the following, we apply these techniques to different components of our latent variables and derive efficient gradient estimators with much lower variance, respectively. In addition, we also consider a stable gradient estimator based on an alternative variational objective.

The VIMCO Estimator

Let f,(, q)

=

p(Y |,q)p( Q( )Q (q

,q) | )

.

The naive gradient of the lower bound

w.r.t. the tree variational parameters  has the form (derivation in Appendix A):

K

LK (, ) = EQ,( 1:K , q1:K )

L^K (, ) - w~j  log Q( j)

j=1

(5)

where L^K (, ) = log

1 K

K i=1

f, (

i,

qi)

and w~j =

f, ( j ,qj )

K i=1

f, (

i ,q i )

are

the

stochastic

lower

bound and the self-normalized importance weight, respectively. While the importance weights are

well-behaved, the stochastic lower bound could be problematic since it does not distinguish the

5

Under review as a conference paper at ICLR 2019

samples according to their fitness for the data and assigns the same learning signal to all of them. Moreover, the magnitude of this learning signal can be extremely large, especially early in training when all the samples from the variational proposal Q explain the data poorly. By utilizing the independence between the multiple samples and the regularity of the learning signal, Mnih & Rezende (2016) propose a localized learning signal strategy that estimates the gradient as follows

K

LK (, ) = EQ,( 1:K , q1:K )

L^jK|-j (, ) - w~j  log Q( j )

j=1

(6)

where



L^Kj|-j (, )

:=

L^K (,

)

-

log

1 K



f,( i, qi) + f^,( -j , q-j )

i=j

is the per-sample local learning signal, with f^,( -j, q-j) being some estimate of f,( j, qj) for sample j using the rest of samples (e.g., the geometric mean). The equality in equation 6 holds since the modification to the learning signal for each sample is independent of the sample itself. This gives the following VIMCO estimator

LK (, )

K
L^jK|-j (, ) - w~j  log Q( j ) with  j , qj iid Q,(, q)
j=1

(7)

The Reparameterization Trick The above VIMCO estimator also works for the branch length
gradient. However, as branch lengths are continuous latent variables, we can use an alternative
approach, the reparameterization trick, to estimate the gradient. Suppose there exists a differentiable mapping q = g( | ) such at q  Q(q| ) with  p ( ). Then the expectation of a function F (q) over the distribution Q(q| ) can be computed as EQ(q|)F (q) = Ep ( )F (g( | )). A typical example is the Gaussian reparameterization: q  N (µ, 2)  q = µ +  ,  N (0, 1). Note that the Log-normal distribution that we choose also has a simple reparameterization: q  Lognormal(µ, 2)  q = exp(µ +  ),  N (0, 1). Now we apply the reparameterization trick
to the lower bound, obtaining



LK (, )

=

EQ,

( 1:K ,

1 1:K ) log  K

K j=1

p(Y

| j, g( j| j))p( j, Q( j )Q(g( j |

g ( j )|

j

j | )

j

))



.

Then the gradient of the lower bound w.r.t.  is (see Appendix A for derivation)

K
LK (, ) = EQ, (1:K , 1:K ) w~j  log f,( j , g( j | j ))
j=1

(8)

where w~j =

f, ( j ,g ( j | j ))

K i=1

f, ( i,g (

i| i))

is

the

same

normalized

importance

weight

as

in

equation

5.

Therefore, we can form the Monte Carlo estimator of the gradient

LK (, )

K
w~j  log f,( j , g( j | j )) with  j iid Q( ), j iid p ( ).
j=1

(9)

Self-normalized Importance Sampling Estimator In addition to the standard variational formulation equation 2, one can reformulate the optimization problem by minimizing the reversed KL divergence, which is equivalent to maximizing the likelihood of the variational approximation

Q, (, q), where ,  = arg max L~(, ), L~(, ) = Ep(,q|Y ) log Q,(, q). (10)
,

We can use an importance sampling estimator to compute the gradient of the objective

L~(, )

=

Ep(,q|Y ) log Q,(, q)

=

1 p(Y

)

EQ,

(

,q)

p(Y Q

|, q)p( ( )Q(q

, q) | )



log Q( )

K
w~j  log Q( j ) with  j , qj iid Q,(, q)
j=1

(11)

6

Under review as a conference paper at ICLR 2019

with the same importance weights w~j as in equation 6 (a detailed derivation can be found in Appendix A). This can be viewed as a multi-sample generalization of the wake-sleep algorithm (Hinton et al., 1995) and was first used in the reweighted wake-sleep algorithm (Bornschein & Bengio, 2015) for training deep generative models. We therefore call the gradient estimator in equation 11 the RWS estimator. Note that all importance weights are between 0 and 1, so the RWS estimator provides a well-behaved alternative to the VIMCO estimator for tree variational parameter gradient estimation. Like the VIMCO estimator, the RWS estimator also provides gradients for branch lengths

L~(, )

K
w~j log Q(qj| j)
j=1

However, we find in practice that equation 9 that uses the reparameterization trick is more useful and often leads to faster convergence, although it uses a different optimization objective. A better understanding of this phenomenon would be an interesting subject of future research.

All stochastic gradient estimators introduced above can be used in conjunction with stochastic optimization methods such as SGA or some of its adaptive variants (e.g. ADAM (Kingma & Ba, 2015)) to maximize the lower bounds. See algorithm 1 in Appendix B for a basic variational Bayesian phylogenetic inference (VBPI) approach.

4 EXPERIMENTS

Throughout this section we evaluate the effectiveness and efficiency of our variational framework for inference over phylogenetic trees. Training was performed by following the gradient estimators in section 3.2 of an annealed version (if needed) of the lower bound. The simplest SBN (the one with a full and complete binary tree structure) is used for the phylogenetic tree topology variational distribution; we have found it to provide sufficiently accurate approximation. For real datasets, we estimate the CPT supports from ultrafast maximum likelihood phylogenetic bootstrap trees using UFBoot (Minh et al., 2013), which is a fast approximate bootstrap method based on efficient heuristics. We compare the performance of the VIMCO estimator and the RWS estimator with different variational parameterizations for the branch length distributions, while varying the number of samples in the training objective to see how these affect the quality of the variational approximations. For VIMCO, we use ADAM for stochastic gradient ascent with learning rate 0.001 (Kingma & Ba, 2015). For RWS, we also use AMSGRAD (Sashank et al., 2018), a recent variant of ADAM, when ADAM is unstable. Results were collected after 200,000 parameter updates.

4.1 SIMULATED SCENARIOS

To empirically investigate the representative power of SBNs to approximate distributions on phylogenetic trees under the variational framework, we first conduct experiments on a simulated setup. We use the space of unrooted phylogenetic trees with 8 leaves, which contains 10395 unique trees in total. Given an arbitrary order of trees, we generate a target distribution p0( ) by drawing a sample from the symmetric Dirichlet distributions Dir(1) of order 10395, where  is the concentration parameter. The target distribution becomes more diffuse as  increases; we used  = 0.008 to provide enough information for inference while allowing for adequate diffusion in the target. Note that there are no branch lengths in this simulated model and the lower bound is

LK () = EQ( 1:K ) log

1 K p0( i) K i=1 Q( i)

0

with the exact evidence being log(1) = 0. We then use both the VIMCO and RWS estimators to optimize the above lower bound based on 20 and 50 samples (K). We use a slightly larger learning rate (0.002) in AMSGRAD for RWS. In addition to the lower bound estimate, KL divergence from the variational approximation to the target distribution is also provided to measure the performance of different methods.

Figure 3 shows the empirical performance of different methods. From the left plot, we see that the lower bounds converge rapidly and the gaps between lower bounds and the exact model evidence are close to zero, demonstrating the expressive power of SBNs on phylogenetic tree probability estimations. The evolution of KL divergences (middle plot) is consistent with the lower bounds. All

7

Under review as a conference paper at ICLR 2019

Evidence Lower Bound KL Divergence
Variational approximation

0.0

0.5 0.00 1.0 0.02

1.5 EXACT

2.0

VIMCO(20) VIMCO(50)

2.5

RWS(20) RWS(50)

3.0 0 Th50ousan1d00Iterati1o5n0s 200

VIMCO(20) VIMCO(50) RWS(20) 100 RWS(50)
10 1
0 Th50ousan1d00Iterati1o5n0s 200

10 1 VIMCO(50) RWS(50)
10 2

10 3

10 140 4

1G0 r3ound tru1t0h2

10 1

Figure 3: Comparison of multi-sample objective on approximating a challenging distribution over unrooted phylogenetic trees with 8 leaves using VIMCO and RWS gradient estimators. Left: Evidence lower bound. Middle: KL divergence. Right: Variational approximations vs ground truth probabilities. The number in parentheses specifies the number of samples used in the training objective.

methods benefit from using more samples, with VIMCO performing better in the end and RWS learning slightly faster at the beginning.1 The slower start of VIMCO is partly due to the regularization term in the lower bounds, which turns out to be beneficial for the overall performance since the regularization encourages the diversity of the variational approximation and leads to more sufficient exploration in the starting phase, similar to the exploring starts (ES) strategy in reinforcement learning (Sutton & Barto, 1998). The right plot compares the variational approximations obtained by VIMCO and RWS, both with 50 samples, to the ground truth p0( ). We see that VIMCO slightly underestimates trees in high-probability areas as a result of the regularization effect. While RWS provides better approximations for trees in high-probability areas, it tends to underestimate trees in low-probability areas which deteriorates the overall performance. We expect the biases in both approaches to be alleviated with more samples.

4.2 REAL DATA PHYLOGENETIC POSTERIOR ESTIMATION

In the second set of experiments we evaluate the proposed variational Bayesian phylogenetic inference (VBPI) algorithms at estimating unrooted phylogenetic tree posteriors on 8 real datasets commonly used to benchmark phylogenetic MCMC methods (Lakner et al., 2008; Ho¨hna & Drummond, 2012; Larget, 2013; Whidden & Matsen IV, 2015) (Table 1). We concentrate on the most challenging part of the phylogenetic model: joint learning of the tree topologies and the branch lengths. We assume a uniform prior on the tree topology, an i.i.d. exponential prior (Exp(10)) for the branch lengths and the simple Jukes & Cantor (1969) substitution model. We consider two different variational parameterizations for the branch length distributions as introduced in section 3.1. In the first case, we use the simple split-based parameterization that assigns parameters to the splits associated with the edges of the trees. In the second case, we assign additional parameters for the primary subsplit pairs (PSP) to better capture the between-tree variation. We form our ground truth posterior from an extremely long MCMC run of 10 billion iterations (sampled each 1000 iterations with the first 25% discarded as burn-in) using MrBayes (Ronquist et al., 2012), and gather the support of CPTs from 10 replicates of 10000 ultrafast maximum likelihood bootstrap trees (Minh et al., 2013). Following Rezende & Mohamed (2015), we use a simple annealed version of the lower bound which was found to provide better results. The modified bound is:

LKt (, ) = EQ,( 1:K , q1:K ) log

1 K [p(Y | i, qi)]t p( i, qi)

K
i=1

Q( i)Q(qi| i)

where t  [0, 1] is an inverse temperature that follows a schedule t = min(0.001, t/100000), going from 0.001 to 1 after 100000 iterations. We train the variational approximations with VIMCO and RWS using 10 and 20 sample objectives and use ADAM with learning rate 0.001 for both methods.

1Although we use larger learning rates for RWS in this experiment, we found RWS generally learns slightly faster than VIMCO at the beginning. See Figure 4 for the real data phylogenetic inference problems in section 4.2 where we use ADAM with learning rate 0.001 for both methods.

8

Under review as a conference paper at ICLR 2019

KL Divergence KL Divergence
VBPI

VIMCO(10)

VIMCO(10) + PSP

101

VIMCO(20) RWS(10)

101

VIMCO(20) + PSP RWS(10) + PSP

RWS(20)

RWS(20) + PSP

MCMC MCMC

100 100

10 1 0

Th50ousan1d00Iterati1o5n0s 200

10 1 0

Th50ousan1d00Iterati1o5n0s 200

7036 7037 7038 7039 7040 7041 7042
7042

7040GSS 7038

7036

Figure 4: Performance on DS1. Left: KL divergence for methods that use the simple split-based parameterization for the branch length distributions. Middle: KL divergence for methods that use PSP. Right: Per-tree marginal likelihood estimation (in nats): VBPI vs GSS. The number in brackets specifies the number of samples used in the training objective. MCMC results are averaged over 10 independent runs. The results for VBPI were obtained using 1000 samples and the error bar shows one standard deviation over 100 independent runs.

Figure 4 (left and middle plots) shows the resulting KL divergence to the ground truth on DS1 as a function of the number of parameter updates. The results for methods that adopt the simple split-based parameterization of variational branch length distributions are shown in the left plot. We see that the performance of all methods improves significantly as the number of samples is increased. As before, RWS learns faster at the beginning while VIMCO performs better in the end. The middle plot, containing the results using PSP for variational parameterization, clearly indicates that a better modeling of between-tree variation of the branch length distributions is beneficial for all method / number of samples combinations. Specifically, PSP enables more flexible branch length distributions across trees which makes the learning task much easier, evidenced by the considerably smaller gaps between the methods. To benchmark the learning efficiency of VBPI, we also compare to MrBayes 3.2.5 (Ronquist et al., 2012), a standard MCMC implementation. We run MrBayes with 4 chains and 10 runs for two million iterations, sampling every 100 iterations. For each run, we compute the KL divergence to the ground truth every 50000 iterations with the first 25% discarded as burn-in. For a relatively fair comparison (in terms of the number of likelihood evaluations), we compare 10 (i.e. 2·20/4) times the number of MCMC iterations with the number of 20-sample objective VBPI iterations.2 Although MCMC converges faster at the start, we see that VBPI methods (especially those with PSP) quickly surpass MCMC and arrive at good approximations with much less computation. This is because VBPI iteratively updates the approximate distribution of trees (e.g., SBNs) which in turn allows guided exploration in the phylogenetic tree space.
The variational approximations provided by VBPI can be readily used to perform importance sampling for phylogenetic inference (see more details in Appendix C). Thanks to the structured variational parameterization for the branch length distributions, VBPI can provide variational approximations on branch lengths for any tree that is covered by the subsplit support. We, therefore, can use them to estimate the marginal likelihood of trees via importance sampling. The right plot of figure 4 compares VBPI using VIMCO with 20 samples and PSP to the state-of-the-art generalized stepping-stone (GSS) (Fan et al., 2011) algorithm for estimating the marginal likelihood of trees in the 95% credible set of DS1. For GSS, we use 50 power posteriors and for each power posterior we run 1,000,000 MCMC iterations, sampling every 1000 iterations with the first 10% discarded as burn-in. The reference distribution for GSS was obtained from an independent Gamma approximation using the maximum a posterior estimate. We see that the estimates given by VBPI closely match GSS estimates and have small variance in general. Similarly, we can use VBPI to estimate the marginal likelihood of the data (i.e., the model evidence) via importance sampling. We use VIMCO to train our variational approximation since it provides better overall performance in general. Table 1 shows the marginal likelihood estimates using different VIMCO approximations and one of the state-of-the-art methods, the stepping-stone (SS) algorithm (Xie et al., 2011), which unlike GSS is able to also integrate out tree topologies. We see that VBPI is competitive with SS and provides estimates with much less variance (hence more reproducible and reliable). Again, the extra flexibility
2the extra factor of 2/4 is because the likelihood and the gradient can be computed together in twice the time of a likelihood (Schadt et al., 1998) and we run MCMC with 4 chains.

9

Under review as a conference paper at ICLR 2019

Table 1: Data sets used for variational phylogenetic posterior estimation, and marginal likelihood estimates of different methods across datasets. The marginal likelihood estimates of all variational methods are obtained by importance sampling using 1000 samples. We run stepping-stone in MrBayes using default settings with 4 chains for 10,000,000 iterations and sampled every 100 iterations. The results are averaged over 10 independent runs with standard deviation in brackets.

DATA SET
DS1 DS2 DS3 DS4 DS5 DS6 DS7 DS8

REFERENCE
HEDGES ET AL. (1990) GAREY ET AL. (1996) YANG & YODER (2003) HENK ET AL. (2003) LAKNER ET AL. (2008) ZHANG & BLACKWELL (2001) YODER & YANG (2004) ROSSMAN ET AL. (2001)

(#TAXA, #SITES)
(27, 1949) (29, 2520) (36, 1812) (41, 1137) (50, 378) (50, 1133) (59, 1824) (64, 1008)

VIMCO(10)
-7108.43(0.26) -26367.70(0.12) -33735.08(0.11) -13329.90(0.31) -8214.36(0.67) -6723.75(0.68) -37332.03(0.43) -8653.34(0.55)

MARGINAL LIKELIHOOD (NATS)

VIMCO(20) VIMCO(10) + PSP VIMCO(20) + PSP

-7108.35(0.21) -26367.71(0.09) -33735.11(0.11) -13329.98(0.20) -8214.74(0.38) -6723.71(0.65) -37331.90(0.49) -8651.54(0.80)

-7108.41(0.16) -26367.72(0.08) -33735.10(0.09) -13329.94(0.18) -8214.61(0.38) -6724.09(0.55) -37331.90(0.32) -8650.63(0.42)

-7108.42(0.10) -26367.70(0.10) -33735.07(0.11) -13329.93(0.22) -8214.55(0.43) -6724.34(0.45) -37332.03(0.23) -8650.55(0.46)

SS
-7108.42(0.18) -26367.57(0.48) -33735.44(0.50) -13330.06(0.54) -8214.51(0.28) -6724.07(0.86) -37332.76(2.42) -8649.88(1.75)

enabled by PSP alleviates the demand for larger number of samples used in the training objective, making it possible to achieve high quality variational approximations with less samples.
5 DISCUSSION
In this work we introduced VBPI, a general variational framework for Bayesian phylogenetic inference. By combining subsplit Bayesian networks, a recent framework that provides flexible distributions of trees, and efficient structured parameterizations for branch length distributions, VBPI provides competitive performance to MCMC methods with less computational demand. In addition to computational savings, VBPI enjoys a number of other advantages compared to MCMC methods: (i) variational approaches that use SBNs can learn and utilize the current approximation of the posterior of trees and therefore provide guided explorations in the tree space during training; (ii) the variational approximation for the branch lengths can be readily used to compute the marginal likelihood of trees via importance sampling; (iii) the overall variational approximation can be used to compute a lower bound of the evidence (whose tightness can be improved by using more samples and is asymptotically unbiased) and hence provides a useful proxy for Bayesian model selection. We report promising numerical results demonstrating the effectiveness and efficiency of VBPI on a benchmark of real data Bayesian phylogenetic inference problems.
VBPI admits many straightforward extensions: (i) flexible approximations for the branch length distributions. Many appropriate distributions that have non-negative support, such as the Gamma distribution and the Beta-prime distribution, can be used instead of the Log-normal distribution. More generally, we can obtain rich complex distributions from simple distributions using normalizing flows (Rezende & Mohamed, 2015). Other than primary subsplit pairs, more complicated local structures can be associated with the edges of trees and more sophisticated dependency on those parameters (e.g., deep networks) can be adopted to improve the modeling of between-tree variation; (ii) efficient support estimation. Throughout the paper, we assume the support of CPTs is known (e.g., from ultrafast bootstrap ML trees). In practice, alternative bootstrap approaches and more sophisticated compatibility-based or distance-based methods can be helpful to provide fast estimation of the support. (iii) efficient training. We observed that VIMCO provides better overall performance and RWS provides better approximations for trees in high-probability areas. An interesting future research direction is to combine these methods to get the best of both worlds. Note that this is a general subject of variational learning for discrete latent variables, not limited to variational inference for phylogenetic models.
REFERENCES
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859­877, 2017.
Jo¨rg Bornschein and Yoshua Bengio. Reweighted wake-sleep. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.
Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. In ICLR, 2016.

10

Under review as a conference paper at ICLR 2019
Vu Dinh, Arman Bilge, Cheng Zhang, and Frederick A Matsen IV. Probabilistic Path Hamiltonian Monte Carlo. In Proceedings of the 34th International Conference on Machine Learning, pp. 1009­1018, July 2017. URL http://proceedings.mlr.press/v70/dinh17a.html.
Gytis Dudas, Luiz Max Carvalho, Trevor Bedford, Andrew J Tatem, Guy Baele, Nuno R Faria, Daniel J Park, Jason T Ladner, Armando Arias, Danny Asogun, Filip Bielejec, Sarah L Caddy, Matthew Cotten, Jonathan D'Ambrozio, Simon Dellicour, Antonino Di Caro, Joseph W Diclaro, Sophie Duraffour, Michael J Elmore, Lawrence S Fakoli, Ousmane Faye, Merle L Gilbert, Sahr M Gevao, Stephen Gire, Adrianne Gladden-Young, Andreas Gnirke, Augustine Goba, Donald S Grant, Bart L Haagmans, Julian A Hiscox, Umaru Jah, Jeffrey R Kugelman, Di Liu, Jia Lu, Christine M Malboeuf, Suzanne Mate, David A Matthews, Christian B Matranga, Luke W Meredith, James Qu, Joshua Quick, Suzan D Pas, My V T Phan, Georgios Pollakis, Chantal B Reusken, Mariano Sanchez-Lockhart, Stephen F Schaffner, John S Schieffelin, Rachel S Sealfon, Etienne Simon-Loriere, Saskia L Smits, Kilian Stoecker, Lucy Thorne, Ekaete Alice Tobin, Mohamed A Vandi, Simon J Watson, Kendra West, Shannon Whitmer, Michael R Wiley, Sarah M Winnicki, Shirlee Wohl, Roman Wo¨lfel, Nathan L Yozwiak, Kristian G Andersen, Sylvia O Blyden, Fatorma Bolay, Miles W Carroll, Bernice Dahn, Boubacar Diallo, Pierre Formenty, Christophe Fraser, George F Gao, Robert F Garry, Ian Goodfellow, Stephan Gu¨nther, Christian T Happi, Edward C Holmes, Brima Kargbo, Sakoba Ke¨ita, Paul Kellam, Marion P G Koopmans, Jens H Kuhn, Nicholas J Loman, N'faly Magassouba, Dhamari Naidoo, Stuart T Nichol, Tolbert Nyenswah, Gustavo Palacios, Oliver G Pybus, Pardis C Sabeti, Amadou Sall, Ute Stro¨her, Isatta Wurie, Marc A Suchard, Philippe Lemey, and Andrew Rambaut. Virus genomes reveal factors that spread and sustained the ebola epidemic. Nature, April 2017. ISSN 0028-0836, 1476-4687. doi: 10.1038/nature22040. URL http://dx.doi.org/10.1038/nature22040.
Y. Fan, R. Wu, M.-H. Chen, L. Kuo, and P. O. Lewis. Choosing among partition models in Bayesian phylogenetics. Mol. Biol. Evol., 28(1):523­532, 2011.
J. Felsenstein. Inferring Phylogenies. Sinauer Associates, 2nd edition, 2003.
Roberto Feuda, Martin Dohrmann, Walker Pett, Herve´ Philippe, Omar Rota-Stabelli, Nicolas Lartillot, Gert Wo¨rheide, and Davide Pisani. Improved modeling of compositional heterogeneity supports sponges as sister to all other animals. Curr. Biol., 27(24):3864­3870.e4, December 2017. ISSN 0960-9822, 1879-0445. doi: 10.1016/j.cub.2017.11.008. URL http: //dx.doi.org/10.1016/j.cub.2017.11.008.
J. R. Garey, T. J. Near, M. R. Nonnemacher, and S. A. Nadler. Molecular evidence for Acanthocephala as a subtaxon of Rotifera. Mol. Evol., 43:287­292, 1996.
S. B. Hedges, K. D. Moberg, and L. R. Maxson. Tetrapod phylogeny inferred from 18S and 28S ribosomal RNA sequences and review of the evidence for amniote relationships. Mol. Biol. Evol., 7:607­633, 1990.
D. A. Henk, A. Weir, and M. Blackwell. Laboulbeniopsis termitarius, an ectoparasite of termites newly recognized as a member of the Laboulbeniomycetes. Mycologia, 95:561­564, 2003.
G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The wake-sleep algorithm for unsupervised neural networks. Science, 268:1158­1161, 1995.
S. Ho¨hna, T. A. Heath, B. Boussau, M. J. Landis, F. Ronquist, and J. P. Huelsenbeck. Probabilistic graphical model representation in phylogenetics. Syst. Biol., 63:753­771, 2014.
Sebastian Ho¨hna and Alexei J. Drummond. Guided tree topology proposals for Bayesian phylogenetic inference. Syst. Biol., 61(1):1­11, January 2012. ISSN 1063-5157. doi: 10.1093/sysbio/ syr074. URL http://dx.doi.org/10.1093/sysbio/syr074.
J. P. Huelsenbeck and F. Ronquist. MrBayes: Bayesian inference of phylogeny. Bioinformatics, 17: 754­755, 2001.
J. P. Huelsenbeck, F. Ronquist, R. Nielsen, and J. P. Bollback. Bayesian inference of phylogeny and its impact on evolutionary biology. Science, 294:2310­2314, 2001.
11

Under review as a conference paper at ICLR 2019
M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. Machine Learning, 37:183­233, 1999.
T. H. Jukes and C. R. Cantor. Evolution of protein molecules. In H. N. Munro (ed.), Mammalian protein metabolism, III, pp. 21­132, New York, 1969. Academic Press.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014.
D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. The MIT Press, 2009.
C. Lakner, P. van der Mark, J. P. Huelsenbeck, B. Larget, and F. Ronquist. Efficiency of Markov chain Monte Carlo tree proposals in Bayesian phylogenetics. Syst. Biol., 57:86­103, 2008.
Bret Larget. The estimation of tree posterior probabilities using conditional clade probability distributions. Syst. Biol., 62(4):501­511, July 2013. ISSN 1063-5157. doi: 10.1093/sysbio/syt014. URL http://dx.doi.org/10.1093/sysbio/syt014.
Nicolas Lartillot and Herve´ Philippe. A Bayesian mixture model for across-site heterogeneities in the amino-acid replacement process. Mol. Biol. Evol., 21(6):1095­1109, June 2004. ISSN 07374038. doi: 10.1093/molbev/msh112. URL http://dx.doi.org/10.1093/molbev/ msh112.
B. Mau, M. Newton, and B. Larget. Bayesian phylogenetic inference via Markov chain Monte Carlo methods. Biometrics, 55:1­12, 1999.
B. Q. Minh, M. A. T. Nguyen, and A. von Haeseler. Ultrafast approximation for phylogenetic bootstrap. Mol. Biol. Evol., 30:1188­1195, 2013.
A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In Proceedings of The 31th International Conference on Machine Learning, pp. 1791­1799, 2014.
Andriy Mnih and Danilo Rezende. Variational inference for monte carlo objectives. In Proceedings of the 33rd International Conference on Machine Learning, pp. 1791­1799, 2016.
Radford Neal. MCMC using hamiltonian dynamics. In S Brooks, A Gelman, G Jones, and XL Meng (eds.), Handbook of Markov Chain Monte Carlo, Chapman & Hall/CRC Handbooks of Modern Statistical Methods. Taylor & Francis, 2011. ISBN 9781420079425. URL http://books. google.com/books?id=qfRsAIKZ4rIC.
J. W. Paisley, D. M. Blei, and M. I. Jordan. Variational bayesian inference with stochastic search. In Proceedings of the 29th International Conference on Machine Learning ICML, 2012.
R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. In AISTATS, pp. 814­822, 2014.
D. Rezende and S. Mohamed. Variational inference with normalizing flow. In Proceedings of The 32nd International Conference on Machine Learning, pp. 1530­1538, 2015.
F. Ronquist, M. Teslenko, P. van der Mark, D. L. Ayres, A. Darling, S. Hohna, B. Larget, L. Liu, M. A. Shchard, and J. P. Huelsenbeck. MrBayes 3.2: efficient Bayesian phylogenetic inference and model choice across a large model space. Syst. Biol., 61:539­542, 2012.
A. Y. Rossman, J. M. Mckemy, R. A. Pardo-Schultheiss, and H. J. Schroers. Molecular studies of the Bionectriaceae using large subunit rDNA sequences. Mycologia, 93:100­110, 2001.
J. R. Sashank, K. Satyen, and K. Sanjiv. On the convergence of adam and beyond. In ICLR, 2018.
Eric E. Schadt, Janet S. Sinsheimer, and Kenneth Lange. Computational advances in maximum likelihood methods for molecular phylogeny. Genome Res., 8:222­233, 1998. doi: 10.1101/gr.8. 3.222.
12

Under review as a conference paper at ICLR 2019

K. Strimmer and V. Moulton. Likelihood analysis of phylogenetic networks using directed graphical models. Molecular biology and evolution, 17:875­881, 2000.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. The MIT Press, 1998.
M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Maching Learning, 1(1-2):1­305, 2008.
Chris Whidden and Frederick A Matsen IV. Quantifying MCMC exploration of phylogenetic tree space. Syst. Biol., 64(3):472­491, May 2015. ISSN 1063-5157, 1076-836X. doi: 10.1093/sysbio/ syv006. URL http://dx.doi.org/10.1093/sysbio/syv006.
W. Xie, P. O. Lewis, Y. Fan, L. Kuo, and M.-H. Chen. Improving marginal likelihood estimation for Bayesian phylogenetic model selection. Syst. Biol., 60:150­160, 2011.
Z. Yang and B. Rannala. Bayesian phylogenetic inference using DNA sequences: a Markov chain Monte Carlo method. Mol. Biol. Evol., 14:717­724, 1997.
Z. Yang and A. D. Yoder. Comparison of likelihood and Bayesian methods for estimating divergence times using multiple gene loci and calibration points, with application to a radiation of cutelooking mouse lemur species. Syst. Biol., 52:705­716, 2003.
A. D. Yoder and Z. Yang. Divergence datas for Malagasy lemurs estimated from multiple gene loci: geological and evolutionary context. Mol. Ecol., 13:757­773, 2004.
Cheng Zhang and Frederick A Matsen IV. Generalizing tree probability estimation via Bayesian networks. arXiv preprint arXiv:1805.07834, 2018.
N. Zhang and M. Blackwell. Molecular phylogeny of dogwood anthracnose fungus (Discula destructiva) and the Diaporthales. Mycologia, 93:355­365, 2001.

A GRADIENT DERIVATION FOR THE MULTI-SAMPLE OBJECTIVES

In this section we will derive the gradient for the multi-sample objectives introduced in section 3. We start with the lower bound



LK (, )

=

1 EQ,( 1:K , q1:K ) log  K

K j=1

p(Y | j, qj)p( j, qj) Q( j)Q(qj| j) 



1 = EQ,( 1:K , q1:K ) log  K

K

f,( j , qj ) .

j=1

Using the product rule and noting that  log f,( j, qj) = - log Q( j),



 Lk (,

)

=

EQ, ( 1:K ,

q1:K ) 

log

1 K

K

f,( j , qj ) +

j=1

EQ, ( 1:K ,

q1:K )

K j=1

Q( j ) Q( j )

log

1 K

K

f,( i, qi)

i=1

K
= EQ, ( 1:K , q1:K )
j=1

f,( j , qj )

K i=1

f,

(

i,

qi)



log

f, (

j,

qj

)+

K

EQ, ( 1:K , q1:K )

log

j=1

1 K

K

f,( i, qi)

i=1

 log Q( j)

K

= EQ, ( 1:K , q1:K )

L^K (, ) - w~j  log Q( j).

j=1

13

Under review as a conference paper at ICLR 2019

This gives the naive gradient of the lower bound w.r.t. .

Using the reparameterization trick, the lower bound has the form



LK (, )

=

EQ,

( 1:K ,

1 1:K ) log  K

K j=1

p(Y

| j, g( j| j))p( j, g( j| j)) Q( j )Q(g( j | j )| j ) 



1 = EQ, ( 1:K , 1:K ) log  K

K

f,( j , g( j | j ))

j=1

Since  is not involved in the distribution with respect to which we take expectation,



LK (, )

=

EQ,

( 1:K ,

1:K ) 

1 log  K

K

f,( j , g( j | j ))

j=1

K
= EQ, ( 1:K , 1:K )
j=1

f,( j , g( j |

K i=1

f,

(

i,

g

(

j i

)) | i

))



log

f,

(

j

,

g

(

j | j ))

K

= EQ, ( 1:K , 1:K )

w~j  log f,( j , g( j | j )).

j=1

Next, we derive the gradient of the multi-sample likelihood objective used in RWS

L~(, ) = Ep(,q|Y ) log Q,(, q).

Again, p(, q|Y ) is independent of , , and we have

L~(, ) = Ep(,q|Y ) log Q,(, q) p(, q|Y )
= EQ,(,q) Q( )Q(q| )  log Q,(, q) 1
= p(Y ) EQ,(,q)f,(, q) log Q( )

K j=1

f,( j

k i=1

f,

, qj) ( i, qi)



log

Q(

j

)

with

j, qj

iid Q,(,

q).

K
= w~j log Q( j)
j=1

The second to last step uses self-normalized importance sampling with K samples. L~(, ) can be computed in a similar way.

B THE VARIATIONAL BAYESIAN PHYLOGENETIC INFERENCE ALGORITHM

Algorithm 1 The variational Bayesian phylogenetic inference (VBPI) algorithm.
1: ,   Initialize parameters
2: while not converged do 3:  1, . . . ,  K  Random samples from the current approximating tree distribution Q( ) 4: 1, . . . , K  Random samples from the multivariate standard normal distribution N (0, I) 5: g  ,LK (, ;  1:K , 1:K ) (Use any gradient estimator from section 3.2) 6: ,   Update parameters using gradients g (e.g. SGA)
7: end while
8: return , 

14

Under review as a conference paper at ICLR 2019

C IMPORTANCE SAMPLING FOR PHYLOGENETIC INFERENCE VIA VARIATIONAL APPROXIMATIONS

In this section, we provide a detailed importance sampling procedure for marginal likelihood estimation for phylogenetic inference based on the variational approximations provided by VBPI.

C.1 ESTIMATING MARGINAL LIKELIHOOD OF TREES

For each tree  that is covered by the subsplit support,

Q(q| ) =

pLognormal (qe | µ(e,  ), (e,  ))

eE( )

can provide accurate approximation to the posterior of branch lengths on  , where the mean and variance parameters µ(e,  ), (e,  ) are gathered from the structured variational parameters  as introduced in section 3.1. Therefore, we can estimate the marginal likelihood of  using importance sampling with Q(q| ) being the importance distribution as follows

p(Y |, q)p(q) p(Y | ) = EQ(q|) Q(q| )

1 M

M p(Y |, qj)p(qj) j=1 Q(qj | )

with qj iid Q(q| )

C.2 ESTIMATING MODEL EVIDENCE

Similarly, we can estimate the marginal likelihood of the data as follows

p(Y |, q)p(, q) p(Y ) = EQ, (, q) Q( )Q(q| )

1 K p(Y | j, qj)p( j, qj)

K
j=1

Q( j )Q(qj | j )

with

 j , qj iid Q,(, q).

In our experiments, we use K = 1000. When taking a log transformation, the above Monte Carlo estimate is no longer unbiased (for the evidence log p(Y )). Instead, it can be viewed as one sample Monte Carlo estimate of the lower bound

LK (, ) = EQ,( 1:K , q1:K ) log

1 K p(Y | i, qi)p( i, qi)

K
i=1

Q( i)Q(qi| i)

 log p(Y )

(12)

whose tightness improves as the number of samples K increases. Therefore, with a sufficiently large K, we can use the lower bound estimate as a proxy for Bayesian model selection.

15

