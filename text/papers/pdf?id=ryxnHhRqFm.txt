Under review as a conference paper at ICLR 2019
GLOBAL-TO-LOCAL MEMORY POINTER NETWORKS FOR TASK-ORIENTED DIALOGUE
Anonymous authors Paper under double-blind review
ABSTRACT
End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share an external knowledge. The encoder encodes dialogue history, modifies global contextual representation that is shared with the decoder, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers which points to specific entries in the external knowledge. We empirically show that our model can improve copy accuracy and mitigate the common outof-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue and human-human Stanford Multi-domain Dialogue datasets on automatic and human evaluation.
1 INTRODUCTION
Task-oriented dialogue systems aim to achieve specific user goals such as restaurant reservation or navigation inquiry within a limited dialogue turns via natural language. Traditional pipeline solutions are composed of natural language understanding, dialogue management and natural language generation (Young et al., 2013; Wen et al., 2017), where each module is designed separately and expensively. In order to reduce human effort and scale up between domains, end-to-end dialogue systems, which input plain text and directly output system responses, have shown promising results based on recurrent neural networks (Zhao et al., 2017; Lei et al., 2018) and memory networks (Sukhbaatar et al., 2015). These approaches have the advantages that the dialogue states are latent without hand-crafted labels and eliminate the needs to model the dependencies between modules and interpret knowledge bases (KB) manually.
However, despite the improvement by modeling KB with memory network (Bordes & Weston, 2017; Madotto et al., 2018), end-to-end systems usually suffer from effectively incorporating external KB into the system response generation. The main reason is that a large, dynamic KB is equal to a noisy input and hard to encode and decode, which makes the generation unstable. Different from chit-chat scenario, this problem is especially harmful in task-oriented one, since the information in KB is usually the expected entities in the response. For example, in Table 1 the driver will expect to get the correct address to the gas station other than a random place such as a hospital. Therefore, pointer networks (Vinyals et al., 2015) or copy mechanism (Gu et al., 2016) is crucial to successfully generate system responses because directly copying essential words from the input source to the output not only reduces the generation difficulty, but it is also more like a human behavior. For example, in Table 1, when human want to reply others the Valero's address, they will need to "copy" the information from the table to their response as well.
Therefore, in the paper, we propose the global-to-local memory pointer (GLMP) networks, which is composed of a global memory encoder, a local memory decoder, and a shared external knowledge. Unlike existing approaches with copy ability (Gulcehre et al., 2016; Gu et al., 2016; Eric & Manning, 2017; Madotto et al., 2018), which do not include global contextual representation between encoder and decoder, and the only information passed to decoder is the encoder hidden states, our model shares the external knowledge and leverages the encoder and the external knowledge to learn a
1

Under review as a conference paper at ICLR 2019

Table 1: An in-car assistant example on the navigation domain. The left part is the KB information and the right part is the conversation between a driver and our system.

Point of interest (poi) Toms house Coupa Panda express Stanford express care Valero Starbucks

Distance 3 miles 2 miles 2 miles 5 miles 4 miles 1 miles

Traffic heavy moderate no no heavy heavy

Poi type friend's house coffee or tea place Chinese restaurant hospital gas station coffee or tea place

Address 580 Van Ness Ave 394 Van Ness Ave 842 Arrowhead Way 214 El Camino Real 200 Alester Ave 792 Bedoin St

Driver System
Driver System
Driver

I need gas GLMP: There is a gas station locally Valero is 4 miles away Gold: Valero is 4 miles away What is the address ? GLMP: Valero is located at 200 Alester Ave Gold: Valero is at 200 Alester Ave Thank you!

global memory pointer. It is then propagated to the decoder and modifies the external knowledge, filtering words that are not necessary for copying. Afterward, instead of generating system responses directly, the local memory decoder first uses a sketch RNN to obtain sketch responses without slot values but sketch tags, which can be considered as learning a latent dialogue management to generate dialogue action template. Then the decoder generates local memory pointers to copy words from external knowledge and replace those sketch tags.
We empirically show that GLMP can achieve superior performance using the combination of global and local memory pointers. In the simulated bAbI dialogue dataset, GLMP achieves 90.5% taskcompletion rate and improves the out-of-vocabulary (OOV) tasks by 19.1% while compared to existing end-to-end approaches. In the human-human dialogue dataset, GLMP is able to surpass the previous state of the art on both automatic and human evaluation, which further confirms the effectiveness of our double pointers usage.

2 GLMP MODEL
Our model 1 is composed of three parts: global memory encoder, external knowledge, and local memory decoder, as shown in Figure 1(a). The dialogue history X = (x1, . . . , xn) and the KB information B = (b1, . . . , bl) are the input, and the system response Y = (y1, . . . , ym) is the expected output. First, the global memory encoder uses a context RNN to encode dialogue history and writes its hidden states into the external knowledge. Then the last hidden state is used to read the external knowledge and generate the global memory pointer at the same time. On the other hand, during the decoding stage, the local memory decoder first generates sketch responses by a sketch RNN. Then the global memory pointer and the sketch RNN hidden state are passed to the external knowledge as a filter and a query. The local memory pointer returns from the external knowledge can copy plain text from the external knowledge to replace the sketch tags and obtain the final system response.
2.1 EXTERNAL KNOWLEDGE
Our external knowledge contains the global contextual representation that is shared with the encoder and the decoder. To incorporate external knowledge into a learning framework, end-to-end memory networks (MN) are used to store word-level information for both structural KB (KB memory) and temporal-dependent dialogue history (dialogue memory), as shown in Figure 1(b). In addition, the MN is well-known for its multiple hop reasoning ability (Sukhbaatar et al., 2015), which is appealing to strengthen copy mechanism.
Global contextual representation. In the KB memory module, each element bi  B is represented in the triplet format as (Subject, Relation, Object) structure, which is a common format used to represent KB nodes (Miller et al., 2016; Eric et al., 2017). For example, the KB in the Table 1 will be denoted as {(Toms house, distance, 3 miles), ..., (Starbucks, address, 792 Bedoin St)}. On the other hand, the dialogue context X is stored in the dialogue memory module, where the speaker and temporal encoding are included as in Bordes & Weston (2017) like a triplet format. For instance, the first utterance from the driver in the Table 1 will be denoted as {($user, turn1, I), ($user, turn1, need), ($user, turn1, gas)}. For the two memory modules, a bag-of-word representation is used as the memory embeddings. During the inference time, we copy the object word once a memory position is pointed to, for example, 3 miles will be copied if the triplet (Toms house, distance, 3 miles) is selected. We denote Object(.) function as getting the object word from a triplet.
1The source code will be released soon.

2

Under review as a conference paper at ICLR 2019

(a) Block diagram

(b) External knowledge

Figure 1: The proposed (a) global-to-local memory pointer networks for task-oriented dialogue systems and the (b) external knowledge architecture.

Knowledge read and write. Our external knowledge is composed of a set of trainable embedding matrices C = (C1, . . . , CK+1), where Ck  R|V |×demb , K is the maximum memory hop in the MN, |V | is the vocabulary size and demb is the embedding dimension. We denote memory in the external knowledge as M = [B; X] = (m1, . . . , mn+l), where mi is one of the triplet components mentioned. To read the memory, the external knowledge needs a initial query vector q1. Moreover,
it can loop over K hops and computes the attention weights at each hop k using

pik = Softmax((qk)T cki ),

(1)

where cki = B(Ck(mi))  Rdemb is the embedding in ith memory position using the embedding matrix Ck, qk is the query vector for hop k, and B(.) is the bag-of-word function. Note that pk 
Rn+l is a soft memory attention that decides the memory relevance with respect to the query vector. Then, the model reads out the memory ok by the weighted sum over ck+1 and update the query
vector qk+1. Formally,

ok = pikcki +1, qk+1 = qk + ok.

(2)

i

2.2 GLOBAL MEMORY ENCODER

In Figure 2(a), a context RNN is used to model the sequential dependency and encode the context X. Then the hidden states are written into the external knowledge as shown in Figure 1(b). Afterward, the last encoder hidden state serves as the query to read the external knowledge and get two outputs, the global memory pointer and the memory readout. Intuitively, since it is hard for MN architectures to model the dependencies between memories (Wu et al., 2018), which is a serious drawback especially in conversational related tasks, writing the hidden states to the external knowledge can provide sequential and contextualized information, and the common OOV challenge can be mitigated as well. In addition, using the encoded dialogue context as a query can encourage our external knowledge to read out memory information related to the hidden dialogue states or user intention. Moreover, the global memory pointer that learns a global memory distribution is passed to the decoder along with the encoded dialogue history and KB information.

Context RNN. A bi-directional gated recurrent unit (GRU) (Chung et al., 2014) is used to encode dialogue history into the hidden states H = (h1e, . . . , hne ), and the last hidden state hen is used to query the external knowledge as the encoded dialogue history. In addition, the hidden states H are
written into the dialogue memory module in the external knowledge by summing up the original
memory representation with the corresponding hidden states. In formula,

cik = cki + hemi if mi  X and k  [1, K + 1],

(3)

Global memory pointer. Global memory pointer G = (g1, . . . , gn+l) is a vector containing real
values between 0 and 1. Unlike conventional attention mechanism that all the weights sum to one, each element in G is an independent probability. We first query the external knowledge using hne

3

Under review as a conference paper at ICLR 2019

(a) Global memory encoder

(b) Local memory decoder Figure 2: The proposed (a) global memory encoder and the (b) local memory decoder architecture.

until the last hop, and instead of applying the Softmax function as in (1), we perform an inner product followed by the Sigmoid function. The memory distribution we obtained is the global memory pointer G, which is passed to the decoder. To further strengthen the global pointing ability, we add an auxiliary loss to train the global memory pointer as a multi-label classification task. We show in the ablation study that adding this additional supervision does improve the performance. Lastly, the memory readout qK+1 is used as the encoded KB information.
In the auxiliary task, we define the label Glabel = (g1l , . . . , gnl +l) by checking whether the object words in the memory exists in the expected system response Y . Then the global memory pointer is trained using binary cross-entropy loss Lossg between G and Glabel. In formula,

gi = Sigmoid((qK )T ciK ),

gil =

1 0

if Object(mi)  Y , otherwise

Lossg = -

n+l i=1

[gil

×

log

gi

+

(1

-

gil)

×

log

(1

-

gi)].

(4)

2.3 LOCAL MEMORY DECODER

Given the encoded dialogue history hen, the encoded KB information qK+1, and the global memory pointer G, our local memory decoder first initializes its sketch RNN using the concatenation of hne and qK+1, and generates a sketch response that excludes slot values but includes the sketch tags. For example, sketch RNN will generate "@poi is @distance away", instead of "Starbucks is 1 mile away." At each decoding time step, the hidden state of the sketch RNN is used for two purposes: 1) predict the next token in vocabulary, which is the same as standard sequence-to-sequence (S2S) learning; 2) serve as the vector to query the external knowledge. If a sketch tag is generated, the global memory pointer is passed to the external knowledge, and the expected output word will be picked up from the local memory pointer. Otherwise, the output word is the word that generated by the sketch RNN. For example in Figure 2(b), a poi tag is generated at the first time step, therefore, the word Starbucks is picked up from the local memory pointer as the system output word.

Sketch RNN. We use a GRU to generate a sketch response Y s = (y1s, . . . , yms ) without real slot values. The sketch RNN learns to generate a dynamic dialogue action template based on the encoded dialogue (hen) and KB information (qK+1). At each decoding time step t, the sketch RNN hidden state hdt and its output distribution Ptvocab are defined as

htd = GRU(C1(y^ts-1), htd-1), Ptvocab = Softmax(W htd)

(5)

We use the standard cross-entropy loss to train the sketch RNN, we define Lossv as.

m
Lossv = - log(Ptvocab(yts)).
t=1

(6)

4

Under review as a conference paper at ICLR 2019

We replace the slot values in Y into sketch tags based on the provided entity table. The sketch tags ST are all the possible slot types that start with a special token, for example, @address stands for all the addresses and @distance stands for all the distance information.

Local memory pointer. Local memory pointer L = (L1, . . . , Lm) contains a sequence of pointers. At each time step t, the global memory pointer G first modify the global contextual representation
using its attention weights,

cki = cki × gi, i  [1, n + l] and k  [1, K + 1],

(7)

and then the sketch RNN hidden state htd queries the external knowledge. The memory attention in the last hop is the corresponding local memory pointer Lt, which is represented as the memory distribution at time step t. To train the local memory pointer, a supervision on top of the last hop
memory attention in the external knowledge is added. We first define the position label of local memory pointer Llabel at the decoding time step t as

Lltabel =

max(z) n+l+1

if z s.t. yt = Object(mz), otherwise.

(8)

The position n+l+1 is a null token in the memory that allows us to calculate loss function even if yt does not exist in the external knowledge. Then, the loss between L and Llabel is defined as

m
Lossl = - log(Lt(Ltlabel)).
t=1

(9)

Furthermore, a record R  Rn+l is utilized to prevent from copying same entities multiple times. All the elements in R are initialized as 1 in the beginning. During the decoding stage, if a memory position has been pointed to, its corresponding position in R will decay with a learned scalar r. That is, the global contextual representation is softly masked out if the corresponding token has been copied. During the inference time, y^t is defined as

y^t =

arg max(Ptvocab) Object(marg max(Lt R))

if arg max(Ptvocab)  ST, otherwise,

(10)

where is the element-wise multiplication. Lastly, all the parameters are jointly trained by minimizing the sum of three losses:

Loss = Lossg + Lossv + Lossl

(11)

3 EXPERIMENTS
3.1 DATASETS
We use two public multi-turn task-oriented dialogue datasets to evaluate our model: the bAbI dialogue (Bordes & Weston, 2017) and Stanford multi-domain dialogue (SMD) (Eric et al., 2017). The bAbI dialogue includes five simulated tasks in the restaurant domain. Task 1 to 4 are about calling API calls, modifying API calls, recommending options, and providing additional information, respectively. Task 5 is the union of tasks 1-4. There are two test sets for each task: one follows the same distribution as the training set and the other has OOV entity values. On the other hand, SMD is a human-human, multi-domain dialogue dataset. It has three distinct domains: calendar scheduling, weather information retrieval, and point-of-interest navigation. The key difference between these two datasets is, the former has longer dialogue turns but the regular user and system behaviors, the latter has few conversational turns but variant responses, and the KB information is much more complicated. The dataset statistics and the training details are reported in the Appendix.

3.2 RESULTS
bAbI Dialogue. In Table 2, we follow Bordes & Weston (2017) to compare the performance based on per-response accuracy and task-completion rate. Note that for utterance retrieval methods, such as QRN, MN, and GMN, cannot correctly recommend options (T3) and provide additional information (T4), and a poor generalization ability is observed in OOV setting, which has around

5

Under review as a conference paper at ICLR 2019

Table 2: Per-response accuracy and completion rate (in the parentheses) on bAbI dialogues. GLMP achieves the least out-of-vocabulary performance drop. Baselines are reported from QRN (Seo et al., 2017), MN (Bordes & Weston, 2017), GMN (Liu & Perez, 2017), Ptr-Unk (Gulcehre et al., 2016), and Mem2Seq (Madotto et al., 2018).

Task T1 T2 T3 T4 T5
T1 oov T2 oov T3 oov T4 oov T5 oov

QRN 99.4 (-) 99.5 (-) 74.8 (-) 57.2 (-) 99.6 (-) 83.1 (-) 78.9 (-) 75.2 (-) 56.9 (-) 67.8 (-)

MN 99.9 (99.6) 100 (100) 74.9 (2.0) 59.5 (3.0) 96.1 (49.4)
72.3 (0) 78.9 (0) 74.4 (0) 57.6 (0) 65.5 (0)

GMN 100 (100) 100 (100) 74.9 (0) 57.2 (0) 96.3 (52.5) 82.4 (0) 78.9 (0) 75.3 (0) 57.0 (0) 66.7 (0)

S2S+Attn 100 (100) 100 (100) 74.8 (0) 57.2 (0) 98.4 (87.3) 81.7 (0) 78.9 (0) 75.3 (0) 57.0 (0) 65.7 (0)

Ptr-Unk 100 (100) 100 (100) 85.1 (19.0) 100 (100) 99.4 (91.5) 92.5 (54.7) 83.2 (0) 82.9 (13.4) 100 (100) 73.6 (0)

Mem2Seq 100 (100) 100 (100) 94.7 (62.1) 100 (100) 97.9 (69.6) 94.0 (62.2) 86.5 (12.4) 90.3 (38.7) 100 (100) 84.5 (2.3)

GLMP K1 100 (100) 100 (100) 96.3 (75.6) 100 (100) 99.3 (90.5) 100 (100) 100 (100) 95.5 (65.7) 100 (100) 90.7 (17.2)

GLMP K3 100 (100) 100 (100) 96.0 (69.4) 100 (100) 99.0 (86.5) 100 (100) 100 (100) 96.7 (72.9) 100 (100) 91.0 (17.7)

GLMP K6 100 (100) 100 (100) 96.0 (68.7) 100 (100) 99.2 (89.7) 99.3 (95.9) 99.4 (94.6) 95.9 (67.7) 100 (100) 91.8 (21.4)

Table 3: In SMD dataset, our model achieves highest BLEU score and entity F1 score over baselines, including previous state-of-the-art result from Madotto et al. (2018). (Models with * are reported from Eric et al. (2017), where the problem is simplified to the canonicalized forms.)

Automatic Evaluation

Rule-Based* KVR* S2S S2S + Attn Ptr-Unk Mem2Seq GLMP H1 GLMP H3 GLMP H6

BLEU

6.6

13.2 8.4

9.3

8.3

12.6

13.85

13.61

14.12

Entity F1

43.8

48.0 10.3 19.9 22.7 33.4

53.11

55.38

53.5

Schedule F1

61.3

62.9 9.7

23.4

26.9 49.3

66.52

68.26

69.18

Weather F1

39.5

47.0 14.1 25.6

26.7 32.8

51.59

52.61

58.03

Navigation F1

40.4

41.3 7.0

10.8

14.9 20.0

46.21

49.47

41.93

Appropriate Humanlike

Mem2Seq 3.89 3.80

Human Evaluation GLMP 4.15 4.02

Human 4.6 4.54

30% performance difference in Task 5. Although previous generation-based approaches (Ptr-Unk, Mem2Seq) have mitigated the gap by incorporating copy mechanism, the simplest cases such as generating and modifying API calls (T1, T2) still face a 6-17% OOV performance drop. On the other hand, GLMP achieves a highest 90.5% task-completion rate in full dialogue task and surpasses other baselines by a big margin especially in the OOV setting. No per-response accuracy loss for T1, T2, T4 using only the single hop, and only decreases 7-9% in task 5.
Stanford Multi-domain Dialogue. For human-human dialogue scenario, we follow previous dialogue works (Eric et al., 2017; Zhao et al., 2017; Madotto et al., 2018) to evaluate our system on two automatic evaluation metrics, BLEU and entity F1 score 2. As shown in Table 3, GLMP achieves a highest 14.12 BLEU and 55.38% entity F1 score, which is a slight improvement in BLEU but a huge gain in entity F1. In fact, for unsupervised evaluation metrics in task-oriented dialogues, we argue that the entity F1 might be a more comprehensive evaluation metric than per-response accuracy or BLEU, as shown in Eric et al. (2017) that humans are able to choose the right entities but have very diversified responses. Note that the results of rule-based and KVR are not directly comparable because they simplified the task by mapping the expression of entities to a canonical form using named entity recognition and linking 3.
Moreover, human evaluation of the generated responses is reported. We compare our work with previous state-of-the-art model Mem2Seq 4 and the original dataset responses as well. We randomly select 200 different dialogue scenarios from the test set to evaluate three different responses. Amazon Mechanical Turk is used to evaluate system appropriateness and human-likeness on a scale from 1 to 5. As the results shown in Table 3, we see that GLMP outperforms Mem2Seq in both measures, which is coherent to previous observation. We also see that human performance on this assessment sets the upper bound on scores, as expected. More details about the human evaluation are reported in the Appendix.
Ablation Study. The contributions of the global memory pointer G and the memory writing of dialogue history H are shown in Table 4. We compare the results using GLMP with K = 1 in
2BLEU: multi-bleu.perl script; Entity F1: Micro-average over responses. 3For example, they compared in "@poi is @poi distance away," instead of "Starbucks is 1 mile away." 4Mem2Seq code is released and we achieve similar results stated in the original paper.

6

Under review as a conference paper at ICLR 2019

Table 4: Ablation study using single hop model.

GLMP GLMP w/o H GLMP w/o G

T1 100 (-) 90.4 (-9.6) 100 (-0)

bAbI Dialogue OOV

Per-response Accuracy

T2 T3 T4

100 (-)

95.5 (-) 100 (-)

85.6 (-14.4) 95.4 (-0.1) 100 (-0)

91.7 (-8.3) 95.5 (-0) 100 (-0)

T5 90.7 (-) 85.2 (-5.5) 92.5 (+1.8)

SMD Entity F1
All 53.11 (-) 51.40 (-1.71) 44.82 (-8.29)

[sigon[at_ofwarnm_ae[nt[r[h[[c[sdt6jhahe_a[_[3emgie_cc3_8carvko8p[aol_vssrue3aracohy_[nkmn_emh6no_ute][rto5]rpe1nthyr[[ypu7]nppnan_ie_]ovs_timopfoso[[rn[[_ifpeeahr[tsi_o8_i5[hgicmeo[[cr]om[ttcogtli66f4s5oe_h_r]srrihrpmagoai47ese_ac_ttsii[tstioneser_mtdfenm_n]ysf1]r[[ehof]_fisnea8al7_neo_ei_pid]]osictcifc[aasdfla38lbrt_lsoroimthpfeep]pe][eead[svstvi0iea3ap5i[eepgcsrorostt_tr__etdean__tr_ya]a]r__e]na]riarihitir]arra]nnaooa_miefyn_ivpddseherpodnflnra[sgof_anftspem_ciyossisoc[f5uoiffittrcgds_rtdssil6seoei_t_soraiamics_e_mac]ctae[_tdta_tdaht_mradelf_6a_asue_tssssrtntrnurmferfeooiyin][aiy]rsn_i_iiitiefnrnenoa]irggggchuadiiptasmplcfcphihdsaoctsfrfe_]ofoooolaofsteoeneeoeese_]isoeeninfprsipenncnndmstrirltlseiaat]]trclnttttea]ttttta_aaad]]]rahhhhnennnaoooov_ve]pstdta____trafipdeyteeeoeoooynwywww]]reffffoniaff]dsaaaaapo______i___i__fcspdinnfnndscitaccc_ctttttttdfrrrrioeecstaoi___f___rrrrrrdrmmmmtlllldhsra_iaaaaaaeeeaeitaaaaynicjjjjrent_dieyaanaagffffffmfmmmeeeeepnnnnna_cstffffffffaprcccccairrsrryoeediidddiiiiinfsenccecccceeecvkskksksseoessp____csqf____nnnnccccssccssy_cccc4165556oeesttttmmmmu____hhhhhh_tstttoooaoaaap_______t____hhhhtiteeeeeehhhhaammmmmmumuuuiiittirclaaaarhhhhagwo_o__oo_ehhhvvvvovvooooakvknnnnptrrrrotoooppppaoyuuuuaiiiiiiirrrrrreeuetghhakmkkkmmmifeiolllllllttttittttnnoaaaoaoohooooofaeeeeeesesssscerrerererrrrteaatteseeeiiinnnnnnnnennngdduedeeeeeeeeeeeeeooyayyyklscssssss?sstttt!!ttttlllll addwrethhsaie?sst

G

Gold: 783_arcDFaiendlaeialx_Gipceal nilsizeetrhadetiGoaendn:decrrheaestsviorfonon:r @cishpeaovtir7ios8na3gt_aa@rsca_asddtdairatei_ospnsl

Pointer w/o G

Final Pointer

0123

0123

Figure 3: Memory attention visualization in the SMD navigation domain. Left column is the global

memory pointer G, middle column is the memory pointer without global weighting, and the right

column is the final memory pointer.

bAbI OOV setting and SMD. GLMP without H means that the context RNN in the global memory encoder does not write the hidden states into the external knowledge. As one can observe, our model without H has 5.5% more loss in the full dialogue task. On the other hand, GLMP without G means that we do not use the global memory pointer to modify the external knowledge, and an 8.29% entity F1 drop can be observed in SMD dataset. Note that a 1.8% increase can be observed in task 5, it suggests that the use of global memory pointer may impose a wrong prior probability before decoding in the OOV setting. However, in most of the cases, our global memory pointer still improves the performance.
Visualization and Qualitative Evaluation. Analyzing the attention weights has been frequently used to interpret deep learning models. In Figure 3, we show the attention vector in the last hop for each generation time step. Y-axis is the external knowledge that we can copy, including the KB information and the dialogue history. Based on the question "what is the address?" asked by the driver in the last turn, the gold answer and our generated response are on the top, and the global memory pointer G is shown in the left column. One can observe that in the right column, the final memory pointer successfully copy the entity chevron in step 0 and its address 783 Arcadia Pl in step 3 to fill in the sketch utterance. On the other hand, the memory attention without global weighting is

7

Under review as a conference paper at ICLR 2019
reported in the middle column. One can find that even if the attention weights focus on several point of interests and addresses in step 0 and step 3, the global memory pointer can mitigate the issue as expected. More dialogue visualization and generated results including several negative examples and error analysis are reported in the Appendix.
4 RELATED WORKS
Task-oriented dialogue systems. Machine learning based dialogue systems are mainly explored by following two different approaches: modularized and end-to-end. For the modularized systems (Williams & Young, 2007; Wen et al., 2017), a set of modules for natural language understanding (Young et al., 2013; Chen et al., 2016), dialogue state tracking (Lee & Stent, 2016; Zhong et al., 2018), dialogue management (Su et al., 2016), and natural language generation (Sharma et al., 2016) are used. These approaches achieve good stability via combining domain-specific knowledge and slot-filling techniques, but additional human labels are needed. On the other hand, end-to-end approaches have shown promising results recently. Some works view the task as a next utterance retrieval problem, for examples, recurrent entity networks share parameters between RNN (Wu et al., 2017), query reduction networks modify query between layers (Seo et al., 2017), and memory networks (Bordes & Weston, 2017; Liu & Perez, 2017; Wu et al., 2018) perform multi-hop design to strengthen reasoning ability. In addition, some approaches treat the task as a sequence generation problem. Lei et al. (2018) incorporates explicit dialogue states tracking into a delexicalized sequence generation. Serban et al. (2016); Zhao et al. (2017) use recurrent neural networks to generate final responses and achieve good results as well. Although it may increase the search space, these approaches can encourage more flexible and diverse system responses by generating utterances token-by-token.
Pointer network. Vinyals et al. (2015) uses attention as a pointer to select a member of the input source as the output. Such copy mechanisms have also been used in other natural language processing tasks, such as question answering (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and text summarization (See et al., 2017). In task-oriented dialogue tasks, Eric & Manning (2017) first demonstrated the potential of the copy-augmented Seq2Seq model, which shows that generationbased methods with simple copy strategy can surpass retrieval-based ones. Later, Eric et al. (2017) augmented the vocabulary distribution by concatenating KB attention, which at the same time increases the output dimension. Recently, Madotto et al. (2018) combines end-to-end memory network into sequence generation, which shows that the multi-hop mechanism in MN can be utilized to improve copy attention. These models outperform utterance retrieval methods by copying relevant entities from the KBs.
Others. Zhao et al. (2017) proposes entity indexing and Wu et al. (2018) introduces recorded delexicalization to simplify the problem by record entity tables manually. In addition, our approach that utilizing the recurrent structure to query external memory can be viewed as the memory controller in Memory augmented neural networks (MANN) (Graves et al., 2014; 2016). Similarly, memory encoders have been used in neural machine translation (Wang et al., 2016) and meta-learning applications (Kaiser et al., 2017). However, different from other models that use a single matrix representation for reading and writing, GLMP leverages end-to-end memory networks to perform multiple hop attention, which is similar to the stacking self-attention strategy in the Transformer (Vaswani et al., 2017).
5 CONCLUSION
In the work, we present an end-to-end trainable model called global-to-local memory pointer networks for task-oriented dialogues. The global memory encoder and the local memory decoder are designed to incorporate the shared external knowledge into the learning framework. We empirically show that the global and the local memory pointer are able to effectively produce system responses even in the out-of-vocabulary scenario, and visualize how global memory pointer helps as well. As a result, our model achieves state-of-the-art results in both the simulated and the human-human dialogue datasets, and holds potential for extending to other tasks such as question answering and text summarization.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Antoine Bordes and Jason Weston. Learning end-to-end goal-oriented dialog. International Conference on Learning Representations, abs/1605.07683, 2017.
Yun-Nung Chen, Dilek Hakkani-Tu¨r, Jianfeng Gao, and Li Deng. End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding. 2016.
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. NIPS Deep Learning and Representation Learning Workshop, 2014.
Mostafa Dehghani, Sascha Rothe, Enrique Alfonseca, and Pascal Fleury. Learning to attend, copy, and generate for session-based query suggestion. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM '17, pp. 1747­1756, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4918-5. doi: 10.1145/3132847.3133010. URL http://doi. acm.org/10.1145/3132847.3133010.
Mihail Eric and Christopher Manning. A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 468­473, Valencia, Spain, April 2017. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/E17-2075.
Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value retrieval networks for task-oriented dialogue. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pp. 37­49. Association for Computational Linguistics, 2017. URL http://aclweb.org/anthology/W17-5506.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, 2014.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwin´ska, Sergio Go´mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538 (7626):471­476, 2016.
Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1631­1640, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.aclweb.org/ anthology/P16-1154.
Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointing the unknown words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 140­149, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ P16-1014.
Shizhu He, Cao Liu, Kang Liu, and Jun Zhao. Generating natural answers by incorporating copying and retrieving mechanisms in sequence-to-sequence learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 199­ 208, Vancouver, Canada, July 2017. Association for Computational Linguistics. URL http: //aclweb.org/anthology/P17-1019.
Lukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events. International Conference on Learning Representations, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.
Sungjin Lee and Amanda Stent. Task lineages: Dialog state tracking for flexible interaction. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pp. 11­21, 2016.
9

Under review as a conference paper at ICLR 2019
Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren, Xiangnan He, and Dawei Yin. Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1437­1447, 2018.
Fei Liu and Julien Perez. Gated end-to-end memory networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pp. 1­10, Valencia, Spain, April 2017. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/E17-1001.
Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1468­1478. Association for Computational Linguistics, 2018. URL http://aclweb.org/ anthology/P18-1136.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. International Conference on Learning Representations, 2017.
Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1400­1409, Austin, Texas, November 2016. Association for Computational Linguistics. URL https://aclweb. org/anthology/D16-1147.
Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1073­1083, Vancouver, Canada, July 2017. Association for Computational Linguistics. URL http://aclweb.org/ anthology/P17-1099.
Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Query-reduction networks for question answering. International Conference on Learning Representations, 2017.
Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau. Building end-to-end dialogue systems using generative hierarchical neural network models. In AAAI, pp. 3776­3784, 2016.
Shikhar Sharma, Jing He, Kaheer Suleman, Hannes Schulz, and Philip Bachman. Natural language generation in dialogue using lexicalized and delexicalized data. International Conference on Learning Representations, 2016.
Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, and Steve Young. On-line active reward learning for policy optimisation in spoken dialogue systems. Association for Computational Linguistics, 2016.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440­2448, 2015.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000­6010, 2017.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2692­2700. Curran Associates, Inc., 2015. URL http://papers.nips. cc/paper/5866-pointer-networks.pdf.
Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu. Memory-enhanced decoder for neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 278­286, Austin, Texas, November 2016. Association for Computational Linguistics. URL https://aclweb.org/anthology/D16-1027.
10

Under review as a conference paper at ICLR 2019
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina Maria Rojas-Barahona, Pei hao Su, Stefan Ultes, David Vandyke, and Steve J. Young. A network-based end-to-end trainable task-oriented dialogue system. In EACL, 2017.
Jason D Williams and Steve Young. Partially observable markov decision processes for spoken dialog systems. Computer Speech & Language, 21(2):393­422, 2007.
Chien-Sheng Wu, Andrea Madotto, Genta Winata, and Pascale Fung. End-to-end recurrent entity network for entity-value independent goal-oriented dialog learning. In Dialog System Technology Challenges Workshop, DSTC6, 2017.
Chien-Sheng Wu, Andrea Madotto, Genta Winata, and Pascale Fung. End-to-end dynamic query memory network for entity-value independent task-oriented dialog. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6154­6158, April 2018.
Steve Young, Milica Gasic´, Blaise Thomson, and Jason D Williams. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160­1179, 2013.
Tiancheng Zhao, Allen Lu, Kyusong Lee, and Maxine Eskenazi. Generative encoder-decoder models for task-oriented spoken dialog systems with chatting capability. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pp. 27­36. Association for Computational Linguistics, August 2017. URL http://aclweb.org/anthology/W17-5505.
Victor Zhong, Caiming Xiong, and Richard Socher. Global-locally self-attentive dialogue state tracker. In Association for Computational Linguistics, 2018.
11

Under review as a conference paper at ICLR 2019

A TABLES
A.1 TRAINING DETAILS
The model is trained end-to-end using Adam optimizer (Kingma & Ba, 2015), and learning rate annealing starts from 1e-3 to 1e-4. The number of hop K is set to 1,3,6 to compare the performance difference. All the embeddings are initialized randomly, and a simple greedy strategy is used without beam-search during the decoding stage. The hyper-parameters such as hidden size and dropout rate are tuned with grid-search over the development set (per-response accuracy for bAbI Dialogue and BLEU score for the SMD). In addition, to increase model generalization and simulate OOV setting, we randomly mask a small number of input source tokens into an unknown token. The model is implemented in PyTorch and the hyper-parameters used for each task are listed below.

Table 5: Selected hyper-parameters in each datasets for different hops. The values is the embedding dimension and the GRU hidden size, and the values between parenthesis is the dropout rate. For all the models we used learning rate equal to 0.001, with a decay rate of 0.5.

GLMP K1 GLMP K3 GLMP K6

T1 64 (0.5) 64 (0.3) 64 (0.3)

T2 64 (0.7) 64 (0.7) 64 (0.5)

T3 64 (0.1) 64 (0.5) 64 (0.3)

T4 64 (0.9) 64 (0.9) 64 (0.7)

T5 128 (0.3) 128 (0.3) 128 (0.2)

SMD 256 (0.2) 128 (0.2) 128 (0.1)

A.2 DATASET STATISTICS

Table 6: Dataset statistics for 2 datasets.

Task
Avg. User turns Avg. Sys turns
Avg. KB results Avg. Sys words Max. Sys words Nb. Slot Types Nb. Distinct Slot values
Vocabulary Train dialogues
Val dialogues Test dialogues Total Nb. Dialogues

1 4 6 0 6.3 9
4000

234
6.5 6.4 3.5 9.5 9.9 3.5 0 24 7 6.2 7.2 5.7 998
7 3747 1000 1000 1000 + 1000 OOV 4000 4000 4000

5 12.9 18.4 23.7 6.5
9
4000

Calendar
6 79
1034

SMD Weather
2.6 2.6 66.1 8.6 87 4 65 1601 2425 302 304 997

Navigation
5 140
1000

A.3 HUMAN EVALUATION
Appropriateness 5: Correct grammar, correct logic, correct dialogue flow, and correct entity provided 4: Correct dialogue flow, logic and grammar but has slightly mistakes in entity provided 3: Noticeable mistakes about grammar or logic or entity provided but acceptable 2: Poor grammar, logic and entity provided 1: Wrong grammar, wrong logic, wrong dialogue flow, and wrong entity provided
Human-Likeness (Naturalness) 5: The utterance is 100% like what a person will say 4: The utterance is 75% like what a person will say 3: The utterance is 50% like what a person will say 2: The utterance is 25% like what a person will say 1: The utterance is 0% like what a person will say

12

Under review as a conference paper at ICLR 2019
Figure 4: Appropriateness and human-likeness scores according to 200 dialogue scenarios.
B ERROR ANALYSIS
For bAbI dialogues, the mistakes are mainly from task 3, which is recommending restaurants based on their rating from high to low. We found that sometimes the system will keep sending those restaurants with the higher score even if the user rejected them in the previous turns. On the other hand, SMD is more challenging for response generation. First, we found that the model makes mistakes when the KB has several options corresponding to the user intention. For example, once the user has more than one doctor appointment in the table, the model can barely recognize. In addition, since we do not include the domain specific and user intention supervision, wrong delexicalized responses may be generated, which results in an incorrect entity copy. Lastly, we found that the copied entities may not be matched to the generated sketch tags. For example, an address tag may result in a distance entity copy. We leave the space of improvement to future works.
C VISUALIZATION
13

Under review as a conference paper at ICLR 2019

Final GDeneelerxaGitciooalnldi:z:etwhdeeGanereneaer3re_asmttioiglner:ostcaheweryan_yesaftroroermestisw@wilplioollwio_wsty_smp_emariaskre@ktepbtou,it3,t_@hmedirlieesstiasanawceacayaraw_ctao4yl0lias9ti_o@bno_alnldaedrarder_bssyst

Pointer w/o G

Final Pointer

[394_van_ness_ave] address coupa [coffee_or_tea_place] poi_type coupa [road_block_nearby] traffic_info coupa
[6_miles] distance coupa [coupa] poi coffee_or_tea_place road_block_nearby 6_miles
[9981_archuleta_ave] address peets_coffee [coffee_or_tea_place] poi_type peets_coffee [moderate_traffic] traffic_info peets_coffee
[4_miles] distance peets_coffee [peets_coffee] poi coffee_or_tea_place moderate_traffic 4_miles
[91_el_camino_real] address 76 [gas_station] poi_type 76
[car_collision_nearby] traffic_info 76 [5_miles] distance 76
[76] poi gas_station car_collision_nearby 5_miles [899_ames_ct] address stanford_childrens_health
[hospital] poi_type stanford_childrens_health [moderate_traffic] traffic_info stanford_childrens_health
[5_miles] distance stanford_childrens_health [stanford_childrens_health] poi hospital moderate_traffic 5_miles
[481_amaranta_ave] address palo_alto_garage_r [parking_garage] poi_type palo_alto_garage_r [no_traffic] traffic_info palo_alto_garage_r [5_miles] distance palo_alto_garage_r
[palo_alto_garage_r] poi parking_garage no_traffic 5_miles [842_arrowhead_way] address panda_express [chinese_restaurant] poi_type panda_express [moderate_traffic] traffic_info panda_express [5_miles] distance panda_express
[panda_express] poi chinese_restaurant moderate_traffic 5_miles [113_arbol_dr] address jing_jing
[chinese_restaurant] poi_type jing_jing [car_collision_nearby] traffic_info jing_jing
[3_miles] distance jing_jing [jing_jing] poi chinese_restaurant car_collision_nearby 3_miles
[409_bollard_st] address willows_market [grocery_store] poi_type willows_market [car_collision_nearby] traffic_info willows_market
[3_miles] distance willows_market [willows_market] poi grocery_store car_collision_nearby 3_miles
give me directions to the closest grocery_store

G

0123456789 0123456789

Figure 5: Memory attention visualization from the SMD navigation domain.

14

Under review as a conference paper at ICLR 2019

Final GenerationD: ethleexniceaalirzeesdt hGoesGnpeoitrldaa:ltiisostnas:ntatfhonerfdon_redea_xerpexrseptsres@_scpsa_ocri_eatryiespaaettis2211@44p__eoeill__acctaa@mmiainndood__rrreeesaasll

Pointer w/o G

Final Pointer

[583_alester_ave] address philz [coffee_or_tea_place] poi_type philz [car_collision_nearby] traffic_info philz
[4_miles] distance philz [philz] poi coffee_or_tea_place car_collision_nearby 4_miles
[842_arrowhead_way] address panda_express [chinese_restaurant] poi_type panda_express
[heavy_traffic] traffic_info panda_express [2_miles] distance panda_express
[panda_express] poi chinese_restaurant heavy_traffic 2_miles [481_amaranta_ave] address palo_alto_garage_r [parking_garage] poi_type palo_alto_garage_r
[road_block_nearby] traffic_info palo_alto_garage_r [2_miles] distance palo_alto_garage_r
[palo_alto_garage_r] poi parking_garage road_block_nearby 2_miles [580_van_ness_ave] address toms_house [friends_house] poi_type toms_house [no_traffic] traffic_info toms_house [2_miles] distance toms_house
[toms_house] poi friends_house no_traffic 2_miles [669_el_camino_real] address p_._f_._changs [chinese_restaurant] poi_type p_._f_._changs [moderate_traffic] traffic_info p_._f_._changs [6_miles] distance p_._f_._changs
[p_._f_._changs] poi chinese_restaurant moderate_traffic 6_miles [214_el_camino_real] address stanford_express_care [hospital] poi_type stanford_express_care [moderate_traffic] traffic_info stanford_express_care [5_miles] distance stanford_express_care
[stanford_express_care] poi hospital moderate_traffic 5_miles [338_alester_ave] address midtown_shopping_center [shopping_center] poi_type midtown_shopping_center [no_traffic] traffic_info midtown_shopping_center [5_miles] distance midtown_shopping_center
[midtown_shopping_center] poi shopping_center no_traffic 5_miles [394_van_ness_ave] address coupa
[coffee_or_tea_place] poi_type coupa [no_traffic] traffic_info coupa [5_miles] distance coupa
[coupa] poi coffee_or_tea_place no_traffic 5_miles find the
address to a
hospital or
clinic

G

0123456 0123456

Figure 6: Memory attention visualization from the SMD navigation domain.

15

Under review as a conference paper at ICLR 2019

Gold:Fitnhael cGlDoeesnleeesrxtaictpiaoalnrizk:eintdhgeG_gneaneraearrgeaestitoispnac:irtvkhiicen_gnc_eegnaartreearsg_tge@airspaocgii_evtiyc, p_loceceianstte@edrp_4go_aimr,a@igledesis,at4wa_namcyeilaeatsw2aa7wy0a_ayatlat@atiar5ed__dmwriealeslkss

Pointer w/o G

Final Pointer

[783_arcadia_pl] address chevron [gas_station] poi_type chevron
[moderate_traffic] traffic_info chevron [3_miles] distance chevron
[chevron] poi gas_station moderate_traffic 3_miles [271_springer_street] address mandarin_roots [chinese_restaurant] poi_type mandarin_roots [moderate_traffic] traffic_info mandarin_roots [4_miles] distance mandarin_roots
[mandarin_roots] poi chinese_restaurant moderate_traffic 4_miles [408_university_ave] address trader_joes [grocery_store] poi_type trader_joes [no_traffic] traffic_info trader_joes [5_miles] distance trader_joes
[trader_joes] poi grocery_store no_traffic 5_miles [638_amherst_st] address sigona_farmers_market [grocery_store] poi_type sigona_farmers_market
[no_traffic] traffic_info sigona_farmers_market [4_miles] distance sigona_farmers_market
[sigona_farmers_market] poi grocery_store no_traffic 4_miles [347_alta_mesa_ave] address jills_house [friends_house] poi_type jills_house [heavy_traffic] traffic_info jills_house [4_miles] distance jills_house
[jills_house] poi friends_house heavy_traffic 4_miles [270_altaire_walk] address civic_center_garage [parking_garage] poi_type civic_center_garage [no_traffic] traffic_info civic_center_garage [4_miles] distance civic_center_garage
[civic_center_garage] poi parking_garage no_traffic 4_miles [434_arastradero_rd] address ravenswood_shopping_center
[shopping_center] poi_type ravenswood_shopping_center [heavy_traffic] traffic_info ravenswood_shopping_center
[4_miles] distance ravenswood_shopping_center [ravenswood_shopping_center] poi shopping_center heavy_traffic 4_miles
what are the
directions to the
closest parking_garage

G

0123456789 0123456789

Figure 7: Memory attention visualization from the SMD navigation domain.

16

Under review as a conference paper at ICLR 2019

GoFlidn:atlhGeerneearraetDiwoenhle:oxtlheic_eafolnizoeedadsreG2s_etmngeirloreacsteiaorwyn_a:sytthoaernendiessaisrgiegosontna@a_f_pafoarmir_mteyerpsre_sm_imsaa@rkrpkeeot ti4,,_@m4_idmleissitleaasnwcaaewyaawwyhaaeytre8a1td9@o_aawdlemdgraeo_ss?st

Pointer w/o G

Final Pointer

[580_van_ness_ave] address toms_house [friends_house] poi_type toms_house [no_traffic] traffic_info toms_house [1_miles] distance toms_house
[toms_house] poi friends_house no_traffic 1_miles [773_alger_dr] address stanford_shopping_center [shopping_center] poi_type stanford_shopping_center [moderate_traffic] traffic_info stanford_shopping_center
[2_miles] distance stanford_shopping_center [stanford_shopping_center] poi shopping_center moderate_traffic 2_miles
[819_alma_st] address whole_foods [grocery_store] poi_type whole_foods [heavy_traffic] traffic_info whole_foods
[2_miles] distance whole_foods [whole_foods] poi grocery_store heavy_traffic 2_miles
[638_amherst_st] address sigona_farmers_market [grocery_store] poi_type sigona_farmers_market
[no_traffic] traffic_info sigona_farmers_market [4_miles] distance sigona_farmers_market
[sigona_farmers_market] poi grocery_store no_traffic 4_miles [271_springer_street] address mandarin_roots [chinese_restaurant] poi_type mandarin_roots [moderate_traffic] traffic_info mandarin_roots [4_miles] distance mandarin_roots
[mandarin_roots] poi chinese_restaurant moderate_traffic 4_miles [481_amaranta_ave] address palo_alto_garage_r [parking_garage] poi_type palo_alto_garage_r [moderate_traffic] traffic_info palo_alto_garage_r [2_miles] distance palo_alto_garage_r
[palo_alto_garage_r] poi parking_garage moderate_traffic 2_miles [329_el_camino_real] address the_westin [rest_stop] poi_type the_westin [no_traffic] traffic_info the_westin [4_miles] distance the_westin
[the_westin] poi rest_stop no_traffic 4_miles give me
directions to the
closest grocery_store

G

0123456789 0123456789

Figure 8: Memory attention visualization from the SMD navigation domain.

17

Under review as a conference paper at ICLR 2019

DelexicalizedFiGnaeGlnGoelerdan:teiiorwanit:liloosnkea:tyoak,arseyemt,tisinnedgtetiarngfroearmdrieninmdneienrrdfaoetrr7yfooprumryo,@ufoerrvdetihnnent_ew6rittwhhiot@hf ptmhaiarstryime@o7ndptamhtewaatitthth@met_ai6mrtihee

Pointer w/o G

Final Pointer

i

need

a

reminder

for

dinner

what

time

shall

i

set

a

dinner

reminder

?

set

my

reminder

for

dinner

at

7pm

,

for

the_6th

of

this

month

with

my

beloved

marie
G 0 1 2 3 4 5 6 7 8 9101112 0 1 2 3 4 5 6 7 8 9101112

Figure 9: Memory attention visualization from the SMD schedule domain.

18

Under review as a conference paper at ICLR 2019

Gold: your tennis_aDcetFlievinxitaiyclaiGsliezoenndetrhGaetei_on4net:rhaytaoiotun5r:ptyemonunarins@d_aeycovtieuvnrittsyiissitsoeonrnw@tihldleab_te4etaahtttae@tnt5dipminmge

Pointer w/o G

Final Pointer

[sister] party swimming_activity

[the_4th] date swimming_activity

[10am] time swimming_activity

[tom] party doctor_appointment

[the_5th] date doctor_appointment

[3pm] time doctor_appointment

[martha] party yoga_activity

[the_10th] date yoga_activity

[11am] time yoga_activity

[sister] party optometrist_appointment

[thursday] date optometrist_appointment

[2pm] time optometrist_appointment

[aunt] party dinner

[sunday] date dinner

[7pm] time dinner

[sister] party tennis_activity

[the_4th] date tennis_activity

[5pm] time tennis_activity

car

,

find

the

date

and

time

for

my

tennis_activity

G

0123456 0123456

Figure 10: Memory attention visualization from the SMD schedule domain.

19

Under review as a conference paper at ICLR 2019

[aunt] party doctor_appointment [thursday] date doctor_appointment
[3pm] time doctor_appointment [go_over_quarterly_report] agenda conference
[conference_room_100] room conference [infrastructure_team] party conference [the_6th] date conference [6pm] time conference [alex] party doctor_appointment [the_17th] date doctor_appointment [7pm] time doctor_appointment [tom] party tennis_activity [the_13th] date tennis_activity [5pm] time tennis_activity [aunt] party dinner [the_17th] date dinner [6pm] time dinner [mother] party tennis_activity [the_4th] date tennis_activity [5pm] time tennis_activity [jon] party tennis_activity [the_11th] date tennis_activity [7pm] time tennis_activity [brother] party football_activity [the_9th] date football_activity [6pm] time football_activity what time is my next tennis_activity and who will be attending ?

GoldD:FeiylneoaxulircGaneleinzxeetdrtaeGtnioennnis:e_ryaaoctuitorivnni:teyyxoitsutroennnnetixhste_a_@4cetthivveiatntyt5iispsmoonnwt@hitehd_ay1to3eutahrtma@tot6tihpmemer

Pointer w/o G

Final Pointer

G 01234567 01234567

Figure 11: Memory attention visualization from the SMD schedule domain.

20

Under review as a conference paper at ICLR 2019

Gold: are you talkDineglexaibcoaulitzethdeFGdienonaceltrGoaret_niaoepnrp:aoytiioonuntmr: @yeoneutvroednnotwcitseodornnies@soddnaayttheoera_5ttht@hetaoimtne6epowmnitthwhi@eth_p5aathlretxy?

Pointer w/o G

Final Pointer

[tom] party football_activity [the_5th] date football_activity
[9am] time football_activity [alex] party dentist_appointment [sunday] date dentist_appointment [2pm] time dentist_appointment [discuss_the_merger] agenda conference [conference_room_50] room conference [management] party conference
[friday] date conference [10am] time conference [father] party doctor_appointment [wednesday] date doctor_appointment [7pm] time doctor_appointment [sister] party swimming_activity [the_13th] date swimming_activity [1pm] time swimming_activity [alex] party doctor_appointment [the_5th] date doctor_appointment [6pm] time doctor_appointment [jon] party dentist_appointment [thursday] date dentist_appointment [2pm] time dentist_appointment [discuss_the_company_picnic] agenda meeting [100_conference_room] room meeting [management] party meeting [the_3rd] date meeting
[2pm] time meeting what time is my doctor
apartment and who is it with ?

G

012345678 012345678

Figure 12: Memory attention visualization from the SMD schedule domain.

21

Under review as a conference paper at ICLR 2019

Delexicalized GenerationF:initalwGGiolelldnn:eotrthabeteiroen@w:wiitlelwabtielhlnenroo_tadbtrtiezrizbdleuritzienzlierneid@nwlrooecodadwt_ioocointdy@_tchwitieysewwkeelyee_kkteeimnndde

[[4[120[[[0[6[0[[[[86f[[[[8[b410[3c][5[0r[[65f[[s00[[[[c0972]7al[3[[00l[6f[[0[[60[9hfhc[00[t[i[ef8lf45]5[9[94f7][z010[[i060[[4[o086f0l[e0][l]f[[0f[0ai4[fnf3][[a[7o61h5]0e800[[40z0]g[h50h0][06f[[[0]5f]f70f[[hr[0hf[[0a0f3[[df[[fih4df[h0i60]r6w]0]5[6]sla[0[[0[[]1u5[a04]f[m0[9[]3ff[[60f[fl7[[lrfhi[0fn]uf[]fh000r[0fohi[rfolffl0_rd6]dfo]c4[4g]][34]o1[i0s8]ri[3[4fto][0a]fr[]g[[5o0for0]0hfh0r[a]h0m0]f]l05hf00_0fl[rg0[h[i[g[hfgcm1isf5[]ffw3hdif]iohtod]l[oa2df]nrwod0e]yz90]0h_g0]0ww[w0fl0vg[04hl90i]w4o4lfzg[hhw][sw0]f]lo[r]fflhiaifi0f]fhi0]fmf[llkf[hflhoihyio[no]]0d0or70]irvonig[shz9wir]g0]2g]rl]2wo]eoiw[[]fa]olg]0fg]fhz]ge]f[f[0k]9lfhh4f0si0f[0li0g[hr[gi[[unflioieehedmhhfioi9ihfiyw]]wwoiwiw]smlg]w]od]]klg]08]rewef]owwww0f[l3lgi[h[ri0]7hz8elal2gfh0wn[wa0iwht]zg[hhhi]foirhittthyssshfe0li0fllfflf]hhonh]gd]]we]o[ddda5inw]niir]06]iesga[w2oi]g]ohwo[6om]m[mh]hhhwrhchzf0w]lt]tt]ll]f0lhzgheeesligf0g00rghfaaad[ydf[liw[]heiieeei]ahifi3hfow]ooidog]dhyo]nwts]siscunununwww0]lwowwo]gslf0gig]hg][tttlw0sg_sssas5wwlhwr[0ligu]uu][ih]ftttsssn[hfdddhiholhfhht]tofltnfhyyre]s]odddiegmimmdwhh0ghiuuuwwoootttaw]wr]]wa]geo]3mmmoshh[heeee]eenawahwatfdttth]]l]fhhnuufhuffslgfh0aaglaylfeeehrrreeieisissh]wunnuunhiw]]ioi]]]dowsssy[]uonununor]rrwwwh]lruufghunnnnnkns]ngiwwhgw0gsszsssssttssttutuuttsstotstihesssh]rrri]ltththoflhuuufdddooo]ddnd]iiiyieeeeuguueeiwooommmw]hhueuhmmfnmfu]fohwhhtttoteeeeweettthi]lrrhrdddddddddddddddzdddddduuuffofaaaglaahagghlfhwrrrweeeiierrwr]iosssnnnnn]nuunnunsssounu]nun]rrrowwwssssssssslnnnnnhng]gwwuurrudrwluussssusssgsstttisssgaaaaaaaaaaaaattaaaaaataarriirittt]hhohdddiiieuuuooouuusooogwdddddddddddddddddddddw]eeeeeemmm]ehheeeheewtttuuufffudduddddddddddddudfdddfdddhfaaahrrryrrlrheeeyyyyyyyyyyyyyyyyyyyyygw]i]sssrrruunnunrrr]nnnnnonnnnnnnhssssssasaaassaaaaaaaaaaaaaaaaasssssssssuuugtttsrrsrsrraaaaaaaaaaaaraaaaaaaaattt[]iiiiiiddduuyuooommmeeeeehhehwttddddddtdddddddddddddddddddddddddddddddddddduuufffmmmmmmmmmmmmmmmmmmmmmmyyyyyyyyyyyyyyyyyyyyyaaarrrhyyyyyyyyyyyyyyyyyyyyysssunununrrr]nnnnnnsssssssssuuuaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaarrrtttiiiuuuooorrrrrrrrrrrrrrrrrrrrrreeeeeeooooooooooooooooooooooddddddddddddddddddddduufuffgggggggggggggggggggggrrryyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyeeeeeeeeeeeeeeeeeeeeeerrrnnnnnnsssssssssnuuuuuuuuuuuuuuuuuuuuuaaaaaaaaaaaaaaaaaaaaarrrrrrrrrrrrrrrrrrrrrrrriiiddddddddddddddddddddddmmmmmmdmdmddmmmdmdddmdmdddddddmmmmmdmdmdddsssssssasassassasasasaaaaaaassaassaaasaaasdnnnnnnnnnnnnnnnnnnnnnyyyyyyyyyyyyyyyyyyyyywwwwwwwwwwwwwwwwwwwwwwaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaanaaannnnnannnannnnnnanannannnatttttttttttttttttttttaaaaaaaaaaaaaaaaaaaaawcccccccccccccccccccccnnnnnnnnnnnnnnnnnnnnnaaaaaaaaaaaaaaaaaaaaayyyyyyyyyyyyyyyyyyyyydddddddddddddddddddddooooooooooooooooooooooynnnnnnnnnnnnnnnnnnnnnoooooooooooooooooooooiiiiiiiiiiiiiiiiiii_ii____________________e_____________________oooooooooooooooooooooo]nnnnnnnnnnnnnnnnnnnnnccccccccccccccccccccchhhhhhhhhhhhhhhhhhhhhdmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrreddddddddddddddddddddddt_____________________aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaataaaaaaaaaaaaaaaaaaaaarko___________________vvv_vvvvv_vvvv_vvvvvvvvvppppppppppppppppppphppaaaaaaaaaaaaaaaaaaaaairrrrrrrrrrrrrrrrrrrrrppppppppppppppppppppptttttttttttttttttttttzedtccccccccccccccccccccccsssssssssssssssssssssiiiiiiiiiiiiiiiiiiiiitttttttwttttttttttttttttttttttttttttttttetttttttttttttttttttttttteeeeeeeieeieieeeiieieiiiiiiieiieeiiieiieiehziiiiiiiiiiiiiiiiiniiiiiaaaaaaooaaooooaaoaaoooooaoaooooooooooooooaooaaooooaaaaooooooooboeeeeeeeeeeeeeeeeeeeeeadddddddddddddddddddddrttttttttttttttttttttttiliwwwwwwwwwwwwwwwwwwwwwinnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnndnoooooooooooooooooooeooeeyyyyyyyyyyyyyyyyyyyyyyylsssss?sssssssssssssssssl

G

Pointer w/o G

Final Pointer

01234567 01234567

Figure 13: Memory attention visualization from the SMD weather domain. 22

Under review as a conference paper at ICLR 2019

Delexicalized GenFeinraatlioGne:ntehrearteiown:iltlhbGeeore@ldw:wdielleabwtehiescrlp_earaetrtd_rsiicbktuieetdse iiinnn dd@aalnnovvciiallllteeiooonnnottnhhuu@rrssdddaaateyy

[[o8[[[[[v6[6110[1[1e[50000f[[1[[06[[[[[0]r[c6[08[8f[[[7[5f90[f04[0[[[0c9[09]4[5[[[77l][so0[[6[[[[06[h3f0[01[9[10e00f7ff0f02ao76[3[f0646]6[f0[[[4[0t[0[[9[l[100g][]fo0ohf0i0[f[[f0]0c6[]0o[1[[af553[f2[fo02[[[74of5[fs[[v0]g[8[0[0]0f000]0]f6[087fl[f0[[60][[[4]ff98g][[gf[[g5[]f[[94]i[1[h[f2hl[[1f6[[[60s[]4ofw[0]0rth0]000rr]h0[f[0v70e0[[70]5g][6r5foo[h[h5]0cf[df58]d[fhf3]fff8[f5[o0hff030][mldf[l400[0]00y_g]gh04mans00h0il9b]0ir00l[h3]5]a[0wf]o60]]]o[d]f[o[f[4o][i[10hfeffif]lhfr0ffhl40f[h0lgguow00g[oo0[fhhe00r[i0h0v0sfil2]gaf0fl9]gro]n80]ff5o2l]]]dfofd]ffy]iy]i]f0[of]fiwww0cl0lgmf0fi]fi00gg7oflwo0fwrwhghy]0[hr0fi]i]ollhho[nfl]g]i0[wg]ifofgd]]hih[offtwhyk]fnw]g]ii[ef3fo][f]ifrfi]0hsgo]foa0wh[ou0ohf]wg0]0yez6]arhzh[c]wwwwfldlhg]g]fl]9f]lhhfgl]d]whfnf]ghw0]]]rf[][l]migiwfiee0e]i[otihi][[hhghfofoih]wyoyiohiho]]ryrthhw]lh[i]0[]lolhg]g]wzwgfrlhz]g[hfwgwa]dndlhfi]]awhflsfe0igid]ntthrt]iehsshsfmos0hioheoihl]yflloyacfhdwwweey]eldoddlg]swwwofl]l[ghghmi]ww]]ig]il]wgw]wligilgmmiomraihhohofwotthtihhthoo]lhohhoiosihhlirggaayuasggo]tlftthtrssdolsgdwgiewggfwiwwww]aw]lt]giw]oi]]gdidsodrwssonsuwnunnuho]hiwwwhoolgyhl]yygmmlwwmeewehhwhhhorlighwwyeeuuetttsnut]gi]wwiw]gwi]aaattgith]whmohhyow]or]hyhhoshhtsuuugooohwgssstsdtgth]uwnunusnssg]t]tteweeeeelssste]ewe]]uhu]tffufdddhw]uuiuddhdlrr]yrheeeyolgtttmmmheewh]hethimmmwttthhht]wtttw]grriruutuottt]ooaaaossnnnnnsnhaaaho]heeeeeessssshssssgttdd[tduussfufsfirrgryssttst]uunnnusssssrsr]rununun]mmddii]dimhhhwtttyddduuduuuuaaammrrmrmhhhhwttnnnnntttntdddddmddddmddddddddmddddttthhhwtttassassssassshuuusssooounanuanuauuurrr]oooheeeeeeeeeeeeiiisssununnuuuuffuufu]uuffufsssuununnrr]rtttrrruuudddddddddddddddddddddaaaaaaaaaaaaaaaaaaaaauuuuuutttooorrrrrrnnnnntntteeeeeennnnnnuuuoossssssossusuufoffsssssssssuuuoooeeeeererrrrrrrruueeeeufefefiiiiiuiuffufrrraaaaaaaaaaaaaaaaaaaaayyyyyyyyryyyyyryyyyyryyyrrrnnnnnnddddddddddddddddddddsdssssssssrrrdddddddddddddddddddddnnnnnnnrrrrrrnnnnnnsssssssssiiisssssssssrrrrrryyyyyyyyyyyyyiyiyyyyyyyidddddddddddddddiiddddddiaaaaaaaaaaaaaaaaaaaaacaaaaaaaaaaaaaaaaaaaaaddddddddddddddddddddddbbbbbbbbbbbbbbbbbbbbbdddddddddddddddddddddaaaaaaaaaaaaaaaaaaaaalyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaarrrrrrrrrrrrrrrrrrrrraaaaaaaaaaaaaaaaaaaaaetyyyyyyyyyyyyyyyyyyyyyeeeeeeeeeeeeeeeeeeeeellllllllllllllyyyylllyyyylyylyyyylylyyyyyyyyyyyyyyyyyyyyyyyyyyyyhaoooooooooooooooooooooddddddddddddddddddddddhhhhhhhhhhhhhhhhhhhhhnnnnnnnnnnnnnnnnnnnnn]sssssssssssssssssssssruaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeaaaeaeeaeeeeeeeeaeeaaeaaeeeaaeeefffffffffffffffffffffttttttttttttttttttttt_eeeeeeeeeeeeeeeeeeeeetrrrrrrrrrrrrrrrrrrrrrrkkkkkkkkkkkkkkkkkkkkkwwwwwwwwwwwwwwwwwwwnwwnnnnnnnnnnnnnnnnnnnnntmmmmmmmmmmmmmmxxxmxmmxxxxmxmxxxxxmxmxxxxxxsoeeeeeeeeeeeeeeeeeeeeesaaaaaaaaaaaaaaaaaaaaahlllllllllllllllllllllvvvvvvvvvvevvveveeveeeeveevveeeeveveveeeveekaaaaaaaaaaaaaaaaaaaaaoooooooooooooooooooooddssssssssssssssssssssstttttttttttttttttttttbbbbbbbbbbbbbbbbbbbbbweiiiiiiiiiiiiiiiiiiiiiitttttttttttttttttttttitttttttttttttttttttttnnnnnnnnnnnnnnnnnnnnnoononoonnooonnonnnonooonnnonnoonoononnoonnboaalllllllllllllllllllllleeeeeeeeeeeeeeeererererrrrrrrrerrreerrrrrrrrilllllllllllllllllllllllllllllllllllllllllllidddddddddddddddddddddddddddddddddddddddddnndeeeeeeeeeoeeeoeeoeoooooeooeeeoeaoooeaoeaoeaeeaeaeaoeaeaeeaaeeaoeoeaeaoeeeeoaaaeaaeeaaeeyyl?srrrrrrrrrrrrrrrrrrrrrl

G

Pointer w/o G

Final Pointer

01234567 01234567

Figure 14: Memory attention visualization from the SMD weather domain. 23

