Under review as a conference paper at ICLR 2019
ENTROPIC GANS MEET VAES: A STATISTICAL APPROACH TO COMPUTE SAMPLE LIKELIHOODS IN GANS
Anonymous authors Paper under double-blind review
ABSTRACT
Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs). VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function. GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data. The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems. In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on sample likelihoods, an approach that VAEs are based on. In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework. Our numerical results on several datasets demonstrate consistent trends with the proposed theory.
1 INTRODUCTION
Learning generative models is becoming an increasingly important problem in machine learning and statistics with a wide range of applications in self-driving cars (Santana & Hotz, 2016), robotics (Hirose et al., 2017), natural language processing (Lee & Tsao, 2018), domain-transfer (Tzeng et al., 2017), computational biology (Ghahramani et al., 2018), etc. Two modern approaches to deal with this problem are Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Variational AutoEncoders (VAEs) (Makhzani et al., 2015; Rosca et al., 2017; Tolstikhin et al., 2017; Mescheder et al., 2017).
VAEs compute a generative model by maximizing a variational lower-bound on average sample likelihoods using an explicit probability distribution for the data. GANs, however, compute a generative model by minimizing a distance between observed and generated distributions without considering an explicit model for the data. Empirically, GANs have been shown to produce higher-quality generative samples than that of VAEs (Karras et al., 2017). However, since GANs do not consider an explicit probability model for the data, we are unable to compute sample likelihoods using their generative models. Computations of sample likelihoods and posterior distributions of latent variables are critical in several statistical inference. Inability in obtaining such statistics within GAN's framework severely limits their application in such statistical inference problems.
In this paper, we resolve these issues for a general formulation of GANs by providing a theoreticallyjustified approach to compute sample likelihoods using GAN's generative model. Our results can open new directions to use GANs in massive-data applications such as model selection, sample selection, hypothesis-testing, etc (see more details in Section 5). Below, we state our main results informally without going into technical conditions while precise statements of our results are presented in Section 2. Let Y and Y^ = G(X) represent observed (i.e. real) and generative (i.e. fake or synthetic) variables, respectively. X (i.e. the latent variable) is the randomness used as the input to the generator G(.).
1

Under review as a conference paper at ICLR 2019

likelihood of latent variable
E[log fX(x)]

coupling entropy H(P )*^Y|Y= ytest

distance to the model
E[l(Y=ytest, G*(X))]
ytest

fX(x)

generator (G*)

x
input randomness
density low high

generative variable Y^ = G*(X)

Figure 1: A statistical framework for GANs. By training a GAN architecture, we first compute optimal generator function G and optimal coupling between the real variable Y and the latent variable X. The sample likelihood of a test sample ytest can then be lower-bounded using a combination of three terms: (1) the expected distance of ytest to the distribution learnt by the generative model, (2) the entropy of the coupled latent variable given ytest and (3) the average likelihood of the coupled latent variable with ytest.

Consider the following explicit probability model of the data given a latent sample X = x:

fY X=x(y)  exp(- (y, G(x))),

(1.1)

where (., .) is a loss function. Under this explicit probability model, we show that minimizing

the objective of an optimal transport GAN (e.g. Wasserstein GAN Arjovsky et al. (2017)) with the cost function (., .) and an entropy regularization (Cuturi, 2013; Seguy et al., 2017) maximizes a

variational lower-bound on average sample likelihoods. I.e.

average sample likelihoods  - (entropic GAN objective) + constants.

(1.2)

If (y, y^) = y - y^ 2, the optimal transport (OT) GAN simplifies to WGAN (Arjovsky et al., 2017) while if (y, y^) = y - y^ 22, the OT GAN simplifies to the quadratic GAN (or, W2GAN) (Feizi et al., 2017). The precise statement of this result can be found in Theorem 1. This result provides

a statistical justification for GAN's optimization and puts it in par with VAEs whose goal is to

maximize a lower bound on sample likelihoods. We note that the entropy regularization has been

proposed primarily to improve computational aspects of GANs (Cuturi, 2013). Our results provide

an additional statistical justification for this regularization term. Moreover, using GAN's training,

we obtain a coupling between the observed variable Y and the latent variable X. This coupling provides the conditional distribution of the latent variable X given an observed sample Y = y. The

explicit model of equation 1.1 acts similar to the decoder in the VAE framework, while the coupling

computed using GANs acts as an encoder in the VAE framework.

Another key question that we address here is how to estimate the likelihood of a new sample ytest
given the generative model trained using GANs. For instance, if we train a GAN on stop-sign images, upon receiving a new image, one may wish to compute the likelihood of the new sample ytest
coming from the trained generative model. In standard GAN formulations, the support of the gener-
ative distribution lies on the range of the optimal generator function. Thus, if the observed sample ytest does not lie on that range (which is very likely in practice), there is no way to assign a sensible
likelihood score to that sample. Below, we show that using the explicit probability model equation 1.1 characterized for entropic GANs, we can lower-bound the likelihood of this sample ytest
from the trained generative model. This is similar to the variational lower-bound on sample like-
lihoods used in VAEs. Our numerical results show that this lower-bound well-reflect the expected
trends of the true sample likelihoods.

2

Under review as a conference paper at ICLR 2019

Let G and PY,X be the optimal latent variables, respectively. The

generator function and optimal coupling PY,X

the can

optimal coupling between be computed efficiently for

real and entropic

GANs as we explain in Section 3. For other GAN architectures, one may approximate such optimal

couplings as we explain in Section 4. Let ytest be a new observed sample. We can lower-bound the

log likelihood of this sample as

log

fY

(ytest)



-

EP X

Y

=ytest

(ytest, G(x))

+H

P
X Y =ytest

log likelihood

distance to the generative model

coupling entropy

+ EP X

Y

=ytest

- x2 2

.

likelihood of latent variable

(1.3)

We present the precise statement of this result in Corollary 2). This result combines three components in order to approximate the likelihood of a sample given a trained generative model:

· The distance between ytest to the generative model: the larger this distance, the smaller is the likelihood of observing ytest from the generative model.
· The entropy of the coupled latent variable: larger entropy means larger the randomness of the coupled latent variable which translates to larger sample likelihood.
· The likelihood of the coupled latent variable: the larger the likelihood of the coupled latent samples, the larger the observed sample likelihood.

Figure 2a provides a pictorial illustration of these components.
In what follows, we explain the technical ingredients of our main results. In Section 3, we present computational methods for GANs and entropic GANs, while in Section 4, we provide numerical experiments on MNIST(LeCun et al., 1998) and CIFAR-10 (Krizhevsky, 2009) datasets.

2 MAIN RESULTS

Let Y  Rd represent the real-data random variable with a probability density function fY (y). GAN's goal is to find a generator function G  Rr  Rd such that Y^ = G(X) has a similar

distribution to Y . Let X be an r-dimensional random variable with a fixed probability density

function fX (x). Here, we assume fX (.) is the density of a normal distribution. In practice, we

oyN^biost=eerGvthe(amxt it)hsaefmonrpu1lmesb{eiyr1o,fm..g..,eyWnmeer}artefivproeremssaeYmntpatnlhedessgemeenmecrpaaintreibcmael

samples from Y^ distributions by arbitrarily large.

, i.e., {y^1, ..., y^m } where PY and PY^ , respectively. Finally, we assume that

the generator function G is injective which is often the case in practice since G maps from an r dimensional space to a d dimensional one where r  d.

GAN computes the optimal generator G by minimizing a distance between the observed distribu-

tion PY and the generative one PY^ . Common distance measures include optimal transport measures (e.g. Wasserstein GAN (Arjovsky et al., 2017), WGAN+Gradient Penalty (Gulrajani et al., 2017),

GAN+Spectral Normalization (Miyato et al., 2018), WGAN+Truncated Gradient Penalty (Petzka

et al., 2017), relaxed WGAN (Guo et al., 2017)), and divergence measures (e.g. the original GAN's

formulation (Goodfellow et al., 2014), f -GAN (Nowozin et al., 2016)), etc.

In this paper, we focus on GANs based on optimal transport (OT) distance (Villani, 2008; Arjovsky et al., 2017) defined for a general loss function (., .) as follows

W

(PY

,

PY^

)

=

min
PY,Y^

E

(Y, Y^ ) .

(2.1)

PY,Y^ is the joint distribution whose marginal distributions are equal to PY and PY^ , respectively.

If by

(y, y^) = W1(., .),

y - y^ 2 while if

, this distance is called the first-order Wasserstein (y, y^) = y - y^ 22, this measure is referred to by

distance and is referred to W22(., .) where W2 is the

second-order Wasserstein distance (Villani, 2008).

The optimal transport (OT) GAN is formulated using the following optimization (Arjovsky et al., 2017; Villani, 2008):

min
GG

W

(PY

, PY^ ),

(2.2)

3

Under review as a conference paper at ICLR 2019

where G is the set of generator functions. Examples of the OT GAN are WGAN (Arjovsky et al.,

2017) corresponding to the first-order Wasserstein W2GAN) (Feizi et al., 2017) corresponding to the

sdeisctoanndc-eoWrde1r(.W, .a)sasnerdsttehienqduiastdarnactiecWGA2(N.,

(or, .).

the

Note that optimization equation 2.2 is a min-min optimization. The objective of this optimization is not smooth in G and it is often computationally expensive to obtain a solution (Sanjabi et al., 2018). One approach to improve computational aspects of this optimization is to add a regularization term to make its objective strictly convex (Cuturi, 2013; Seguy et al., 2017). A common strictly-convex regularization term is the negative Shannon entropy function defined as H(PY,Y^ ) = -E log PY,Y^ . This leads to the following optimal transport GAN formulation with the entropy regularization, or for simplicity, the entropic GAN formulation:

min min E
GG PY,Y^

(Y, Y^ ) - H PY,Y^

,

where  is the regularization parameter.

(2.3)

There are two approaches to solve the optimization problem 2.3. The first approach uses an iterative method to solve the min-min formulation (Genevay et al., 2017). Another approach is to solve an equivelent min-max formulation by writing the dual of the inner minimization part (Seguy et al., 2017; Sanjabi et al., 2018). The latter is often referred to as a GAN formulation since the min-max optimization is over a set of generator functions and a set of discriminator functions. We will explain these computational approaches in more details in Section 3.

In the following, we present an explicit probability model for entropic GANs under which they can be viewed as maximizing a lower bound on average sample likelihoods.

Theorem 1 Let the loss function be shift invariant, i.e., (y, y^) = h(y - y^). Let fY X=x(y) = C exp(- (y, G(x)) ),
be an explicit probability model for Y given X = x for a well-defined normalization

(2.4)

C

=

yRd

exp(-

1 (y, G(x))

) .

(2.5)

Then, we have

EPY

[log fY

(Y

)]

-

1 

ave. sample likelihoods

EPY,Y^ (Y, Y^ ) - H PY,Y^
entropic GAN objective

+constants.

(2.6)

In words, the entropic GAN maximizes a lower bound on sample likelihoods according to the explicit probability model of equation 2.4.

The proof of this theorem is presented in Section A. This result has a similar flavor to that of VAEs (Makhzani et al., 2015; Rosca et al., 2017; Tolstikhin et al., 2017; Mescheder et al., 2017) where a generative model is computed by maximizing a lower bound on sample likelihoods.
Having a shift invariant loss function is critical for Theorem 1 as this makes the normalization term C independent from G and x (to see this, one can define y = y - G(x) in equation A.4). The most standard OT GAN loss functions such as the L2 for WGAN (Arjovsky et al., 2017) and the quadratic loss for W2GAN (Feizi et al., 2017) satisfy this property.
One can further simplify this result by considering specific loss functions. For example, we have the following result for the entropic GAN with the quadratic loss function.
Corollary 1 Let (y, y^) = y - y^ 2 2. Then, fY X=x(.) of equation 2.4 corresponds to the multivariate Gaussian density function and C = 1 . In this case, the constant term in equation A.4
(2)d
is equal to - log(m) - d log(2) 2 - r 2 - log(2) 2.
Let G and PY,X be optimal solutions of an entropic GAN optimization 2.3 (note that the optimal coupling can be computed efficiently for entropic GAN using equation 3.7). Let ytest be a newly

4

Under review as a conference paper at ICLR 2019

observed sample. An important question is what the likelihood of this sample is given the trained generative model. Using the explicit probability model of equation 2.4 and the result of Theorem 1, we can (approximately) compute sample likelihoods using the trained generative model. We explain this result in the following corollary.

Corollary 2

Let G

and

P
Y

,Y^

(or,

alternatively PY,X )

be optimal

solutions of the entropic

GAN

equation 2.3. Let ytest be a new observed sample. We have

log

fY

(ytest)



-

1 

EP X

Y

=ytest

(ytest, G(x))

- H

P
X Y =ytest

(2.7)

+ EP X

Y

=ytest

- x2 2

+ constants.

The inequality becomes tight iff DKL

P
X

Y

=ytest

fX Y =ytest

= 0 where DKL(. .) is the Kullback-

Leibler divergence between two distributions.

3 GAN'S DUAL FORMULATION
In this section, we discuss dual formulations for OT GAN equation 2.2 and entropic GAN equation 2.3 optimizations. These dual formulations are min-max optimizations over two function classes, namely the generator and the discriminator. Often local search methods such as alternating gradient descent (GD) are used to compute a solution for these min-max optimizations.
First, we discuss the dual formulation of OT GAN optimization equation 2.2. Using the duality of the inner minimization, which is a linear program, we can re-write optimization equation 2.2 as follows (Villani, 2008):

min
GG

max
D1 ,D2

E [D1(Y )] - E [D2(G(X))] ,

(3.1)

where D1(y) - D2(y^)  (y, y^) for all (y, y^). The maximization is over two sets of functions D1

and D2 which are coupled using the loss function. Using the Kantorovich duality Villani (2008), we

can further simplify this optimization as follows:

min
GG

max
D -convex

E [D(Y )] - E

D( )(G(X))

,

(3.2)

where D( )(Y^ ) = infY (Y, Y^ ) + D(Y ) is the -conjugate function of D(.) and D is restricted to

-convex functions (Villani, 2008). The above optimization provides a general formulation for OT

GANs. If the loss function is . 2, then the optimal transport distance is referred to as the first order Wasserstein distance. In this case, the min-max optimization equation 3.2 simplifies to the following

optimization (Arjovsky et al., 2017):

min
GG

max
D:1-Lip

E [D(Y )] - E [D(G(X))] .

(3.3)

This is often referred to as Wasserstein GAN, or simply WGAN (Arjovsky et al., 2017). If the loss

function is quadratic, then the OT optimization is referred to as the quadratic GAN (or, W2GAN)

(Feizi et al., 2017).

Similarly, the dual formulation of the entropic GAN equation 2.3 can be written as the following optimization (Cuturi, 2013; Seguy et al., 2017) 1:

min
GG

max
D1 ,D2

E [D1(Y )] - E [D2(G(X))] - EPY ×PY^

[exp (v(y, y^)

)] ,

(3.4)

where

v(y, y^) = D1(y) - D2(y^) - (y, y^).

(3.5)

Note that the hard constraint of optimization equation 3.1 is being replaced by a soft constraint in optimization equation 3.2. In this case, optimal primal variables PY,Y^ can be computed according to the following lemma (Seguy et al., 2017):

1Note that optimization equation 3.4 is dual of optimization equation 2.3 when the terms H(PY ) + H(PY^ ) have been added to its objective. Since we have assumed that G is injective, these terms are constants and thus can be ignored from the optimization objective without loss of generality.

5

Under review as a conference paper at ICLR 2019

Lemma 1 Let D1 and D2 be the optimal discriminator functions for a given generator function G according to optimization equation 3.4. Let

v(y, y^) = D1(y) - D2(y^) - (y, y^).

(3.6)

Then,

PY,Y^ (y, y^) = PY (y)PY^ (y^) exp (v (y, y^) ).

(3.7)

This lemma is important for our results since it provides an efficient way to compute the optimal

coupling between real discriminators (D1 and

and generative variables (i.e. D2) of optimization equation

PY,Y^ ) using the 3.4. It is worth to

optimal generator (G) and note that without the entropy

regularization term, computing the optimal coupling in OT GAN using the optimal generator and

discriminator functions is not straightforward in general (unless in some special cases such as that of

the W2GAN (Villani, 2008; Feizi et al., 2017)). This is another additional computational benefit of

using entropic GAN. We use the algorithm presented in Sanjabi et al. (2018) to solve optimization

equation 3.4.

4 EXPERIMENTAL RESULTS

In this section, we supplement our theoretical results with experimental validations. One of the main

objectives of our work is to provide a framework to compute sample likelihoods in GANs. Such

likelihood statistics can then be used in several statistical inference applications that we discuss in

Section 5. With a trained entropic WGAN, the likelihood of a test sample can be lower-bounded

using Corollary 2. As shown in Lemma 1, WGAN with entropy regularization provides a closed-

form solution to the conditional density of the latent variable. From equation 3.7, we have

PY^ Y =ytest (y^)  PY^ (y^) exp v ytest, y^  .

By change of variables (and under the assumption that the generator is injective), we have

PX Y =ytest (x)  PX (x) exp v ytest, G(x)  .

(4.1)

In order to compute our proposed likelihood surrogate of Corollary 2, we need to draw samples from the distribution PX Y =ytest (x). One approach is to use a Markov chain Monte Carlo (MCMC) method to sample from this distribution. In our experiments, however, we found that MCMC demonstrates poor performance owing to the high dimensional nature of X. A similar issue with MCMC has been reported for VAEs in Kingma & Welling (2013). Thus, we use a different estimator to compute the likelihood surrogate which provides a better exploration of the latent space. We present our sampling procedure in Alg. 1.

Algorithm 1 Estimating sample likelihoods in GANs

1: 2: 3:

CSNaoommrmpplaueltieNzeuptioo=ignePttsXpxr(ioxibi.ai).bdeixPliptXi(e(vsxp)i(y=test,iNu=G1i ui(xi))

)

4:

Compute

L

=

-

1 

[iN=1

pi

l(ytest

,

G(xi))

+



Ni=1

pi

log

pi

]

-

iN=1

pi

xi 2 2

5: Return L

4.1 LIKELIHOOD EVOLUTION IN GAN'S TRAINING
In experiments of this section, we study how sample likelihoods vary as GAN's training progresses. An entropic WGAN is first trained on MNIST dataset. Then, we randomly choose ns = 1, 000 samples from MNIST test-set to compute the surrogate likelihoods using Algorithm 1 at different training iterations. We expect sample likelihoods to increase over training iterations as the generative model improves. A proper surrogate likelihood function should capture this trend. Fig. 2a demonstrates the evolution of sample likelihood distributions at different training iterations of the entropic WGAN. At iteration 1, surrogate likelihood values are very low as GAN's generated images are merely random noise. The likelihood distribution shifts towards high values during the training and saturates beyond a point. Details of this experiment are presented in Appendix C.

6

Under review as a conference paper at ICLR 2019

(a) (b)
Figure 2: (a) Distributions of surrogate sample likelihoods at different iterations of entropic WGAN's training using MNIST dataset. (b) Distributions of surrogate sample likelihoods of MNIST, MNIST-1 and SVHM datasets using a GAN trained on MNIST-1.

4.2 LIKELIHOOD COMPARISON ACROSS DIFFERENT DATASETS
The previous experiment studies how our likelihood estimate improves over the iterations for samples from the same dataset. In this section, we perform experiments across different datasets. An entropic WGAN is first trained on a subset of samples from the MNIST dataset containing digit 1 (which we call the MNIST-1 dataset). With this trained model, likelihood estimates are computed for (1) 1,000 samples from the entire MNIST dataset, and (2) 1,000 samples from the Street View House Numbers (SVHN) dataset (Netzer et al., 2011) (Fig. 2b). We note that highest likelihood estimates are obtained for samples from MNIST-1 dataset, the same dataset used in GAN's training. Moreover, the likelihood distribution for the MNIST dataset is bimodal with one mode peaking inline with the MNIST-1 mode. Samples from this mode correspond to digit 1 in the MNIST dataset. The other mode, which is the dominant mode, contains the rest of the digits and has relatively low likelihood estimates. The SVHN dataset, on the other hand, has much lower likelihood estimates than that of MNIST as its distribution is significantly different than that of MNIST images. Furthermore, we observe that the likelihood distribution of SVHN samples has a large spread (variance). This is because samples of the SVHN dataset is more diverse with varying backgrounds and styles than samples from MNIST. We note that SVHN samples with high likelihood estimates correspond to images that are similar to MNIST digits, while samples with low scores are very different than MNIST samples. Details of this experiment are presented in Appendix C.

4.3 APPROXIMATE LIKELIHOOD COMPUTATION IN UN-REGULARIZED GANS

Most standard GAN architectures do not have the entropy regularization. Likelihood lower bounds

of Theorem 1 and Corollary 2, however, hold even for those GANs as long as we obtain the opti-

mopatlimcoaul pcloinugpliPnYg,Y^PYin,Y^adfrdoimtiotnhetodtuhael

optimal generator G from GAN's training. formulation of OT GAN can be done when

Computation of the loss function

is quadratic (Feizi et al., 2017). In this case, the gradient of the optimal discriminator provides the

optimal coupling between Y and Y^ (Villani, 2008) (see Lemma. 2 in Appendix A).

For

a

general

GAN

architecture,

however, the

exact

computation of

optimal

coupling

P
Y

,Y^

may

be

difficult. One sensible approximation is to couple Y = ytest with a single latent sample x~ (we are

assuming the conditional distribution PX Y =ytest is an impulse function). To compute x~ correspond-

ing to a ytest, we sample k latent samples {xi }ik=1 and select the xi whose G(xi) is closest to ytest.

This heuristic takes into account both the likelihood of the latent variable as well as the distance

between ytest and the model (similarly to equation 3.7). We can then use Corollary 2 to approximate

sample likelihoods for various GAN architectures.

7

Under review as a conference paper at ICLR 2019

(a) (b)
Figure 3: (a) Sample likelihood estimates of MNIST, Office and CIFAR datasets using a GAN trained on the CIFAR dataset. (b) Sample likelihood estimates of MNIST, Office and LSUN datasets using a GAN trained on the LSUN dataset.

We use this approach to compute likelihood estimates for the CIFAR-10 (Krizhevsky, 2009) and LSUN-Bedrooms (Yu et al., 2015) datasets. For CIFAR-10, we train DCGAN while for LSUN, we train WGAN (details of these experiments can be found in Appendices ?? and ??, respectively).
Fig. 3a demonstrates sample likelihood estimates of different datasets using a GAN trained on the CIFAR dataset. Likelihoods assigned to samples from MNIST and Office datasets are lower than that of the CIFAR dataset. Samples from the Office dataset are assigned to higher likelihood values than samples of the MNIST dataset. We note that the Office dataset is indeed more similar to the CIFAR dataset than MNIST. A similar experiment has been repeated for LSUN-Bedrooms (Yu et al., 2015) dataset. Similar performance trends have been observed in this experiment (Fig. 3b).

5 CONCLUSION
In this paper, we have provided a statistical framework to understand GANs. Our main result shows that the entropic GAN optimization can be viewed as maximization of a variational lower-bound on average log-likelihoods, an approach that VAEs are based upon. This result makes a connection between two most-popular generative models, namely GANs and VAEs. More importantly, our result constructs an explicit probability model for GANs that can be used to compute a lower-bound on sample likelihoods. Our experimental results on various datasets demonstrate that this likelihood surrogate can be a good approximation of the true likelihood function. Although in this paper we mainly focus on understanding the behavior of the sample likelihood surrogate in different datasets, the proposed statistical framework of GANs can be used in various applications. For example, our proposed likelihood surrogate can be used as a quantitative measure to evaluate the performance of different GAN architectures in various datasets, it can be used to quantify the domain shifts across different datasets, it can be used to select a proper generator class for a given dataset, it can be used to detect outlier samples, it can be used in statistical tests such as hypothesis testing, etc. We leave exploring these directions for future work.

REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. arXiv:1701.07875, 2017.

Wasserstein GAN.

arXiv preprint

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pp. 2292­2300, 2013.

Soheil Feizi, Changho Suh, Fei Xia, and David Tse. Understanding GANs: the LQG setting. arXiv preprint arXiv:1710.10793, 2017.

8

Under review as a conference paper at ICLR 2019
Aude Genevay, Gabriel Peyre´, and Marco Cuturi. Sinkhorn-autodiff: Tractable wasserstein learning of generative models. arXiv preprint arXiv:1706.00292, 2017.
Arsham Ghahramani, Fiona M Watt, and Nicholas M Luscombe. Generative adversarial networks uncover epidermal regulators and predict single cell perturbations. bioRxiv, pp. 262501, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of Wasserstein GANs. arXiv preprint arXiv:1704.00028, 2017.
Xin Guo, Johnny Hong, Tianyi Lin, and Nan Yang. Relaxed wasserstein with applications to GANs. arXiv preprint arXiv:1705.07164, 2017.
Noriaki Hirose, Amir Sadeghian, Patrick Goebel, and Silvio Savarese. To go or not to go? a near unsupervised learning approach for robot navigation. arXiv preprint arXiv:1709.05439, 2017.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. URL http://arxiv. org/abs/1710.10196.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 1998.
Hung-yi Lee and Yu Tsao. Generative adversarial network and its applications to speech signal and natural language processing. IEEE International Conference on Acoustics, Speech and Signal Processing, 2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 5, 2011.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of Wasserstein GANs. arXiv preprint arXiv:1709.08894, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987, 2017.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. Solving approximate Wasserstein GANs to stationarity. arXiv preprint arXiv:1802.08249, 2018.
9

Under review as a conference paper at ICLR 2019

Eder Santana and George Hotz. Learning a driving simulator. arXiv preprint arXiv:1608.01230, 2016.
Vivien Seguy, Bharath Bhushan Damodaran, Re´mi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large-scale optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283, 2017.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein autoencoders. arXiv preprint arXiv:1711.01558, 2017.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Computer Vision and Pattern Recognition (CVPR), 2017.
Ce´dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.

APPENDIX A PROOFS

Using the Baye's rule, one can compute the log-likelihood of an observed sample y as follows:

log fY (y) = log fY X=x(y) + log fX (x) - log fX Y =y(x)

= log C(x) -

(y,

G(x))

-

log

 2

-

x 2

2

-

log fX

Y =y(x),

(A.1)

where the second step follows from equation 2.4.

Consider a joint density function PX,Y such that its marginal distributions match PX and PY . Note that the equation equation A.1 is true for every x. Thus, we can take the expectation of both sides
with respect to a distribution PX Y =y. This leads to the following equation:

log fY (y) =EPX Y =y

-

(y, G(x))



+

log C(x)

-

1 2

log

2

-

x 2

2

-

log

fX

Y

=y (x)

=EPX Y =y

-

(y, G(x))



+

log

C (x)

-

1 2

log

2

-

x 2

2

-

log fX

Y

=y (x)

(A.2)

+ log PX Y =y(x) - log PX Y =y(x)

= - EPX Y =y

[

(y, G(x))

] -

1 2

log

2

+

EPX

Y

=y

+ KL PX Y =y fX Y =y + H PX Y =y , where H(.) is the Shannon-entropy function.

log C(x) -

x2 2

Next we take the expectation of both sides with respect to PY :

E [log fY (Y )] = -

1  EPX,Y

[

(y, G(x))] -

1 2

log 2 + EfX

log C(x) -

x2 2

(A.3)

+ EPY KL PX Y =y fX Y =y + H (PX,Y ) - H (PY ) .

Here, we replaced the expectation over PX with the expectation over fX since one can generate

an arbitrarily large number of samples from the generator. Since the KL divergence is always non-

negative, we have

E

[log

fY

(Y

)]



-

1 

EPX,Y [ (y, G(x))] - H (PX,Y )

+EfX

[log

C (x)]

-

log(m)

-

r

+

log 2

2

sample likelihood

GAN objective with entropy regularizer

(A.4)

10

Under review as a conference paper at ICLR 2019

This inequality is true for every PX,Y satisfying the marginal conditions. Thus, similar to VAEs, we can pick PX,Y to maximize the lower bound on average sample log-likelihoods. This leads to the entropic GAN optimization equation 2.3.

Lemma 2 Let PY be absolutely continuous whose support contained in a convex set in Rd. Let Dopt be the optimal discriminator for a given generator G in W2GAN. This solution is unique. Moreover,
we have

Dopt(Y ) d=ist G(X),

(A.5)

where d=ist means matching distributions.

APPENDIX B SINKHORN LOSS

In practice, it has been observed that a slightly modified version of the entropic GAN demonstrates

improved computational properties (Genevay et al., 2017; Sanjabi et al., 2018). We explain this

modification in this section. Let

W

,(PY

,

PY^

)

=

min
PY,Y^

E

(Y, Y^ ) + DKL PY,Y^

,

(B.1)

where DKL(. .) from that of the

is the KullbackLeibler divergence. Note entropic GAN optimization equation 2.3

that the object by a constant

of this optimization differs term H(PY ) + H(PY^ ).

A sinkhorn distance function is then defined as (Genevay et al., 2017):

W¯ ,(PY , PY^ ) = 2W ,(PY , PY^ ) - W ,(PY , PY ) - W ,(PY^ , PY^ ).

(B.2)

W¯ is called the Sinkhorn loss function. Reference Genevay et al. (2017) has shown that as   0, W¯ ,(PY , PY^ ) approaches W ,(PY , PY^ ). For a general , we have the following upper and lower bounds:

Lemma 3 For a given  > 0, we have W¯ ,(PY , PY^ )  2W ,(PY , PY^ )  W¯ ,(PY , PY^ ) + H(PY ) + H(PY^ ).

(B.3)

fWsP¯oirnroc,ooepf(WtPiFmYr,i,ozP(maPYt^iYto)hn,e2Ped+Yqeu)fiant2iitoHHino(n(BPPe.YY1q))u)aa(+ttnihodinss2ciBHma.n2i(l,baPrewYl^yse)e.Whenav,bey(WPusY^i,n, gP(PYa^Yn),idPeY^Hn)ti(tPyY^cWo)¯u, pw,lie(nPhgYaav,sePaYW^fe)a,2si(.bPlMeYos,orPeluYo^tv)ioern,

Since H(PY ) + H(PY^ ) is constant in our setup, optimizing the GAN with the Sinkhorn loss is equivalent to optimizing the entropic GAN. So, our likelihood estimation framework can be used with models trained using SInkhorn loss as well. This is particularly important from a practical standpoint as training models with Sinkhorn loss tends to be more stable in practice.

APPENDIX C TRAINING ENTROPIC GANS
In this section, we discuss how WGANs with entropic regularization is trained. As discussed in Section 3, the dual of the entropic GAN formulation can be written as

where

min
GG

max
D1 ,D2

E [D1(Y )] - E [D2(G(X))] - EPY ×PY^

[exp (v(y, y^)

)] ,

v(y, y^) = D1(y) - D2(y^) - (y, y^).

We can optimize this min-max problem using alternating optimization. A better approach would be to take into account the smoothness introduced in the problem due to the entropic regularizer, and solve the generator problem to stationarity using first-order methods. Please refer to Sanjabi et al. (2018) for more details. In all our experiments, we use Algorithm 1 of Sanjabi et al. (2018) to train our GAN model.

11

Under review as a conference paper at ICLR 2019

C.1 TRAINING DETAILS FOR MNIST DIGITS
MNIST dataset constains 28×28 grayscale images. As a pre-processing step, all images were resized in the range [0, 1]. The Discriminator and the Generator architectures used in our experiments are given in Tables. 1,2. Note that the dual formulation of GANs employ two discriminators - D1 and D2, and we use the same architecture for both. The hyperparameter details are given in Table 3. Some sample generations are shown in Fig. 4
C.2 TRAINING DETAILS FOR CIFAR DATASET
We trained a DCGAN model on CIFAR dataset using the discriminator and generator architecture used in Radford et al. (2015). The hyperparamer details are mentioned in Table. 4. Some sample generations are provided in Figure 6

Table 1: Generator architecture

Layer
Input Fully connected
Reshape BatchNorm+ReLU Deconv2d (5 × 5, str 2) BatchNorm+ReLU Remove border row and col. Deconv2d (5 × 5, str 2) BatchNorm+ReLU Deconv2d (5 × 5, str 2)
Sigmoid

Output size
128 4.4.256 256 × 4 × 4 256 × 4 × 4 128 × 8 × 8 128 × 8 × 8 128 × 7 × 7 64 × 14 × 14 128 × 8 × 8 1 × 28 × 28 1 × 28 × 28

Filters
128  256
256  128 128  64 64  1 -

Table 2: Discriminator architecture

Layer
Input Conv2D(5 × 5, str 2)
LeakyReLU(0.2) Conv2D(5 × 5, str 2)
LeakyReLU(0.2) Conv2d (5 × 5, str 2)
LeakyRelU(0.2) Reshape
Fully connected

Output size 1 × 28 × 28 32 × 14 × 14 32 × 14 × 14 64 × 7 × 7 64 × 7 × 7 128 × 4 × 4 128 × 4 × 4
128.4.4
1

Filters
1  32
32  64
64  128
2048  1

Table 3: Hyper-parameter details for MNIST experiment

Parameter
 Generator learning rate Discriminator learning rate
Batch size Optimizer Optimizer params Number of critic iters / gen iter Number of training iterations

Config
5 0.0002 0.0002
100 Adam 1 = 0.5, 2 = 0.9
5 10000

12

Under review as a conference paper at ICLR 2019

Figure 4: Samples generated by Entropic GAN trained on MNIST

Figure 5: Samples generated by Entropic GAN trained on MNIST-1 dataset
C.3 TRAINING DETAILS FOR LSUN-BEDROOMS DATASET We trained a WGAN model on LSUN-Bedrooms dataset with DCGAN architectures for generator and discriminator networks (Arjovsky et al., 2017). The hyperparameter details are given in Table. 5, and some sample generations are provided in Fig. 7

Table 4: Hyper-parameter details for CIFAR-10 experiment

Parameter
Generator learning rate Discriminator learning rate
Batch size Optimizer Optimizer params Number of training epochs

Config
0.0002 0.0002
64 Adam 1 = 0.5, 2 = 0.99 100

Table 5: Hyper-parameter details for LSUN-Bedrooms experiment

Parameter
Generator learning rate Discriminator learning rate
Clipping parameter c Number of critic iters per gen iter
Batch size Optimizer Number of training iterations

Config
0.00005 0.00005
0.01 5 64
RMSProp 70000

13

Under review as a conference paper at ICLR 2019
Figure 6: Samples generated by DCGAN model trained on CIFAR dataset
Figure 7: Samples generated by WGAN model trained on LSUN-Bedrooms dataset 14

