Under review as a conference paper at ICLR 2019
DON'T LET YOUR DISCRIMINATOR BE FOOLED
Anonymous authors Paper under double-blind review
ABSTRACT
Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel. Some of the most promising adversarial models today minimize a Wasserstein objective. It is smoother and more stable to optimize. In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties. By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss. We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively. The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions.
1 INTRODUCTION
Generative adversarial networks (GANs) (Goodfellow et al., 2014a) are at the forefront of generative modeling. They cast generative modeling as a never ending duel between two networks: a generator network produces synthetic samples from random noise and a discriminator network distinguishes synthetic from real data samples. GANs produce visually appealing samples, but are often hard to train. Much recent work, tries to stabilize GAN training through novel architecture (Radford et al., 2015; Denton et al., 2015) or training objectives(Mao et al., 2017; Arjovsky et al., 2017; Gulrajani et al., 2017).
In this paper, we show that GAN objectives significantly stabilize, if the discriminator is robust to adversarial attacks. Specifically, we show that a robust discriminator leads to a robust minimax objective for the generator irrespective of the training objective used. In addition, any training objective is smooth and Lipschitz as long as the discriminator is Lipschitz. Finally, we show that this robustness does not need to be enforced for every single input of the discriminator, but rather just in expectation over the generated samples. We present two new regularization terms, borrowed from the adversarial training literature. Both terms ensure the robustness of the discriminator. They are easy to optimize and can be added to any existing GAN objective.
Our experiments show that adversarial robustness both improves the visual quality of the results, as well as stabilizes the training procedure across a wide range of architectures, hyper-parameters and training objectives. We will publish the code and data used to perform our experiments upon acceptance.
2 RELATED WORKS
The scope of generative modeling changed significantly with the emergence of photo realistic image generation like Generative Adversarial Networks (GANs) (Goodfellow et al., 2014a), Variational AutoEncoders (Kingma & Welling, 2013), or Pixel Convolutional Networks (Oord et al., 2016). The visual quality of these generative models enabled applications such as, generative image processing (Pathak et al., 2016), image editing (Zhu et al., 2016), and image translation (Isola et al., 2017; Zhu et al., 2017). They also started the race for ever prettier generated images. Two major areas of improvement are architectures (Radford et al., 2015; Denton et al., 2015) and loss functions (Mao et al., 2017; Arjovsky et al., 2017; Gulrajani et al., 2017). Denton et al. (2015) showed that a Laplacian pyramid generator greatly improves the quality of the generated images, while Radford et al. (2015) showed
1

Under review as a conference paper at ICLR 2019

standard discriminator

robust discriminator

Figure 1: Results from four generative adversarial models. The leftmost model is trained with a standard discriminator, while the models on the right use an increasingly more robust discriminator. We show that a robust discriminator leads to a smoother and more stable training objective, resulting in a better generative model. Best viewed on screen.

impressive results by carefully balancing the expressive power of the generator and discriminator networks. In this work, we use Radford et al. (Radford et al., 2015) architecture, and solely focus on the loss function.
GANs come in three popular loss functions: the original Jensen-Shanon divergence objective (Goodfellow et al., 2014a), least squares GANs (LSGAN) (Mao et al., 2017), and Wasserstein distance (WGAN) (Arjovsky et al., 2017). Each objectives minimizes special case of a f-divergence between the generative and data distribution (Nowozin et al., 2016). Arjovsky et al. (2017) augment the GAN objective with a smoothness condition in the form of a Lipschitz-1 constraint on the discriminator. However, enforcing a Lipschitz-1 discriminator is difficult. Arjovsky et al. (2017) clip the weights of the discriminator to enforce the Lipschitz condition, while Gulrajani et al. (2017) apply a penalty on the gradient of the discriminator. The gradient penalty (WGAN-GP) generally yields better results, however it looses some of the appealing theoretical properties of WGANs. Arjovsky et al.show that in theory most GAN objective are non-continuous and non-Lipschitz, while the Wasserstein objective is Lipschitz, smooth, and more stable to optimize. In this paper, we extend their analysis and show that any GAN objective can be made smooth as long as the discriminator is robust to adversarial perturbations. In addition, the discriminator only needs to be robust to adversarial attacks in expectation over all generated samples. A simple penalty function is sufficient to carry the theoretical smoothness results of WGAN other GAN objectives.
Our theoretical analysis is related to Bottou et al. (2017), who study the geometry to common distance measures for generative modeling. We build on their analysis and show that any GAN objective is Lipschitz and smooth, as long as the generator and discriminator are Lipschitz.
Adversarial Examples Despite success of deep networks, their differentiable nature makes them vulnerabilities to adversarial attacks. Small perturbation of the input can significantly change the output of a network (Szegedy et al., 2013). The initial work of Szegedy et al. (2013) set off an arms race between novel attack methods (Goodfellow et al., 2014b; Carlini & Wagner, 2017; Papernot et al., 2016a; Moosavi Dezfooli et al., 2016) and defenses (Kannan et al., 2018; Papernot et al., 2016b) against these attacks. The goal of an attacker is to find an perturbation in an attack region that yields a large change in output. Some of the fastest attacks simply perturb the input in the gradient direction of the network (Goodfellow et al., 2014b), while more complex attacks optimize for an attack vector (Carlini & Wagner, 2017). Many attacks can be directly used in a defense through dataset augmentation (Papernot et al., 2016b). In this work, we primarily use the fast gradient methods of Goodfellow et al. (2014b), in particular we use the normalized gradient attack.
3 PRELIMINARIES
Let z  PZ be a sample from a noise distribution PZ , e.g. uniform noise. Let x  PR be a sample form a data distribution PR. The goal of sampling based generative modeling is to find a function
2

Under review as a conference paper at ICLR 2019

G(z) that maps samples from PZ to PR. More specifically, we want PY = {y = G(z)|z  PZ } to closely resemble PR.

Generative Adversarial Networks A generative adversarial network (GAN) optimizes a two player game between a generative model G and a discriminator D. The generator G maps noise to data samples. A discriminator D then judges if the transformed noise is close enough to the true data distribution PR. A GAN jointly optimizes both generator and discriminator in a minimax game. The GAN objective L minimizes a generator G and maximizes a discriminator D over

L(G, D) = ExPR [f (-D(z))] + EzPZ [f (D(G(z)))], where f is differentiable, usually concave, loss function. The generator minimizes

(1)

(G) = max L(G, D).
DD

(2)

In practice, both generator G(z; ) and discriminator D(x; ) are deep neural networks with parame-

ters  and  respectively. The discriminator D produces a single scalar output, which is then passed

through the loss function f . Different choices of f lead to different GAN models. The original GAN

objective uses a sigmoid log likelihood loss for f (z) = - log(1 + exp(-x)), and corresponds to the

Jensen-Shannon divergence between data and generator (Goodfellow et al., 2014a). A Euclidean

loss

f (z)

=

-

1 2

(1

+

x)2

leads

to

least

squares

GAN

(LSGAN)

(Mao

et

al.,

2017).

A

Lipschitz-1

discriminator D and identity loss functions f (z) = x corresponds to the Wasserstein distance in

WGAN (Arjovsky et al., 2017). In all these examples f is concave, and for GAN and LSGAN

non-positive.

Adversarial attacks We follow the standard adversarial attack definition in literature (Carlini & Wagner, 2017; Kannan et al., 2018). Definition 3.1 (robustness). A function h(z) is robust to adversarial perturbations  for an input x if and only if
|h(z) - h(x + )| <  for all  p < , where the p-norm · p defines the local attack region.
For simplicity of the analysis, we focus on distance norms | · | instead of general distance functions of Carlini & Wagner (2017). Our definition includes logit pairing of Kannan et al. (2018).
Definition 3.1 focuses on individual inputs x drawn from an empirical data distribution of training or testing images. We extend this definition to generative distributions G(z).
Definition 3.2 (expected robustness). A function h, e.g. a discriminator, is robust to adversarial perturbations for a generative distribution G(z) if and only if
EzPZ [|h(G(z)) - h(G(z) + u(z))|] <  for all functions u with u(z) p < , where the p-norm · p defines the local attack region, and d measures the distance between the network outputs.
This definition only requires robustness in expectation, but not for every single sample z  PZ. A discriminator D is robust if it satisfies definition 3.2 for h = D. A discriminator loss f is robust for h = f  D. For notational simplicity, we define the additive combination of a generator G and a perturbation function u as G + u where (G + u)(z) = G(z) + u(z).

4 ROBUST GENERATIVE ADVERSARIAL NETWORKS
In this section, we show that the GAN objective is robust to adversarial perturbations, as long as the discriminator is robust. This property even holds as the discriminator adapts with the perturbation applied. We first analyze the case when the loss of the discriminator is robust to an adversarial attack, and then study the robustness of the discriminator directly. Theorem 4.1 (robust loss). If the loss of a discriminator D is robust to adversarial perturbations u(z) p <  in expectation
EzPZ [|f (D(G(z))) - f (D(G(z) + u(z)))|] <  then the adversarial objective is robust
| (G) - (G + u)| < 

3

Under review as a conference paper at ICLR 2019

Proof. Let DG be the optimal discriminator for generator G. For this discriminator we have

L(G, DG) = max L(G, D)  L(G, DG+u),
D

(3)

by definition. This allows us to bound the difference in the objective for a generator G and its perturbation G + u:

(G) - (G + u)  L(G, DG) - L(G + u, DG)

Inequality 3

= Ezµz [f (DG(G(z))) - f (DG(G(z) + u(z)))] <  linearity of expectation

We can equivalently bound (G + u) - (G) < , and obtain | (G) - (G + u)| < .

The above proof uses the (global) optimality of the discriminator in Equation (3). However, the bound holds for any discriminator that improves the objective, without reaching a local or global minima, and could thus be generalized.
Next, we focus our attention to robust discriminators. Here, we rely on an additional assumption that all loss functions f we consider are concave. This is the case for the three most popular architectures: original GAN, LSGAN and WGAN. Theorem 4.2 (robust discriminator). For a concave loss f and a discriminator D that is robust to perturbations u(z) p <  in expectation
EzPZ [|D(G(z)) - D(G(z) + u(z))|] <  the adversarial objective is robust
| (G) - (G + u)| < C
where C = EzPZ [max(|f (D(G(z)))| , |f (D(G(z) + u(z)))|)] is the expected loss gradient.
Proof. The proof follows directly from the definition of concavity f (a) - f (b)  f (a)(a - b) and the fairly conservative bound |f (a) - f (b)|  max(|f (a)|, |f (b)|)|a - b|. In expectation, this bound reduces the robust discriminator to a robust loss
EzPZ [|f (D(G(z))) - f (D(G(z) + u(z)))|]  EzPZ [|D(G(z)) - D(G(z) + u(z))|] C < C. The rest of the proof follows Theorem 4.1.

For the original GAN C < 1, for WGAN C = 1, and for LSGAN C  EzPZ [|1 + D(G(z))|] + , where |1 + D(G(z))| is directly minimized in the objective and is close to zero.
Theorem 4.1 and Theorem 4.2 have some interesting implications for general GANs. First, any GAN that is trained with a robust discriminator or loss has a robust and hence smooth objective. This directly extends the theoretical properties of WGANs (Arjovsky et al., 2017) to other GAN model. It further relaxes the strict Lipschitz constraints to adversarial robustness in expectation. This is much easier to enforce in practice as we will show in the next section.
Second, Theorem 4.2 allows us to analyze the continuity and Lipschitzness of any GAN objective. In particular, we can show any GAN objective is continuous or Lipschitz in its parameters as long as the discriminator and generator are continuous or Lipschitz. This is in direct contradiction to Arjovsky et al. (2017), that show only WGAN to be continuous. However, their counter example relies on a discontinuous discriminator with infinitely large weights. See supplemental material for a detailed derivation and proof.
Robust discriminators or losses can be hard to train in practice, as they require constrained optimization or a carefully tuned architecture. Next, we show how a regularized discriminator objective can lead to a robust discriminator without the need for any constraints.

4.1 TRAINING A ROBUST DISCRIMINATOR

Our goal is to train a discriminator D such that it is robust to adversarial perturbations in expectation. We do this by augmenting the original discriminator training objective with an additional adversarial regularization

(D, G) = ExPZ

max (D(G(z)) - D(G(z) + v))2
v: v p<

.

(4)

4

Under review as a conference paper at ICLR 2019

In adversarial defense, this is known as distillation (Papernot et al., 2016b) or logit pairing (Kannan

et al., 2018), where v is the attack vector in an attack region v p < . For the main evaluation,

we find the attack vector using the fast normalized gradient attack (Goodfellow et al., 2014b):

v=

D(G(z)) D(G(z))

2

.

Additional

results

using

other

attack

methods

are

in

the

supplementary

material.

This adversarial regularization is sufficient to ensure a robust discriminator.

Theorem 4.3 (regularized robustness). For a non-positive loss f and a regularized discriminator objective
maximize L(D, G) - (D, G)
DD
the optimal discriminator D is robust

ExPZ [|D(G(z)) - D(G(z) + u(z))|]  for all u(z) p < .

-2f (0) 

Proof. We know that the objective value of the optimal discriminator is larger than the objective of a constant zero discriminator:
L(D, G) - (D, G)  L(0, G) - (0, G) = L(0, G) (D, G)  L(D, G) - L(0, G)  -L(0, G) = -2f (0),

where the last inequality holds due to the non-positively of the objective. Using Jensens inequality we can further reduce

u:

max
u(z) p

<

ExPZ

[|D(G(z))

-

D(G(z)

+

u(z))|]2



u:

max
u(z) p

<

ExPZ

|D(G(z)) - D(G(z) + u(z))|2

= (D, G)  -2f (0) . 

As we increase the weight  of the robustness term, the bound in Theorem 4.3 tightens and the discriminator is more robust to adversarial perturbations. A similar bound can be derived for an absolute instead of squared loss .
While the adversarial regularization  yields all the nice theoretical benefits of a robust discriminator, in practice it often does not provide enough regularization. A much stronger regularization is to match the features (x) of the penultimate layer of the discriminator network D(x) = w (x). We call this robust feature matching (RFM). Our robust feature matching minimizes

r(D, G) = EzPz

max
v: v p<

(G(z)) - (G(z) + v)

2 2

+

w

2 2

(5)

where v is the attack direction, w is the weight of the last linear layer, and   R+ is a hyperparameter. We use  = 0.05 and  = 10-4 in all our experiments. Similar to Theorem 4.3, robust feature matching ensures the robustness of the optimal discriminator

ExPZ

[|D(G(z))

-

D(G(z)

+

u(z))|]



-2f (0) . 

For a detailed proof see supplement.

Next, we will give some intuition on how adversarial defense changes the loss landscape of a GAN on a small toy example.

4.2 A TOY EXAMPLE
We verify the above robustness properties on the simple one dimensional Dirac-GAN of Mescheder (2018). It allows us to separate practical optimization issues from the form and regularity of the loss function. We can easily optimize the discriminator of Dirac-GAN to convergence with or without

5

Under review as a conference paper at ICLR 2019

1.0 WGAN

0.8 0.6

LSGAN GAN

0.4

0.2

0.0 2 1 0 1 2

2 10 1 2

2 10 1 2

2 10 1 2

(a) unconstrained

(b) robust loss

(c) robust discriminator (d) regularized objective

Figure 2: Dirac-GAN objective value as a function of the generator parameter  for linear (WGAN), least squares (LSGAN) and Jensen-Shanon (GAN) objectives: (a) without constraints, (b) constrains on the loss function, (c) constrains the discriminator output, (d) a regularizer on the discriminator output. Adding adversarial robustness to the discriminator smoothes out any objective. Best viewed in color.

robustness constraints or penalties. In the Dirac-GAN, the true data distribution pD is represented by a Dirac distribution centered at zero. The generator produces samples from another Dirac distribution centered at , where  is the only learnable generator parameter. The discriminator is a linear function D(x) = x with a single scalar parameter . This experimental setup is analogous to the two dimensional toy example of Arjovsky et al. (2017). The GAN objective of Dirac-GAN reduces to L(G) = max f () + f (0). We plot this objective in Figure 2a for unconstrained GAN, LSGAN and WGAN respectively. Note, that all objectives are zero if the data and generative distributions match and constant otherwise. Without any regularization, neither one of these objectives will provide a meaningful gradient signal.
Adding a robust loss Figure 2b, Theorem 4.1, or a robust discriminator Figure 2d, Theorem 4.2, will smooth out all loss functions and provide a meaningful gradient signal throughout training. Finally, the regularized loss shows an equally smooth loss curve. This serves as an illustrative example of the effect of robustness on the overall loss function of a GAN.
5 EXPERIMENTS
We perform all our experiments on the MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky, 2009) and CelebA (Liu et al., 2015) datasets. We use a slightly modified DCGAN (Radford et al., 2015) architecture. All convolutional blocks are replaced with residual blocks (He et al., 2016), the generator employs batch normalization (Ioffe & Szegedy, 2015) and ReLU nonlinearities, while the discriminator uses instance normalization (Ulyanov et al., 2016) and Leaky ReLU. We train using a weight decay term  = 10-4, with batch size n = 64 and optimize using ADAM (Kingma & Ba, 2014) with h = 2 · 10-4 and 0 = 0, 1 = 0.9 for 50 epochs on CIFAR10 and 25 epochs on CelebA. We use a latent vector of dimension z = 128 and use a unit Gaussian for our sampling distribution. For complete training and architecture details see supplement.
Robust GAN training We compare original Jenson-Shannon divergence loss (GAN) (Goodfellow et al., 2014a), the least squares loss (LSGAN) (Mao et al., 2017), and the Wasserstein distance (WGAN) (Arjovsky et al., 2017) on CIFAR10 and CelebA. Along with these losses, we add a regularization method: Instance Noise (IN) (Sønderby et al., 2016), Gradient Penalty (GP) (Gulrajani et al., 2017), Adversarial Regularization (AR) , and Robust Feature Matching (RFM) r. We tuned the hyper-parameters for each combination of loss functions and robustness terms (or none).
We present quantitative results in terms of Fréchet Inception Distance (FID) (Heusel et al., 2017), as well as qualitative results. The FID score measures the distance between two Gaussian distributions that are estimated from the deep feature statistics from the real and generated distributions. Distributions that are close have a lower FID score, distributions that are large have a higher score. Heusel et al. (2017) showed that the FID metric corresponds well with human judgment of image quality. In our experiments, we use FID as the primary quantitative measure of image quality. As we are mainly interested in the relative improvements of different methods we did not extensively tune the architecture. See the supplementary for additional details on experimental setup.
6

Under review as a conference paper at ICLR 2019

GAN

LSGAN

WGAN

None

GP

DS RFM

Figure 3: Random image samples from CIFAR10 for various GAN objectives (GAN, LSGAN, WGAN) and robust regularizations (none, GP, AR, RFM). Best viewed on screen.

Table 1 shows the quantitative results. Without any regularization the original and least squares objective work best, while the linear (WGAN) objective does not train well. This is not surprising, as an unconstrained linear objective easily large discriminator outputs and unstable gradients. Adding instance noise (IN) exacerbates this problem for CIFAR-10, but slightly helps for Celeb-A. The gradient penalty (GP) fixes some of the instabilities for both CIFAR-10 and Celeb-A, and helps most for the linear (WGAN) model. However, the gradient penalty does not nearly perform as well as our robust regularizations (AR and RFM), both of which improve the quality of the synthesized images throughout different GAN objectives. Figures 3 and 4 show the corresponding qualitative results. More generated images as well as additional results measuring the effect of different adversarial attacks can be found in supplementary material.

loss
GAN LSGAN WGAN

None
34.25 32.03 41.58

regularization
IN GP AR
32.28 30.22 24.06 32.89 29.72 27.42 49.80 26.20 28.42

RFM
24.50 25.22 24.90

None
34.63 36.27 53.50

regularization
IN GP AR
14.23 19.61 12.04 14.06 21.56 15.60 19.41 16.12 14.16

RFM
12.15 11.45 13.40

(a) FID on CIFAR10

(b) FID on Celeb-A

Table 1: FID scores on CIFAR10 and FID on Celeb-A for various GAN losses and regularizations. Lower is better for FID.

Stability across architectures/hyperparameters We follow Gulrajani et al. (2017) and evaluate the robustness of our regularization across a variety of experimental setups on CIFAR-10. We randomly sample 55 different experiment setups from a large pool of commonly used losses (JS, LSGAN, WGAN), batch size (8, 64), learning rate (10-3, 10-4), network blocks (vanilla convolution, ResNet), filter size (32, 128), nonlinearities (ReLU, LeakyReLU, Tanh) and normalizations (BatchNorm, InstanceNorm, none). For each random setup, we train four separate models: without regularization, with additive instance noise, with gradient penalty, and with RFM. We rank them from 1 to 4 in order of increasing FID score. We then count how many times a given method performs the

7

Under review as a conference paper at ICLR 2019

GAN

LSGAN

WGAN

None

GP

DS RFM

Figure 4: Random image samples from CelebA for various GAN objectives (GAN, LSGAN, WGAN) and robust regularizations (none, GP, AR, RFM). Best viewed on screen.

best and worst out of the batch. Table 2 shows the results. Our robust feature matching consistently outperform other regularization, performing best in over two thirds of the experimental setups.
Experimentally, a robust loss or penalty leads to better convergence, see supplement. However, the theoretical implications of adversarial robustness on local convergence is not yet well understood and warrants further investigation.

None Noise GP RFM
# Top Perf. 1 6 11 37 # Worst Perf. 39 9 4 3 Average Rank 3.6 2.7 2.2 1.5
Table 2: Performance under different experimental setups.

6 CONCLUSION
In this paper, we established a clear connection between robust discriminators in generative adversarial networks and the overall smoothness of the optimization and the quality of the results. To our knowledge, we are the first to show that a robustness regularization guarantees a smooth and robust loss function for any GAN objective. Finally, our results suggest that robust regularization leads to better training and visual results than standard gradient penalties.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. 2017.
Leon Bottou, Martin Arjovsky, David Lopez-Paz, and Maxime Oquab. Geometrical insights for implicit generative modeling. 2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39­57. IEEE, 2017.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, pp. 1486­1494, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672­2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. CoRR, 2014b.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, pp. 5769­5779, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770­778, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Günter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. pp. 5967­5976, 2017.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, 2009.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, December 2015.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, pp. 2813­2821. IEEE, 2017.
Lars Mescheder. On the convergence properties of gan training. arXiv preprint arXiv:1801.04406, 2018.
Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In CVPR, 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In NIPS, pp. 271­279, 2016.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. ICML, 2016.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), IEEE, pp. 372­387. IEEE, 2016a.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 582­597, 2016b.
9

Under review as a conference paper at ICLR 2019 Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:
Feature learning by inpainting. In CVPR, pp. 2536­2544, 2016. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. CoRR, 2015. Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszár. Amortised map inference
for image super-resolution. arXiv preprint arXiv:1610.04490, 2016. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. CoRR, 2013. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast
stylization. arXiv preprint arXiv:1607.08022, 2016. Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the
natural image manifold. In ECCV, pp. 597­613. Springer, 2016. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. In ICCV, 2017.
10

Under review as a conference paper at ICLR 2019

A CONTINUITY AND LIPSCHITZNESS
We start by showing that any GAN with a continuous generator and disciminator has itself a continuous objective. Corollary A.1. If generator, discriminator and loss function fF in a GAN are continuous, then the GAN objective (G()) is continuous in the generator parameters .
Proof. By definition of continuity we have G(z; ) - G(z;  + ) < G for all  <  and all noise samples z, and |fF (D(x)) - fF (D(x + x))| < D for all x < x. Using Theorem 4.1, we know | (G()) - (G() + u)| < D for any perturbation u(z) = G(z;  + ) - G(z; ) as long as u(z) < x. Hence, | (G()) - (G( + ))| < D for all  < , if x = G.
The same properties hold for Lipschitz functions. Corollary A.2. In a GAN, if the generator is K-Lipschitz and the discriminator is L-Lipschitz, then the GAN objective is (KLC)-Lipschitz in the generator parameters , where C is the expected slope of the loss function.
Proof. By definition of Lipschitz continuity any perturbation u(z) = G(z;  + ) - G(z; ) is bounded u(z) < K  . Furthermore the composition of Lipschitz functions is bounded by |D(G(z;  + )) - D(G(z; ))| < KL x . Combining x = u(z) with Theorem 4.2 yields | (G()) - (G( + ))| < KLC  . Hence the GAN objective is (KLC)-Lipschitz.

B ROBUST FEATURE MATCHING BOUND

Here we show that for a optimal discriminator D(x) = w (x) in robust feature matching

r(D, G) = EzPz

max
v: v p<

(G(z)) - (G(z) + v)

2 2

+

w

2 2

is robust. Theorem B.1. For a non-positive loss f and a robust feature matching objective

maximize
DD

L(D,

G)

-

r

(D,

G)

the optimal discriminator D is robust

ExPZ

[|D(G(z))

-

D(G(z)

+

u(z))|]



-f (0) 

for all u(z) p < .

(6)

Proof. We know that the objective value of the optimal discriminator is larger than the objective of a constant zero discriminator:
L(D, G) - r(D, G)  L(0, G) - r(0, G) = L(0, G) r(D, G)  L(D, G) - L(0, G)  -L(0, G) = -2f (0),

where the last inequality holds due to the non-positively of the objective. For every element of the

RFM objective

(G(z)) - (G(z) + v)

2 2

+



w

2 2



 2 

w

2

(G(z)) - (G(z) + v)

2

2  w 

(G(z)) - w

(G(z) + v) 2

= 2 |D(G(z)) - D(G(z) + v)|.

Square expansion Cauchy­Schwarz

Hence

ExPZ

[|D(G(z))

-

D(G(z)

+

u(z))|]



1 2

r

(D,

G)



-f (0) . 

11

Under review as a conference paper at ICLR 2019

C NETWORK ARCHITECTURE

We use a network architecture almost identical to WGAN-GP for consistency. The generator employs pre-activation batch normalization with ReLU, while the discriminator uses instance normalization with Leaky ReLU for better gradients. We have run experiments using a variety of architectures, including DCGAN, which WGAN-GP builds off of. We find that the results are similar and decided on the architecture from WGAN-GP for the computation efficiency due to the constant filter size.

Layer
Latent vector Linear Reshape
Residual Block Residual Block
Upsample Residual Block Upsample Residual Block Upsample Residual Block
Conv 1x1 Sigmoid

Output shape
128 2048 4 × 4 × 128
4 × 4 × 128 4 × 4 × 128
8 × 8 × 128 8 × 8 × 128 16 × 16 × 128 16 × 16 × 128 32 × 32 × 128 32 × 32 × 128
32 × 32 × 3 32 × 32 × 3

(a) Generator architecture.

Layer
Image Conv 3x3 Residual Block
Downsample Residual Block Downsample Residual Block Downsample Residual Block
Residual Block Residual Block Residual Block
Global Average Pooling Linear

Output shape
32 × 32 × 3 32 × 32 × 128 32 × 32 × 128
16 × 16 × 128 16 × 16 × 128
8 × 8 × 128 8 × 8 × 128 4 × 4 × 128 4 × 4 × 128
4 × 4 × 128 4 × 4 × 128 4 × 4 × 128
128 1

(b) Discriminator architecture.

Table 3: GAN Architecture for our experiments on CIFAR-10 and CelebA. Note that CelebA follows the same architecture, but the generator has a few more upsampling and processing blocks to output an image of the correct size.

D USING A ROBUST LOSS

Here, we present additional quantitative and qualitative results for robust loss f , instead of discriminator. For all regularization, we penalize the gradient or robustness of the loss directly, instead of the discriminator output. The results here agree with our theoretical analysis that a robust loss is beneficial for the generator. However, here the gradient penalty performs worse for GAN and LSGAN, which might be due to vanishing gradients in the loss function close to an optimal discriminator.

loss
GAN LSGAN WGAN

regularization
None GP AR
34.25 50.66 24.06 32.03 46.99 28.33 41.58 26.20 28.42

regularization
None GP AR
34.63 53.25 11.96 36.27 57.01 15.42 53.50 16.12 14.74

(a) FID on CIFAR10

(b) FID on Celeb-A

Table 4: FID scores using a robust loss.

E DIRAC-GAN
To better understand the effects that a robust loss or discriminator have on the GAN objective function, we use the Dirac-GAN setup. Each example in Figure 5 shows the vector field of the given GAN objective as a function of (, ), where the initial points 0 = 0 = 1, and the global optimum  =  = 0. We ran simultanious updates, for an unconstrained GAN, a GAN with hard robustness
12

Under review as a conference paper at ICLR 2019

constraints, and with a robustness regularization. The unregularized GAN does not converge. The hard constraint GAN comes within distance d of the global optimum, where d depends on the strength of the robustness. The smaller  the smaller is d. Finally, adversarial regularization leads to a convergent Dirac-GAN objective. However, a thorough theoretical analysis is warranted to fully understand this local convergence property.

222 111 000 111 22 1 0 1 2 22 1 0 1 2 22 1 0 1 2

(a) Unregularized GAN

(b) Robust GAN

(c) Adv. Regularization (GAN)

222

111

000

111

22 1 0 1 2 22 1 0 1 2 22 1 0 1 2

(d) Unregularized WGAN

(e) Robust WGAN

(f) Adv. Reg. (WGAN)

Figure 5: Convergence of Dirac-GAN using various regularization methods (or none) optimized with alternating gradient descent.

F EFFECTS OF USING DEFENSES AGAINST DIFFERENT ATTACKS
In our main experiments we use an FGM attack (scaled normalized gradient) for simplicity, as other attacks have more hyperparameters such as number of iterations and constants. To show, that this is a result not exclusive to FGM, we provide additional experiments using defenses against other popular attacks - Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD) and Carlini Wagner (CW) using robust feature matching (RFM) and adversarial regularization (AR). As expected, these defense techniques also provide improvements to the FID score.
FGSM FGSM PGD PGD CW CW +AR +RFM +AR +RFM +AR +RFM GAN (JS) 23.94 26.92 26.00 27.43 24.72 30.35 LSGAN 27.26 25.34 28.88 26.59 25.43 28.02 WGAN 29.80 25.22 30.16 29.32 40.57 27.90 Figure 6: FID for additional defenses on CIFAR-10. Results are compatible with Table 2(a) in submission.
G ADDITIONAL GENERATED SAMPLES
Finally, we present additinal generations for various models.
13

Under review as a conference paper at ICLR 2019

(a) Robust discriminator using WGAN+RFM.

(b) Robust loss using GAN+DS.

Figure 7: Comparison of the best models from training using a robust discriminator and robust loss.

Figure 8: Original GAN.

Figure 9: Original LSGAN.

Figure 10: Original WGAN-GP. 14

Under review as a conference paper at ICLR 2019

(a) Robust discriminator using WGAN+RFM.

(b) Robust loss using GAN+RFM.

Figure 11: Comparison of the best models from training using a robust discriminator and robust loss.

Figure 12: Original GAN.

Figure 13: Original LSGAN. 15

Under review as a conference paper at ICLR 2019
Figure 14: Original WGAN-GP. 16

