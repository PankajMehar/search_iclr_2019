Under review as a conference paper at ICLR 2019
NADPEX: AN ON-POLICY TEMPORALLY CONSISTENT
EXPLORATION METHOD FOR DEEP REINFORCEMENT
LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Reinforcement learning agents need exploratory behaviors to escape from local optima. These behaviors may include both immediate dithering perturbation and temporally consistent exploration. To achieve these, a stochastic policy model that is inherently consistent through a period of time is in desire, especially for tasks with either sparse rewards or long term information. In this work, we introduce a novel on-policy temporally consistent exploration strategy - Neural Adaptive Dropout Policy Exploration (NADPEx) - for deep reinforcement learning agents. Modeled as a global random variable for conditional distribution, dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients' alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy's stable improvement. Our experiments demonstrate that NADPEx solves tasks with sparse reward while naive exploration and parameter noise fail. It yields as well or even faster convergence in the standard mujoco benchmark for continuous control.
1 INTRODUCTION
Exploration remains a challenge in reinforcement learning, in spite of its recent successes in robotic manipulation and locomotion (Schulman et al., 2015b; Mnih et al., 2016; Duan et al., 2016; Schulman et al., 2017b). Most reinforcement learning algorithms explore with stochasticity in stepwise action space and suffer from low learning efficiency in environments with sparse rewards (Florensa et al., 2017) or long information chain (Osband et al., 2017). In these environments, temporally consistent exploration is needed to acquire useful learning signals. Though in off-policy methods, autocorrelated noises (Lillicrap et al., 2015) or separate samplers (Xu et al., 2018) could be designed for consistent exploration, on-policy exploration strategies tend to be learnt alongside with the optimal policy, which is non-trivial. A complementary objective term should be constructed because the purpose of exploration to optimize informational value of possible trajectories (Osband et al., 2016) is not contained directly in the reward of underlying Markov Decision Process (MDP). This complementary objective is known as reward shaping e.g. optimistic towards uncertainty heuristic and intrinsic rewards. However, most of them require complex additional structures and strong human priors when state and action spaces are intractable, and introduce unadjustable bias in end-to-end learning (Bellemare et al., 2016; Ostrovski et al., 2017; Tang et al., 2017; Fu et al., 2017; Houthooft et al., 2016; Pathak et al., 2017).
It would not be necessary though, to teach agents to explore consistently through reward shaping, if the policy model inherits it as a prior. This inspires ones to disentangle policy stochasticity, with a structure of time scales. One possible solution is policy parameter perturbation in a large time scale. Though previous attempts were restricted to linear function approximators (Ru¨ckstieß et al., 2008; Osband et al., 2014), progress has been made with neural networks, through either network section duplication (Osband et al., 2016) or adaptive-scale parameter noise injection (Plappert et al., 2017; Fortunato et al., 2017). However, in Osband et al. (2016) the episode-wise stochasticity is unadjustable, and the duplicated modules do not cooperate with each other. Directly optimizing the mean of distribution of all parameters, Plappert et al. (2017) adjusts the stochasticity to the learning progress heristically. Besides, as all sampled parameters need to be stored in on-policy exploration,
1

Under review as a conference paper at ICLR 2019
it is not directly salable to large neural networks, where the trade-off between large memory for multiple networks and high variance with sinlge network is non-trivial to make.
This work proposes Neural Adaptive Dropout Policy Exploration (NADPEx) as a simple, scalable and improvement-guaranteed method for on-policy temporally consistent exploration, which generalizes Osband et al. (2016) and Plappert et al. (2017). Policy stochasticity is disentangled into two time scales, step-wise action noise and episode-wise stochasticity modeled with a distribution of subnetworks. With one single subnetwork in an episode, it achieves inherent temporal consistency with only a little computational overhead and no crafted reward, even in environments with sparse rewards. And with a set of subnetworks in one sampling batch, agents experience diverse behavioral patterns. This sampling of subnetworks is executed through dropout (Hinton et al., 2012; Srivastava et al., 2014), which encourages composability of constituents and facilitates the emergence of diverse maneuvers. As dropout could be excerted on connections, neurons, modules and paths, NADPEx naturally extends to neural networks with various sizes, exploiting modularity at various levels. To align separately parametrized stochasticity to each other, this sampling is made differentiable for all possible dropout candidates. We further discuss the effect of a first-order KL regularizer on dropout policies to improve stability and guarantee policy improvement. Our experiments demonstrate that NAPDEx solves challenging exploration tasks with sparse rewards while achieving as efficient or even faster convergence in the standard mujoco benchmark for state-of-theart PPO agents, which we believe can be generalized to any deep reinforcement learning agents.

Figure 1: (a) Conceptual graph to illustrate the hierarchical stochasticity in NADPEx. Agent spontaneously explores towards different directions with different dropouts. (b) Graphical model for an MDP with NADPEx. z is a global random variable in one episode  .

2 PRELIMINARIES

We consider a standard discrete-time finite-horizon discounted Markov Decision Process (MDP)

setting for reinforcement learning, represented by a tuple M = (S, A, P, r, 0, , T ), with a state set S, an action set A, a transitional probability distribution P : S × A × S  R+, a bounded reward

function r : S × A  R+, an initial state distribution 0 : S  R+, a discount factor   [0, 1] and

horizon T . We denote a policy parametrized by  as  : S×A  R+. The optimization objective of

this policy is to maximize the expected discounted return () = E [

T t=0

tr(st, at)],

where



=

(s0, a0, ...) denotes the whole trajectory, s0  0(s0), at  (at|st) and st+1  P(st+1|st, at). In

experimental evaluation, we follow normal setting and use undiscounted return E [

T t=0

r(st,

at)].

2.1 PROXIMAL POLICY OPTIMIZATION (PPO)
Theoretically speaking, NADPEx works with any policy gradient based algorithms. In this work, we consider the recent advance in on-policy policy gradient method, proximal policy optimization (PPO) (Schulman et al., 2017b).
Policy gradient methods were first proposed by Williams (1992) in REINFORCE algorithm to maximize the aforementioned objective in a gradient ascent manner. The gradient of an expectation

2

Under review as a conference paper at ICLR 2019

() is approximated with a Monte Carlo average of policy gradient:

1N () = N T

T
 log (ati|sit)(Rti - bti),

i=1 t=o

(1)

where N is the number of episodes in this batch, Rti =

T t =t

t

-trti

and bit is a baseline for

variance reduction.

Schulman et al. (2015b) proposes a policy iteration algorithm and proves its monotonicity in im-
provement. With the 's corresponding discounted state-visitation frequency , the sampling policy old and the updated policy , it solves the following constrained optimization problem with a line search for an appropriate step size within certain bound KL, called trust region:

argmax

Esold

,aold

[

 (a|s) old (a|s)

Aold

(s,

a)]

s.t. Esold [DKL(old(·|s)|(·|s))]  KL

(2)

where DKL(·||·) is the Kullback-Leibler (KL) divergence, Aold (s, a) is calculated with Generalized Advantage Estimation (GAE) (Schulman et al., 2015c).
Proximal Policy Optimization (PPO) transforms (2) to a unconstrained optimization and solves it with first-order derivatives, embracing established stochastic gradient-based optimizer like Adam (Kingma & Ba, 2014). Noting that DKL(old(·|s)|(·|s)) is actually a relaxation of total variational divergence according to Schulman et al. (2015b), whose first-order derivative is 0 when  is close to old, DKL((·|s)||old(·|s)) is a natural replacement. Combining the first-order derivative of DKL((·|s)||old(·|s)) (Schulman et al., 2017a) (check Appendix A for a proof):
DKL((·|s)|old (·|s))] = Ea [ log (a|s)(log (a|s) - log old (a|s))], (3)

PPO optimizes the following unconstrained objective, called KL PPO loss1:

LKL

=

Esold ,aold [r(s, a)Aold (s, a)

-

 (log
2

 (a|s)

-

log old (a|s))2],

(4)

where r(s, a)

=

. (a|s)
old (a|s)

Schulman et al. (2017b) also proposes a clipping version PPO, as a

lower bound to (4):

Lclip = Esold ,aold [min(r(s, a), clip(r(s, a), 1 - , 1 + ))Aold (s, a)]

(5)

In this work, we try both KL PPO and clipping PPO.

2.2 DROPOUT

Dropout is a technique used in deep learning to prevent features from co-adaptation and parameters
from overfitting, by randomly dropping some hidden neuron units in each round of feed-forward
and back-propagation (Hinton et al., 2012; Srivastava et al., 2014). This is modeled by multiplying a Bernoulli random variable zjk to each hidden unit, i.e. neuron activation hjk, for j = 1...m, where m is the number of hidden units in kth layer. Then the neuron activation of the k + 1th layer is

hk+1 = (W(k+1)T Dkz hk + b(k+1)),

(6)

where Dz = diag(z)  respectively and we simply
function. The parameter of

dRtheminso×Btmeet,rhnWeomul(lwki+irt1ah)ndaon=m.d<bvaW(rki+a,b1b)le>airs.epwjk(xe=.i)giPhstt(shzeajknn=donb0lii)an,seaeaksranatethuekrod+nroa1pctothiuvtalartiyaoetner

of the jth hidden unit at kth layer. In supervised learning, these dropout parameters are normally

fixed during training in some successful practice (Hinton et al., 2012; Wan et al., 2013). And there

are some variants to dropout connections, modules or paths (Wan et al., 2013).

1Though the concrete form is not provided in Schulman et al. (2017b), it is given in Dhariwal et al. (2017) publicly released by the authors

3

Under review as a conference paper at ICLR 2019

3 METHODOLOGY

3.1 NEURAL ADAPTIVE DROPOUT POLICY EXPLORATION (NADPEX)

Designing an exploration strategy is to introduce a kind of stochasticity during reinforcement learning agents' interaction with the environment to help them get rid of some local optima. While action space noise (parametrized as a stochastic policy ) might be sufficient in environments where stepwise rewards are provided, they have limited effectiveness in more complex environments (Florensa et al., 2017; Plappert et al., 2017). As their complement, an exploration strategy would be especially beneficial if it can help either sparse reward signals (Florensa et al., 2017) or significant long-term information (Osband et al., 2016) to be acquired and back-propagated through time (BPTT) (Sutton & Barto, 1998). This motivates us to introduce a hierarchy of stochasticity and capture temporal consistency with a separate parametrization.

Our algorithm, Neural Adaptive Dropout Policy Exploration (NADPEx), models stochasticity at large time scales with a distribution of plausible subnetworks. For each episode, one specific subnetworks is drawn from this distribution. Inspected from a large time scale, this encourages exploration towards different directions among different episodes in one batch of sampling. And from a small time scale, temporal correlation is enforced for consistent exploration. Policies with a hierarchy of stochasticity is believed to represent a complex action distribution and larger-scale behavioral patterns (Florensa et al., 2017).

We achieve the drawing of subnetwork through dropout. As introduced in Section 2.2, originally, dropout is modeled through multiplying a binary random variable zjk  Bernoulli(1 - pkj ) to each neuron activation hjk. In Srivastava et al. (2014), this binary dropout is soften as continuous random variable dropout, modeled with a Gaussian random variable z  N (I, 2), normally referred to
as Gaussian multiplicative dropout. Here we denote both distributions with q(z). Later we will
introduce how both of them could be made differentibale and thus adaptative to the learning progress.

During the sampling process, at the beginning of every episode  i, a vector of dropout random variables zi is sampled from q(z), giving us a dropout policy |zi for step-wise action distribution. zi is fed to the stochastic computation graph (Schulman et al., 2015a) for the whole episode until
termination, when a new round of drawing initiates. Similar as observations, actions and rewards,
this random variable is stored as sampled data, which will be fed back to the stochastic computation
graph during training. Policies with this hierarchy of stochasticity, i.e. NADPEx policies, can be
represented as a joint distribution:

making the the objective:

,(·, z) = q(z)|z(·),

(7)

T
(,) = Ezq(z)[E|z[ tr(st, at)]].
t=0

(8)

If we only use the stochasticity of network architecture as a bootstrap, as in BootstrapDQN (Osband et al., 2016), the bootstrapped policy gradient training is to update the network parameters  with the following gradients:

1 (,) = Ezq(z)[(|z)] = Ezq(z)[(|z)]  N

N

L(, zi)

i=1

(9)

L(, zi) is the surrogate loss of dropout policy ,zi , variates according to the specific type of reinforcement learning algorithm. In next subsection, we discuss some pitfalls of bootstrap and
provide and our solution to it.

3.2 TRAINING NADPEX
STOCHASTICITY ALIGNMENT
One concern in separating the parametrization of stochasticity in different levels is whether they can adapt to each other elegantly to guarantee policy improvement in terms of objective (8). Policy

4

Under review as a conference paper at ICLR 2019

gradients in (9) alway reduces |z's entropy (i.e. stochasticity at small time scale) as the policy improves, which corresponds to the increase in agent's certainty. But this information is not propagated to q for stochasticity at large time scale in bootstrap. As in this work q is a distribution of subnetworks, this concern could also be intuitively understood as component space composition may not guarantee the performance improvement in the resulting policy space, due to the complex non-linearity of reward function. Gangwani & Peng (2017) observe that a naive crossover in the parameter space is expected to yield a low-performance composition, for which an extra policy distillation stage is designed.

We investigate likelihood ratio trick (Ranganath et al., 2014; Schulman et al., 2015a) for gradient back-propragation from the same objective (8) through discrete distribution e.g. binary dropout and reparametrization trick (Kingma & Welling, 2013; Rezende et al., 2014) for continuous distribution e.g. Gaussian multiplicative dropout, thus covering all possible dropout candidates (The proof is provided in Appendix B):

,(,)



1 N

N
(L(, zi) +  log q(zi)A(si0, a0i )),

i=1

(10)

, (, )



1 N

N

,L(, I + 

i=1

i).

(11)

In (10)  is the parameters for Bernoulli distributions, A(si0, a0i ) is the GAE (Schulman et al., 2015c) for the ith trajectory from the beginning, i.e. (si0, ai0). In (11) z = I +  i, is an element-wise multiplication, and i is the dummy random variable i  N (0, I).

POLICY SPACE CONSTRAINT

However, simply making the dropout ditribution differentiable may not guarantee the policy improment. As introduced in Section 2.1, Schulman et al. (2015b) reduce the monotonic policy improvement theory to a contraint on KL divergence in policy space. In this work, the analytical form of NADPEx policy , is obtainable as q is designed to be fully factorizable. Thus ones can leverage the full-fledged KL constraint to stablize the training of NADPEx.

Here we give an example of PPO-NADPEx. Similar as (3), we leverage a first-order approximated

KL divergence for NADPEx policies. As credit assignment with likelihood ratio trick in (3) may

suffer from curse of dimensionality for NADPEx policies, we simply choose the mean policies

 = |z as a representative, where z is the mean of dropout random variable vectors. Then the objective is:

LKdrLopout

=

E[r|z(s, a)Aold (s, a) -

 2

(log



(a|s)

-

log

old

|z

(a|s))2

]

(12)

where r|z(s, a)

=

.|z (a|s)
old|z (a|s)

A

proof

for

the second

term

is

in

Appendix

C.

Intuitively,

the

KL

divergence between dropout polices |z and mean policies  is added as a regularization term.

Optimizing the lower bound of (12), clipping PPO could adjust the clip ratio accordingly.

3.3 RELATIONS TO PARAMETER NOISE
Episode-wise parameter noise injection (Plappert et al., 2017) is another way to introduce the hierarchy of stochasticity, which could be regarded as a special case of NADPEx, with Gaussian mulitplicative dropout on connection and a heurstic adaptation of variance. That dropout at neurons is a local reparametrization of noisy networks is proved in Kingma et al. (2015). A replication of the proof is provided in Appendix D. They also prove this local reparametrization trick has a lower variance in gradients. And this reduction in variance could be enhanced if ones choose to drop modules or paths in NADPEx.
More importantly, as we scope our discussion down to on-policy exploration strategy, where z from q(z) need to be stored for training, NADPEx is more scalable to much larger neural networks with possibly larger mini-batch size for stochastic gradient-based optimizers. As a base case, to dropout neurons rather than connections, NADPEx has a complexity of O(N m) for both sampling and memory, comparing to O(N m2) of parameter noise, with m denoting the number of neurons in one layer. This reduction could also be enhanced if ones choose to drop modules or paths.

5

Under review as a conference paper at ICLR 2019
4 EXPERIMENTS
In this section we evaluate NADPEx by answering the following questions through experiments.
(i) Can NADPEx achieve state-of-the-art result in standard benchmarks? (ii) Does NADPEx drive temporally consistent exploration in sparse reward environments? (iii) Is the end-to-end training of the hierarchical stochasticity effective? (iv) Is KL regularization effective in preventing divergence between dropout policies and the
mean policy?
We developed NADPEx based on openai/baselines (Dhariwal et al., 2017). We run all experiments with PPO as reinforcement learning algorithm. Especially, we developed NADAPEx based on the GPU optimized version PPO. Details about network architecture and other hyper-parameters are available in Appendix E. During our implemented parameter noise in the same framework as NADPEx, we encountered out of memory error for a mini-batch size 64 and training batch size 2048. This proves our claim of sampling/memory complexity reduction with NADPEx. We had to make some trade-off between GPU parallelism and more iterations to survive. For all experiment with NADPEx, we test the following configurations for the dropout hyperparameter, the initial dropout rate: (a) pjk = 0.01, (b) pkj = 0.1, (c) pjk = 0.3. For Gaussiasn dropout, we estimate the dropout rate as pkj = jk/(1 + jk). And for experiments with parameter noise, we set the initial variance accordingly: (a) jk = 0.001, (b) jk = 0.01, (c) jk = 0.05, to ensure same level of stochasticity. For experiments with fixed KL version PPO, we run with  = 0.0005. All figures below show statistics of experiments with 5 randomly generated seeds.
4.1 NADPEX IN STANDARD ENVIRONMENTS
First, we evaluate NADPEx's overall performance in standard environments on the continuous control environments implemented in OpenAI Gym (Brockman et al., 2016). From Figure 2 we can see that they show similar pattern with Gaussian dropout and binary dropout, given identical dropout rates. Between them, Gaussian dropout is slightly better at scores and consistency among initial dropout rates. With the three initial dropout rates listed above, we find pkj = 0.1 shows constistent advantage over others. On the one hand, when the initial dropout rate is small (pkj = 0.01), NADPEx agents learn to earn reward faster than agents with only action space noise. It is even possible that these agents learn faster than ones with pkj = 0.1 in the beginning. However, their higher variance between random seeds indicates that some of them are not exploring as efficiently and the NADPEx policies may not be optimal, therefore normally they will be surpassed by ones with pik = 0.1 in the middle stage of training. On the other hand, large initial dropout rate (pkj = 0.3) tends to converge slowly, possibly due to the claim in Molchanov et al. (2017) that a large dropout rate could induce large varaince in gradients, making a stable convergence more difficult.
Figure 2: Experiments with NADPEx in standard envs, three pkj are presented for Gaussian dropout, as well as the best of binary dropout. Extensive comparison is given in Appendix F.
6

Under review as a conference paper at ICLR 2019
4.2 TEMPORALLY CONSISTENT EXPLORATION We then evaluate how NADPEx with Gaussian dropout performs in environments where reward signals are sparse. Comparing with environments with stepwise rewards, these environments are more challenging as they only yield non-zero reward signals after milestone states are reached. Temporally consistent exploration therefore plays a crucial role in these environments. As in Plappert et al. (2017) we run experiments in rllab (Duan et al., 2016), modified according to Houthooft et al. (2016). Specifically, we have: (a) SparseDoublePendulum, which only yields a reward if the agent reaches the upright position, and (b) SparseHalfCheetah, which only yields a reward if the agent crosses a target distance, (c) SparseMountainCar, which only yields a reward if the agent drives up the hill. We use the same time horizon of T = 500 steps before resetting and gradually increase the difficulty until the performance of NADPEx and parameter noise differentiates. We would like to refer readers to Appendix G for more details about the sparse reward environments. As shown in Figure 3, we witness success with temporally consistent exploration through NADPEx, while action perturbation fails. In all enviroments we examed, larger initial dropout rates can achieve faster or better convergence, revealing a stronger exploration capability at large time scales. Comparing to parameter noise, NADPEx earns higher score in shorter time, possibly indicating a higher exploration efficiency.
Figure 3: Experiments with NADPEx and parameter noise in sparse reward envs.
4.3 EFFECTIVENESS OF STOCHASTICITY ADAPTATION One hypothesis NADPEx builds upon is end-to-end training of separately parametrized stochasticity can appropriately adapt the temporally-extended exploration with the current learning progress. In Figure 4 we show our comparison between NADPEx and bootstrap, as introduced in Section 3. And we can see that though the difference is subtle in simple task like Hopper, the advantage NADPEx has over bootstrap is obvious in Walker2d, which is a task with higher complexity. In Humanoid, the task with highest dimension of actions, the empirical result is interesting. Though bootstrap policy learns almost as fast as NADPEx in the begining, that the dropout is not adaptative drives it to over-explore when some dropout policies are close to converge. Being trapped at a local optimum, bootstrap policy earns 500 scores less than NADPEx policy.
Figure 4: Comparison between NADPEx and bootstrap with Gaussian dropout.
7

Under review as a conference paper at ICLR 2019
4.4 KL DIVERGENCE BETWEEN DROPOUT POLICIES
To evaluate the effect of the KL regularizer, we also run experiments with KL PPO. Though in the original paper of Schulman et al. (2017b) clipping PPO empirically performs better than KL PPO, we believe including this KL term explicitly in the objective makes our validation self-explanatory. Figure 5 left shows the experiment result in Walker2d. Different from clipping PPO, NADPEx with small initial dropout rate performs best, earning much higher score than action noise. As shown in Figure 5 right, the KL divergence between dropout polices and mean policies is bounded.
Figure 5: NADPEx KL PPO in Walker2d. Left: learning curves; right: true DKL(old|z|||z).
5 RELATED WORKS
On-policy reinforcement learning methods have gained attention in recent years (Schulman et al., 2015b; Mnih et al., 2016; Schulman et al., 2017b), mainly due to their elegance with theoretical grounding and stability in policy iteration (Henderson et al., 2017b). Despite of the effectiveness, to improve their data efficiency remains an active research topic. In this work, we consider the exploration strategies for on-policy reinforcement learning methods. Most of the aforementioned works employ naive exploration, with stochasticity only in action space. However, they fail to tackle some tasks with either sparse rewards (Florensa et al., 2017) or longterm information (Osband et al., 2016), where temporally consistent exploration is needed. One solution to this challenge is to shape the reward to encourage more directed exploration. The specific direction has various foundations, including but not restricted to state visitation count (Jaksch et al., 2010; Tang et al., 2017), state density (Bellemare et al., 2016; Ostrovski et al., 2017; Fu et al., 2017), self-supervised prediction error (Pathak et al., 2017) etc. Some of them share the Probably Approximately Correct (PAC) with discrete and tractable state space (Jaksch et al., 2010). But when state space and action space are intractable, all of them need additional computational structures, which take non-trivial efforts to implement and non-negligible resources to execute. Orthogonal to them, methods involving a hierarchy of stochasticity are proposed. Based on hierarchical reinforcement learning, Florensa et al. (2017) models the stochasticity at large time scales with a random variable - option - and model low-level policies with Stochastic Neural Networks. However, authors employ human designed proxy reward and staged training. Almost concurrently, Osband et al. (2016) and Plappert et al. (2017); Fortunato et al. (2017) propose network section ensemble and parameter noise injection respetively to disentangle stochasticity. Under the banner of Stochastic Neural Networks, NADPEx generalize them all. BootstrapDQN is a special case of NADPEx without stochasticity adapation and parameter noise is a special case with high variance and complexlity, as well as some heursitic approximation. Details are discussed in Section 3. In NADPEx, this stochasticity at large time scales is captured with a distribution of plausible neural subnetworks from the same complete network. We achieve this through dropout (Srivastava et al., 2014). In spite of its success in supervised deep learning literature and Bayesian deep learning literature, it is the first time to combine dropout to reinforcement learning policies for exploration. The closest ones are Gal & Ghahramani (2016); Henderson et al. (2017a), which use dropout in value network to capture agents' uncertainty about the environment. According to Osband et al. (2016), Gal & Ghahramani (2016) even fails in environment requiring temporally consistent exploration.
8

Under review as a conference paper at ICLR 2019
There are also some attempts from the Evolutionary Strategies and Genetic Algorithms literature (Salimans et al., 2017; Such et al., 2017; Gangwani & Peng, 2017) to continuous control tasks. Though they model the problem much differently from ours, the relation could be an interesting topic for future research.
6 CONCLUSION
Building a hierarchy of stochasticity for reinforcement learning policy is the first step towards more structured exploration. We presented a method, NADPEx, that models stochasticity at large time scale with a distribution of plausible subnetworks from the same complete network to achieve onpolicy temporally consistent exploration. These subnetworks are sampled through dropout at the beginning of episodes, used to explore the environment with diverse and consistent behavioral patterns and updated through simultaneous gradient back-propagation. A learning objective is provided such that this distribution is also updated in an end-to-end manner to adapt to the action-space policy. Thanks to the fact that this dropout transformation is differentiable, KL regularizers on policy space can help to further stabilize it. Our experiments exhibit that NADPEx successfully solves continuous control tasks, even with strong sparsity in rewards.
REFERENCES
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 1471­1479, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/ openai/baselines, 2017.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pp. 1329­1338, 2016.
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement learning. arXiv preprint arXiv:1704.03012, 2017.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.
Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2574­2584, 2017.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050­1059, 2016.
Tanmay Gangwani and Jian Peng. Policy optimization by genetic distillation. arXiv preprint arXiv:1711.01012, 2017.
Peter Henderson, Thang Doan, Riashat Islam, and David Meger. Bayesian policy gradients via alpha divergence dropout inference. arXiv preprint arXiv:1712.02037, 2017a.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017b.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.
9

Under review as a conference paper at ICLR 2019
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pp. 1109­1117, 2016.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563­1600, 2010.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pp. 2575­2583, 2015.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928­1937, 2016.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. arXiv preprint arXiv:1701.05369, 2017.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. arXiv preprint arXiv:1402.0635, 2014.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In Advances in neural information processing systems, pp. 4026­4034, 2016.
Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized value functions. arXiv preprint arXiv:1703.07608, 2017.
Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Re´mi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial Intelligence and Statistics, pp. 814­822, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Thomas Ru¨ckstieß, Martin Felder, and Ju¨rgen Schmidhuber. State-dependent exploration for policy gradient methods. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 234­249. Springer, 2008.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. In Advances in Neural Information Processing Systems, pp. 3528­ 3536, 2015a.
10

Under review as a conference paper at ICLR 2019

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015b.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015c.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft qlearning. arXiv preprint arXiv:1704.06440, 2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2750­ 2759, 2017.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International Conference on Machine Learning, pp. 1058­1066, 2013.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pp. 5­32. Springer, 1992.
Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore via meta-policy gradient. In International Conference on Machine Learning, pp. 5459­5468, 2018.

A KL DIVERGENCE FIRST ORDER APPROXIMATION

We derive the first order derivatitve of KL divergence from a refernce policy  to a parametrized policy  to :

DKL((·|s)||(·|s)) =  (a|s)(log (a|s) - log (a|s))da

= ((a|s)(log (a|s) - log (a|s)))da = (a|s)(log (a|s) - log (a|s) + 1)da

(13)

= (a|s) log (a|s)(log (a|s) - log (a|s) + 1)da
= Ea [ log (a|s)(log (a|s) - log (a|s))]
Note that line 3 derives line 4 with likelihood ratio trick, line 4 derives line 5 as (a|s) log (a|s)da = 0

11

Under review as a conference paper at ICLR 2019

B GRADIENTS OF NADPEX

Here we provide a full derivation of NADPEx's gradients. Specifically, gradients for three type of dropout are discussed.
As shown in (7), a NADPEx policy ,z is a joint distribution, which could be factorized with a dropout distribution q(z) and a conditional distribution, i.e. the dropout policy, |z, z  q(z) is the dropout random varible vector.
In reinforcement learning with NADPEx, we use gradient based optimization to maximize the objective (8).

T
(,) = Ezq(z)[E|z[ tr(st, at)]].
t=0

(14)

B.1 DISCRETE DROPOUT

Normally the dropout distribution q(z) is a discrete distribution, for example, z  Bernoulli(), ones can use likelihood ratio trick to calculate (14):

T
,(,) = ,Ezq [E|z[ tr(st, at)]]
t=0
= , q(z) p|z,R d dz

= q(z) p|z,R d dz +  q(z) p|z,R d dz

= Ezq [L(, z) +  log q(z)A(s0|z, a0|z)]

1 N

N
(L(, zi) +  log q(zi)A(s0i , a0i )),

i=1

(15)

where L(·) is the surrogate loss, varying from reinforcement learning algorihtms, A(si0, a0i ) is the GAE (Schulman et al., 2015c) for the ith trajectory from the beginning, i.e. (s0i , ai0), such that the gradient has low variance.

B.2 GAUSSIAN MULTIPLICATIVE DROPOUT

In Srivastava et al. (2014), Gaussian multiplicative dropout is proposed, where z  N (I, 2) is the

multiplicative dropout random varible vector, with the same expectation of discrete dropout P (z =

1)

=

1 1+

.

Reparametirzed

with

a

dummy

random

variable

 N (0, I) (Kingma & Welling, 2013;

Rezende et al., 2014), we have z = I +  , where is an element-wise multiplication, such that

(14) could be calculated as:

T
,(,) = ,EzN (I,2)[E|z[ tr(st, at)]]
t=0

= ,EzN (I,2)[(|z)]

= ,E N (0,I)[(|I+ )]

= E N (0,I)[,(|I+ )]

= E N (0,I)[,L(, I +  )]

1 N

N

,L(, I + 

i=1

i)

(16)

12

Under review as a conference paper at ICLR 2019

C NADPEX KL DIVERGENCE FIRST ORDER APPROXIMATION

In NADPEx, the dropout distribution is designed to be fully factorizable, thus the gradient of KL divergence in the NADPEx policy space is obtainable:

,DKL(,(·|s)||old,old (·|s))

=,

,(a,

z|s)

log

,(a, z|s) old,old (a, z|s)

dadz

=,

q (z )|z (a|s)

log

q (z )|z (a|s) qold (z)old|z(a|s)

dadz

=

q(z)

|z (a|s)

log

q(z)|z(a|s) dadz qold (z)old|z(a|s)

+



q (z )|z (a|s)

log

q(z)|z(a|s) dzda. qold (z)old|z(a|s)

(17)

And thus the same first-order approximation could be done just as in Appendix A. However, note that likelihood ratio trick is used during the derivation, which should be applied to the random variable vectors a and z here. As normally z could have a much higher dimension than a, making this likelihood ratio trick suffer from curse of dimensionality, we drop the second term with a little bias introduced. This gives us

DKL(,(·|s)||old,old (·|s))

=

q(z)

|z (a|s)

log

q(z)|z(a|s) dadz qold (z)old|z(a|s)

=

q(z)

|z (a|s)(log

|z (a|s) old |z (a|s)

+

log

q(z) )dadz qold (z)

=

q(z)

|z (a|s)

log

|z (a|s) old |z (a|s)

dadz.

(18)

Note

that

from

line

3

to

line

4

we

simply

remove

log

q (z ) qold (z)

,

which

can

be

regarded

as

substracting

it as a baseline b(z) to reduce variance. We further replace , with a mean policy  = |z to

enforce the idea that dropout policy had better to be close to each other. Therefore, we have

DKL((·|s)||old,old (·|s))

= q(z) (a|s)(log (a|s) - log old|z(a|s))dadz

= q(z) (a|s) log (a|s)(log (a|s) - log old|z(a|s))dadz
=Ezq,a|z [ log (a|s)(log (a|s) - log old|z(a|s))] =Ezq,a|z [(log (a|s) - log old|z(a|s))2].

(19)

D VARITAIONAL DROPOUT AND LOCAL REPARAMETRIZATION

As introduced in Section 2.1, multiplicative dropout at neuron activation of kth layer can also be viewed as a multiplicative noise for the input at k + 1th layer:

hk+1 = (W(k+1)T Dkz hk + b(k+1)). After a simple rearrangement W~ (k+1)T = W(k+1)T Dzk, we have:
hk+1 = (W~ (k+1)T hk + b(k+1)).

(20) (21)

That is, the stochastic neuron activation in networks with Gaussian multiplicatice dropout can also be regarded as stochastic neuron activation in Noisy Networks with correlated Gaussian parameter noise, whose means equal the corresponding ones in networks with dropout.

13

Under review as a conference paper at ICLR 2019

E HYPERPARAMETERS
For most of the hyperparamters, we follow the setting in original PPO paper. Though, to emphasis the scalability of our method, we use 2 parallel enviroments and only 1 minibatch in each epoch.

Table 1: Hyperparamters for PPO Hyperparameters Value

Horizon
Adam stepsize Num. epochs Num. minibatch Discount () GAE  PPO clip Num. layers Num. hidden units

2048 3 × 10-4
10 1 0.99 0.95 0.2 2 64

F EXPERIMENT RESULTS IN STANDARD ENVIRONMENTS

Figure 6: NADPEx in standard envs where Gaussian dropout is used

Figure 7: NADPEx in standard envs where binary dropout is used 14

Under review as a conference paper at ICLR 2019
Figure 8: NADPEx in standard envs, camparing the best of Gaussian dropout and binary dropout
G ENVIRONMENTS WITH SPARSE REWARDS
We use the same sparse reward environments from rllab Duan et al. (2016), modified by Houthooft et al. (2016):
· SparseHalfCheetah (S  R17, A  R6), which only yields a reward if the agent crosses a distance threshold,
· SparseMountainCar (S  R2, A  R), which only yields a reward if the agent drives up the hill,
· SparseDoublePendulum (S  R6, A  R), which only yields a reward if the agent reaches the upright position.
15

