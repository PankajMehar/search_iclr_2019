Under review as a conference paper at ICLR 2019
DUAL LEARNING: THEORETICAL STUDY AND ALGORITHMIC EXTENSIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Dual learning has been successfully applied in many machine learning applications including machine translation, image-to-image transformation, etc. The high-level idea of dual learning is very intuitive: if we map an x from one domain to another and then map it back, we should recover the original x. Although its effectiveness has been empirically verified, theoretical understanding of dual learning is still missing. In this paper, we conduct a theoretical study to understand why and when dual learning can improve a mapping function. Based on the theoretical discoveries, we extend dual learning by introducing more related mappings and propose highly symmetric frameworks, cycle dual learning and multipath dual learning, in both of which we can leverage the feedback signals from additional domains to improve the qualities of the mappings. We prove that both cycle dual learning and multipath dual learning can boost the performance of standard dual learning under mild conditions. Experiments on WMT 14 EnglishGerman and MultiUN EnglishFrench translations verify our theoretical findings on dual learning, and the results on the translations among English, French, and Spanish of MultiUN demonstrate the efficacy of cycle dual learning and multipath dual learning.
1 INTRODUCTION
Most machine learning tasks can be formulated as learning a mapping from one domain to another one, like image classification (from image to label), neural machine translation (from source language to target language), speech recognition (from voice to text), etc. Among them, quite a few tasks are of dual forms, like image classification v.s. image generation (from label to image), the neural machine translation between two languages (EnglishFrench v.s. FrenchEnglish), speech recognition v.s. speech synthesis (from text to voice), etc. Such duality can be utilized to improve the model qualities.
One prominent framework is dual learning, first proposed by He et al. (2016) for machine translation and then applied to many other applications like image translation (Kim et al., 2017; Zhu et al., 2017), question answering and generation (Tang et al., 2017), etc. In dual learning, two mapping functions between two domains are trained simultaneously so that one function is close to the inverse of the other. The intuition is that, if we translate a sentence from English to French and then translate the obtained French sentence back to English, we should get the same sentence or a very simlar one.
Dual learning is of great interest because it can accommodate any unidirectional architecture, e.g. a transformer (Vaswani et al., 2017), and provide a performance boost. Moreover, dual learning can be used in semi-supervised learning, which is highly desirable since deep neural networks are generally thirst for labeled data.
Despite of the empirical success of dual learning, the theoretical understanding is largely missing. In this paper, we conduct both theoretical analyses and empirical studies to answer the following questions: Q1: Why and when does dual learning improve a mapping function? Q2: Can we further improve the performance of a mapping function?
1

Under review as a conference paper at ICLR 2019
1.1 OUR CONTRIBUTIONS
Our contributions are in two folds: a theoretical study of dual learning and the frameworks of cycle dual learning and multipath dual learning, which subsume dual learning as a special case. Without loss of generality, we take machine translation as the example for the study and algorithm presentation.
Dual learning. We take a novel statistical approach to model the problem. Suppose there are two baseline translators between two language spaces, one forward and one backward. Based on our Theorem 1, dual learning outperforms both baseline translators as long as the baseline translators are better than random mappings under natural assumptions. Empirical studies show that an improvement is observed even if the reconstruction is far from perfect.
Cycle dual learning and multipath dual learning. We propose cycle dual learning and multipath dual learning frameworks by extending dual learning. Both frameworks use dual learning as the basic building block and leverage a third, a fourth, or more languages to help boost the translator qualities between the original two languages. We prove that under mild conditions, the two symmetric frameworks outperform dual learning (Theorems 2 and 3). Our experiments on MultiUN dataset show a significant improvement (1.45 BLEU points, see Table 4) against dual learning.
1.2 RELATED WORK
Dual learning was first proposed by He et al. (2016) in the context of machine translation, where the two dual translators are updated in a reinforcement learning manner with the reconstructed distortion as the feedback signal. A similar approach proposed by Cheng et al. (2016) has the same high-level idea but a very different implementation. Since then dual learning architectures have been proposed for other applications including image processing (Kim et al., 2017; Zhu et al., 2017), sentiment analysis (Xia et al., 2017a), image segmentation (Luo et al., 2017), etc.
Built upon the dual learning framework, Xia et al. (2017b) and Wang et al. (2018) consider the joint distribution constraint, which says the joint distribution of sample over two domains is invariant when computing from either domain. We relax this constraint for simplicity of analysis. Xia et al. (2018) proposed model-level dual learning, which shares components between the primary direction and the dual direction. Dual learning was also leveraged for unsupervised learning (Lample et al., 2018; Artetxe et al., 2018).
Despite the vast number of works related to dual learning, none of them explain why it works or when it fails. Galanti et al. (2018) claim that dual learning does not circumvent the alignment problem, where a sentence is translated wrong by the forward translator but translated back to it by the backward translator. We show that the alignment problem occurs with a small probability under dual learning compared to the baseline translator, and this probability can be further reduced by our cycle dual learning and multipath dual learning. Furthermore, their hypothesis that the translator should not be too complex is not verified in the context of machine translation.
2 PRELIMINARIES
In this section, we introduce the notations and the essential assumptions used for theoretical analysis. A table summarizing the notations can be found in Appendix A. Let S1, . . . , Sk be k language spaces, composed of sentences in each language. For any Si, we denote the distribution of sentences in Si by µ(i) and let X(i) be the random variable, i.e. Pr(X(i) = x) = µ(i)(x). For any x(i)  Si, we denote the correct translation in Sj by x(j). In practice, we may select a threshold BLEU (Papineni et al., 2002a) score, above which the translation is considered correct. Let Tij denote the baseline translator that translates from a sentence in Si to one in Sj, i.e. the desired mapping is Tij(x(i)) = x(j). When a random sentence x(i) is sampled from Si according to µ(i), it is possible that this sentence is translated incorrectly. We use accuracy of the translator pij, the accuracy of the translator, to describe the probability of translating a sentence correctly when this sentence is randomly sampled from Si according to µ(i). Formally, pij = Prx(i)µ(i) (Tij (x(i)) = x(j)) =
1 1 1E [ ] =x(i)µ(i) Tij (x(i))=x(j) x(i)Si Tij (x(i))=x(j) µ(i)(x(i)), where is the indicator function.
2

Under review as a conference paper at ICLR 2019

We sometimes omit the subscript x(i)  µ(i) for simplicity. It is also easy to see that

Pr(Tij (x(i)) = x(j)) = 1E[ ]Tij(x(i))=x(j) = 1 - pij .

(1)

In order to characterize reconstruction accuracy, we let X(j,r) denote the random variable that fol-
lows the distribution Tij(X(i)) where X(i)  µ(i). We define µ(j,r)(x) = Pr(X(j,r) = x),
1and further define pjri = PrX(j,r)µ(j,r) (Tji(x(j)) = x(i)) = E [X(j,r)µ(j,r) ]Tji(x(j))=x(i) = x(j)Sj 1Tji(x(j))=x(i) µ(j,r)(x(j)). The superscript r means "reconstruction". The difference be-
tween prji and pji lies in the distributions of samples in space Sj. We assume that Tij and Tji are independent translators. This is natural because Tij and Tji are usually trained independently. Then,
for any j = i, we have

Pr(Tij (x(i)) = x(j), Tji(x(j)) = x(i)) = pij pjri.

(2)

Further, we make the following assumption.

Assumption 1.

For any j

=

i, any y(j)



1S , E[ ]j y(j)=x(j),Tij (x(i))=y(j)

=

1-pij m

,

where

m

is

a

constant.

The interpretation of this assumption is as follows. For all x(i)  Si that are incorrectly translated into Sj, there are m possibly incorrect translations (in Sj) with positive probabilities. Assumption 1 states that each of the incorrect translations is chosen uniformly at random.

Justification of Assumption 1. It is generally not true in the real-world since the probabilities

of different translations are usually very different. However, this assumption can be approximated

by partitioning the possibly wrong translations into several clusters, where the probability of the

translation falling into each cluster is close. The best case is that the incorrect translation with the

largest probability pmax is treated as one cluster. All the other possibly incorrect translations are

clustered

so

that

the

probability

of

each

cluster

is

close

to

pmax.

Then

we

have

m



.1-pij
pmax

A

small

pmax (a large m) is desirable. Note that the correct translation is indeed also a group of translations

if we set a threshold BLEU score, above which the translation is considered correct.

To analyze the efficacy of dual learning, we consider the following two cases: (Case 1) Tji(Tij (x(i))) = x(i); (Case 2) Tji(Tij (x(i))) = x(i).

Dual learning will detect Case 2 and train the translators so that Case 2 is minimized. We use superscript d for the translators trained from dual learning. For any given xi  Si which fails in reconstruction (Case 2), we define

 = Pr(Tidj (x(i)) = x(j), Tjdi(Tidj (x(i))) = x(i)|Case 2)  = Pr(Tidj (x(i)) = x(j), Tjdi(Tidj (x(i))) = x(i)|Case 2)  = Pr(Tjdi(Tidj (x(i))) = x(i)|Case 2),

(3) (4) (5)

where "Case 2" denotes the condition Tji(Tij(x(i))) = x(i). Here  can be viewed as the probability of correcting the wrong translations by dual learning,  the probability of the occurrence of the alignment problem under Case 2, and  the probability of nonzero reconstruction error.  models the imperfectness of dual learning, which should be zero in the ideal case. It is easy to see + + = 1.

3 THEORETICAL STUDY

In this section, we provide a theoretical study of why dual learning outperforms the baseline translatorby the following theorem.

Theorem 1. Under Assumption 1, for any language spaces Si and Sj, the accuracy of dual learning

outcome

Tidj

is

pidj

=

(1

-

)pij pjri

+

(1

-

(1-pij

)(1-prji m

)

),

where



is

defined

in

equation

3.

This theorem is proved by explicitly computing the probabilities of each case. During dual learning, Case 2 is redistributed to  case (a desirable subcase of Case 1, defined in equation 3),  case (an undesirable but unavoidable subcase of Case 1, defined in equation 4), and  case (remaining in Case 2, defined in equation 5). See Appendix B for the full proof. We have the following observations from Theorem 1.

3

Under review as a conference paper at ICLR 2019

Relation to the baseline translators. The accuracy is dual learning improvement is positively correlated to the baseline translators of both directions. The larger the pij or prji is, the higher
accuracy of Tidj dual learning can achieve.

The role of m and . When pij, prji are fixed, a large m is desired for better accuracy. As 

increases, the outcome of dual learning improves. In the optimistic (but maybe unrealistic) case

where  = means dual

1,  =  = 0, we learning provides

have pidj - pij = 1 an improvement as

l-on(g1-aspimjm)(1-p1rjia)n-d pprjiij

=

.(1-pij )(m-1+pjri)
m

> 0, which is trivial.

This

A hypothesis. We consider the case where the probabilities of redistribution to  case and  case

are proportional to  and .

Formally,

 

=

Pr(Tij (x(i))=x(j),Tji(Tij (x(i)))=x(i)) Pr(Tij (x(i))=x(j),Tji(Tij (x(i)))=x(i))

=

.mpij prji
(1-pij )(1-prji)

Then we have 

=

mpij pjri(1-) mpij prji+(1-pij )(1-prji)

and

pidj = Pr(Tidj (x(i)) = x(j), Tjdi(x(j)) = x(i))

=

mpij pjri(1

-

(1

-

pij prji

-

(1-pij )(1-prji m

)

))

mpij pjri + (1 - pij )(1 - prji)

=

mpij

mpij prji(1 - ) pjri + (1 - pij )(1

-

prji)

,

(6)

where



=

(1-pij pjri-

(1-pij )(1-pjri) m

).

To

compare

pidj

with

the

accuracy

of

the

original

translator,

we compute the difference

pidj

-

pij

=pij

(

mpij

prji

mpjri + (1

(1 -

- ) pij )(1

-

pjri)

-

1)

=pij ( mpij pjri

+

mpjri (1 - pij)(1

-

pjri)

-

1

-

mpij pjri

+

mprji (1 - pij)(1

-

prji) )

=pij

(

(1 - pij)((m + 1)pjri mpij prji + (1 - pij )(1

- -

1) prji

)

-

mpij prji

+

mprji (1 - pij)(1

-

prji) )

Ideally,

we

have



=

0,

which

means



=

0.

If

pjri

>

1 m+1

,

the outcome

of dual learning

is

better

than the baseline translator. This means, as long as the overall probability of a correct translation is

greater than choosing from the m + 1 translations (1 correct and m incorrect) uniformly at random,

dual learning outperforms the baseline translator. The expression with the  factor is negative, which

is consistent with the intuition that  should be minimized. For the nonideal case where  > 0, if

both

prji

>

1 m+1

and



<

(1 - pij)(1 +

1 m

-

)1
mpjri

hold,

dual learning

also improves

against the

baseline translator.

Figure 1: The proposed multipath dual learning and cycle dual learning.
4 EXTENSIONS: CYCLE DUAL LEARNING AND MULTIPATH DUAL LEARNING
In Theorem 1, we found that both pij and prji play a positive role in improving pidj under mild assumptions. A natural question is whether this probability could be further enhanced by exploiting multiple language domains. Therefore, we propose the two frameworks, cycle dual learning and multipath dual learning, both of which leverage multiple languages and significantly extend the standard dual learning. In this section, we mainly present the cycle dual learning framework due to the space constraint.
4

Under review as a conference paper at ICLR 2019

4.1 FRAMEWORK

The proposed frameworks are illustrated in Figure 1. Let S1 and S2 denote the source language space

and the target language space respectively. To use these frameworks, we first train the following

translators: S1  S2, S1  Sk and S2  Sk where k  3. Then, for cycle dual learning, we

require a sentence from S2 to be very similar to S2  Sk  S1  S2 (or equivalently, a sentence

from S1 to be very similar to S1  S2  Sk  S1); for multipath dual learning, we require

the translation S1  S2 to be the very similar S1  Sk  S2. In this way, we build another

constraint, where the translation S1  S2 could leverage the information pivoted by the domain

Sk. In practice, to use cycle dual learning to enhance the S1  S2 model, we need to minimize

x(2)S2 D(T12(Tk1(T2k(x(2)))), x(2)), where D(·, ·) measures the differences of two inputs; to

use multipath update could

dual also

learning, we be applied to

need to S2 

minimize x(1)S1 D(T12(x(1)), S1 translation and leverage more

Tk2(T1k(x(1)))). Similar language domains. If no

auxiliary domain is provided, both cycle dual learning and multipath dual learning will degenerate

to the standard dual learning. We design sampling based algorithms for the two frameworks. This

section focuses on the cycle dual learning framework with corresponding theoretical analysis. See

Appendix D and F for the multipath dual learning framework. In both frameworks, let ij denote

the parameters of translator Tij.

Algorithm 1: Cycle Dual Learning Framework
1 Input: Samples from spaces S1 . . . Sk, initial translators T12, T21 and T1i, Ti1 i = 3, . . . , K; learning rates ;
2 Train each of T12, T21 and T1i, Ti1 i = 3, . . . , k by dual learning; 3 Randomly sample a k from {3, 4, · · · , K}; randomly sample one x(1)  S1 and one x(2)  S2; 4 Generate x~(2) by Tk2  T1k(x(1)) and generate x~(1) by Tk1  T2k(x(2)); 5 Update the parameters of T12 and T21, denoted as 12 and 21, as follows:
12  12 + 12 log P (x(2)|x~(1); 12); 21  21 + 21 log P (x(1)|x~(2); 21); (7)
Repeat Step 3 to Step 5 until convergence;

4.2 THEORETICAL ANALYSIS

The theoretical analyses of cycle dual learning and multipath dual learning are highly symmetric. In this section, we take cycle dual learning as an example. The analysis for multipath dual learning can be found in Appendix F.

For simplicity, we focus on the triangle structure that contains only S1, S2 and S3. Now dual learning serves as the basic structure. For simplicity, we use qij to denote the accuracy of Tidj, i.e. qij = pdij. We further define q3(21) = PrX(3)T2d3(X(1))(T3d1(x(3)) = x(1)), which as the accuracy of T3d1 when the sentence in S3 is translated from a random sentence in S2. This is analogous to reconstruction accuracy. To analyze cycle dual learning, we consider the following two cases (analogous to Case 1 and Case 2 for dual learning): Case 1: x(1) = T3d1(T2d3(T1d2(x(1)))); and Case 2: x(1) = T3d1(T2d3(T1d2(x(1)))).
Cycle dual learning will detect Case 2 and train the translators so that Case 2 is minimized. To quantify this effect, we define the following probabilities:

 = Pr(T1c2(x(1)) = x(2), T3c1(T2c3(x(2))) = x(1)|Case 2)  = Pr(T1c2(x(1)) = x(2), T3c1(T2c3(T1c2(x(1)))) = x(1)|Case 2)  = Pr(x(1) = T3c1(T2c3(T1c2(x(1))))|Case 2),

(8)

where Case 2 denotes the condition x(1) = T3d1(T2d3(T1d2(x(1)))) and superscripts d and c denote the translators obtained from dual learning and cycle dual learning, respectively.  ,  ,  can be viewed
as the probability of correcting the wrong translations by cycle dual learning, the probability of the
occurrence of the alignment problem under Case 2, and the probability of nonzero reconstruction

5

Under review as a conference paper at ICLR 2019

error.  models the imperfectness of dual learning. And we have  +  +  = 1. We assume that T2d3 and T3d1 are independent translators. That implies,

Pr(T2d3(x(2)) = x(3), T3d1(x(3)) = x(1)) = q23q3(21)

(9)

We will also need a similar assumption to Assumption 1 as follows.

Assumption 2.

For any j

=

i, any y(j)



1S , E[ ]j y(j)=x(j),Tidj (x(i))=y(j)

=

1-qij m

,

where

m

is

a

constant.

The interpretation and justification of Assumption 2 is similar to those of Assumption 1. Then we have the following theorem about the triangle structure. The more general case can be viewed as adding one path a time so Theorem 2 can be applied.

Theorem 2. Given languages spaces S1, S2, and S3, where the objective is to train a translator that maps from S1 to S2, the accuracy of cycle dual learning outcome is

pc12

=

q12(1

-



)(q23q3(11)

+

(1

-

q23)(1 m

-

q3(11)) )

+



(1

-

(1

-

q12)(q23

+

q3(11)

- (m m2

+

1)q23q3(11)

+

m

-

1) )

(10)

under Assumption 2.

Theorem 2 is also proved by explicitly computing the probability of each case. The full proof can be found in the Appendix C. Theorem 2 is much harder to interpret than Theorem 1, but we can still observe how factors affect the accuracy of T1c2.
The role of T1d2. The accuracy increases as q12 increases. So a good initial translator is desirable. However, if T1d2 is arbitrarily bad, it is still possible to improve it significantly. This implies the application of this framework on low-resource or even zero-shot machine translation.

A similar hypothesis. Similar to the analysis of dual learning, we consider the condition where

 

= and define Pr(T1d2(x(1))=x(2),T3d1(T2d3(x(2)))=x(1))
Pr(T1d2 (x(1) )=x(2) ,T3d1 (T2d3 (T1d2 (x(1) )))=x(1) )

=  Pr(x(1) = T3d1(T2d3(T1d2(x(1))))).

Then the accuracy simplifies to

p1c2

=

 (1 -   +

)

=

q12(q23q3(21)

+

q12(q23q3(21)

+

(1-q23

)(1-q3(21) m

)

)(1

-



)

) +(1-q23)(1-q3(21))
m

(1-q12 )(q23 +q3(21) -(m+1)q23 q3(21) +m-1) m2

1-

1-

= = 1 + M(1-q12)(q23+q3(21)-(m+1)q23q3(21)+m-1) 1 + mq12(mq23q3(21)+(1-q23)(1-q3(21)))

1-q12 q12

where M = .q23+q3(21)-(m+1)q23q3(21)+m-1
m(mq23 q3(21) +(1-q23 )(1-q3(21) ))
and M = 1, it simplifies to q12, which

We observe that When is the accuracy of dual

 = 0 (and learning and

therefore  = 0) that pc12 increases

as M decreases. To characterize the condition when p1c2 > q12, we let M < 1. We have

q13 +q3(12) -(m+1)q13 q3(12) +m-1 m(mq13 q3(12) +(1-q13 )(1-q3(12) ))

<

1, which leads to

((m + 1)q13 - 1)((m + 1)q3(12) - 1) > 0.

When

q13, q3(12)

>

1 m+1

,

which

means

the

probability

of

correctly

translating

a sentence

is

greater

than choosing from potentially wrong translations uniformly at random, cycle dual learning outper-

forms dual learning.

We also prove a similar theorem for multipath dual learning (see Appendix F). The accuracy of multipath dual learning has a similar form. We observe similar empirical performances as well (see Appendix E).

5 EXPERIMENTS OF DUAL LEARNING
Since previous works (He et al., 2016; Xia et al., 2017a;b; Wang et al., 2018; Xia et al., 2018) have demonstrated strength of dual learning, we aims at providing some theoretical insights. We choose

6

Under review as a conference paper at ICLR 2019

WMT2014 EnglishGermen translation1 and MultiUN (Eisele & Chen, 2010) EnglishFrench translation2 to verify our theoretical analysis for dual learning. For ease of reference, denote English,
French and German as En, Fr and De respectively.

5.1 SETTINGS
Datasets Following the common practice in NMT, for the EnDe, we preprocess the data in the same way as that used in Vaswani et al. (2017) and eventually obtain 4.5M training sentence pairs. We concatenate newstest2012 and newstest2013 as the validation set (6K sentence pairs) and choose newstest2014 as the test set (3K sentence pairs). For the MultiUN EnFr translation, we sample 2M/6K/3K sentence pairs as the training/validation/test sets. All the sentence are split into wordpiece following (Johnson et al., 2016). To leverage dual learning as that used in (He et al., 2016), for WMT'14 EnDe translation, we choose 8M monolingual English sentences and 8M monolingial German sentences from newscrawl. For MultiUN EnFr translation, we randomly sample 1M English and 1M French sentences as the monolingual data to construct the duality loss.
Architecture & Optimization We use Transformer (Vaswani et al., 2017), the state-of-the-art NMT system, for each direction. For WMT EnDe translation, we choose the transformer big configuration, in which the word embedding dimension, hidden dimension filter sizes and number of multihead attention are 1024, 1024 and 4096 respectively. For MultiUN EnFr translation, we choose the transformer base configuration, in which the aforementioned four numbers are 512, 512, 2048 and 8 respectively. The optimization algorithm is Adam with learning rate 2 × 10-4, 1 = 0.99 and 7168 tokens per GPU. We train our models on 8 M40 GPUs for 7 days.
Evaluation For WMT 2014 EnDe translation, following Vaswani et al. (2017), we use beam search with beam width 4 and length penalty 0.6 to generate candidates. The evaluation metric is BLEU score (Papineni et al., 2002b), which is a geometric mean of n-gram precisions (n = 1, 2, 3, 4). A large BLEU score indicates a better translation quality.

5.2 RESULTS
Translation qualities The BLEU scores of each translation tasks are summarized in Table 1, in which the second row and third row represent the results of the standard Transformer and dual learning. We can see that after applying dual learning, the performances of all tasks are boosted. Specially, on EnDe and DeEn translations, we can improve the baseline from 28.40 to 29.97, and from 32.15 to 35.16. On the other task, dual learning can achieve 0.65 and 0.86 point improvement, which demonstrates its effectiveness. We found that on MultiUN, we do not achieve as much improvement as WMT. The reason is that MultiUN dataset is a collection of translated documents from the United Nations, which are usually of formal and simple patterns that are easy to learn. As a result, introducing more data might not increase the BLEU so much.

EnDe DeEn EnFr FrEn

Standard

28.40

Dual Learning 29.97

32.04 34.93

50.26 50.56 50.91 51.42

Table 1: BLEU scores of WMT2014 EnDe and MultiUN EnFr translations tasks.

The translator accuracy We interpret the results in terms of accuracy when different thresholds are selected. We vary the threshold BLEU score from 10 to 40, and the accuracy of each translator is shown in Table 2. Let S1 and S2 denote English and German respectively for the EnDe task (English and French respectively for the EnFr task). Values are percentages of translations that are above the threshold.
Qualitively, we observe that dual learning outcomes are better than single transformers. More interesting observations lie in the following quantitative analysis.
1Data available at http://www.statmt.org/wmt14/translation-task.html. 2Data available at http://opus.nlpl.eu/MultiUN.php

7

Under review as a conference paper at ICLR 2019

EnDe

EnFr

Threshold BLEU p12 p21 p1d2 pd21 p12 p21 pd12 pd21

10 0.42 0.55 0.66 0.75 0.82 0.80 0.82 0.81 20 0.27 0.38 0.55 0.67 0.77 0.74 0.78 0.75 30 0.13 0.21 0.38 0.50 0.67 0.64 0.68 0.66 40 0.06 0.10 0.24 0.32 0.55 0.53 0.56 0.55

Table 2: Accuracy of translators using different threshold BLEU scores.

The roles of m and  It appears that all theories throughout this paper are independent of the selection of threshold score. However, in practice, different thresholds result in different m and  values. A higher threshold BLEU score usually means there are more incorrect possible translations, and it will be harder reconstruct the original sentence well. Despite of this, we assume m and  are constants and performed a regression analysis using equation 6, where m and  are independent variables. we compute p12, p21, and pd12 for all integer threshold scores between 1 and 100, and fit equation 6 by minimizing the absolute value of differences between the predicted p1d2 and true pd12, resulting in m = 92 and  = 0.45 for the EnDe task (m = 12 and  = 0.57 for the EnFr task). The high  value on one hand implies that machine translation is generally a hard task, and on the other hand means there is still space to improve.

6 EXPERIMENTS OF MULTIPATH DUAL LEARNING

To verify the effectiveness of cycle dual learning and multipath dual learning, we focus on the translation between English (En), French (Fr) and Spanish (Es). Again, we choose the MultiUN dataset to build the translation models since any two of the aforementioned three languages have bilingual sentence pairs. We study two different settings, where for each language pair, we are provided with 2M or 0.2M bilingual sentence pairs. For both settings, we choose 1M monolingual sentences for each language. We use transformer base for all experiments in this section, where the model is a six-block network, with word embedding size, hidden state size and filter size 512, 512 and 2048. The training process is the same as that in Section 5.

Baseline Dual Learning
Cycle

EnFr
50.26 50.91 51.28

FrEn
50.56 51.42 51.89

EnEs
55.15 55.51 55.97

EsEn
55.23 55.77 56.17

EsFr
47.75 48.23 48.62

FrEs
48.13 48.52 48.87

Table 3: Experimental Results on MultiUN (2M bilingual data)
The experimental results of using 2M bilingual data and 1M monolingual data are shown in Table 3. We can see that on average, dual learning can boost the six baselines (i.e., standard transformer) by 0.55 point. Although dual learning can achieve verify high scores on MultiUN translation tasks, our proposed cycle dual learning can still improve it by 0.41 point on average.

Baseline Dual Learning
Cycle

EnFr
43.12 45.54 47.23

FrEn
43.26 45.44 46.72

EnEs
49.28 51.07 52.56

EsEn
47.80 50.31 51.65

EsFr
41.47 42.81 43.52

FrEs
41.21 42.57 44.97

Table 4: Experimental Results on MultiUN (0.2M bilingual data)

The results of using 0.2M bilingual data plus 1M monolingual data is shown in Table 4. We have the following observations: (1) Since there are fewer bilingual sentences, the baselines of the six translation tasks are not as good as those in Table 3. (2) For this setting, dual learning can improve the BLEU scores by 1.93 points on average, which is consistent with the discovery in He et al.

8

Under review as a conference paper at ICLR 2019
(2016) that dual learning can obtain more improvements when the number of bilingual sentences is small.(3) When cycle dual learning is added to the conventional dual learning, we can achieve an extra 1.45 improvements.
7 CONCLUSIONS
We provide the first theoretical study of dual learning and characterize conditions when dual learning outperforms baselines. We also propose two algorithmic extensions of dual learning, the cycle dual learning framework and multipath dual learning framework, which are provably better than dual learning under mild conditions. Our dual learning experiments demonstrate the efficacy of dual learning w.r.t. accuracy and provide insights on the potential power of dual learning. Our cycle dual learning framework achieves a new state-of-the-art BLEU score.
REFERENCES
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine translation. In International Conference on Learning Representations (ICLR 2018), 2018.
Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Semisupervised learning for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1965­ 1974, 2016.
Andreas Eisele and Yu Chen. Multiun: A multilingual corpus from united nation documents. In Proceedings of the Seventh conference on International Language Resources and Evaluation, pp. 2868­2872, 5 2010.
Tomer Galanti, Lior Wolf, and Sagie Benaim. The role of minimal complexity functions in unsupervised learning of semantic mappings. In 6th International Conference on Learning Representations, 2018.
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. Dual learning for machine translation. In Advances in Neural Information Processing Systems, pp. 820­828, 2016.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vie´gas, Martin Wattenberg, Greg Corrado, et al. Google's multilingual neural machine translation system: enabling zero-shot translation. arXiv preprint arXiv:1611.04558, 2016.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. In International Conference on Machine Learning, pp. 1857­1865, 2017.
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only. In International Conference on Learning Representations (ICLR 2018), 2018.
Ping Luo, Guangrun Wang, Liang Lin, and Xiaogang Wang. Deep dual learning for semantic image segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, pp. 21­26, 2017.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311­318. Association for Computational Linguistics, 2002a.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311­318. Association for Computational Linguistics, 2002b.
Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, and Ming Zhou. Question answering and question generation as dual tasks. arXiv preprint arXiv:1706.02027, 2017.
9

Under review as a conference paper at ICLR 2019
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998­6008, 2017.
Yijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao Qin, Guiquan Liu, and T Liu. Dual transfer learning for neural machine translation with marginal distribution regularization. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Yingce Xia, Jiang Bian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Dual inference for machine learning. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pp. 3112­3118, 2017a.
Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning. In International Conference on Machine Learning, pp. 3789­3798, 2017b.
Yingce Xia, Xu Tan, Fei Tian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Model-level dual learning. In International Conference on Machine Learning, pp. 3789­3798, 2018.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In IEEE International Conference on Computer Vision, 2017.
10

Under review as a conference paper at ICLR 2019

A NOTATIONS
k number of language spaces Si i-th language space µ(i) distribution of sentences in Si X(i) the random variable that follows µ(i) x(i) one sample (sentence) in Si x(j) the correct translation of x(i) in SJ Tij the baseline translatorthat translates from Si to Sj pij accuracy of the Tij Tidj translator that translates from Si to Sj trained using dual learning pdij accuracy of T dij Ticj translator that translates from Si to Sj trained using cycle dual learning picj accuracy of Ticj Timj translator that translates from Si to Sj trained using multipath dual learning pmij accuracy of Timj
Table 5: Notations

B PROOF OF THEOREM 1

Proof. Consider a random sample x(1) and the translation from x(1)  S1 to S2. Before dual learning, the accuracy is p12. We analyze the two cases defined earlier in this section.

Case 1. T21(T12(x(1))) = x(1). Case 1 consists of two subcases:

Case 1.1: T12(x(1)) = x(2); Case 1.2: T12(x(1)) = x(2).

Although Case 1.2 is not desired, dual learning does not detect it. From equation 2, the probability
of Case 1.1 is Pr(T12(x(1)) = x(2), T21(T12(x(1))) = x(1)) = p12p2r1, According to Assumption 1, we have

Pr(T12(x(1)) = x(2), T21(T12(x(1))) = x(1))

=

Pr(T12(x(1))

=

y(2), T21(y(2))

=

x(1))

=

(1

- p12)(1 m

- p2r1)

y (2) S2 ,y (2) =x(2)

Case 2. T21(T12(x(1))) = x(1). Dual learning will train the translators so that this case is minimized. The probability of this case is simply the complement of Case 1.

Pr(Case

2)

=

1

-

p12pr21

-

(1

-

p12)(1 m

-

p2r1)

After dual learning, Case 2 is redistributed to the two subcases of Case 1, with probabilities  and  respectively. So we have

Pr(T1d2(x(1))

=

x(2), T2d1(T1d2(x(1)))

=

x(1))

=

p12pr21

+

(1

-

p12pr21

-

(1

-

p12)(1 m

-

pr21) )

=

(1

-

)p12p2r1

+

(1

-

(1

-

p12)(1 m

-

pr21) )

(11)

Pr(T1d2(x(1))

=

x(2), T2d1(T1d2(x(1)))

=

x(1))

=

(1

-

)(1

- p12)(1 m

-

p2r1)

+

(1

-

p12pr21)

Pr(T2d1(T1d2(x(1)))

=

x(1))

=

(1

-

p12p2r1

-

(1

-

p12)(1 m

-

p2r1) ),

where equation 11 computes the accuracy of dual learning.

11

Under review as a conference paper at ICLR 2019

C PROOF OF THEOREM 2

Proof. We focus on the mapping from x(1)  S1 to S2 and consider the following two cases:
Case 1: T3d1(T2d3(T1d2(x(1)))) = x(1); Case 2: T3d1(T2d3(T1d2(x(1)))) = x(1).
We first characterize the probability of Case 1.

Case 1: According to whether the translation in S2 is correct, Case 1 can be partitioned into two subcases:

Case 1.1: T1d2(x(1)) = x(2), T3d1(T2d3(x(2))) = x(1);

Case 1.2: T1d2(x(1)) = x(2), T3d1(T2d3(T1d2(x(1)))) = x(1).

Case 1.1. For the path S1  S2, we have Pr(T1d2(x(1)) = x(2)) = q12. For the path S2  S3  S1, since we only care about the translation in S1, we have

Pr(T3d1(T2d3(x(2))) = x(1)) = Pr(T2d3(x(2)) = x(3)) Pr(T3d1(T2d3(x(2))) = x(1)|T2d3(x(2)) = x(3))

+ Pr(T2d3(x(2)) = x(3)) Pr(T3d1(T2d3(x(2))) = x(1)|T2d3(x(2)) = x(3))

=q23q3(11)

+

(1

-

q23)(1 m

-

q3(11))

where the first equality is obtained by the law of total probability, and the second quality is obtained by equation 9 and Assumption 2. Then the probability of Case 1.1 is computed as follows.

Pr(T1d2(x(1))

=

x(2), T2d3(T1d3(x(1)))

=

x(2))

=

q12(q23q3(11)

+

(1

-

q23)(1 m

-

q3(11)) )

(12)

Case 1.2. For the S1  S2 path we have Pr(T1d2(x(1)) = x(2)) = 1 - q12. Let y(2)  S2 denote this incorrect translation. Now we compute Pr(T3d1(T2d3(x(2))) = y(1)). There are three cases depending on the translation at S3: x(3), y(3), and others. They are computed as follows.

Pr(T3d1(T2d3(y(2))) = x(1)|T2d3(x(2)) = x(3)) = q3(11)

Pr(T3d1(T2d3(y(2)))

=

x(1)|T2d3(x(2))

=

y(3))

=

1 - q3(11) m

Pr(T3d1(T2d3(y(2)))

=

x(1)|T2d3(x(2))

=

x(3), y(3))

=

1

- q3(11) m

Using the law of total probability we have

Pr(T3d1(T2d3(y(2)))

=

x(1))

= q3(11)(1 - m

q23)

+

q23(1 - m

q3(11))

+

(m

-

1)(1

- q23)(1 m2

-

q3(11))

= m(q32

+

q3(11)

-

2q23q3(11))

+ (m m2

-

1)(1

-

q23)(1

-

q3(11))

= q23

+

q3(11)

-

(m

+ 1)q23q3(11) m2

+

m

-

1

Then the probability of Case 1.2 is

Pr(T1d2(x(1))

=

x(2), T3d1(T2d3(T1d2(x(1))))

=

x(1))

=

(1 - q12)(q23

+ q3(11)

- (m + 1)q32q3(11) m2

+ m - 1)

(13)

Case 2. The probability of Case 2 is simply the complement of Case 1.

Pr(T3d1(T2d3(T1d2(x(1)))) = x(1)) = 1 - Pr(T3d1(T2d3(T1d2(x(1)))) = x(1))

=1

-

q12(q23q3(11)

+

(1

-

q23)(1 m

-

q3(11)) )

-

(1

-

q12)(q23

+

q3(11)

- (m m2

+

1)q23q3(11)

+

m

-

1) .

(14)

12

Under review as a conference paper at ICLR 2019

Then the accuracy of this triple learning is
pm12 = Pr(T1d2(x(1)) = x(2), T3d1(T2d3(x(2))) = x(1)) +  Pr(T3d1(T2d3(T1d2(x(1)))) = x(1)), (15) where  is defined in equation 8. equation 10 is obtained by substitute equation 12 and equation 14 into equation 15.
D MULTIPATH DUAL LEARNING FRAMEWORK
The algorithm of multipath dual learning is shown in Algorithm 2.
Algorithm 2: Multi-path Dual Learning Framework 1 Input: Samples from spaces S1 . . . Sk, initial translators T12, T21 and T1i, Ti1 i = 3, . . . , K;
learning rates ; 2 Train each of T12, T21 and T1i, Ti1 i = 3, . . . , k by dual learning; 3 Randomly sample a k from {3, 4, · · · , K}; randomly sample one x(1)  S1 and one x(2)  S2; 4 Sample one x^(k)  T1k(x(1)), x^(2)  Tk2(x^(k)); x~(k)  T2k(x(2)) and x^(2)  Tk2(x~(k)); 5 Update the parameters of T12 and T21, denoted as 12 and 21, as follows:
12  12 - 12 log P (x^(2)|x(1); 12); 21  21 - 21 log P (x^(1)|x(2); 21); (16) Repeat Step 3 to Step 5 until convergence;

It is well-known that the two gradients in equation 16 are unbiased estimators of

12 DKL(P (·|x(1); Tk2  T1k) P (·|x(1); 12)) ; 21 DKL(P (·|x(1); Tk2  T1k) P (·|x(1); 12)). (17)
The reason is shown as follows. For any given x(1)  S1,

min DKL(P (·|x(1); 1k, k2) P (·|x(1); 12))
12

=

x(2) S2

P

(x(2)|x(1);

1k ,

k2)

log

P

(x(2)|x(1); 1k, k2 P (x(2)|x(1); 12)

)

,

(18)

which is equivalent to

max

P (x(2)|x(1); 1k, k2) log P (x(2)|x(1); 12)

12

x(2) S2

= P (x(2), x(k)|x(1); 1k, k2) log P (x(2)|x(1); 12)

x(2)S2 x(k)Sk

= P (x(k)|x(1); 1k, k2)P (x(2)|x(k), x(1); 1k, k2) log P (x(2)|x(1); 12)

x(2)S2 x(k)Sk

= P (x(k)|x(1); 1k)P (x(2)|x(k); k2) log P (x(2)|x(1); 12)

x(2)S2 x(k)Sk

=E Ex(k)P (·|x(1);1k) x(2)P (·|x(k);k2) log P (x(2)|x(1); 12).

(19)

Therefore, we can first sample x^(k) from P (·|x(1); 12), then sample x^(2) from P (·|x^(k); k2), and then maximize log P (x^(2)|x(1); 12). Obviously, -12 log P (x^(2)|x(1); 12) is the unbiased estimator of 12 DKL(P (·|x(1); 1k, k2) P (·|x(1); 12)).
The performance of multipath dual learning is shown in Table 6 in comparison with other translators. We observe that multipath dual learning has a very similar performance with cycle dual learning, which is consistent with our theoretical analysis.

13

Under review as a conference paper at ICLR 2019

Baseline Dual Learning
Multi-path Cycle

EnFr
50.26 50.91 51.27 51.28

FrEn
50.56 51.42 51.74 51.89

EnEs
55.15 55.51 55.89 55.97

EsEn
55.23 55.77 56.06 56.17

EsFr
47.75 48.23 48.34 48.62

FrEs
48.13 48.52 48.76 48.87

Table 6: Experimental Results on MultiUN (2M bilingual data)

E DERIVATION OF CYCLE DUAL LEARNING ALGORITHM

We follow the notations used in Appendix D. Let R(x(2)) denote the event that after passing S2  Sk  S1  S2, x(2) is reconstructed to x(2). We have that

log P (R(x(2))) = log P (x(2)|start from x(2); 2k, k1, 12)

= log P (x(2), x(1), x(k)|start from x(2); 2k, k1, 12)

x(k)Sk x(1)S1

= log P (x(1), x(k)|start from x(2); 2k, k1)·

x(k)Sk x(1)S1
P (x(2)|start from x(2), x(1), x(k); 2k, k1, 12)

 P (x(1), x(k)|x(2); 2k, k1) log P (x(2)|start from x(2), x(1), x(k); 2k, k1, 12)

x(k)Sk x(1)S1

= P (x(1), x(k)|x(2); 2k, k1) log P (x(2)|x(1); 12)

x(k)Sk x(1)S1

= P (x(k)|x(2); 2k)P (x(1)|x(k); k1) log P (x(2)|x(1); 12)

x(k)Sk x(1)S1

=E Ex(k)P (·|x(2);2k) x(1)P (·|x(k));k1 log P (x(2)|x(1); 12).

(20)

Then, we can use the algorithm proposed in Algorithm 1 to solve the above optimization problem,

i.e., min - log P (R(x(2))).

F THEORETICAL ANALYSIS FOR MUTIPATH DUAL LEARNING

We define q3(12) = PrX(3)T1d3(X(1))(T3d2(x(3)) = x(2)), which as the accuracy of T3d2 when the sentence in S3 is translated from a random sentence in S1. This is analogous to reconstruction accuracy, and is reduced to the reconstruction accuracy when S2 = S1. To analyze multipath learning, we consider the following cases (analogous to Case 1 and Case 2 for dual learning and cycle dual learning): Case 1: T12(x(1)) = T32(T13(x(1))); Case 2: T12(x(1)) = T32(T13(x(1))).
multipath dual learning will detect Case 2 and train the translators so that the difference between translations from the two paths is minimized. To quantify this effect, we define the following probabilities:

 = Pr(T1m2 (x(1)) = x(2), T3m2 (T1m3 (x(1))) = x(2)|Case 2)  = Pr(T1m2 (x(1)) = x(2), T3m2 (T1m3 (x(1))) = T1m2 (x(2))|Case 2)  = Pr(T2m1 (T1m2 (x(1))) = x(1)|Case 2),

(21)

where Case 2 denotes the condition T12(x(1)) = T32(T13(x(1))) and superscripts d and m denote the translators obtained from dual learning and multipath learning, respectively.  ,  ,  can be viewed as the probability of correcting the wrong translations by multipath learning, the probability of the occurrence of the alignment problem under Case 2, and the probability of nonzero reconstruction error.  models the imperfectness of dual learning. And again, we have  +  +  = 1. We

14

Under review as a conference paper at ICLR 2019

assume that T1d3 and T3d2 are independent translators before multipath learning. That implies,

Pr(T1d3(x(1)) = x(3), T3d2(T1d3(x(1))) = x(2)) = Pr(T1d3(x(1)) = x(3), T3d2(x(3)) = x(2)) = q13q3(12) (22)
Then we have the following theorem, which focuses one the triangle structure containing only S1, S2, and S3. The more general case can be viewed as adding one path a time so Theorem 3 can be applied.

Theorem 3. Given languages spaces S1, S2, and S3, where the objective is to train a translator that maps from S1 to S2, the accuracy of two-path learning (Figure 1) outcome is

pm12

=

q12(1

-



)(q13q3(12)

+

(1

-

q13)(1 m

-

q3(12)) )

+



(1

-

(1

-

q12)(q13

+

q3(12)

- (m m2

+

1)q13q3(12)

+

m

-

1) )

(23)

under Assumption 2.

Proof. Similar to the analysis of Theorem 1, we focus on the mapping from x(1)  S1 to S2 and consider the following two cases:

Case 1: T1d2(x(1)) = T3d2(T1d3(x(1))); Case 2: T1d2(x(1)) = T3d2(T1d3(x(1))).
We first characterize the probability of Case 1.

Case 1: According to whether the translation in S2 is correct, Case 1 can be partitioned into two subcases:
Case 1.1: T1d2(x(1)) = x(2), T2d3(T1d3(x(1))) = x(2);
Case 1.2: T1d2(x(1)) = x(2), T2d3(T1d3(x(1))) = T1d2(x(1)). Case 1.1. For the path S1  S2, we have

Pr(T1d2(x(1)) = x(2)) = q12. For the path S1  S3  S2, since we only care about the translation in S2, we have

Pr(T3d2(T1d3(x(1))) = x(2))

= Pr(T1d3(x(1)) = x(3)) Pr(T3d2(T1d3(x(1))) = x(2)|T1d3(x(1)) = x(3))

+ Pr(T1d3(x(1)) = x(3)) Pr(T3d2(T1d3(x(1))) = x(2)|T1d3(x(1)) = x(3))

=q13q3(12)

+

(1

-

q13)(1 m

-

q3(12))

where the first equality is obtained by the law of total probability, and the second quality is obtained by equation 22 and Assumption 2. Then the probability of Case 1.1 is computed as follows.

Pr(T1d2(x(1))

=

x(2), T2d3(T1d3(x(1)))

=

x(2))

=

q12(q13q3(12)

+

(1

-

q13)(1 m

-

q3(12)) )

(24)

Case 1.2. For the S1  S2 path we have Pr(T1d2(x(1)) = x(2)) = 1 - q12. Let y(2)  S2 denote this incorrect translation. Now we compute Pr(T3d2(T1d3(x(1))) = y(2)). There are three cases depending on the translation at S3: x(3), y(3), and others. They are computed as follows.

Pr(T3d2(T1d3(x(1)))

=

y(2)|T1d3(x(1))

=

x(3))

=

1 - q3(12) m

Pr(T3d2(T1d3(x(1))) = y(2)|T1d3(x(1)) = y(3)) = q3(12)

Pr(T3d2(T1d3(x(1)))

=

y(2)|T1d3(x(1))

=

x(3), y(3))

=

1

- q3(12) m

15

Under review as a conference paper at ICLR 2019

Using the law of total probability we have

Pr(T3d2(T1d3(x(1)))

=

y(2))

= q13(1 - m

q3(12))

+

q3(12)(1 - m

q13)

+

(m

-

1)(1

- q13)(1 m2

-

q3(12))

= m(q13

+

q3(12)

-

2q13q3(12))

+ (m m2

-

1)(1

-

q13)(1

-

q3(12))

= q13

+

q3(12)

-

(m

+ 1)q13q3(12) m2

+

m

-

1

Then the probability of Case 1.2 is

Pr(T1d2(x(1)) = x(2), T2d3(T1d3(x(1))) = T1d2(x(1)))

=

(1

-

q12)(q13

+

q3(12)

- (m m2

+

1)q13q3(12)

+

m

-

1)

Case 2. The probability of Case 2 is simply the complement of Case 1.

(25)

Pr(T1d2(x(1)) = T3d2(T1d3(x(1)))) = 1 - Pr(T1d2(x(1)) = T3d2(T1d3(x(1))))

=1

-

q12(q13q3(12)

+

(1

-

q13)(1 m

-

q3(12)) )

-

(1

-

q12)(q13

+

q3(12)

- (m m2

+

1)q13q3(12)

+

m

-

1) .

(26)

Then the accuracy of this two-path learning is

p1m2 = Pr(T1d2(x(1)) = x(2), T2d3(T1d3(x(1))) = x(2)) +  Pr(T1d2(x(1)) = T3d2(T1d3(x(1)))), (27)
where  is defined in equation 21. equation 23 is obtained by substitute equation 24 and equation 26 into equation 27.

The role of T1d2. The accuracy increases as q12 increases. So a good original translator trained from dual learning is desirable. However, if T1d2 is arbitrarily bad, it is still possible to improve it significantly. This implies the application of this framework on low-resource or even zero-shot
machine translation.

A similar hypothesis. Similar to the analysis of dual learning, we consider the condition where

 

= and define Pr(T1d2(x(1))=x(2),T2d3(T1d3(x(1)))=x(2))
Pr(T1d2 (x(1) )=x(2) ,T2d3 (T1d3 (x(1) ))=x(2) )

=  Pr(T1d2(x(1)) = T3d2(T1d3(x(1)))).

Then the accuracy simplifies to

pm12

 (1 -  =
 +

)

=

q12(q13q3(12)

+

q12(q13q3(12)

+

(1-q13

)(1-q3(12) m

)

)(1

-



)

) +(1-q13)(1-q3(12))
m

(1-q12 )(q13 +q3(12) -(m+1)q13 q3(12) +m-1) m2

1-

1-

= = 1 + M(1-q12)(q13+q3(12)-(m+1)q13q3(12)+m-1) 1 + mq12(mq13q3(12)+(1-q13)(1-q3(12)))

1-q12 q12

where M simplifies

= to

.q13 +q3(12) -(m+1)q13 q3(12) +m-1
m(mq13 q3(12) +(1-q13 )(1-q3(12) ))
q12, which is the accuracy of

When  = 0 (and therefore  = 0) and M = 1, it dual learning. Observing pm12 increases as M decreases,

we let M < 1 so that pm12 > q12. We have

q13 + q3(12) - (m + 1)q13q3(12) + m - 1 m(mq13q3(12) + (1 - q13)(1 - q3(12)))

<

1

((m + 1)q13 - 1)((m + 1)q3(12) - 1) > 0

when q13, q3(12)

>

1 m+1

,

which

means the probability of correctly

translating

a

sentence is

greater

than choosing from potentially wrong translations uniformly at random, the two-path learning out-

performs dual learning.

16

