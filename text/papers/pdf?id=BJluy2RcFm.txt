Under review as a conference paper at ICLR 2019

JANOSSY POOLING: LEARNING DEEP PERMUTATIONINVARIANT FUNCTIONS FOR VARIABLE-SIZE INPUTS
Anonymous authors Paper under double-blind review

ABSTRACT
We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.

1 INTRODUCTION

Pooling is a fundamental operation in deep learning architectures (LeCun et al., 2015). The role of pooling is to merge a collection of related features into a single, possibly vector-valued, summary feature. A prototypical example is in convolutional neural networks (CNNs)(LeCun et al., 1995), where linear activations of features in neighborhoods of image locations are pooled together to construct more abstract features. A more modern example is in neural networks for graphs, where each layer pools together embeddings of neighbors of a vertex to form a new embedding for that vertex, see for instance, (Kipf & Welling, 2016; Atwood & Towsley, 2016; Hamilton et al., 2017; Velickovic et al., 2017; Monti et al., 2017; Xu et al., 2018; Liu et al., 2018; Liben-Nowell & Kleinberg, 2007; van den Berg et al., 2017; Duvenaud et al., 2015; Gilmer et al., 2017; Ying et al., 2018).

A common requirement of a pooling operator is invariance to the ordering of the input features. In CNNs for images, this allows invariance to translations of objects, while for graphs, this allows invariance to relabeling of graph vertices. Existing pooling operators are mostly limited to predefined heuristics such as max-pool, min-pool, sum, or average. Another desirable characteristic of pooling layers is the ability to take variable-size inputs. This is less important in images, where neighborhoods are usually fixed a priori. However in applications involving graphs, the number of neighbors of different vertices can vary widely. Our goal is to design flexible and learnable pooling operators satisfying these two desiderata.

Abstractly, we will view pooling as a permutation-invariant (or symmetric) function acting on finite

but arbitrary length sequences h. All elements hi of the sequences are features belonging to some

space H (which itself could be a high-dimensional Euclidean space Rn or some subset thereof,

n > 0). The sequences h are elements of the union of products of the H-space: h 

 j=0

Hj

.

Throughout the paper, we will use n to represent the set of all permutations of the integers 1 to n,

where n will often be clear from the context. In particular, h,   |h|, will represent a reordering

of the elements of a sequence h according to , where |h| is the length of the sequence h. We will

use the double bar superscript f to indicate that a function is permutation invariant, returning the

same value no matter the order of its arguments: f (h) = f (h),   |h|. We will use the arrow
superscript f to indicate general functions on sequences h which may or may not be permutationinvariant. Functions f without any markers are `simple' functions, acting on elements in H, scalars or any other argument that is not a sequence of elements in H.

1

Under review as a conference paper at ICLR 2019

Our goal in this paper is to model and learn permutation-sensitive functions f that can be used to construct flexible and learnable permutation-invariant neural networks. A recent step in this direction is work on deep sets by Zaheer et al. (2017), who argued for learning permutation-invariant functions through the following composition:

y(x; (), (f), (h)) =  f (|h|, h; (f)); () , where

(1)

f (|h|, h; (f)) =

1 |h|

|h|

f (hj ; (f))

and

h  h(x; (h)).

j=1

(2)

Here (a) x  X is the training data, h  H is the embedding space of h, and h : X×Ra 

 j=0

Hj

,

a > 0 is the embedding given by lower layers of the neural network with parameters (h)  Ra,

and (b) f : H × Rb  F is a middle-layer embedding function with parameters (f)  Rb, b > 0, and F is the embedding (output) space of f ; and (c)  : F × Rc  Y is a neural network

with parameters ()  Rc, c > 0, that maps to the final output space Y. Typically H and F are high-dimensional real-valued spaces; Y is often Rd in d-dimensional regression problems or the

simplex in classification problems. Effectively, the neural network f learns an embedding for each

element in H, and given a sequence h, its component embeddings are added together before a second

neural network transformation  is applied. Note that the function h may be the identity mapping

h(x; ·) = x that makes f act directly on the input data. Zaheer et al. (2017) argues that, under reasonable assumptions, the above architecture with suitably flexible functions f and  is capable
of approximating any symmetric function on h-sequences, which justifies the widespread use of

average (sum) pooling to make neural networks permutation invariant in Duvenaud et al. (2015),

Hamilton et al. (2017), Kipf & Welling (2016), Atwood & Towsley (2016), Niepert et al. (2016),

among other works.

In practice, however, there is a gap between flexibility and learnability. While the architecture

of equations 1 and 2 is a universal approximator to all permutation-invariant functions, it does not

allow structural knowledge about the function y to be encoded easily. Consider for instance trying

to learn the permutation-invariant function y (x) = maxi,j|x| |xi - xj|. With such higher-order interactions between the elements of h, the functions f of equation 2 cannot capture any useful

intermediate representations towards the final output, with the burden shifted entirely to the function

. Learning  means learning to undo mixing performed by the summation layer f (|h|, h; (f)) =

1 |h|

|h| j=1

f

(hj

;

(f

)).

Unfortunately,

as

we

show

in

our

experiments,

in

many

applications

this

is

too much to ask of .

Contributions. In this work we investigate a learnable permutation-invariant pooling layer for variable-size inputs inspired by the Janossy density framework (widely used in the theory of point processes (Daley & Vere-Jones, 2003, Chapter 7)). This approach, which we call Janossy pooling, directly allows the user to model what higher-order dependencies in h are relevant in the pooling.
Figure 1 summarizes a neural network with
a single pooling layer f : given an input embedding h, we apply a learnable (permutationsensitive) function f to every permutation h of the input sequence h. These outputs are added together, and fed to the second function . Examples of function f include feedfoward and recurrent neural networks. We call the op-
eration used to construct f from f the Janossy pooling. A more detail description of Janossy

x
<latexit sha1_base64="iQq7hfCeWkc81YcZLO1HLp72y/g=">AAAHEHic3VXNbhMxEHYLDSX8tXDk4lJFCqiNNhEIJFSpggsSlyLRHym7qrzeSWLFXq9sp21q7UPAEV6EG+LKG/AcvACz2VVomrY3kMDSar+dmW9n5hvLjjMprAuCHwuL164v1W4s36zfun3n7r2V1ft7Vo8Mh12upTYHMbMgRQq7TjgJB5kBpmIJ+/HwdeHfPwJjhU7fu3EGkWL9VPQEZw5N+z6Me/QkP1xZD1rBZNF50K7AOqnWzuHq0s8w0XykIHVcMmu77SBzkWfGCS4hr4cjCxnjQ9aHbnIkMpsyBTbyJ5OSc9pAf0J72uCTOjqxniV5pqwdqxgjFXMDe95XGC/ydUeu9yLyIs1GDlJeJuqNJHWaFv3TRBjgTo4RMG4Elkv5gBnGHao0k6X4t9Na2rw+k0GxIRitVeQ5SznIWbd3YnhamQooRWyYGXs7YBnYlhKWozqTfiU4nzAz7BuANPeT19qzYC2W+CsMSuGYa6VYmjzx4dsejjyOTT/3PrRYeeasOIV6WCTpWs4kbAWtduTrjUaDQtIH7CYx7LjrBiKNaLOzETymm5u0GfwGnRJ0ELxEHsWdgoJMmT0h5VYsR7BRfE5QVPG5MDhn2mx3MofcM/EGkjIcQVQluTx62n/JmX5GZVVXMOfq6lxQV17P54TEwSvU8U8JOXVdqugVPf9z2l60S1Hbv75L/ydNz0jqQ2b6SuDpEOoMDHPaFAfpsXADKZRw1lf+2UmUNHZyNQ3952mxG4BjxfBiLZPiANYyLG0YitdE+/ylMA/2Oq02zvjd0/XtV9WFsUwekkekSdrkOdkmb8gO2SWcDMkH8ol8rn2sfal9rX0rQxcXKs4DMrNq338BFk9MWg==</latexit>

Janossy pooling <latexit sha1_base64="gEHQWIkoMr756YZLGKdESr+vQmk=">AAAHIXic3VXNbhMxEHYLDSX8tSBx4eJSRQqojTYRCCRUqRIXBJci0R8pu6q83klixV6vbKd/Zl8GjvAi3BA3xGPwAsxmV6Fp2t5AAktRvp2ZzzPzjWXHmRTWBcH3ufkrVxdq1xav12/cvHX7ztLy3R2rR4bDNtdSm72YWZAihW0nnIS9zABTsYTdePiy8O8egLFCp+/ccQaRYv1U9ARnDk37S/dDB0fOv2aptvaYZlrjTv18f2k1aAXjRWdBuwKrpFpb+8sLP8NE85GC1HHJrO22g8xFnhknuIS8Ho4sZIwPWR+6yYHIbMoU2MgfjXvIaQP9Ce1pg7/U0bH1NMkzhQWqGCMVcwN71lcYz/N1R673PPIizUYOUl4m6o0kdZoWgtBEGOBOHiNg3Agsl/IBM4w7lG0qS7G3Q4FsXp/KoNgQjNYq8pylHOS02zsxPKlMBZQiNswceztgGdiWEpajOuN+JTifMDPsG4A09+O/lafBSixxKwxK4ZBrpViaPPbhmx6egTg2/dz70GLlmbPiBOphkaRrOZOwEbTaka83Gg0KSR+wm8Sww64biDSizc5a8Iiur9Nm8Bt0StBB8AJ5FI8OCjJh9oSUG7EcwVrxOUZRxefC4Jxps93JHHJPxRtIynAEUZXk4uhJ/yVn8hmVVV3CnKmrc05deT2fERIHr1DHPyXkxHWhopf0/M9pe94pRW3/+in9nzQ9JakPmekrgbdDqDMwzGlTXKSHwg2kUMJZX/mnJ1HS2NHlNPSfpcVuAI4Vw4u1TIoLWMuwtGEoPhPts4/CLNjptNo447dPVjc3qwdjkTwgD0mTtMkzsklekS2yTTh5Tz6QT+Rz7WPtS+1r7VsZOj9Xce6RqVX78Qt7u1OD</latexit>

*
f (n,
<latexit sha1_base64="Q81w7t8zrDMzVqfY/gkjq1CaXy0=">AAAHnHic3VXbbtNAEHXKJSXcWnhEoC0lkKI2siMQSAipAh6QKqQi0VIpNtF6PY5X2Yu1u24bVv4IPo/v4IFX1rEpTVv6BhKslPjszhzP7JmRJ84Z1cb3v7YWLly8dLm9eKVz9dr1GzeXlm/talkoAjtEMqn2YqyBUQE7hhoGe7kCzGMGH+PJ68r+cR+UplJ8MNMcIo7HgqaUYOOORktfbMixybRRhQn5BJQY9J/yAqWo3m0EAS9ChamGQd+Hw24HoTCL5aF9EGqiaG6afzNlECo6zkyGVS6lKPIHZdkT6zaMU5SVIxvmdGRFWb5A7kiyRE+5e6DQZGBw+cn20rVyDXVGS6t+358tdBoEDVj1mrU9Wr70LUwkKTgIQxjWehj4uYksVoYSBmUnLDTkmEzwGIbJPs21wBx0ZA9n8pWo6+wJSqVyP2HQ7PQ4yWKuq2Sd50yrk7bq8CzbsDDp88hSkRcGBKkDpQVDRqKqFiihCohhUwewU9Gli4hTDxPjKjYXpXq3kZLpsjMXgWNXJCl5ZAkWBNi82Ro6+dwcVZDRWGE1tTrDOeg+p5o4dWb3ZWBsgtVkrABEaWePlaf+Sszcq5yTgAMiOccieWzDrdS1XxyrcWntzy6gn6ETVkGGmmAGL/1+ENlOt9tFkIzB3SZR+GBoMioi1Bus+2toYwP1/F9gUIOBAy8cD7mudYIcMVPK2MuYFbBebWcoaviEKldn1AsGuXHcY/4KktrdgagJ8nvvo/vXnKNtVGd1DvNUXoMz8io75SkhXeG50/FPCXlk+q2i59z5n9P2rC512v71Lv2fND0mqQ2xGnPqvg6hzEFhI1X1IT2gJmOUU6NtY5+vRE3Dh+fTnP0kLa4Hgz0+LZph4VzdmAhODoXTYHfQD1yN3z9Z3XzVDIxF74533+t5gffM2/Teetvejke87627rYetR+177Tftrfa72nWh1XBue3OrvfsDJ9B9OA==</latexit>

hn ;

(f ) )

n
<latexit sha1_base64="rsfgRhiO5/hmq0K9Qh9nQB7sLYE=">AAAHDnic3VXdahNBFJ5WG2v8a/XSm6klEKUNu8E/kELBG8GbCqYtZJcyO3uSDJmfZWa2aTrkGfRSX8Q78dZX8Dl8AWezS2yatncKOrDst+ecb8853xlmkowzY4Pgx9LytesrtRurN+u3bt+5e29t/f6+Ubmm0KGKK32YEAOcSehYZjkcZhqISDgcJMPXhf/gGLRhSr634wxiQfqS9Rgl1ps6UcaO5NHaZtAKpgsvgrACm6hae0frKz+jVNFcgLSUE2O6YZDZ2BFtGeUwqUe5gYzQIelDNz1mmZFEgIndybTgCW54f4p7SvtHWjy1niU5IowZi8RHCmIH5ryvMF7k6+a29zJ2TGa5BUnLRL2cY6tw0T1OmQZq+dgDQjXz5WI6IJpQ6zWay1L82yrFzaQ+l0GQIWilROwokRT4vNtZNjytTAXkLNFEj50ZkAxMSzBDvTrTfjlYlxI97GsAOXHT18azYCPh/lc+SMKIKiGITJ+46G3PDzxJdH/iXGR85Zk17BTqUZGkayjhsBO0wtjVG40GhrQPvptUk1HXDpiMcbO9FTzG29u4GfwG7RK0PXjledjvEy/IjNljnO8kPIet4nOK4opPmfZzxs2wnVnPPROvIS3DPYirJJdHz/ovObPPuKzqCuZCXe0L6prUJwtC+sELr+OfEnLmulTRK3r+57S9aJd6bf/6Lv2fND0jqYuI7gvmT4dIZaCJVbo4SEfMDjgTzBpX+ecnUdLIydU07z9PS+wALCmGlyieFgew4lFp86H+mgjPXwqLYL/dCv2M3z3d3H1eXRir6CF6hJooRC/QLnqD9lAHUcTQB/QJfa59rH2pfa19K0OXlyrOAzS3at9/ARqvS4g=</latexit>

n! <latexit sha1_base64="CqSxLA7RzqOeE+U8OSS/9b1O8a4=">AAAHI3ic3VXNahRBEO5Es8b1L9GL4KVjsrBKsswsioIEgl4ELxHMD+wMoaendrfZ7umhuyfJphmfRo/6It7Eiwefwhew54c1m01yU9CGYb6pqm+q6qumO0o508bzvs/NX7m60Li2eL154+at23eWlu/uapkpCjtUcqn2I6KBswR2DDMc9lMFREQc9qLRq8K/dwhKM5m8M+MUQkEGCeszSowzHSzdDwwcG7uWrKzhFJTITOnQ+cHSqtfxyoVngV+DVVSv7YPlhZ9BLGkmIDGUE617vpea0BJlGOWQN4NMQ0roiAygFx+yVCdEgA7tcdlFjlvOH+O+VO5JDC6tp0mWCK3HInKRgpihPusrjOf5epnpPw8tS9LMQEKrRP2MYyNxIQmOmQJq+NgBQhVz5WI6JIpQ44SbylL820jJdd6cyiDICJSUIrSUJBT4tNsaNjqpTQXkLFJEja0ekhR0RzBNnTplvxyMjYkaDRRAktvytfLUW4m4+5ULSuCISiFIEj+2wZu+2wVRpAa5tYF2ladGsxNoBkWSnqaEw6bX8UPbbLVaGOIBuG5iRY56ZsiSELe7694jvLGB295v0K1A14EXjofd5nGCTJh9xvlmxDNYLz5LFNZ8ypSbM2773dQ47ql4BXEV7kBYJ7k4etJ/xZl8hlVVlzBn6uqeU1fezGeEdIMXTsc/JeTEdaGil/T8z2l73i512v71Xfo/aXpKUhsQNRDMnQ6BdCc2MVIVB+kRM0POBDPa1v7pSVQ0cnw5zfnP0iIzBEOK4UWSx8UBLHlQ2Vyouyb8s5fCLNjtdnw347dPVrde1hfGInqAHqI28tEztIVeo220gyh6jz6gT+hz42PjS+Nr41sVOj9Xc+6hqdX48QvAFlO8</latexit>

permutations

1
<latexit sha1_base64="d6CUvJJ0zqV1ykkHggo2Zv2YMs8=">AAAHDnic3VXNbtNAEN4WGkr4a+HIxaWKFFAb2RF/EqpUiQsSlyKRtlJsVev1JFll12vtrpumqzwDHOFFuCGuvALPwQswjq3QNG1vIMFKlj/PzOeZ+Wa1G2eCG+v7P5aWr11fqd1YvVm/dfvO3Xtr6/f3jco1gw5TQunDmBoQPIWO5VbAYaaByljAQTx8XfgPjkEbrtL3dpxBJGk/5T3OqEVTJ8z4UXC0tum3/OnyFkFQgU1Srb2j9ZWfYaJYLiG1TFBjuoGf2chRbTkTMKmHuYGMsiHtQzc55plJqQQTuZNpwROvgf7E6ymNT2q9qfUsyVFpzFjGGCmpHZjzvsJ4ka+b297LyPE0yy2krEzUy4VnlVd07yVcA7NijIAyzbFcjw2opsyiRnNZin9bpYSZ1OcySDoErZSMHKMpAzHvdpYPTytTAQWPNdVjZwY0A9OS3DBUZ9qvAOsSqod9DZBO3PS18czfiAX+CoNSGDElJU2TJy5828OBx7HuT5wLDVaeWcNPoR4WSbqGUQE7fiuIXL3RaHiQ9AG7STQdde2Ap5HXbG/5j73tba/p/wbtErQRvEKeh/sEBZkxe1yInVjksFV8TlFU8RnXOGevGbQzi9wz8RqSMhxBVCW5PHrWf8mZfUZlVVcwF+pqX1DXpD5ZEBIHL1HHPyXkzHWpolf0/M9pe9EuRW3/+i79nzQ9I6kLqe5LjqdDqDLQ1CpdHKQjbgeCS26Nq/zzkyhp9ORqGvrP02I7AEuL4cVKJMUBrERY2jAUr4ng/KWwCPbbrQBn/O7p5u7z6sJYJQ/JI9IkAXlBdskbskc6hBFOPpBP5HPtY+1L7WvtWxm6vFRxHpC5Vfv+C4UDS0s=</latexit>

X
<latexit sha1_base64="k6uzAXhrGnbUz9nsdUn8NmN+AAQ=">AAAHDXic3VXdbtMwFPYGK6P8bXDJjcdUqaCtSioQSGjSBDdI3AyJ/UhNNDnOaWPVjiPb2dZZeQW4hBfhDnHLM/AcvABOE5W13XoHEliK8uWc8+Wc8x3LjjLOtPG8H0vL166vNG6s3mzeun3n7r219fsHWuaKwj6VXKqjiGjgLIV9wwyHo0wBERGHw2j4uvQfnoDSTKbvzSiDUJBByvqMElOaAp2L47VNr+ONF54Hfg02Ub32jtdXfgaxpLmA1FBOtO75XmZCS5RhlEPRDHINGaFDMoBefMIynRIBOrRn43oL3HL+GPelck9q8Nh6kWSJ0HokIhcpiEn0rK80Xubr5ab/IrQszXIDKa0S9XOOjcRl8zhmCqjhIwcIVcyVi2lCFKHGSTSVpfy3kZLrojmVQZAhKClFaClJKfBptzVseF6bSshZpIgaWZ2QDHRHME2dOuN+ORgbEzUcKIC0sOPXxjNvI+LuVy4ohVMqhSBp/MQGb/tu3lGkBoW1gXaVZ0azc2gGZZKepoTDjtfxQ9tstVoY4gG4bmJFTnsmYWmI290t7zHe3sZt7zfoVqDrwEvHw26bOEEmzD7jfCfiOWyVn2MU1nzKlJszbvvdzDjuhXgFcRXuQFgnuTp60n/FmXyGVVULmHN1dS+pq2gWc0K6wQun458ScuK6UtEFPf9z2l62S522f32X/k+aXpDUBkQNBHOnQyAzUMRIVR6kp8wknAlmtK3905OoaORsMc35Z2mRScCQcniR5HF5AEseVDYX6q4Jf/ZSmAcH3Y7vZvzu6ebuq/rCWEUP0SPURj56jnbRG7SH9hFFCfqAPqHPjY+NL42vjW9V6PJSzXmAplbj+y/YCks5</latexit>

...

y(x)
<latexit sha1_base64="Ll1vtmM4aAwXthixjBRO27YNDOc=">AAAHLHic3VXNbhMxEHYLDSX8tXDk4lJVSlGJNhEIJFSpggsSlyLRHym7qrzeSWLFXq9sp21q7ZmngSO8CBeEuPICvACz2VXaNG1vIIGl1X47M9/OzDeWHWdSWBcE3+bmr11fqN1YvFm/dfvO3XtLy/d3rR4aDjtcS232Y2ZBihR2nHAS9jMDTMUS9uLB68K/dwjGCp2+d6MMIsV6qegKzhyaDpZWQo3ugu1P0SjPGz6Mu/Q4X68fLK0GzWC86CxoVWCVVGv7YHnhV5hoPlSQOi6ZtZ1WkLnIM+MEl5DXw6GFjPEB60EnORSZTZkCG/njcTs5XUN/Qrva4JM6OraeJXmmrB2pGCMVc3173lcYL/J1hq77IvIizYYOUl4m6g4ldZoW2tBEGOBOjhAwbgSWS3mfGcYdKjiVpfi301ravD6VQbEBGK1V5DlLOchpt3dicFKZCihFbJgZedtnGdimEpajOuN+JTifMDPoGYA09+PXyrNgJZb4KwxK4YhrpViaPPbh2y5uhzg2vdz70GLlmbPiBOphkaRjOZOwGTRbka+vra1RSHqA3SSGHXVcX6QRbbQ3gnX65AltBKegXYI2gpfIo7g5UJAJsyuk3IzlEDaKzzGKKj4XBudMG6125pB7Jt5AUoYjiKokl0dP+i85k8+orOoK5kxd7Qvqyuv5jJA4eIU6/ikhJ65LFb2i539O24t2KWr713fp/6TpGUl9yExPCTwdQp2BYU6b4iA9Eq4vhRLO+so/PYmSxo6vpqH/PC12fXCsGF6sZVIcwFqGpQ1D8Zponb8UZsFuu9nCGb97urr1qrowFslD8og0SIs8J1vkDdkmO4STD+Qj+Uy+1D7Vvta+136UofNzFecBmVq1n78BkLJX0w==</latexit>

... ...
...

input <latexit sha1_base64="28/KLmXyApINSxilaaz2CyjAyvs=">AAAHFXic3VXNbtNAEN4WGkr4a+HIxaWKFFAb2RF/EqpUiQsSlyLRHym2qvV6kqyy67V2x23TlV8DjvAi3BBXzjwHL8A6tkLTtL2BBCtZ/jwzn2fmm9VunAlu0Pd/LCxeu77UuLF8s3nr9p2791ZW7+8ZlWsGu0wJpQ9iakDwFHaRo4CDTAOVsYD9ePS69O8fgTZcpe9xnEEk6SDlfc4oOlMYIpyg5WmWY3G4su53/Mny5kFQg3VSr53D1aWfYaJYLiFFJqgxvcDPMLJUI2cCimaYG8goG9EB9JIjnpmUSjCRPZnUXXgt50+8vtLuSdGbWM+SLJXGjGXsIiXFoTnvK40X+Xo59l9GVVuQsipRPxceKq8UwUu4BoZi7ABlmrtyPTakmjJ0Us1kKf+NSglTNGcySDoCrZSMLKMpAzHrtshHp7WphILHmuqxNUOagelIbphTZ9KvALQJ1aOBBkgLO3mtPfPXYuF+5YJSOGZKSpomT2z4tu/mHsd6UFgbGld5hoafQjMsk/QMowK2/E4Q2War1fIgGYDrJtH0uIdDnkZeu7vhP/Y2N722/xt0K9B14JXjeW67OEGmzD4XYisWOWyUnxMU1XzGtZuz1w66GTrumXgNSRXuQFQnuTx62n/FmX5GVVVXMOfq6l5QV9Es5oR0g5dOxz8l5NR1qaJX9PzPaXvRLnXa/vVd+j9pekZSG1I9kNydDqHKQFNUujxIjzkOBZccja39s5OoaPTkaprzn6fFOASk5fBiJZLyAFYirGwu1F0TwflLYR7sdTuBm/G7p+vbz+sLY5k8JI9ImwTkBdkmb8gO2SWMZOQD+UQ+Nz42vjS+Nr5VoYsLNecBmVmN778AJPdPFQ==</latexit> h

h(x; (h)

)

<latexit sha1_base64="XZKp7XTCFut/QASC0GIT4rSignE=">AAAHQHic3VXNbtQwEHYLXcry18KRi0u10ha1q+yKCqSqUgUXJC5Foj/SJlSOM7ux1o6D7bTdWnkSngaO8AS8ATeEuHHC2UTLbrftDSSwFOXLzHyZmc+WJ0w508bzvszNX7u+ULuxeLN+6/adu/eWlu/va5kpCntUcqkOQ6KBswT2DDMcDlMFRIQcDsLBi8J/cAxKM5m8McMUAkH6CesxSowzHS1tWj/s4TjHPrzL2DGOmyPDab6FHZA80kPhXtg3MRiSv7XNeC1fO1pa9VreaOFZ0K7AKqrW7tHywg8/kjQTkBjKidbdtpeawBJlGOWQ1/1MQ0rogPShGx2zVCdEgA7s6ajFHDecP8I9qdyTGDyyTpIsEboo1UUKYmJ93lcYL/J1M9N7FliWpJmBhJaJehnHRuJCLxwxBdTwoQOEKubKxTQmilDjVJ3KUvzbSMl1Xp/KIMgAlJQisJQkFPi02xo2OKtMBeQsVEQNrY5JCrolmKZOnVG/HIyNiBr0FUCS29FrZdNbCbn7lQtK4IRKIUgSPbb+q547ImGo+rm1vnaVp0azM6j7RZKupoTDttdqB7beaDQwRH1w3USKnHRNzJIANzvr3hre2MBN7zfolKDjwJbjYXeynCBjZo9xvh3yDNaLzxEKKj5lyu0zbrY7qXHciXgFURnuQFAluTx63H/JGX8GZVVXMGfq6lxQV17PZ4R0Gy+cjn9KyLHrUkWv6Pmf0/aiU+q0/eun9H/SdEJS6xPVF8zdDr5MQREjVXGRnjATcyaY0bbyT+9ESSOnV9Oc/zwtLMeCnZwV1ahwoW5MtM8PhVmw32m13R6/frK687waGIvoIXqEmqiNnqId9BLtoj1E0Xv0AX1Cn2sfa19r32rfy9D5uYrzAE2t2s9fiS5fAQ==</latexit>

Figure 1: A

; (f)

*
f

(n,

h



1

) f(n, h ;
<latexit sha1_base64="dfZjaIgTBrZNfmIF3GEO5IW35MY=">AAAHUXic3VVdb9MwFPUGW0fHxwaPvHhMlTq0VUkFAjFNmkBCSLwMiX1ITZgcx2mt2nFku9s6K/+IXwNPCP4FT7xx00Rdu257AwksJTn2vSf33mPLN8oEN9bzvs3N37q9sFhbulNfvnvv/oOV1YcHRg00ZftUCaWPImKY4Cnbt9wKdpRpRmQk2GHUf1PYD0+YNlylH+0wY6Ek3ZQnnBILS8crbwMF5oLtLlCS58100wVRgns5Dl7h7eIFcyViM5TwwYHtMUvyT66ZbOQb9eOVda/ljQaeBX4F1lE19o5XF34GsaIDyVJLBTGm43uZDR3RllPB8nowMCwjtE+6rBOf8MykRDITurNRzTlugD3GidLwpBaPVidJjkhT5AqektieuWwrFq+ydQY2eRk6nmYDy1JaBkoGAluFCwFxzDWjVgwBEKo5pItpj2hCLcg8FaX4t1VKmLw+FUGSPtNKydBRklImps3O8v55tVRAwSNN9NCZHsmYaUluKKgzqlcw62Ki+13NWJq70WftubcWCfgVOKXslCopSRo/dcH7BM5MFOlu7lxgIPPMGn7O6kERpGMoEWzHa/mhqzcaDcziLoNqYk1OO7bH0xA325veBt7awk3vArRL0AawDTwMJwgEGTMTLsROJAZss5iOUFjxKdewz7jptzML3Al/zeLSHUBYBbnee1x/yRlPwzKrG5gzebWvyCuv5zNCwsZL0PFPCTk2XavoDTX/c9pedUpB279+Sv8nTSckdQHRXcnhdghUxjSxShcX6Sm3PcElt8ZV9umdKGnk7GYa2C/TorIvuMlmUfUKcIU24V9uCrPgoN3yYY8/PFvffV01jCX0GD1BTeSjF2gXvUN7aB9R9Bl9Qd/Rj8Wvi79qqDZfus7PVZxHaGrUln8DhApi0A==</latexit>

<latexit sha1_base64="vGXC2trvkMadf8pCYprVKJwqDmE=">AAAHmnic3VVdb9MwFE0Ho6N8bfA4Hjy2Sh3aqqRiAmmaNI0X0F6GtI1JTagc56axaseR7WzrrPwGfh+/A/GO04Sy7usNJLDU5tj3ntzrc69yw4xRpV33W2Pu3v35B82Fh61Hj588fba49PxYiVwSOCKCCXkSYgWMpnCkqWZwkknAPGTwORy9L+2fT0EqKtJDPc4g4HiY0pgSrO3RYPGr8TnWidIy1z4fgUx73S2eoxhVu03P47kvMVXQ67pw3m4h5CehODdrviKSZrr+12MGvqTDRCdYZkKkebZWFJ10w/hhjJJiYPyMDrxiG9kDwSI15vaBfJ2AxsUX04nXi3XUGiyuul13stB14NVg1anXwWBp/rsfCZJzSDVhWKm+52Y6MFhqShgULT9XkGEywkPoR6c0UynmoAJzPhGvQG1rj1AspP2lGk1OL5MM5qpM1npOlLpqKw9vsvVzHb8LDE2zXENKqkBxzpAWqKwEiqgEotnYAmw1tOkiYrXDRNt6zUQp362FYKpozUTg2JZICB4YglMCbNZsNB1d1EclZDSUWI6NSnAGqsupIladyX0ZaBNhORpKgLQwk8fKlrsSMvsq65TCGRGc4zR6bfz92DZfGMphYcyvHqAX0PLLIH1FMIMdt+sFptVutxFEQ7C3iSQ+6+uEpgHq9DbcdbS5iTrub9CrQM+CbctDtmetIFNmTBnbCVkOG+V2goKaT6i0dUYdr5dpy73kLyGq3C0I6iC3e0/vX3Gm26DK6g7mtbx6N+RVtIprQtrCc6vjnxJyarpV0Tvu/M9pe1OXWm3/epf+T5pektT4WA45tV8HX2QgsRay/JCeUZ0wyqlWprbPVqKi4fO7adZ+lRZWg8Fcnhb1sLCudkx4V4fCdXDc63q2xp/erO7u1QNjwVl2Xjkdx3PeOrvOB+fAOXKI86Ox3FhrtJsvm3vNj839ynWuUXNeODOrefgTT+977w==</latexit>

(f) ) (f ; () ) <latexit sha1_base64="tQ2F39SBCSDEkm6vkwI+uACKg6M=">AAAHRXic3VVNbxMxEHULDSV8tXDksqWKlKI22kQgEFWlCi5IXIpEP6TsUnm9k8SKvV7ZTtvU2v/Cr4EjXPkRnEBcYTa7Spsm7Q0ksLTaZ888z8yz5YlSwY31/a9z89euL1RuLN6s3rp95+69peX7e0YNNINdpoTSBxE1IHgCu5ZbAQepBiojAftR/1Vu3z8CbbhK3tlhCqGk3YR3OKMWlw6XXgS6p+qBQp98C3eGOlm26YJIidgMJf4C2wNLs/eunlPWsrXDpVW/4Y+GNw2aJVgl5dg5XF74HsSKDSQklglqTLvppzZ0VFvOBGTVYGAgpaxPu9COj3hqEirBhO5kVGfm1dAeex2l8UusN1o9T3JUmjxZ9JTU9sxFW744y9Ye2M7z0PEkHVhIWBGoMxCeVV4umhdzDcyKIQLKNMd0PdajmjKL0k5Eyfe2SgmTVSciSNoHrZQMHaMJAzFpdpb3T8ulHAoeaaqHzvRoCqYhuWGozqheAdbFVPe7GiDJ3Oi38tRfiQRuhU4JHDMlJU3ixy5408F7EkW6mzkXGMw8tYafQjXIg7QNowK2/EYzdNVareZB3AWsJtb0uG17PAm9emvdX/M2Nry6fwZaBWgh2ESehxcGBRkzO1yIrUgMYD2fjlBY8hnXeM5evdlKLXLP+WuIC3cEYRnkcu9x/QVnPA2LrK5gTuXVmpFXVs2mhMSDl6jjnxJybLpU0Stq/ue0nXVLUdu/fkv/J03PSeoCqruS4+sQqBQ0tUrnD+kxtz3BJbfGlfbJkyho9ORqGtov0qKiM7gZ3QJdsU00LzaFabDXajTxjN8+Wd1+WTaMRfKQPCJ10iTPyDZ5TXbILmHkA/lIPpMvlU+Vb5UflZ+F6/xcyXlAJkbl129HFGMl</latexit>

output <latexit sha1_base64="ch4B7SqeCc6ES90oZ5sMu0rViZs=">AAAHFnic3VXNbtNAEN4WGkr4a+HIxaWKFFCJ7Ig/CVWqxAWJS5HojxRb1Xo9SVbZ9Zrdddt05eeAI7wIN8SVK8/BCzCOrdA0bW8gwUqWP8/M55n5ZrUbZ4Ib6/s/FhavXF1qXFu+3rxx89btOyurd3eNyjWDHaaE0vsxNSB4CjuWWwH7mQYqYwF78ehV6d87BG24St/ZcQaRpIOU9zmjFk1RaOHYOpXbLLfFwcq63/Eny5sHQQ3WSb22D1aXfoaJYrmE1DJBjekFfmYjR7XlTEDRDHMDGWUjOoBecsgzk1IJJnLHk8ILr4X+xOsrjU9qvYn1NMlRacxYxhgpqR2as77SeJ6vl9v+i8jxFNuClFWJ+rnwrPJKFbyEa2BWjBFQpjmW67Eh1ZRZ1GomS/lvq5QwRXMmg6Qj0ErJyDGaMhCzbmf56KQ2lVDwWFM9dmZIMzAdyQ1DdSb9CrAuoXo00ABp4Savtaf+WizwVxiUwhFTUtI0eeTCN30cfBzrQeFcaLDyzBp+As2wTNIzjArY9DtB5JqtVsuDZADYTaLpUc8OeRp57e6G/9B7/Nhr+79BtwJdBC+R5+F+QUGmzD4XYjMWOWyUnxMU1XzGNc7ZawfdzCL3VLyGpApHENVJLo6e9l9xpp9RVdUlzLm6uufUVTSLOSFx8BJ1/FNCTl0XKnpJz/+ctuftUtT2r+/S/0nTU5K6kOqB5Hg6hCoDTa3S5UF6xO1QcMmtcbV/dhIVjR5fTkP/WVpsh2BpObxYiaQ8gJUIKxuG4jURnL0U5sFutxPgjN8+Wd96Vl8Yy+Q+eUDaJCDPyRZ5TbbJDmHkPflAPpHPjY+NL42vjW9V6OJCzblHZlbj+y+ucE+g</latexit>

neural network with a single Janossy

pooling layer. The embedding h is permuted in all |h|! possible ways, and for each permuta-

tion h, f (|h|, h; (f)) is computed. These results are added together, and the result is passed to a second function  parameterized by (),
which gives the final permutation-invariant output y(x; (h), (f), ()). We discuss how this scheme can be made computationally tractable.

2

Under review as a conference paper at ICLR 2019

pooling is given in Definition 2.1. Further, we contribute the following analysis:
(a) We show deep sets (Zaheer et al., 2017) is a special case of Janossy pooling if the function f depends only on the first element of the vector h. In the most general form of Janossy pooling (as described above), f depends on its entire input sequence h. This naturally raises the possibility of intermediate choices of f that allow practitioners to trade between flexibility and tractability. We will show that functions f that depend on their first k arguments of h allow the Janossy pooling layer to capture up to k-ary dependencies in h.
(b) We show Janossy pooling can be used to learn permutation-invariant neural networks y(x) by sampling a random permutation during training, and then modeling this permutation using a sequence model such as a recurrent neural network (LSTMs (Hochreiter & Schmidhuber, 1997), GRUs (Cho et al., 2014)) or a vector model such as a feedforward network. We call this permutation-sampling learning algorithm -SGD (-Stochastic Gradient Descent). Our analysis explains why this seemly unsound procedure is theoretically justified, which sheds light on the recent puzzling success of permutation sampling and LSTMs in relational models Moore & Neville (2017); Hamilton et al. (2017). We show that this property relates to what we randomized model ensemble techniques.
(c) In Zaheer et al. (2017), the authors describe a connection between deep sets and infinite de Finetti exchangeabilty. We provide a probabilistic connection between Janossy pooling and finite de Finetti exchangeabilty (Diaconis, 1977).

2 JANOSSY POOLING

In this section we formalize the Janossy pooling function f . We start with a function f , parameterized by (f), which can take any variable-size sequence as input: a sequence of matrices (such as a sequence of images), a sequence of vectors (such as a sequence of vector embeddings), or a variablesize sequence of features or embeddings representing the neighbors of a node in an attributed graph.
In practice, we implement f with a neural network. Formalizing Figure 1 from Section 1, we use
f to define f :

Definition 2.1: [Janossy pooling] Consider a function f : N ×

 j=0

Hj

×

Rb



Y

on

variable-

length but finite sequences h, parameterized by (f)  Rb, b > 0. A permutation-invariant function

f:

 j=0

Hj



Y

is

the

Janossy

function

associated

with

f

if

f (h; (f))

=

1 |h|!

f (|h|, h; (f)),

|h|

(3)

where |h| is the set of all permutations of the integers 1 to |h|, and h represents a particular reordering of the elements of sequence h according to   |h|. We denote Janossy pooling the

operation used to construct f from f .



Definition 2.1 provides a conceptually simple approach to constructing permutation-invariant func-

tions from arbitrary sequence-functions. If f is a vector-valued function, then so is f , and in prac-

tice, one might pass this vector output of f through a second function  (e.g. a neural network

parameterized by ()):





y(h;

(f

),

(p))

=





1 |h|!

f (|h|, h; (f)); (p) .

|h|

(4)

Theoretically, equation 3 can capture any permutation-invariant function for a flexible-enough fam-
ily of functions for f , thus, at least theoretically,  in equation 4 provides no additional representational power. In practice,  can improve learnability by capturing common aspects across all terms in the summation. Furthermore, when we look at approximations to equation 3, adding  can help recover some of the lost model capacity. Overall then, equation 4 represents one layer of Janossy
pooling, forming a constituent part of a bigger neural network. Figure 1 summarizes this.

3

Under review as a conference paper at ICLR 2019

Janossy pooling, as defined in equation 3 and 4 is intractable; the computational cost of summing over all permutations (for prediction), and backpropagating gradients (for learning) is likely prohibitive for most problems of interest. Nevertheless, it provides an overarching framework to unify existing methods, and to extend them. In what follows we present strategies for mitigating this computational cost.

2.1 TRACTABILITY THROUGH CANONICAL INPUT ORDERINGS

A simple way to achieve permutation-invariance without the summation in equation 3 is to order the elements of h according to some canonical ordering based on their values, and then feed the reordered sequence to f . This way, all the permutations of h are mapped to the same canonical
sequence. Effectively, this imposes the following structural constraint on f : f (h) equals 0 for all
sequences that do not follow the canonical permutation. This allows us to use more complex f models, such as LSTMs and GRUs, that can capture arbitrary relationships in the canonical ordering of h.

Examples of the canonical ordering approach already exist in the literature. We first describe Niepert

et al. (2016), which orders nodes in a graph according to a user-specified ranking such as between-

ness centrality (say from high to low). This approach is useful only if the canonical ordering is

relevant to the task at hand. Niepert et al. acknowledges this shortcoming by showing an example

where a poor vertex traversal order significantly degrades classification accuracy. As an idealized

enxeanmts pxlei,,1caonndsixdei,r2 insapmutpsleedquienndceepsenhde=ntly(hoif,1e, ahcih,2 )othin=e1r.,

with (hi,1, hi,2)  H = R2, and compoChoosing a canonical ordering to sort h

according to h·,1 when the task at hand depends on a canonical ordering based on h·,2 can lead to

poor prediction accuracy.

Rather than pre-defining a good canonical order, one can try to learn it from the data. This requires searching over the discrete space of all |h|! permutations of the input vector h. In practice, this discrete optimization relies on heuristics (Vinyals et al., 2016; Rezatofighi et al., 2018). Alternatively, instead of choosing a single canonical ordering, one can choose multiple orderings, resulting in ensemble methods that average across multiple permutations. These can be viewed as more refined (possibly data-driven) approximations to equation 3.

2.2 TRACTABILITY THROUGH k-ARY DEPENDENCIES

Here, we provide a different spectrum of options to trade-off flexibility, complexity, and generalizability in Janossy pooling. Now, to simplify the sum over permutations in equation 3, we impose
structural constraints where f (h) depends only on the first k elements of its input sequence. This amounts to the assumption that only k-ary dependencies in h are relevant to the task at hand.

Definition 2.2: [k-ary Janossy pooling] Fix k  N. For any sequence h, define k (h) as its projection to a length k sequence; in particular, if |h|  k, we keep the first k elements, while if |h| < k, we append (k - |h|) zeros at the end of h to make it at least of size k. Then, a k-ary
permutation-invariant Janossy function f is given by

f (|h|, h; (f))

=

1 |h|!

f (k, k(h); (f)).

|h|

(5)



Proposition 2.1 shows that if |h| > k, equation 5 only needs to sum over |h|!/(|h| - k)! terms.

Proposition 2.1.

The Janossy pooling in equation 5 requires summing over only

n! (n-k)!

terms, thus

saving computation when k < |h|.

Proof. Without loss of generality assume |h|  k, we can pad h with zeros if needed. Define two permutations ,   |h| that agree on the first k elements as k-equivalent. Such permutations
satisfy f (k, k (h); (f)) = f (k, k (h ); (f)). These two permutations belong to the same equivalence class, containing a total of (|h| - k)! permutations (obtained by permuting the last

4

Under review as a conference paper at ICLR 2019

(|h| - k) elements). Overall, we then have a total of |h|!/(|h| - k)! equivalence classes. Write the set of equivalence classes as k|h|, and represent each by one of its elements. Then,

f k(h; (f))

=

1 |h|!

f (k, k(h); (f))

|h|

=

(|h| - k)! |h|!

f k, k(h)); (f)

|kh|

(6)

is now a summation over only |h|!/(|h| - k)! terms.

The sum in equation 6 can be written equivalently as fi1,i2,...,ikI|h| k, hi1,i2...,ik ; (f) , where In is the set of all permutations of {1, 2, . . . , n} taken k at a time. Note that the value of k balances

computational savings and model flexibility; it can be selected as a hyperparameter based on a-priori

beliefs or through typical hyperparameter tuning strategies.

Remark 2.1 (Deep sets (Zaheer et al., 2017) is a 1-ary (unary) Janossy pooling). Equation 6 repre-

sented with k = 1 yields the model 

|h| i=1

f (hi; (f)); ()

.

Noting that

f

only takes a

single

input, we can redefine it and recover equation 2.

Not surprisingly, the computational savings obtained from k-ary Janossy pooling come at the cost
of reduced model flexibility. The next result formalizes this. Theorem 2.1. For any k  N, define Fk as the set of all permutation invariant functions that can be represented by Janossy pooling with k-ary dependencies. Then, Fk-1 is a proper subset of Fk if the space H is not trivial (i.e. if the cardinality of H is greater than 1). Thus, Janossy pooling with k-ary dependencies can express any Janossy pooling function with (k - 1)-ary dependencies, but
the converse does not hold.

Proof.

(Fk-1  Fk): Consider any element f k-1  Fk-1. For any order-dependent function f , define f k(h) = f (k(h)). Then, by definition, there exists a function f such that

f k-1(h; (f))

=

1 |h|!

f (k-1(h); (f))

=

1 |h|!

f k-1(h; (f))

|h|

|h|

(7)

=

1 |h|!

f k-1(k(h); (f))

|h|

(8)

(Fk  Fk-1): Here, we need to construct a function f  Fk that is not present in Fk-1. The construction is given in the Supplementary Material.

Corollary 2.1. For k > 1, the deep sets function in equation 1 (Zaheer et al., 2017) pushes the modeling of k-ary relationships to .

Proof. Deep sets functions can be expressed via Janossy pooling with k = 1. Thus, by Theorem 2.1,
f in equation 2 cannot express all functions that can be expressed by higher-order (i.e. k > 1) Janossy pooling operations. Consequently, if the deep sets function can express a premutationinvariant function, the expressive power must have been pushed to .

2.3 TRACTABILITY THROUGH PERMUTATION SAMPLING
Another approach to tractable Janossy pooling uses random permutations of the input h during training. Like the approach in Section 2.1, this offers significant computational savings, allowing more complex models for f such as LSTMs and GRUs. Unlike that earlier approach, this is considerable more flexible, avoiding the need to learn a canonical-ordering or making assumptions about the dependencies between the elements of h and the objective function. The approach of sampling random permutations has been previously used in relational learning tasks (Moore & Neville, 2017; Hamilton et al., 2017) as a heuristic with an LSTM as f . Both these papers report that permutation sampling outperforms or closely marches all other tested neural network models they tried. We provide a theoretical framework to understand and extend such approaches.

5

Under review as a conference paper at ICLR 2019

For simplicity, we will focus on a single Janossy pooling layer and a single sampled permu-
tation, although generalizing these conditions is easy. We assume a supervised learning set-
ting, though our analysis easily extends to unsupervised learning. We are given training data D  {(x1, y1), . . . , (xN , yN )}, where yi  Y is the target output and xi its corresponding input. Our original goal was to minimize the empirical loss

L(D; (), (f), (h))

=

1 N

N
L

yi,  f (h(i); (f)); ()

i=1

,

(9)

where

f (h(i); (f))

=

1 |h(i)|!

f (|h(i)|, h(i); (f))

 |h(i) |

(10)

and h(i) = h(xi; (h))  j=0Hj. Computing the gradient of equation 9 is intractable for large inputs h(i), as the backpropagation computation graph branches out for every permutation in the
sum. To address this computational challenge, we will turn out attention to stochastic optimization.

Permutation sampling. Consider replacing the Janossy sum in equation 10 with the estimate
f^(h; (f)) = f (|h|, hs; (f)),
where s is a random permutation sampled uniformly, s  Unif(|h|). The estimator in equation 2.3 is unbiased: Es[f^(h; (f))] = f (h; (f)). Note however that when f is chained with another nonlinear function  and/or nonlinear loss L, the composition is no longer unbiased: Es[L(y, ( f (|hs|, hs; (f)); ()))] = L(y, (Es[ f (|hs|, hs; (f))]; ())). Using this estimate we propose the following stochastic approximation algorithm for gradient descent:

Definition 2.3: [-SGD] Let B = {(x1, y1), . . . , (xB, yB)} be a mini-batch i.i.d. sampled uniformly from the training data D. At step t, consider the stochastic gradient descent update

t = t-1 - Zt,

(11)

where Zt is the random gradient

Zt

=

1 B

B
 L

yi, 

f (|h(i)|, hs(ii); t(f)); t()

i=1

where h(i) = h(xi; t(h)),  = ((), (f), (h)), Uniform(|h(i)|); the learning rate is t  (0, 1) s.t.

with the

 t=1

t

random permutations

=  and

 t=1

t2

<

{si}Bi=1 .

,

si

 

Effectively, this is a Robbins-Monro stochastic approximation algorithm of gradient descent (Robbins & Monro, 1951; Bottou & Cun, 2004) and optimizes the following modified objective function:

J (D; (), (f), (h))

=

1 N

N

Esi

L

yi, 

f (|h(i)|, hs(ii); (f)); ()

i=1

=

1 N

N

1 |h(i)|!

(12) L yi,  f (|h(i)|, h(i); (f)); () ,

i=1

 |h(i) |

where h(i)  (h(xi; (h))) for a permutation . Observe that the expectation over permutations is now outside the L and  functions. Like equation 9, the loss in equation 12 is also permutation

invariant. In general though, the optima of J are different from those of the original objective
function L. In the setting where  is the identity function (equation 3), and L is convex, the function
J is an upper bound to L (via Jensen's inequality), and can be viewed as a tractable surrogate to the original Janossy objective. While this property no longer holds for general , our results show that
this holds for a composition f    f and still exhibits competitive results. In general, increasing the number of sampled permutations decreases bias and variance, and we recover the exact algorithm when all |h|! permutations are sampled. The next result provides some insight into the covergence properties of our algorithm.

6

Under review as a conference paper at ICLR 2019

Proposition 2.2 (-SGD Convergence). Consider the -SGD algorithm in Definition 2.3. If (a) there exists a constant G such that for all , GtT   G  -  22, where Gt is the true
gradient for the full batch over all permutations, Gt = J (D; t(), t(f), t(h)), where   ((), (f), (h)). (b) there exists a constant  > 0 such that for all , Et[ Zt 22]  2(1 +  -  22), where the expectation is taken with respect to all the data prior to step t. Then, the algorithm in equation 11 converges to  with probability one.

Proof. First, we can show that E[Zt] = Gt by equation 12, the linearity of the derivative operator,

and the fact that the permutations are independently sampled for each training example in the mini-

batch and are assumed independent of . That equation 11 converges to  is a consequence of our

conditions and the supermartingale convergence theorem (Grimmett & Stirzaker, 2001, pp. 481).

The following argument follows Yuille (2004). If At - t -  22(2t2 - G2t), noting that Ct is positive

= for

1 2

t - 

22, Bt

a sufficiently large

t=, an12d2t2t,=a1nBdt

Ct 

= 

by our definition of t. The inequality E[At]  At-1 + Bt-1 - Ct-1, for all t, is a consequence

of E

t - 

2 2

=E

t-1 - t-1Zt-1 - 

2 2

=E

t-1 - 

2 2

- 2t-1E[(t-1 -



)T Zt-1] + t2-1E[

Zt2-1

2 2

]



E

t-1 - 

2 2

- 2t-1(t-1 -  )T Gt-1 + t2-1G2 

E with

t-1 -  probability

22on-e a2ndt-1Gt=1Ct-t 1<- .

2 2

-

t-1(

)T Gt + t2-1G2. Then, At converges to zero

There are also ways to cope with the difference in objective functions between equation 9 and equation 12. An interesting approach adds an output regularization penalty for two distinct sampled permutations si and si, f (hsi (x(i); (h)); (f)) - f (hsi (x(i); (h)); (f)) 22, so as to reduce the variance of the sampled Janossy pooling output. Such a variance-reduction procedure allows Es[L(yi, f (|hs|, hs; (f))]  L(yi, Es[ f (|hs|, hs; (f))]), inducing a near-equivalence between optimizing equation 9 and equation 12, assuming  is the identity or  is combined with f to form
a single function f    f . This type of output-regularization-for-variance-reduction approach has been successfully used before to improve test performance of the average Dropout masks on LSTMs (Zolna et al., 2018).

Inference. The use of -SGD to optimize the Janossy pooling layer optimizes the objective J, and thus has the following implication on how outputs should be calculated at inference time:
Remark 2.2 (Inference). Assume L is convex (e.g., L is the L2, cross entropy, or negative loglikelihood losses), which via Jensen's inequality makes -SGD a proper surrogate to the original Janossy objective. At test time we estimate the output yi of input xi by computing (or estimating)

y^i(xi) = Esi f (|h(i) |, h(sii) ; (f ) ) ,

(13)

where f    f , (f )  ((f) , () ), hs(ii)  (h(xi; (h) ))si and () , (f) , (h) are fixed points of the -SGD optimization. Equation 13 is a permutation-invariant function.

2.4 PROBABILISTIC INTERPRETATION
Our work has a strong connection with finite exchangeability. Some researchers may be more familiar with the concept of infinite exchangeability through de Finetti's theorem (De Finetti, 1937; Diaconis, 1977), which imposes strong structural requirements: the probability of any subsequence must equal the marginalized probability of the original sequence. Finite exchangeability drops this projectivity requirement (Diaconis, 1977), which in general, cannot be simplified beyond first sampling the number of observations m, and then sampling their locations from some exchangeable but non-i.i.d. distribution pmxchgx(x1, . . . , xm) (Daley & Vere-Jones, 2003).
Finite exchangeability also arises from the theory of spatial point processes; our framework of Janossy pooling is inspired by Janossy densities (Daley & Vere-Jones, 2003), which model the finite exchangeable distributions as mixtures of non-exchangeable distributions applied to permutations. This literature also studies simplied exchangeable point processes such as finite Gibbs models (Vo et al., 2018; Moller & Waagepetersen, 2003) that restrict the structure of pxchg to fixed-order dependencies, and are related to k-ary Janossy.

7

Under review as a conference paper at ICLR 2019
3 EXPERIMENTS
In what follows we empirically evaluate two tractable Janossy pooling approaches, k-ary dependencies (section 2.2) and sampling permutations for stochastic optimization (section 2.3), to learn permutation-invariant functions for tasks of different complexities. One baseline we compare against is deep sets (Zaheer et al., 2017); recall that this corresponds to unary (k = 1) Janossy pooling (Remark 2.1). Corollary 2.1 shows that explicitly modeling higher-order correlations simplifies the task of the upper layers of the neural network, and we evaluate the gains of increasing k = 1, 2, 3, |h| over different tasks. We also evaluate Janossy pooling in graph tasks, where it can be used as a permutation-invariant function to aggregate the features and embeddings of the neighbors of a node in the graph. Note that in graph tasks, permutation-invariance is required to ensure that the neural network is invariant to permutations in the adjacency matrix (graph isomorphism).
3.1 ARITHMETIC TASKS ON SEQUENCES OF INTEGERS
We first consider the task of predicting the sum of a sequence of integers (Zaheer et al., 2017), and extend it to consider other harder tasks: range, unique sum, and unique count. In the sum task we predict the sum of a sequence of 5 integers drawn uniformly with replacement from {0, 1, . . . , 99}; the range task also receives a sequence 5 integers distributed the same way, and tries to predict the range (the difference between the maximum and minimum values); the unique sum task receives a sequence of 10 integers, sampled uniformly with replacement from {0, 1, . . . , 9}, and predicts the sum of all unique elements; the unique count task receives a sequence of repeating elements, sampled uniformly with replacement from {0, 1, . . . , 9}, and predicts the number of unique elements. Unlike Zaheer et al. (2017), we work with the digits themselves, rather than with MNIST images. This allows a more direct assessment of the different Janossy pooling approximations, and it is a simple matter to pass digit images through another neural network layer before applying our algorithms. Note that sum task maps directly to the architecture of deeps sets (which computes embeddings of the individual digits and adds them together). However the other tasks we selected involved calculating permutation-invariant quantities that require reasoning about high-order relationships within the sequence.
We explore several Janossy pooling variants. First are Deep Sets, or Janossy (k = 1) and Janossy (k = 2, 3) where f is a feedforward network with a single hidden layer comprised of 30 neurons. Second, full (k = |h|) Janossy pooling where f is an LSTM or a GRU (50 and 80 units in the hidden state, respectively) and the LSTM or GRU hidden state of the last temporal unit is the output of f (the ct of Cho et al. with t = |h|). The full Janossy pooling trained with -SGD is denoted JanossyM inf-LSTM and Janossy-M inf-GRU, where the value of M is the number of sampled permutations used to estimate y^i(xi) of equation 13 at test time for a test sequence xi. To define , we followed the experimental setup of Zaheer et al. (2017), which uses a single dense layer with identity activation. A consequence of this choice is that it provides direct insight into the capacity of f to exploit kwise relationships. We have also tested these tasks where  is a feedforward network with one hidden layer using tanh activations and 100 units; some preliminary results from this more powerful nonlinear  are also reported.
Table 1 shows the accuracy (average 0-1 loss) for 15 runs of the above Janossy pooling approximations under the sum, range, unique sum, and unique count tasks with  being a dense layer with identity activation. Table 3 in the Supplementary Material shows the same results measured by mean absolute error. The data consists of 100,000 training examples and 10,000 test examples. (Sum task) Note that the sum task (Zaheer et al., 2017) is easy for all Janossy pooling approximations. Also note that for the methods trained with -SGD (LSTM and GRU), using 20 samples instead of one to estimate equation 13 yields significant accuracy gains. (Range, unique sum, and unique count tasks) These results corroborate both theorem 2.1 and corollary 2.1, showing that performance increases consistently with k on the tasks that require reasoning about relationships within elements of the sequence. This serves as evidence that optimizing using -SGD yields strong accuracy gains for modeling the entire sequence, albeit over the modified loss J of equation 12. For the methods trained with -SGD (LSTM and GRU), we also see significant gains of using 20 samples instead of one to estimate equation 13, whenever such gains are possible.
When  is a feedforward network with one hidden layer (and 100 hidden units) with tanh activations, our preliminary results (Table 5 in the Supplementary Material) show that Janossy k = 1 performs
8

Under review as a conference paper at ICLR 2019

Table 1: Accuracy (average 0-1 loss) of various Janossy pooling approximations under distinct tasks.
Labels -1inf- and -20inf- refer to the number of permutations sampled to estimate equation 13 for methods learned with -SGD. Standard deviations computed over 15 runs are shown in parentheses.

Tasks Janossy (k = 1)-Deep Sets Janossy (k = 2) Janossy (k = 3) Janossy-1inf-LSTM
Janossy-1inf-GRU
Janossy-20inf-LSTM
Janossy-20inf-GRU

sum
1.000 (.000) 0.999 (.001) 0.996 (.001) 0.886 (.028) 0.978 (.027) 0.955 (.010) 0.990 (.001)

range
0.038 (.002) 0.090 (.004) 0.227 (.004) 0.596 (.057) 0.783 (.024) 0.711 (.046) 0.872 (.017)

unique sum
0.072 (.002) 0.173 (.004) 0.289 (.017) 0.999 (.001) 1.000 (.000) 1.000 (.000) 1.000 (.000)

unique count
0.355 (.006) 0.693 (.049) 0.872 (.015) 0.999 (.001) 1.000 (.000) 1.000 (.000) 1.000 (.000)

Table 2: Performance (Micro-F1) using GraphSAGE with three aggregators shows the benefit of better test-time inference in Janossy pooling approximations trained with -SGD. Standard deviations over 20 runs in parentheses.

CORA
Janossy-1inf-LSTM Janossy (k = 1)-Deep Sets Janossy-20inf-LSTM

CORA 0.821 (.005) 0.863 (.004) 0.857 (.003)

PUBMED 0.812 (.007) .885 (.003) 0.889 (.003)

similarly to Table 1 in the range task, while Janossy k = 2, 3 perform significantly better. For the unique sum and unique count tasks Janossy k = 1, 2, 3 have significant improvements, while Janossy k = 2, 3 either outperforms k = 1 or performs equally. These preliminary results also show that Janossy LSTMs and GRUs perform worse when  is nonlinear, possibly due to the larger divergence between the objective functions equation 9 and equation 12 when  is nonlinear. Investigating this
further is a topic for in future work.

3.2 JANOSSY POOLING AS AN AGGREGATOR FUNCTION FOR VERTEX CLASSIFICATION
Here we consider the task of vertex classification over two graph datasets: Cora and Pubmed (Sen et al., 2008). These datasets are citation networks where vertices represent papers, edges represent citations, and vertex features are bag-of-words representations of the document text. The task is to classify the paper topic. More details of these datasets and our experimental test/train splits are shown in Table 4 in the Supplementary Material.
We use the graph convolutional model of Hamilton et al. (2017) for its simplicity and the use what the authors call Mean Pooling (Janossy pooling k = 1) and LSTM aggregator. The LSTM aggregator is effectively a Janossy pooling approximation using an LSTM for f and trained with -SGD, described as Janossy-1inf-LSTM in our results. At test time, however, Hamilton et al. estimates the class label using equation 13 but with a single permutation. Their experiments indicate that mean pooling (Janossy pooling k = 1) yields better accuracy than the LSTM approximation on citation network tasks.
Here, we show that gains of mean pooling over the LSTM aggregator reported in Hamilton et al. vanish once we provide a better estimate of equation 13. We note in passing that we have closely followed experiment setup of Hamilton et al. detailed in Section C.1 of the Supplementary Material. The Micro-F1 scores (same metric used by Hamilton et al.) are averaged over 21 runs and shown in Table 2, along with the Janossy k = 1 approximation (the mean pooling aggregator of Hamilton et al.). As in (Hamilton et al., 2017), Janossy k = 1 seems to have higher accuracy than Janossy-1infLSTM (one permutation sample for estimating the node label using equation 13). However, with a label estimate averaged over 20 permutation samples (Janossy-20inf-LSTM), these gains disappear and Janossy-20inf-LSTM accuracy is not significantly different from mean pooling (unary Janossy).
The absence of difference between the unary Janossy and Janossy-LSTM pooling approximations points to an easy task. It would have been pointless to try k-ary Janossy pooling approximations in these tasks. It is reasonable to suppose that the topic of a paper can be adequately predicted by computing the average bag-of-words representations of papers in the neighborhood without reasoning about relationships between neighboring papers (i.e. it may be a small-k task). For example, a high proportion of mentions of neural in neighboring papers is suggestive about the paper topic and can be computed independently for each paper.

9

Under review as a conference paper at ICLR 2019
4 RELATED WORK
Under the Janossy pooling framework presented in this work, existing literature falls under one of three approaches to approximating to the intractable Janossy-pooling layer: Canonical orderings, k-ary dependencies, and permutation sampling.
Canonical Ordering Approaches. In section 2.1, we saw how permutation invariance can be achieved by mapping permutations to a canonical ordering. Rather than trying to define a good canonical ordering, one can try to learn it from the data, however searching among all h!| permutations for one that correlates with the task of interest is a difficult discrete optimization problem. Recently, Rezatofighi et al. (2018) proposed a method that computes the posterior distribution of all permutations, conditioned on the model and the data. This posterior-sampling approach is intractable for large inputs, unfortunately. We note in passing that Rezatofighi et al. (2018) is interested in permutation-invariant outputs, and that Janossy pooling is also trivially applicable to these tasks. Vinyals et al. (2016) proposes a heuristic using ancestral sampling while learning the model.
k-ary Janossy Pooling Approaches. In section 2.2 we described k-ary Janossy pooling, which considers k-order relationships in the input vector h to simplify optimization. Deep Sets (Zaheer et al., 2017) can be characterized as unary Janossy pooling (i.e., k-ary for k = 1). Our experiments considered tasks that see significant gains from higher-order k-ary Janossy pooling, k > 1. Ravanbakhsh et al. (2017) proposes a similar unary Janossy pooling to deep sets. Cotter et al. (2018) proposes to add inductive biases to the deep sets model in the form of monotonicity constraints with respect to the vector valued elements of the input sequence by modeling  and  with Deep Lattice Networks (You et al., 2017); one can extend Cotter et al. (2018) by using higher-order k-ary Janossy pooling for k > 1.
Exploiting dependencies within a sequence to learn a permutation invariant function has been discussed elsewhere. For instance Santoro et al. (2017) exploits pairwise relationships to perform relational reasoning about pairs of objects in an image Battaglia et al. (2018) contemplates modeling the center of mass of a solar system by including the pairwise interactions among planets. However, Janossy pooling provides a general framework for capturing dependencies within a permutation invariant pooling layer.
Permutation Sampling Approaches. In section 2.3 we have seen a that permutation sampling can be used as a stochastic gradient procedure (-SGD) to learn a model with a Janossy pooling layer. The learned model provides only an approximate solution to original permutation-invariant function. Permutation sampling has been used as a heuristic (without a theoretical justification) in both Moore & Neville (2017) and Hamilton et al. (2017), which found that randomly permuting sequences and feeding them forward to an LSTM is effective in relational learning tasks that require permutation invariant pooling layers.
5 CONCLUSIONS
Our approach of permutation-invariance through Janossy pooling unifies a number of existing approaches, and opens up avenues to develop both new methodological extensions, as well as better theory. Our paper focused on two main approaches: k-ary interactions and random permutations. The former involves exact Janossy pooling for a restricted class of functions f . Adding an additional neural network  can recover lost model capacity, but now hurts learnability. Placing restrictions on  (convexity, Lipschitz continuity etc.) can allow a more refined control of this trade-off, allowing theoretical and empirical work to shed light on the compromises involved. The random permutation approach involves no clear trade-offs between model capacity, and learnability, instead it approximates the Janossy loss L with a more tractable approximation J. The difference between the two reduces as more permutations are sampled, however, despite excellent empirical performance, it is not clear what problems this is best suited for. Better understanding how the loss-functions L and J relate to each other, can shed light on the slightly black-box nature of this procedure. It is also important to understanding the relationship between the random-permutation approach to canonical ordering, and how one might be used to improve the other. Finally, it is important to apply our methodology to a wider range of applications. Two immediate domains are more challenging tasks involving graphs and tasks involving non-Poisson point processes.
10

Under review as a conference paper at ICLR 2019
REFERENCES
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In NIPS, 2016.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.
Le´on Bottou and Yann L Cun. Large scale online learning. In NIPS, 2004.
Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. EMNLP, 2014.
Franc¸ois Chollet et al. Keras. https://keras.io, 2015.
Andrew Cotter, Maya Gupta, Heinrich Jiang, James Muller, Taman Narayan, Serena Wang, and Tao Zhu. Interpretable set functions. arXiv preprint arXiv:1806.00050, 2018.
Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes: volume II: general theory and structure. Springer, 2003.
Bruno De Finetti. La pre´vision: ses lois logiques, ses sources subjectives. In Annales de l'institut Henri Poincare´, volume 7, pp. 1­68, 1937. [Translated into Enlish: H. E. Kyburg and H.E. Smokler, eds. Studies in Subjective Probability. Krieger 53-118, 1980].
Persi Diaconis. Finite forms of de Finetti's theorem on exchangeability. Synthese, 36(2):271­281, 1977.
Persi Diaconis and David Freedman. De finetti's generalizations of exchangeability. Studies in inductive logic and probability, 2:233­249, 1980.
David K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional Networks on Graphs for Learning Molecular Fingerprints. In NIPS, 2015.
Morris L. Easton. Chapter 8: Finite de Finetti style theorems, volume Volume 1 of Regional Conference Series in Probability and Statistics, pp. 108­120. Institute of Mathematical Statistics and American Statistical Association, Haywood CA and Alexandria VA, 1989. URL https://projecteuclid.org/euclid.cbms/1462061038.
Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. Bayesian Data Analysis. CRC Press, 3 edition, 2014.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017.
Geoffrey Grimmett and David Stirzaker. Probability and random processes. Oxford university press, 2001.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs. In NIPS, jun 2017. URL http://arxiv.org/abs/1706.02216.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Diederik P. Kingma and Jimmy Lei Ba. ADAM: A Method for Stochastic Optimization. International Conference on Learning Representations, ICLR, 2015.
Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks. sep 2016. URL http://arxiv.org/abs/1609.02907.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995.
11

Under review as a conference paper at ICLR 2019
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 2015.
David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019­1031, 2007.
Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, and Le Song. Geniepath: Graph neural networks with adaptive receptive paths. arXiv preprint arXiv:1802.00910, 2018.
Jesper Moller and Rasmus Plenge Waagepetersen. Statistical inference and simulation for spatial point processes. Chapman and Hall/CRC, 2003.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proc. CVPR, volume 1, pp. 3, 2017.
John Moore and Jennifer Neville. Deep collective inference. In AAAI, pp. 2364­2372, 2017.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pp. 2014­2023, 2016.
Peter Orbanz and Daniel M. Roy. Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures. IEEE Trans. Pattern Anal. Mach. Intell., 37(2):437­461, feb 2015. ISSN 0162-8828. doi: 10.1109/TPAMI.2014.2334607. URL http://ieeexplore.ieee.org/ document/6847223/.
Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Deep Learning with Sets and Point Clouds. In ICLR Workshop Track, nov 2017. URL http://arxiv.org/abs/1611.04500.
S Hamid Rezatofighi, Roman Kaskman, Farbod T Motlagh, Qinfeng Shi, Daniel Cremers, Laura Leal-Taixe´, and Ian Reid. Deep perm-set net: Learn to predict sets with unknown permutation and cardinality using deep neural networks. arXiv preprint arXiv:1805.00613, 2018.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 1951.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning. In Advances in neural information processing systems, pp. 4967­4976, 2017.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008.
Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix completion. stat, 1050:7, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order Matters: Sequence to Sequence for Sets. ICLR, 2016.
Ba-Ngu Vo, Nhan Dam, Dinh Phung, Quang N Tran, and Ba-Tuong Vo. Model-based learning for point pattern data. Pattern Recognition, 84:136­151, 2018.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation Learning on Graphs with Jumping Knowledge Networks. In ICML, 2018. URL http://arxiv.org/abs/1806.03536.
Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. arXiv preprint arXiv:1806.08804, 2018.
Seungil You, David Ding, Kevin Canini, Jan Pfeifer, and Maya Gupta. Deep lattice networks and partial monotonic functions. In Advances in Neural Information Processing Systems, pp. 2981­ 2989, 2017.
12

Under review as a conference paper at ICLR 2019

Alan Yuille. The Convergence of Contrastive Divergences. In NIPS, 2004.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep Sets. In NIPS, 2017.
Konrad Zolna, Devansh Arpit, Dendi Suhubdy, and Yoshua Bengio. Fraternal dropout. ICLR, 2018.

A PROOFS OF RESULTS

We restate and prove the remaining portion of theorem 2.1. Theorem 2.1. For any k  N, define Fk as the set of all permutation invariant functions that can be represented by Janossy pooling with k-ary dependencies. Then, Fk-1 is a proper subset of Fk if the space H is not trivial (i.e. if the cardinality of H is greater than 1). Thus, Janossy pooling with k-ary dependencies can express any Janossy pooling function with (k - 1)-ary dependencies, but
the converse does not hold.

Proof. (Fk  Fk1 ):

We will demonstrate the existence of f k  Fk such that f k-1 = f k for all f k-1  Fk-1. It suffices

to consider |x| = k. Let f k(|x|, x; ) =

|x| l=1

x(l)

whence

f k(x;

)

=

|x| l=1

xl.

Thus,

for

any

f k-1,

f k-1(|x|, x; ) f k(|x|, x; )

=

1 |x|!
|x|

f k-1(k - 1, k-1(x); )

|x| l=1

xl

=

1 |x| |x|!
j=1 ~{1,...,|x|}\j

f k-1

k - 1, (x-j)~ ; 

|x| l=1

xl

where {1,...,|x|}\j is the set of permutation functions of {1, 2, . . . , j -1, j +1, . . . , |x|} and (x-j)~ is a permutation of {x1, . . . , xj-1, xj+1, . . . , x|x|}.This can be written as

1 |x| 1 |x|! j=1 xj

f k-1 k - 1, (x-j )~ ; 

~ {1,...,|x|}\j

l=j xl

,

denote by aj,|x|

whence

f k-1(|x|, x; ) f k(|x|, x; )

=

1 |x|!

|x| j=1

1 xj

aj,|x|

.

(14)

Now, f k-1 = f k if and only if their quotient in equation 14 is unity for all x. But this is clearly not possible in general unless X is a singleton, which we have precluded in our assumptions.

B DE FINETTI'S THEOREMS

The infinite form of de Finetti's theorem arises when contemplating an infinite sequence of ex-
changeable random variables or when contemplating a finite sequence whose distribution could arise
as a marginalization of an infinitely long exchangeable sequence (Diaconis, 1977; Orbanz & Roy, 2015; Gelman et al., 2014). When this holds, the data distribution p(X1, X2, . . .) can be represented as a mixture of some prior distribution p() for parameter    and a conditionally iid likelihood for the data, given : in the case of an infinite exchangeable sequence1,



p(X1, X2, . . . , ) =

p(Xi | )p()d

 i=1

(15)

1Here we write a slightly simplified form, as in Gelman et al. (2014) which is satisfactory for our discussion here. A more general form can be found in Orbanz & Roy (2015).

13

Under review as a conference paper at ICLR 2019

Table 3: Mean Absolute Error of various Janossy pooling across various tasks. Labels -1inf- and

-20inf- refer to the number of permutations sampled to estimate equation 13 for methods learned

with -SGD. Standard deviations computed over 15 runs are shown in parentheses.

Tasks

sum

range

unique sum

unique count

Janossy-MLP (k = 1)-Deep Sets Janossy-MLP (k = 2) Janossy-MLP (k = 3) Janossy-1inf-LSTM
Janossy-1inf-GRU
Janossy-20inf-LSTM
Janossy-20inf-GRU

0.026(.005) 0.032(.007) 0.032(.003) 0.510(.055) 0.263(.069) 0.433(.040) 0.194(.011)

9.424(.081) 4.057(.042) 2.265(.051) 0.630(.101) 0.364(.028) 0.482(.079) 0.249(.022)

4.235(.020) 1.972(.023) 1.157(.088) 0.029(.008) 0.01(.001) 0.014(.006) 0.005(.001)

0.828(.008) 0.393(.006) 0.266(.010) 0.017(.005) 0.005(.001) 0.009(.003) 0.003(0)

or if (X1, . . . , Xn) can be extended to an infinitely exchangeable sequence, its distribution is arbitrarily close to the form in equation 15Diaconis (1977).

However, Diaconis (1977) shows that not all finitely exchangeable sequences can be written as
in equation 15, by demonstrating that one can write an exchangeable probability mass function of two binary random variables (over events {0, 0}, {0, 1}, {1, 0}, {1, 1}) where p({0, 1}) = p({1, 0}), which cannot be written in general as a mixture over conditionally iid Bernoulli random variables.

Conversely, a representational form for exchangeable finite sequences has also been studiedDiaconis

& Freedman (1980); Easton (1989). For example, in the case of m binary random variables

X1, X2, . . . , Xm, the probability distribution (1) randomly select the number of successes

is represented by the

m i=1

Xi

=

s

<

m

in

following generating an urn with m balls ,

process: then (2)

randomly draw balls from that urn. Observe that if an urn has s successes, and the first s draws are

all successes, the (s + 1)th draw must be a failure, and thus the draws are not iid.

C EXPERIMENTS: FURTHER RESULTS AND IMPLEMENTATION DETAILS
C.1 RESULTS
Mean Absolute errors for arthimetic tasks on integer sequences are reported in Table 3. These largely corroborate the results from Table 1, with a drop in the mean absolute error as the value of k increases and when using more sampled permutations at test-time (e.g., Janossy-20inf-LSTM versus Janossy-1inf-LSTM). Beyond the performance gains, we also observe a drop in variance when sampling more permutations at test time.

C.2 IMPLEMENTATION DETAILS

Sequence tasks We extended the code from Zaheer et al. (2017), which was written in

Keras(Chollet et al., 2015). Regardless of the model, we always sort the sequence x beforehand.

In

the

notation of Figure 1,

h is

an Embedding with

dimension of

floor(

100 k

),

f

is either

an MLP

with a single hidden layer or an RNN depending on the model (k-ary Janossy or full-Janossy, re-

spectively), and  is always a single dense layer. The MLPs have 30 neurons, the LSTMs have 50

neurons, and the GRUs have 80 hidden neurons. All activations are tanh except for the output layer

which is linear.

We constructed these models to be as comparable as possible; each has a similar number of total

parameters and the same number of layers. We unify the number of parameters by adjusting the

output

dimension

of

the

embedding

(approximately

1 k

)

for

the

different

k-ary

models

and

by

using

fewer neurons in the LSTM than the GRU since LSTMs are more complicated models with more

parameters.

Optimization is done with Adam (Kingma & Ba, 2015) using a learning rate of 10-4. We did not do a tuning parameter sweep. Training was performed on GeForce GTX 1080 Ti GPUs.

14

Under review as a conference paper at ICLR 2019

Table 4: Summary of the graph datasets

CHARACTERISTICS Number of Vertices Average Degree Number of Vertex Features Number of Classes Number of Training Vertices Number of Test Vertices

CORA 2708 3.900 1433 7 256 1000

PUBMED 19717 4.496 500 3 256 1000

Table 5: Preliminary results similar to Table 1: accuracy of various Janossy pooling models across

various tasks where  is an MLP

Tasks

range

unique sum unique count

Janossy-MLP (k = 1)-Deep Sets Janossy-MLP (k = 2) Janossy-MLP (k = 3) Janossy-1inf-LSTM
Janossy-1inf-GRU
Janossy-20inf-LSTM
Janossy-20inf-GRU

0.049 (.002) 0.379 (.023) 0.616 (.022) 0.226 (.014) 0.188 (.015) 0.238 (.020) 0.199 (.028)

0.361 (.040) 0.970 (.015) 0.918 (.024) 0.693 (.042) 0.925 (.024) 0.669 (.202) 0.937 (.052)

1 (0) 1 (0) 1 (0) 0.631(.073) 0.998 (.003) 0.626 (.116) 0.995 (.010)

Graph-based tasks The characteristics of the datasets used are tabulated in Table 4. This task is implemented in PyTorch and trained on the same GeForce GTX 1080 Ti GPUs.
As this experiment is using Janossy pooling as a GraphSAGE (Hamilton et al., 2017) aggregator, we forked the GraphSAGE datasets and code for experimental set-up and the mean-pool aggregator; we implemented our own LSTM as it is not yet available in the GraphSAGE PyTorch repo. Following Hamilton et al. (2017) a depth (i.e. number of aggregator steps) of 2, an Adam optimizer and a hidden LSTM dimension of 256. The remaining hyperparameters are dataset-specific:
· On the Cora dataset, for all models, the learning rate is 0.005; the number of random samples at depth one and two are both five.
· On the Pubmed dataset, for all models the learning rate is 0.01; the number of random samples at depth one and two are 10 and 25, respectively.
Note that the embedding function h, in terms of Figure 1 is an identity in this task.
D SUPPLEMENTARY RESULTS ON SEQUENCE ARITHMETIC TASKS
Here we present our preliminary results on the sequence task where  is modeled as a feedforward network with one hidden layer containing 100 hidden neurons and tanh activations. Other than the model for , the implementation details are as in section C.1. No hyperparameter tuning was performed. We are not considering results for the sum task since the experimental interest is to investigate whether a richer model for  can carry the burden of exploiting dependencies in the sequence when k = 1. The results are shown in Table 5. We can see that deep sets does not perform well across tasks and that increasing k helps to boost performance.

15

