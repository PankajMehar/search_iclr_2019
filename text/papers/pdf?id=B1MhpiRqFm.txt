Under review as a conference paper at ICLR 2019
BOLTZMANN WEIGHTING DONE RIGHT IN REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
The Boltzmann softmax operator can trade-off well between exploration and exploitation according to current estimation in an exponential weighting scheme, which is a promising way to address the exploration-exploitation dilemma in reinforcement learning. Unfortunately, the Boltzmann softmax operator is not a non-expansion, which may lead to unstable or even divergent learning behavior when used in estimating the value function. The convergence of value iteration is guaranteed in a restricted set of non-expansive operators and how to characterize the effect of such non-expansive operators in value iteration remains an open problem. In this paper, we propose a new technique to analyze the error bound of value iteration with the the Boltzmann softmax operator. We then propose the dynamic Boltzmann softmax(DBS) operator to enable the convergence to the optimal value function in value iteration. We also present convergence rate analysis of the algorithm. Using Q-learning as an application, we show that the DBS operator can be applied in a model-free reinforcement learning algorithm. Finally, we demonstrate the effectiveness of the DBS operator in a toy problem called GridWorld and a suite of Atari games. Experimental results show that outperforms DQN substantially in benchmark games.
1 INTRODUCTION
In sequential decision making problem, an agent learns to find an optimal policy that maximizes the expected discounted long-term reward, which can be modeled by a Markov decision process (MDP). In an MDP, the optimal value function is the fixed point of the Bellman operator. Thus, the optimal value function can be computed by iterative updates from arbitrary initial value function, i.e., value iteration (Bellman (2013)). Littman & Szepesva´ri (1996) proposed the generalized MDP considering generalized action selection operators, according to which the value function is estimated. If the generalized action selection operator satisfies the non-expansion property, the uniqueness of the fixed-point of the generalized Bellman operator is guaranteed, thus ensuring the convergence of generalized value iteration algorithm. Examples of non-expansive operators include the max and the mean operators.
Without full information about transition dynamics and reward function of the environment, in reinforcement learning, the agent aims to learn an optimal policy by interacting with the unknown environment from experience. Reinforcement learning has achieved groundbreaking success for many decision making problems, both in discrete and continuous domains, including robotics (Kober et al. (2013)), game playing (Mnih et al. (2015)), and many others. One of the most fundamental challenges is how to balance exploration and exploitation, where the exploration-exploitation dilemma occurs in action selection and value function optimization (Asadi & Littman (2016)). For value function optimization, the agent estimates the value function according to the action selection operator and then updates the estimated value. A number of effective algorithms to address explorationexploitation dilemma have their root in alternating the action selection operator, which falls in the generalized MDP framework.
One of the most important reinforcement learning algorithm, Q-learning, employs the the max operator for value function optimization. The max operator always greedily selects the action that gives the best value and updates current estimation according to the value. As the current estimation for value functions is not accurate, such greedy selection may lead to misbehavior, e.g., the overesti-
1

Under review as a conference paper at ICLR 2019

mation phenomenon (Hasselt (2010)). In fact, the max operator is the extreme case of exploitation, lacking the ability to explore or consider other choices. On the other hand, the mean operator estimates the value by computing the average. Thus, the mean operator solely explores which fails to utilize current estimation.
The Boltzmann softmax operator is a natural summary operator that has been widely applied (CesaBianchi et al. (2017)). Specifically, it is an exponentially weighting scheme, where the weights are computed according to the current estimation and its parameter . The parameter  trades off between exploration and exploitation. When   , it behaves as the max operator which solely favors exploitation while it behaves as the mean operator when   0 which purely focuses on exploration. However, despite from the advantages, it is very challenging to apply the operator in value function optimization. First, the parameter  is difficult to choose (Sutton et al. (1998)). Second, as shown in (Littman & Szepesva´ri (1996); Asadi & Littman (2016)), the Boltzmann softmax operator is not a non-expansion, which may lead to multiple fixed-points and thus the value function of this policy is not well-defined. The non-expansive property is vital to guarantee the uniqueness of the fix-point and the convergence of the learning algorithm. Without such property, the learning algorithm may misbehave or even diverge. Thus, it is widely believed that the Boltzmann softmax operator cannot be used directly due to the violation of the non-expansive property (Littman & Szepesva´ri (1996); Asadi & Littman (2016)). In fact, as far as we know, how to characterize the use of the Boltzmann softmax operator, which violates the property of non-expansion in value iteration, remains an open problem (Littman (1996)).
In this paper, we study the property of the Boltzmann softmax operator and propose a new technique to characterize its error bound in value iteration with fixed parameter . To be specific, we show that for the error bound between the value function induced by the Boltzmann softmax operator and the optimal value function, there remains a term related to  that will not converge to 0. Thus, although the Boltzmann softmax operator guarantees the approximate convergence of value iteration, it would converge to a sub-optimal policy unfortunately. Indeed, the direct use of the Boltzmann softmax operator inevitably introduces performance drop in value iteration.
We then take a step further and study an essential problem, is there a way that the Boltzmann softmax operator be applied in value iteration which guarantees the convergence to the optimal policy?
Based on this technique, we propose the dynamic Boltzmann softmax operator boltzt , termed the DBS operator, to eliminate the loss and enable the convergence of value iteration to the optimal value function. Our core idea is to dynamically change t in value iteration and present its convergence rate analysis. Then, we propose the DBS Q -learning algorithm with the application of the DBS operator in a popular model-free reinforcement learning algorithm, i.e., Q-learning (Watkins & Dayan (1992)), and prove the convergence of the DBS Q-learning.
We conduct experiments to verify the effectiveness and efficiency of our proposed dynamic Boltzmann softmax operator. We first evaluate DBS value iteration and DBS Q-learning on a tabular case, the GridWorld. Results show that the DBS operator leads to smaller error and faster convergence. We then demonstrate that the DBS operator can be extended to large scale problems, Atari games. Using DQN as baseline, we show that DQN with the dynamic Boltzmann softmax operator (abbreviated as DBS-DQN) substantially outperforms DQN in a suite of Atari benchmark games.

2 PRELIMINARIES

A Markov decision process (MDP) is defined by a 5-tuple (S, A, p, r, ), where S and A denote the set of states and actions, p(s |s, a) represents the transition probability from state s to state s
under action a, and r(s, a) is the corresponding immediate reward. The discount factor is denoted by   [0, 1), which controls the degree of importance of future rewards.

At each time, the agent interacts with the environment with its policy , a mapping from state to

action. The objective is to find an optimal policy that maximizes the expected discounted long-term

reward E[

 t=0

trt|],

which

can

be

solved

by

estimating

value

functions.

and state-action value of s and a under policy  are defined as V (s) = E[

Q(s, a) = E[ max V (s) and

Qt=(0s, atr)t=|s0m=axs,

a0 = a]. Q(s, a).

The

optimal

value

functions

are

The state value of s

 t=0

trt|s0

=

s]

and

defined as V (s) =

2

Under review as a conference paper at ICLR 2019

Littman & Szepesva´ri (1996) proposed a general framework for reinforcement learning, where the generalized action selection operator is denoted by . The optimal value function V  satisfies the generalized Bellman equation, which is defined recursively as in Equation (1):

V (s) =

r(s, a) + p(s |s, a)V (s )

(1)

aA

s S

Starting from arbitrary initial value function V0, the optimal value function V  can be computed by value iteration (Bellman (2013)) according to an iterative update: Vk+1 = T Vk, where T is the
generalized the Bellman operator as defined in Equation (2).

(T V )(s) = r(s, a) + p(s |s, a)V (s ) .

aA

s S

(2)

The convergence of value iteration is guaranteed if is a non-expansion, which guarantees the unique solution of the generalized Bellman equation (1), i.e., T V  = V . The non-expansion is

defined as:

Q1(s, a) - Q2(s, a)  ||Q1(s, ·) - Q2(s, ·)||,

(3)

aa

where || · || denotes the -norm.

The Boltzmann softmax operator is one kind of the action selection operator

as:

boltz(X) =

n i=1

xiexi

n i=1

exi

.

, which is defined (4)

3 ANALYSIS OF THE BOLTZMANN SOFTMAX OPERATOR

In this section, we first analyze the property of the Boltzmann softmax operator and then propose a new technique to analyze its error bound in value iteration.
It has been shown that the Boltzmann softmax operator is not a non-expansion ((Littman & Szepesva´ri (1996); Asadi & Littman (2016))) as it does not satisfy Inequality (3). Indeed, the non-expansive property is vital to the convergence of the learning algorithm, which guarantees the uniqueness of the fixed point. We first analyze the property of the Boltzmann softmax operator, which paves the path for studying the effect of using such operators that violates the non-expansive property in value iteration.

Proposition 1 For  > 0 and X, Y  Rn, the Boltzmann softmax operator satisfies the following property:

2 log(n) |boltz(X) - boltz(Y)|  ||X - Y|| +  ,

(5)

where ·  is the -norm in Rn.

In Proposition 1, we show that although the Boltzmann softmax operator is not a non-expansive operator, the degree of the violation of the non-expansive property is controlled by . The larger the value of  is, the closer it is to the non-expansion. Due to space limit, we put the proof of Proposition 1 in Appendix A.
Next, we propose a new technique to characterize the error bound of value iteration with Boltzmann softmax operator in value iteration, where the full proof is referred to Appendix B.

Theorem 1 (Error bound of value iteration with Boltzmann softmax operator) Let Vt be the value function computed by the Boltzmann softmax operator at the t-th iteration and V0 denote the initial value. After t iterations,

||Vt

-

V

||



t||V0

-

V

||

+

4 log(|A|)(1 - (1 - )

t) .

Taking the limit of t in both sides of Inequality (6), we obtain the following result:

(6)

3

Under review as a conference paper at ICLR 2019

Corollary 1 For the Boltzmann softmax operator boltz, the error of value functions is

limt ||Vt+1

-

V

||



4 log(|A|) (1-)

.

Corollary 1 characterizes the error bound of value iteration with the Boltzmann softmax operator.

With

a

fixed

parameter

,

the

error

is

upper

bounded

by

4 log(|A|) (1-)

,

which

decreases

with

an

in-

creasing value of . Thus, the direct use of the Boltzmann softmax operator inevitably introduces

performance drop in practice. Motivated by the theoretical findings, we propose the dynamic Boltz-

mann softmax operator, which enables the convergence to the optimal.

4 DYNAMIC BOLTZMANN SOFTMAX OPERATOR
In this section, we propose the dynamic Boltzmann softmax (DBS) operator boltzt to eliminate the error in value iteration. Next, we give theoretical analysis of the proposed DBS value iteration algorithm. We prove that it converges to the optimal policy if t approaches , as shown in Theorem 2. We then present the convergence rate analysis in Theorem 3. Finally, we show that the DBS operator can be applied in a prominent model-free reinforcement learning algorithm, Q-learning, with convergence guarantee.
From Corollary 1, although the Boltzmann softmax operator can converge, it may suffer from error due to the violation of the non-expansive property. We propose the dynamic Boltzmann softmax (DBS) operator to eliminate the error, which is motivated by Corollary 1 that although boltz is not a non-expansive operator, it performs very close to the non-expansion when  is large enough.
Based on the DBS operator, we design the corresponding DBS value iteration algorithm. DBS value iteration algorithm admits a dynamically changing series {t} (line 1) and update the value function according to the dynamic Boltzmann softmax operator boltzt (line 6). Thus, the way to update the value function is according to the exponential weighting scheme, which is related to both the current estimation value and the parameter t.
Algorithm 1: DBS Value Iteration
Input: An increasing series {t}; termination condition  1 Initialize V (s), s  S arbitrarily 2 for each episode t = 1, 2, ... do 3 0 4 for each s  S do 5 v  V (s) 6 V (s)  boltzt s ,r p(s , r|s, a)[r + V (s )] 7   max(, |v - V (s)|)
8 if  <  then 9 break

4.1 CONVERGENCE ANALYSIS
In Theorem 2, we demonstrate that the DBS operator can enable the convergence of DBS value iteration to the optimal. Due to space limit, see Appendix C for proof of the theorem.
Theorem 2 (Convergence of value iteration with the DBS operator) For a sequence of dynamic Boltzmann softmax operator {t}, if t  , Vt converges to V , where Vt and V  denote the value function after t iterations and the optimal value function.
Theorem 2 implies that DBS value iteration does converge to the optimal policy if t approaches infinity. Although the Boltzmann softmax operator may violate the non-expansive property for some values of , we only need  approaches infinity to guarantee the convergence. The convergence rate of the DBS operator is shown in Theorem 3, where the proof is provided in Appendix C.
4

Under review as a conference paper at ICLR 2019

Theorem 3 (Convergence rate of value iteration with the DBS operator) For any power series t = tp(p > 0), we have that for any  (0, min{0.25, ||R||-1}), after

max{O

log(

1

+log(

log(

1 

)

1 1-

)

),

O

(

1 (1-)

1
)p

} steps, the error ||Vt - V || 

.

For the larger value of p, the convergence rate is faster. Note that when p approaches , the convergence bound is dominated by the first term.
From the above theoretical analysis of the DBS operator, we demonstrate that value iteration can still converge to the optimal even with an operator violating the non-expansive property. Such finding generalizes previous understanding of the convergence of value iteration, which is restricted to a class of non-expansive operators. In addition, the convergence rate is of the same order as the standard Bellman operator. This implies that the DBS operator will not lose too much in terms of the convergence rate in value iteration. These findings provide theoretical background and pave the way for the study of the use of the DBS operator in reinforcement learning algorithms, e.g., Qlearning, which does not have full information about the model. In the absence of full information about the model, the agent has to explores in the environment and exploits the optimal strategy.

4.2 APPLICATION: Q-LEARNING
In this section, we show that the DBS operator can be applied in a model-free Q-learning algorithm (Algorithm 9), which requires careful trade-off between exploration and exploitation.
The DBS Q-learning updates the Q-value according to the DBS operator, where it admits a dynamically changing series t. It is worth noting that in Theorem 3, the larger value of p results in faster convergence rate in value iteration. However, this is not the case in Q-learning. Indeed, Q-learning differs from value iteration in that it knows nothing about the environment, which means the agent has to learn from experience. Thus, the agent needs to balance between exploration and exploitation. If p is too large, it quickly approximates the max operator that favors pure exploitation.
Algorithm 2: DBS Q-learning
Input: An increasing series {t} 1 Initialize Q(s, a), s  S, a  A arbitrarily, and Q(terminal)-(state, ·) = 0 2 for each episode t = 1, 2, ... do 3 Initialize s 4 t = f (t) 5 for each step of episode do 6 choose a from s using policy derived from Q 7 take action a, observe r, s 8 Q(s, a)  Q(s, a) + [r + boltzt (Q(s, ·)) - Q(s, a)] 9 sa

In Theorem 4, we prove that DBS Q-learning converges to the optimal policy under the same additional condition as in DBS value iteration. The proof is based on the stochastic approximation lemma in (Singh et al. (2000)), and the full proof is referred to Appendix E.

Theorem 4 (Convergence of DBS Q-learning) The Q-learning algorithm with dynamic Boltzmann softmax policy given by

Qt+1(st, at) = (1 - t(st, at))Qt(st, at) + t(st, at)[rt + boltzt (Qt(st+1, ·))] converges to the optimal Q(s, a) values if

(7)

1. The state and action spaces are finite.

2. t t(s, a) =  and 3. limt t = 

t t2(s, a) < 

4. Var(r(s, a)) is bounded.

5

Under review as a conference paper at ICLR 2019

L -Loss log( -Loss) log( -Loss) Steps
T

(a) Environment.

300 Q-learning

DBS Q-learning ( t = t)

250

DBS Q-learning ( t = t2) DBS Q-learning ( t = t3)

DBS Q-learning ( t = t7)

200

150

100

50

0 100 200 300 400 500 Episode

(b) Results of Q-learning.

Figure 1: The grid world experiment.

Grid World

t = 0.1

1.0

t=1 t = 10

t = 100

0.8

t = 1000 t=t

t =t2

0.6 t = t3

0.4

0.2

0.0 0

100 200 300 400 500 Episode

(a) Training loss.

0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
0.1 1 10 100
(b) Static .

1000

10
20
30
40
50 t t2
t
(c) Dynamic t.

200000 150000 100000 50000

k=k k =k2 k = k10
1000 800
0.00 0.05 0.10 0.15 0.20 0.25

0 t3 0.00 0.05 0.10 0.15 0.20 0.25

(d) Convergence bound.

Figure 2: Value iteration in the GridWorld.

Note that different from value iteration, Q-learning does not know the full information about the model and has to learn from experience. Thus, it is vital to trade-off exploration and exploitation. Unlike the max operator, the DBS operator enables exploration in the begining of learning with a small value of t. Since the estimated value function is not accurate in the begining of learning, it is better to weight possible choices according to the current estimation rather than greedily selecting the maximum estimated value. The DBS favors exploitation more as t increases, meaning that it is able to utilize the information of current estimation.
5 EXPERIMENTS
5.1 GRIDWORLD
We first evaluate the performace of DBS value iteration DBS Q-learning in a toy problem, the GridWorld (Figure 1(a)), which is a larger variant of the environment of (O'Donoghue et al. (2016)). The GridWorld consists of 10 × 10 grids, with the dark grids representing walls. The agent starts at the upper left corner and aims to eat the apple at the bottom right corner upon receiving a reward of +1. Otherwise, the reward is 0. An episode ends if the agent successfully eats the apple or a maximum number of steps 300 is reached. For this experiment, we consider the discount factor  = 0.9.
The training loss of value iteration is shown in Figure 2(a). As expected, larger value of  leads to smaller loss. Figure 2(b) and Figure 2(c) demonstrate the training loss in logarithmic form for the last episode. For static , the value iteration algorithm suffers from some loss which decreases as  increases. For dynamic t, the performance of t2 and t3 are the same and achieve the smallest loss. The convergence rate is illustrated in Figure 2(d). For higher order p of t = tp, the convergence rate is faster. We also see that the convergence rate of t2 and t10 is very close as discussed before.
Figure 1(b) demonstrates the number of steps the agent spent in each episode. DBS Q-learning with t = t2 achieves the best performance as it best trades off between exploration and exploitation. When the power p of t = tp increases, it performs closer to the max operator for exploitation. When p = 1, it performs worse than Q-learning in this simple game as it explores more. Thus, considering trading off between exploration and exploitation, we choose p = 2 in the following experiments.
6

Under review as a conference paper at ICLR 2019

Mean Median

DQN

495.76% 84.72%

DBS-DQN 1611.49% 103.95%

Table 1: Summary of Atari games.

Figure 3: Relative human normalized score on Atari games.

5.2 ATARI

We evaluate the DBS-DQN algorithm on 49 Atari games from the Arcade Learning Environment
(Bellemare et al. (2013)) by comparing it with DQN. For fair comparison, we use the same setup of
network architectures and hyper-parameters as in Mnih et al. (2015) for both DQN and DBS-DQN.
Note that DBS-DQN estimates the value for the next state according to the DBS operator, where t = ct2 and c is the coefficient. See Appendix F for full implementation details. For each game, we train each algorithm for 50M steps. The evaluation procedure is identical to Mnih et al. (2015), 30 no-op evaluation, where the agent performs a random number (up to 30) of "do nothing" actions
in the beginning of an episode.

Table 1 shows the summary of results in human normalized score, which is defined as (Van Hasselt

et al. (2016)):

scoreagent - scorerandom × 100%, scorehuman - scorerandom

(8)

where human score and random score are taken from Wang et al. (2015). As illustrated in Table 1,

DBS-DQN significantly outperforms DQN in terms of both the mean and the median of the human

normalized score. To better characterize the effectiveness of DBS-DQN, its improvement over DQN

is shown in Figure 3, where the improvement is defined as the relative human normalized score:

scoreagent - scorebaseline

× 100%,

max{scorehuman, scorebaseline} - scorerandom

(9)

with DQN serving as the baseline. In all, DBS-DQN exceeds the performace of DQN in 33 out of 49 Atari games. Full scores of comparison is referred to Appendix G. Figure 4 shows the learning curves for each algorithm. The results provide emprical evidence that the DBS operator trades-off well exploration and exploitation in value function optimization.

6 RELATED WORK
In reinforcement learning, avoiding the exploration-exploitation dilemma is a vital task. The Boltzmann softmax operator is a popular way to balance exploration and exploitation by exponentially

7

Under review as a conference paper at ICLR 2019

Average score per episode

1800 Assault

1600

1400

1200

1000

800

600

400

200

DQN DBS-DQN

0 25 50 75Trainin1g00Epochs125 150 175 200

(a) Assault

Average score per episode

Enduro 500
400
300
200
100 DQN
0 DBS-DQN 0 25 50 75Trainin1g00Epochs125 150 175 200
(b) Enduro

Average score per episode

Riverraid 6000

5000

4000

3000

2000

1000 0

DQN DBS-DQN 25 50 75Trainin1g00Epochs125 150 175 200

(c) Riverraid

RoadRunner

25000 20000

15000 10000

5000 0 0

DQN DBS-DQN 25 50 75 100 125 150 175 200 Training Epochs

(d) RoadRunner

Average score per episode

5000 Seaquest

4000

3000

2000

1000 0 0

DQN DBS-DQN 25 50 75 100 125 150 175 200 Training Epochs

(e) Seaquest

Average score per episode

Zaxxon
2500
2000
1500
1000
500 DQN
0 DBS-DQN 0 25 50 75 100 125 150 175 200 Training Epochs
(f) Zaxxon

Figure 4: Learning curves in Atari games.

Average score per episode

weighting its current estimation (Kaelbling et al. (1996)). To address the problem, a line of research focuses on the exploration strategy where the agent should exploit current best action on the one hand, but it needs to explore whether there are better possibilities on the other hand. (Singh et al. (2000)) studied the convergence of on-policy algorithm, i.e., Sarsa. They show that the strategy needs to be greedy in the limit, which guarantees that the optimal action can be selected. Although they considered a dynamic parameter of  in the Boltzmann softmax operator, it depends on the state, which is impractical in complex problems as Atari games. The other line studies the use of alternative operators in value function estimation. To better trade-off between exploration and exploitation, Asadi & Littman (2016) proposed the "Mellowmax" operator, where the degree of exploration and exploitation is controlled by its parameter. However, although the "Mellowmax" operator can approximate maximization in the limit, it converges to a sub-optimal policy rather than the optimal policy. Haarnoja et al. (2017) utilized the log-sum-exp operator, which enables better exploration and learns deep energy-based policies.
It is worth noting that meta learning cannot be applied to solve the problem since the Boltzmann softmax operator with a fixed parameter would inevitably lead to error in value iteration, so there is no optimal value of .
7 CONCLUSION
We provide a new theoretical technique to analyze the error bound of the value iteration algorithm with the Boltzmann softmax operator. Then, we develop the DBS value iteration algorithm based on our proposed dynamic Boltzmann softmax (DBS) operator which enables convergence to the optimal value function and present convergence rate analysis. We show that the DBS operator can be applied in a model-free reinforcement learning algorithm, Q-learning. Experimental results demonstrate the effectiveness of the DBS operator and show that it can be extended to complex problems as Atari games. For future work, it is worth studying the sample complexity of our proposed DBS Qlearning algorithm. It is also a promising direction to apply the DBS operator to other state-of-the-art DQN-based algorithms, such as Rainbow (Hessel et al. (2017)).
8

Under review as a conference paper at ICLR 2019
REFERENCES
Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 243­252, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253­279, 2013.
Richard Bellman. Dynamic programming. Courier Corporation, 2013.
Nicolo` Cesa-Bianchi, Claudio Gentile, Ga´bor Lugosi, and Gergely Neu. Boltzmann exploration done right. In Advances in Neural Information Processing Systems, pp. 6284­6293, 2017.
Chelo Ferreira and Jose´ L Lo´pez. Asymptotic expansions of the hurwitz­lerch zeta function. Journal of Mathematical Analysis and Applications, 298(1):210­224, 2004.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Hado V Hasselt. Double q-learning. In Advances in Neural Information Processing Systems, pp. 2613­2621, 2010.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237­285, 1996.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238­1274, 2013.
Michael L Littman and Csaba Szepesva´ri. A generalized reinforcement-learning model: Convergence and applications. In Machine Learning, Proceedings of the Thirteenth International Conference (ICML '96), Bari, Italy, July 3-6, 1996, pp. 310­318, 1996.
Michael Lederman Littman. Algorithms for sequential decision making. 1996.
David JC MacKay and David JC Mac Kay. Information theory, inference and learning algorithms. Cambridge university press, 2003.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy gradient and q-learning. arXiv preprint arXiv:1611.01626, 2016.
Satinder Singh, Tommi Jaakkola, Michael L Littman, and Csaba Szepesva´ri. Convergence results for single-step on-policy reinforcement-learning algorithms. Machine learning, 38(3):287­308, 2000.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In AAAI, volume 2, pp. 5. Phoenix, AZ, 2016.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992.
9

Under review as a conference paper at ICLR 2019

A PROPERTY OF THE BOLTZMANN SOFTMAX OPERATOR

Proposition 1 For  > 0 and X, Y  Rn, the Boltzmann softmax operator satisfies the following property:

2 log(n) |boltz(X) - boltz(Y)|  ||X - Y|| +  ,

(10)

where ·  is the -norm in Rn.

Proof 1

Let L denote a log-sum-exp function, i.e., L(X) =

1 

log(

n i=1

exi

)

|boltz(X) - boltz(Y)| = |(boltz(X) - L(X)) - (boltz(Y) - L(Y)) + (L(X) - L(Y))|
 |boltz(X) - L(X)| + |boltz(Y) - L(Y)| + |L(X) - L(Y)| (11)

From

MacKay

&

Mac

Kay

(2003),

we

have

|boltz (X)

-

L (X)|



log(|X|) 

Substitute the above inequality into Equation (11), we have

|boltz (X)

-

boltz (Y)|



2 log(n) 

+

|1 

log(

n

exi ) - 1 log( 

n

eyi )|

i=1 i=1

2 log(n) 1 = + log(


n i=1 n i=1

exi eyi

)

(12)

Let i = |xi - yi| and assume

n i=1

exi



n i=1

eyi

without

loss

of

generality.

Then,

we

have

1 log(


n i=1 n i=1

exi eyi

)

1 

log(

 1 log( 

n i=1

e (yi +i

n i=1

eyi

)

)

n i=1

e (yi +||i || )

n i=1

eyi

)

 ||i||

= ||x - y||

(13)

So we have

|boltz (X)

- boltz(Y)|



2 log(n) 

+ ||x -

y||

(14)

B ERROR BOUND OF THE BOLTZMANN SOFTMAX OPERATOR

Theorem 1 (Error bound of value iteration with Boltzmann softmax operator) Let Vt be the

value function computed by the Boltzmann softmax operator at the t-th iteration and V0 denote the

initial

value.

After

t

iterations,

||Vt

-

V

||



t||V0

-

V

||

+

4

log(|A|)(1- (1-)

t

)

.

Proof 2 Let T and Tm denote the dynamic programming operators for the Boltzmann softmax operator boltz and the max operator respectively, so

(TV )(s) = boltz(Q(s, ·)), (TmV )(s) = max(Q(s, ·))

(15)

Thus,

||(TV1) - (TmV2)||  ||(TV1) - (TV2)|| + ||(TV2) - (TmV2)||
(A) (B)

(16)

10

Under review as a conference paper at ICLR 2019

For the term (A), we have

||(T V1 )

-

(Tt V2)||

=

max
s

|boltz (Q1 (s,

·))

-

boltz (Q2 (s,

·))|



max
s

max
a

|Q1

-

Q2|

+

2 log(|A|) 

 max max 
sa

p(s

|s, a)|V1(s

)

-

V2(s

)|

+

2

log(|A|) 

s



||V1

-

V2||

+

2

log(|A|) 

(17)

For the term (B), we have

||(T V1 )

-

(TmV1)||

=

max
s

|boltz (Q1 (s,

·))

-

max(Q1(s,

·))|

 max |boltz(Q1(s, ·)) - L(Q1(s, ·))| + |L(Q1(s, ·)) - max(Q1(s, ·))|
s

 2 log(|A|) 

(18)

Thus,

||(T V1 )

-

(TmV2)||



||V1

-

V2||

+

4

log(|A|) 

(19)

As for the max operator, Tm is a contraction mapping, then from Banach fixed-point theorem we have TmV  = V 

Let Tbt denote the dynamic programming operator for a sequence of Boltzmann softmax operators, then we have

||Vt - V || = ||TbtV0 - Tmt V ||



||(Tbt-1V0

-

Tmt-1V

||

+

4

log(|A|) 

 ...



t||V0

-

V

||

+

4

log(|A|) 

t

 t-k

k=1

=

t||V0

-

V

||

+

4

log(|A|)(1 - (1 - )

t)

(20)

C CONVERGENCE OF DBS VALUE ITERATION

Theorem 2 (Convergence of value iteration with the DBS operator) For a sequence of dynamic Boltzmann softmax operator {t}, if t  , Vt converges to V , where Vt and V  denote the
value function after t iterations and the optimal value function.

Proof 3 Let Tt and Tm denote the dynamic programming operators for dynamic Boltzmann softmax operator boltzt and the max operator respectively, so (Tt V )(s) = boltzt (Q(s, ·)), (TmV )(s) = max(Q(s, ·)). Thus, we have

||(Tt V1) - (TmV2)||  ||(Tt V1) - (Tt V2)|| + ||(Tt V2) - (TmV2)||

(21)

(A) (B)

By similar techniques as in Theorem 6, we have

4 log(|A|)

||(Tt V1) - (TmV2)||  ||V1 - V2|| +

t

(22)

11

Under review as a conference paper at ICLR 2019

As the max operator Tm is a contraction mapping, then from Banach fixed-point theorem we have TmV  = V 

Let Tbt denote the dynamic programming operator for a sequence of dynamic Boltzmann softmax operators, so Tbt = Tt Tt-1 ...T1 , then we have

||Vt - V || = ||TbtV0 - Tmt V || = ||(Tt ...T1 )V0 - (Tm...Tm)V ||



||(Tt-1 ...T1 )V0

-

(Tm...Tm)V

||

+

4 log(|A|) t

(23)

 ...



t||V0

-

V

||

+

4

log(|A|)

t k=1

 t-k k

Since limk

1 k

=

0, we have that 

1

>

0, K

>

0,

such that k

>

K,

|

1 k

|

<

1. Thus,

t t-k K( 1) t-k

t t-k

=+

k=1 k

k=1 k

k=K( 1)+1 k



1

K( 1)
t-k +

min 

1

t

 t-k

k=1

k=K( 1)+1

1 t-K( 1)(1 - K( 1))

1(1 - t-K( 1))

= min 

1-

+ 1 1-

1 t-K( 1)

 1-

min  + 1

(24)

If t

>

log((

2 (1- )- log 

1) min )

+ K(

1) and

1<

2(1 - ), then

<t t-k
k=1 k

2.

So we obtain that  2 > 0, T > 0, such that t > T, |

t k=1

|t-k
k

<

2.

Thus, limt

t k=1

 t-k k

= 0.

Taking the limit of Equation (23), we have that

lim
t

||Vt+1

-

V

||



lim
t

t||V1

-

V

||

+

4

log(|A|)

t k=1

 t-k k

=0

(25)

D CONVERGENCE BOUND

Theorem 3 (Convergence rate of value iteration with the DBS operator) For any power series t = tp(p > 0), we have that for any  (0, min{0.25, ||R||-1}), after

max{O

log(

1

+log(

log(

1 

)

1 1-

)

),

O

(

1 (1-)

1
)p

} steps, the error ||Vt - V || 

.

Proof 4

t

 t-k kp

= t



-1 kp

-



-1 kp

k=1

k=1

k=t+1

= t Lip(-1) --(t+1) (-1, p, t + 1)

Polylogarithm

Lerch transcendent

(26)

By Ferreira & Lo´pez (2004), we have

Eqaution(26)



t

 -(t+1) -1 - 1

(t

1 + 1)p

=

1 (1 - )(t + 1)p

(27)

12

Under review as a conference paper at ICLR 2019

From Theorem 2 we have

||Vt

-

V

||



t||V1

-

V

||

+

(1

4 log(|A|) - )(t + 1)p



2

max{t||V1

-

V

||,

(1

4 log(|A|) - )(t + 1)p

}

(28)

Thus, for any

>

0,

after

at

most

t

=

max{

log(

1

)+log(

1 1-

)+log(||R||)+log(4)

log(

1 

)

,

8 log(|A|) (1-)

1
p - 1}

steps, we have ||Vt - V ||  .

E CONVERGENCE OF DBS Q-LEARNING

Theorem 4 (Convergence of DBS Q-learning) The Q-learning algorithm with dynamic Boltzmann softmax policy given by

Qt+1(st, at) = (1 - t(st, at))Qt(st, at) + t(st, at)[rt + boltzt (Qt(st+1, ·))] converges to the optimal Q(s, a) values if

(29)

1. The state and action spaces are finite. 2. t t(s, a) =  and t t2(s, a) <  3. limt t = 
4. Var(r(s, a)) is bounded.

Proof 5 Let t(s, a) = Qt(s, a) - Q(s, a) and Ft(s, a) = rt + boltzt (Qt(st+1, ·)) - Q(s, a)
Thus, from (29) we have t+1(s, a) = (1 - t(s, a))t(s, a) + t(s, a)Ft(s, a), which has the same form as the process defined in Lemma 2.

Next, we verify Ft(s, a) meets the required properties.

Ft(s, a) = rt + boltzt (Qt(st+1, ·)) - Q(s, a)

=

(rt

+



max
a+1

Qt(st+1,

at+1)

-

Q(s,

a))

+

(boltzt

(Qt(st+1,

·))

-

max
at+1

Qt(st+1,

at+1))

= Gt(s, a) + Ht(s, a)

(30)

For Gt, it is indeed the Ft function as that in Q-learning with static exploration parameters, which

satisfies

||E[Gt(s, a)]|Pt||w  ||t||w

(31)

For Ht, we have

|E[Ht(s, a)]| = 

p(s

|s,

a)[boltzt (Qt(s

,

·))

-

max
a

Qt(s

,

a

)]

s



msax[boltzt (Qt(s

,

·))

-

max
a

Qt(s

,

a

)]

  max
s

boltzt

(Qt(s

,

·))

-

max
a

Qt(s

,

a

)

=  max
s

(boltzt (Qt(s

,

·))

-

Lt (Qt(s

,

·)))

+

(Lt (Qt(s

,

·))

-

max
a

Qt(s

,

a

))

  max
s

|boltzt (Qt(s

,

·))

-

Lt (Qt(s

,

·))|

+

|Lt (Qt(s

,

·))

-

max
a

Qt(s

,

a

)|





log(|A|) t

+



max | 1 s t

log(
a

et Qt (s
A

,a

))

-

max Qt(s
a

,a

)|

 2 log(|A|) t

(32)

Let

ht

=

2

log(|A|) t

,

so

we

have

||E[Ft(s, a)]|Pt||w  ||t||w + ht,

where ht converges to 0

(33)

13

Under review as a conference paper at ICLR 2019

F IMPLEMENTATION DETAILS

The network architecture is the same as in (Mnih et al. (2015)). The input to the network is a raw pixel image, which is pre-processed into a size of 84 × 84 × 4. Table 2 summarizes the network
architecture.

LAYER
1st 2nd 3rd 4th output

TYPE
convolutional convolutional convolutional fully-connected fully-connected

CONFIGURATION
#filters=32, size=8 × 8, stride=4 #filters=64, size=4 × 4, stride=2 #filters=64, size=3 × 3, stride=1 #units=512 #units=#actions

ACTIVATION FUNCTION
ReLU ReLU ReLU ReLU --

Table 2: Network architecture.

14

Under review as a conference paper at ICLR 2019

G ATARI SCORES

GAMES
Alien Amidar Assault Asterix Asteroids Atlantis Bank Heist Battle Zone Beam Rider Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar H.E.R.O. Ice Hockey James Bond Kangaroo Krull Kung-Fu Master Montezumas Revenge Ms. Pac-Man Name This Game Pong Private Eye Q*Bert River Raid Road Runner Robotank Seaquest Space Invaders Star Gunner Tennis Time Pilot Tutankham Up and Down Venture Video Pinball Wizard Of Wor Zaxxon

DQN
20.18 56.73 780.99 50.03 1.38 1651.23 59.66 79.08 49.89 19.84 732.5 1332.64 25.86 80.81 399.15 659.59 545.45 84.72 163.77 104.05 17.15 395.37 9.44 65.14 76.86 270.09 241.6 639.28 114.78
0.0 41.81 102.76 113.88 0.18 97.46 38.27 504.66 636.08 13.8 101.55 559.34 232.26 78.38 36.3 84.74 13.73 12792.59 51.05 58.32

DBS-DQN 25.84 67.26
851.52 56.69 1.68
23211.93 81.08
103.46 55.04 27.72
730.25 1336.32
37.16 12.0 419.03 473.09 432.58 108.93 196.0 103.95 40.37 556.44 7.89 64.75 75.8 295.29 425.36 574.89 129.87 8.42 37.45 110.91 116.19 2.98 80.44 41.63 573.03 409.02 14.97 76.54 391.4 166.45 163.0 170.04 181.62 8.67 45791.36 54.68 62.15

Figure 5: Human normalized scores across all games, starting with 30 no-op actions.

15

