Under review as a conference paper at ICLR 2019
THE ANISOTROPIC NOISE IN STOCHASTIC GRADIENT DESCENT: ITS BEHAVIOR OF ESCAPING FROM MINIMA AND REGULARIZATION EFFECTS
Anonymous authors Paper under double-blind review
ABSTRACT
Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we theoretically study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We verify our understanding through comparing this anisotropic diffusion with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics) and other types of position-dependent noise.
1 INTRODUCTION
As a successful learning algorithm, stochastic gradient descent (SGD) was originally adopted for dealing with the computational bottleneck of training neural networks with large-scale datasets (Bottou, 1991). Its empirical efficiency and effectiveness have attracted lots of attention. And thus, SGD and its variants have become standard workhorse for learning deep models. Besides the aspect of empirical efficiency, recently, researchers started to analyze the optimization behaviors of SGD and its impacts on generalization.
The optimization properties of SGD have been studied from various perspectives. The convergence behaviors of SGD for simple one hidden layer neural networks were investigated in (Li & Yuan, 2017; Brutzkus et al., 2017). In non-convex settings, the characterization of how SGD escapes from stationary points, including saddle points and local minima, was analyzed in (Daneshmand et al., 2018; Jin et al., 2017; Hu et al., 2017).
On the other hand, in the context of deep learning, researchers realized that the noise introduced by SGD impacts the generalization, thanks to the research on the phenomenon that training with a large batch could cause a significant drop of test accuracy (Keskar et al., 2017). Particularly, several works attempted to investigate how the magnitude of the noise influences the generalization during the process of SGD optimization, including the batch size and learning rate (Hoffer et al., 2017; Goyal et al., 2017; Chaudhari & Soatto, 2017; Jastrzebski et al., 2017). Another line of research interpreted SGD from a Bayesian perspective. In (Mandt et al., 2017; Chaudhari & Soatto, 2017), SGD was interpreted as performing variational inference, where certain entropic regularization involves to prevent overfitting. And the work (Smith & Le, 2018) tried to provide an understanding based on model evidence. These explanations are compatible with the flat/sharp minima argument (Hochreiter & Schmidhuber, 1997; Keskar et al., 2017), since Bayesian inference tends to targeting the region with large probability mass, corresponding to the flat minima.
However, when analyzing the optimization behavior and regularization effects of SGD, most of existing works only assume the noise covariance of SGD is constant or upper bounded by some
1

Under review as a conference paper at ICLR 2019

constant, and what role the noise structure of stochastic gradient plays in optimization and generalization was rarely discussed in literature.
In this work, we theoretically study a general form of gradient-based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. By investigating this general dynamics, we analyze how the noise structure of SGD influences the escaping behavior from minima and its regularization effects. Several novel theoretical results and empirical justifications are made.
1. We derive a key indicator to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency;
2. We further justify that SGD in the context of deep neural networks satisfies these two conditions, and thus provide a plausible explanation why SGD can escape from sharp minima more efficiently, converging to flat minima with a higher probability. Moreover, these flat minima typically generalize well according to various works (Hochreiter & Schmidhuber, 1997; Keskar et al., 2017; Neyshabur et al., 2017; Wu et al., 2017). We also show that Langevin dynamics with well tuned isotropic noise cannot beat SGD, which further confirms the importance of noise structure of SGD;
3. A large number of experiments are designed systematically to justify our understanding on the behavior of the anisotropic diffusion of SGD. We compare SGD with full gradient descent with different types of diffusion noise, including isotropic and positiondependent/independent noise. All these comparisons demonstrate the effectiveness of anisotropic diffusion for good generalization in training deep networks.
The remaining of the paper is organized as follows. In Section 2, we introduce the background of SGD and a general form of optimization dynamics of interest. We then theoretically study the behaviors of escaping from minima in Ornstein-Uhlenbeck process in Section 3, and establish two conditions for characterizing the noise structure that affects the escaping efficiency. In Section 4, we show that the noise of SGD in the context of deep learning meets the two conditions, and thus explains its superior efficiency of escaping from sharp minima over other dynamics with isotropic noise. Various experiments are conducted for verifying our understanding in Section 5, and we conclude the paper in Section 6.

2 BACKGROUND

In general, supervised learning usually involves an optimization process of minimizing an empirical

loss over training data, L() := 1/N

N i=1

(f (xi; ), yi), where {(xi, yi)}Ni=1 denotes the training

set with N i.i.d. samples, the prediction function f is often parameterized by   RD, such as

deep neural networks. And (·, ·) is the loss function, such as mean squared error and cross entropy,

typically corresponding to certain negative log likelihood. Due to the over parameterization and

non-convexity of the loss function in deep networks, there exist multiple global minima, exhibiting

diverse generalization performance. We call those solutions generalizing well good solutions or

minima, and vice versa.

Gradient descent and its stochastic variants A typical approach to minimize the loss function is
gradient descent (GD), the dynamics of which in each iteration t is, t+1 = t - tg0(t), where g0(t) = L(t) denotes the full gradient and t denotes the learning rate. In non-convex op-
timization, a more useful kind of gradient based optimizers act like GD with an unbiased noise,
including gradient Langevin dynamics (GLD), t+1 = t - tg0(t) + t t, t  N (0, I), and stochastic gradient descent (SGD), during each iteration t of which, a minibatch of training samples
with size m are randomly selected, with index set Bt  {1, 2, . . . , N }, and a stochastic gradient is evaluated based on the chosen minibatch, g~(t) = iBt  (f (xi; t), yi)/m, which is an unbiased estimator of the full gradient g0(t). Then, the parameters are updated with some learning rate t as t+1 = t - tg~(t). Denote g() =  ((f (x; ), y), the gradient for loss with a single data point (x, y), and assume that the size of minibatch is large enough for the central limit theorem to

2

Under review as a conference paper at ICLR 2019

hold, and thus g~(t) follows a Gaussian distribution (Mandt et al., 2017; Li et al., 2017),

g~(t)  N

g0(t),

1 m

(t)

,

where

(t)



1 N

N

g(t; xi) - g0(t)

i=1

g(t; xi) - g0(t) T . (1)

Note that the covariance matrix  depends on the model architecture, dataset and the current param-

eter t. Now we can rewrite the update of SGD as,

t+1

= t - tg0(t) +

t m

t,

t  N 0, (t) .

(2)

Inspired by GLD and SGD, we may consider a general kind of optimization dynamics, namely, gradient descent with unbiased noise,

t+1 = t - tg0(t) + t t, t  N (0, t) .

(3)

For small enough constant learning rate t = , the above iteration in Eq. (3) can be treated as the numerical discretization of the following stochastic differential equation (Li et al., 2017; Jastrzebski
et al., 2017; Chaudhari & Soatto, 2017),

dt = -L(t) dt + t2t dWt.

(4)

Considering et al., 2017)

t2t as the coefficient of studied the influence of noise

noise term, magnitude

existing of SGD

works (Hoffer et al., 2017; on generalization, i.e. t2

Jastrzebski = /m.

In this work, we focus on studying the benefits of anisotropic structure of t in SGD helping escape

from minima by bridging the covariance matrix with the Hessian of the loss surface, and its implicit

regularization effects on generalization, especially in deep learning context. For the purpose of

eliminating the influence of the noise magnitude, we constrain it to be a constant when studying

different structures of noise covariance. The noise magnitude could be evaluated as the expectation

of the squared norm of the noise vector,

 E[( t

t

)T

 ( t

t)] = t2E[

T

] = t2 Tr E[

T ] = t2 Tr t.

(5)

Thus, we introduce the following constraint,

given time t, t2 Tr (t) is constant.

(6)

From the statistical physics point of view, Tr(t2t) characterizes the kinetic energy (Gardiner), thus it is natural to force the energy to be unchanging, otherwise it is trivial that the higher the energy

is, the less stable the system is.

For simplicity, we absorb t2 into t, denoting t2t as t. If not pointed out, the subscript t of matrix t is omitted to emphasize that we are fixing t and discussing the varying structure of .

3 THE BEHAVIORS OF ESCAPING FROM MINIMA IN ORNSTEIN-UHLENBECK
PROCESS

For a general loss function L() = EX X () (the expectation could be either population or empirical), where X denotes data example and  denoted parameters to be optimized, under suitable smoothness assumptions, the SDE associated with the gradient variant optimizer as shown in Eq. (4)
can be written as follows (Li et al., 2017; Jastrzebski et al., 2017; Chaudhari & Soatto, 2017; Hu
et al., 2017), with little abuse of notation,

1
dt = -L(t) dt + t2 dWt.

(7)

Let L0 = L(0) be one of the minimal values of L(), then for a fixed t small enough (such that

Lt - L0  0), Et [Lt - L0] characterizes the efficiency of  escaping from the minimum 0 of L().

It is natural to measure the escaping efficiency using E[Lt - L0] since it characterizes the increase

of the potential, i.e., the increase of the loss L. And also note that Lt - L0  0, for any  > 0,

the escaping probability P (Lt - L0  ) can be controlled by the expectation E[Lt - L0] since by

Markov's

inequality,

we

have

P (Lt

-

L0



)



E[Lt -L0 

]

.

3

Under review as a conference paper at ICLR 2019

Proposition 1 (Escaping efficiency for general process). For the process (7), provided mild smoothness assumptions, the escaping efficiency from the minimum 0 is,

E[Lt - L0] = -

t
E
0

LT L

+

t 0

1 2E

Tr(Htt)

dt,

(8)

where Ht denotes the Hessian of L(t) at t.

We provide the proof in Appendix, and the same for the other propositions.

The escaping efficiency for general processes is hard to analyze due to the intractableness of the

integral in Eq. (8). However, we may consider the second-order approximation locally near the

minima 0, where L()



L0

+

1 2

(

-

0)T H(

-

0).

Without losing generality, we suppose

0 = 0. Further, suppose that H is a positive definite matrix and the diffusion covariance t =  is

constant for t. Then the SDE (7) becomes an Ornstein-Uhlenbeck process,

dt

=

-H t

dt

+

1 2

dWt,

0 = 0.

(9)

Proposition 2 (Escaping efficiency of Ornstein-Uhlenbeck process). For Ornstein-Uhlenbeck process (9), with t small enough, the escaping efficiency from minimum 0 = 0 is,

E[Lt

-

L0]

=

1 4

Tr

I - e-2Ht 



t 2

Tr (H) .

(10)

Inspired by Proposition 1 and Proposition 2, we propose Tr (H) as an empirical indicator mea-
suring the efficiency for a stochastic process escaping from minima. Now we turn to analysis which kind of noise covariance structure  will benefit escaping sharp minima, under the constraint Eq. (6).

Firstly,

for

the

isotropic loss

surface,

i.e.,

H

=

I ,

the

escaping efficiency

is

E[Lt - L0]

=

t 2

Tr ,

which is invariant under the constraint that Tr  is constant (Eq. (6)). Thus it is only nontrivial to

study the impact of noise structure when the Hessian of loss surface is anisotropic.

Secondly, H and  being semi-positive definite, to achieve the maximum of Tr(H) under con-

straint (6),  corresponding

should be  = unit eigenvector

o(fTHr . )N·ote1tuh1aut T1th,ewrhanerke-1m1,autr1ixarethise

maximal eigenvalue and highly anisotropic. More

generally, the following Proposition 3 characterizes one kind of anisotropic noise significantly out-

performing isotropic noise in order of number of parameters D, given H is ill-conditioned.

Proposition 3 (The benefits of anisotropic noise). With semi-positive definite H and , assume

(1) H is ill-conditioned. Let 1  2  . . . ,  D  0 be the eigenvalues of H in descent order,

and for some constant k

D

and

d

>

1 2

,

1 > 0,

k+1, k+2, . . . , D < 1D-d,

(11)

(2)  is "aligned" with H. Let ui be the corresponding unit eigenvector of eigenvalue i, for some

projection coefficient a > 0,

u1T

u1



a1

Tr  Tr H

,

(12)

then we have the benefit of the anisotropic noise over the isotropic one in term of escaping efficiency,

which can be characterized by the follow ratio,

Tr (H) Tr(H¯ )

=

O

aD(2d-1)

,

(13)

where

¯

=

Tr  D

I

denotes

the

covariance

of

isotropic

noise,

to

meet

the

constraint

Eq.

(6).

To give some geometric intuitions on the left hand side of Eq. (12), let the maximal eigenvalue and
its corresponding unit eigenvector of  be 1, v1, then the right hand side has a lower bound as u1T u1  uT1 v11v1T u1 = 1 u1, v1 2. Thus if the maximal eigenvalues of H and  are aligned in proportion, 1/ Tr   a11/ Tr H, and the angle of their corresponding unit eigenvectors is close to zero, u1, v1  a2, the second condition Eq. (12) in Proposition 3 holds for a = a1a2.

4

Under review as a conference paper at ICLR 2019

Typically, in the scenario of modern deep neural networks, due to the over-parameterization, Hessian and the gradient covariance are usually ill-conditioned and anistropic near minima, as shown by (Sagun et al., 2017) and (Chaudhari & Soatto, 2017). Thus the first condition in Eq. (11) usually holds for deep neural networks, and we further justify it by experiments in Section 5.3. Therefore, in the following section, we turn to focus on how the gradient covariance, i.e. the covariance of SGD noise meets the second condition of Proposition 3 in the context of deep neural networks.

4 THE ANISOTROPIC NOISE OF SGD IN DEEP NETWORKS

In this section, we mainly investigate the anisotropic structure of gradient covariance in SGD, and explore its connection with the Hessian of loss surface.

Around the true parameter According to the classic statistical theory (Pawitan, 2001, Chap. 8), for population loss L() = EX (), with being the negative log likelihood, when evaluating at the true parameter , there is the exact equivalence between the Hessian H of the population loss and Fisher information matrix F ,

F () := EX [ () ()T ] = EX [2 ()] = 2L() =: H().

(14)

In practice, with the assumptions that the sample size N is large enough (i.e. indicating asymptotic behavior) and suitable smoothness conditions, when the current parameter t is not far from the ground truth, Fisher is close to Hessian. Thus we can obtain the following approximate equality
between gradient covariance and Hessian,

(t) = F (t) - LT (t)L(t)  F (t)  H(t).

The first approximation is due to the dominance of noise over the mean of gradient in the later stage of SGD optimization, which has been shown in (Shwartz-Ziv & Tishby, 2017). A similar experiment as (Shwartz-Ziv & Tishby, 2017) has been conducted to demonstrate this observation, which is left in Appendix due to the limit of space.

In the following, we theoretically characterize the closeness between  and H in the context of one hidden layer neural networks; and show that the gradient covariance introduced by SGD indeed has more benefits than isotropic one in term of escaping from minima, provided some assumptions.

One hidden layer neural network with fixed output layer parameters For binary classification neural network with one hidden layer in classic setups (with softmax and cross-entropy loss), we have following results to globally bound Fisher and Hessian with each other.
Proposition 4 (The relationship between Fisher and Hessian in one hidden layer neural network). Consider the binary classification problem with data {(xi, yi)}iI , y  {0, 1}, and typical (either population or empirical) loss as L() = E[  f (x; )], where f denotes the output of neural network, and  denotes the cross-entropy loss with softmax,

(f (x), y) = -

y log

1

ef (x) + ef(x)

+

(1

-

y) log

1

1 + ef(x)

, y  {0, 1}.

If: (1) the neural network f is with one hidden layer and piece-wise linear activation. And the parameters of output layer are fixed during training; (2) the optimization happens on a set U such that, f (x; )  (-C, C),   U, x, i.e., the output of the classifier is bounded during optimization.
Then, we have the following relationship between (either population or empirical) Fisher F and Hessian H almost everywhere:
e-C F () H() eC F ().
A B means that (B - A) is semi-positive definite.

There are a few remarks on Proposition 4. Firstly, as shown in (Brutzkus et al., 2017), the considered neural networks in Proposition 4 are non-convex and have multiple minima, and thus it is still nontrivial to consider the escaping from minima. Secondly, the Proposition 4 holds in both population and empirical sense, since the proof does not distinguish the two circumstances. Thirdly, the bound

5

Under review as a conference paper at ICLR 2019

between F and H holds "globally" in the set U where the output f is bounded, rather than merely around the true global minima as discussed previously.

By Proposition 4, the following relationship between gradient covariance and Hessian could be derived.

Proposition 5 (The relationship between gradient covariance and Hessian in one hidden layer neural network). Assume the conditions in Proposition 4 hold, then for some small  > 0 and for  close enough to minima  (local or global),

uT

u



e-2(C+)

Tr  Tr H

holds for any positive eigenvalue  and its corresponding unit eigenvector u of Hessian H.

(15)

As a direct corollary of Proposition 5, for such neural networks, the second condition Eq. (12) in Proposition 3 holds in a very loose sense.
Therefore, based on the discussion on population loss around the true parameters and one hidden layer neural network with fixed output layer parameters, given the ill-conditioning of H due to the over-parameterization of modern deep networks, according to Proposition 3, we can conclude the noise structure of SGD helps to escape from sharp minima much faster than the dynamics with isotropic noise, and converge to flatter solutions with a high probability. These flat minima typically generalize well (Hochreiter & Schmidhuber, 1997; Keskar et al., 2017; Neyshabur et al., 2017; Wu et al., 2017). Thus, we attribute such properties of SGD on its better generalization performance comparing to GD, GLD and other dynamics with isotropic noise (Hoffer et al., 2017; Goyal et al., 2017; Keskar et al., 2017).
In the following, we conduct a series of experiments systematically to verify our understanding on the behavior of escaping from minima and its regularization effects for different optimization dynamics.

5 EXPERIMENTS

To better understanding the behavior of anisotropic noise different from isotropic ones, we introduce dynamics with different kinds of noise structure to empirical study with, as shown on Table 1.

Table 1: Compared dynamics defined in Eq. (3). For GLD dynamic, GLD diagonal, GLD Hessian and GLD 1st eigvec(H), t are adjusted to make t t share the same expected norm as that of SGD. For GLD leading, t is same as in SGD. Note that GLD 1st eigvec(H) achieves the best escaping efficiency as our indicator suggested.

Noise t

Remarks

SGD
GLD constant GLD dynamic

t  N 0, stgd
t  N (0, I) t  N (0, I)

tsgd

is

defined

as

in

Eq.

(1),

and

t

=

t m

t is a tunable constant

t is adjusted to make t t share the same expected

norm as that of SGD

GLD diagonal

t  N 0, diag(t) The covariance diag(t) is the diagonal of the covariance of SGD noise.

GLD leading GLD Hessian

t  N 0, ~ t t  N 0, H~t

~ t =

k i=1

iviviT

.

i,

vi

are

the

first

k

leading

eigen-

values and corresponding eigenvalues of the covariance

of SGD noise, respectively. (A low rank approximation

of tsgd)

H~t is a low rank approximation of the Hessian matrix

of loss L() by its the first k leading eigenvalues and

corresponding eigenvalues.

GLD 1st eigven(H) t  N 0, 1u1uT1

1, u1 are the maximal eigenvalue and its corresponding unit eigenvector of the Hessian matrix of loss L(t).

5.1 TWO-DIMENSIONAL TOY EXAMPLE
We design a 2-D toy example L(w1, w2) with two basins, a small one and a large one, corresponding to a sharp and flat minima, (1, 1) and (-1, -1), respectively, both of which are global minima.

6

Under review as a conference paper at ICLR 2019

Please refer to Appendix for the detailed constructions. We initialize the dynamics of interest with the sharp minimum (w1, w2) = (1, 1), and run them to study their behaviors escaping from this sharp minimum.
To explicitly control the noise magnitude, we only conduct experiments on GD, GLD const, GLD diag, GLD leading (with k = 2 = D in Table 1, or in other words, the exactly covariance of SGD noise), GLD Hessian (k = 2) and GLD 1st eigven(H). And we adjust t in each dynamics to force their noise to share the same expected squared norm as defined in Eq. (6). Figure 1(a) shows the trajectories of the dynamics escaping from the sharp minimum (1, 1) towards the flat one (-1, -1), while Figure 1(b) presents the success rate of escaping for each dynamic during 100 repeated experiments.
As shown in Figure 1, GLD 1st eigvec(H) achieves the highest success rate, indicating the fastest escaping speed from the sharp minimum. The dynamics with anisotropic noise aligned with Hessian well, including GLD 1st eigvec(H), GLD Hessian and GLD leading, greatly outperform GD, GLD const with isotropic noise, and GLD diag with noise poorly aligned with Hessian. These experiments are consistent with our theoretical analysis on Ornstein-Uhlenbeck process shown Proposition 2 and 3, demonstrating the benefits of anisotropic noise for escaping from sharp minima.

w2 7.500
success rate (%) Tr(Ht t)

3.000 4.500

1.5 1.0

7.500 4.500

6.000 1.500

10.570.0590.0000

6.000

0.5 0.0

1.500

3.000
GD

GLD const

0.5 GLD diag GLD leading

1.0

GLD Hessian GLD 1st eigven(H)

w11.0 0.5 0.0 0.5 1.0 1.5

35 30 25 20 15 10 5 0 GD cGoLnDst dGiLaDg leGadLDing HeGsLsDian eGigLvDen1(sHt)

2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00
0

GLD const GLD diag GLD leading GLD Hessian GLD 1st eigven(H) 50 100 15i0ter2a0t0ion250 300 350 400

(a) (b) (c)

Figure 1: 2-D toy example. Compared dynamics are defined in Table 1, k = 2, t2 is tuned to keep noise of all dynamics sharing same expected squared norm, 0.01. All dynamics are run by 500 iterations with learning rate 0.005. (a) The trajectory of each compared dynamics for escaping from the sharp minimum in one run. (b) Success rate of arriving the flat solution in 100 repeated runs. (c) Tr(Htt) of compared dynamics in one run.

5.2 ONE HIDDEN LAYER NEURAL NETWORK WITH FIXED OUTPUT LAYER PARAMETERS

We empirically show that in one hidden layer neural net-

work with fixed output layer parameters, the anisotropic noise induced by SGD indeed helps escape from sharp minima more efficiently than isotropic noise. Three net-

102 101 100

Tr(Htt) Tr(Ht¯ t)
20 hidden nodes 200 hidden nodes 2,000 hidden nodes

works are trained to binary classify 1, 000 linearly separa-

10-1

ble two-dimensional points. The number of hidden nodes

for each network varies in {20, 200, 2000}. We plot the

empirical indicator Tr (H) in Figure 2. We can eas-

ily observe that as the increase of the number of hidden

nodes,

the

ratio

Tr(H )
Tr(H¯ )

is

enlarged

significantly,

which

is

consistent with the Eq. (13) described in Proposition 3.

10-2 10-3 10-4 10-5
0

5 10 15 20 25 30
iteration

Figure 2: One hidden layer neural net-

works with fixed output layer parameters.

5.3 PRACTICAL DATASETS

The solid and the dotted lines represent the value of Tr(H) and Tr(H¯ ), respec-

tively. The number of hidden nodes varies In this part, we conduct a series of experiments in real in {20, 200, 2000}, the results of which are deep learning scenarios to demonstrate the behavior of denoted in different colors.

SGD noise and its implicit regularization effects. We con-

struct a noisy training set based on FashionMNIST dataset1. Concretely, the training set consist of

1000 images with correct labels, and another 200 images with random labels. All the test data are

with clean labels. A small LeNet-like network is utilized such that the spectrum decomposition over

1https://github.com/zalandoresearch/fashion-mnist

7

Under review as a conference paper at ICLR 2019

gradient covariance matrix and Hessian matrix are computationally feasible. The network consists of two convolutional layers and two fully-connected layers, with 11, 330 parameters in total.
We firstly run the standard gradient decent for 3000 iterations to arrive at the parameters G D near the global minima with near zero training loss and 100% training accuracy, which are typically sharp minima that generalize poorly (Neyshabur et al., 2017). And then all other compared methods are initialized with G D and run for optimization with the same learning rate t = 0.07 and same batch size m = 20 (if needed) for fair comparison2.

Verification of SGD noise satisfying the conditions in Proposition 3 To see whether the noise of

SGD in real deep learning circumstance satisfies the two conditions in Proposition 3, we run SGD optimizer initialized from G D, i.e. the sharp minima found by GD.

Figure 3(a) shows the first 400 eigenvalues of Hessian at G D, from which we see that the

140th eigenvalue has already decayed to about 1% of the first eigenvalue. Note that Hessian

H  RD×D, D = 11330, thus H around G D approximately meets the ill-conditioning require-

ment in Proposition 3.

Figure 3(b) shows the projection coefficient estimated by a^

=

uT1 u1 Tr H 1 Tr 

along the trajectory of SGD. The plot indicates that the projection coefficient is in a descent scale

comparing to D2d-1, thus satisfying the second condition in Proposition 3. Therefore, Proposition 3

ensures that SGD would escape from minima G D faster than GLD in order of O(D2d-1), as shown

in Figure 3(c). An interesting observation is that in the later stage of SGD optimization, Tr(H) be-

comes significantly (107 times) smaller than in the beginning stage, implying that SGD has already

converged to minima being almost impossible to escape from. This phenomenon demonstrates the

reasonability to employ Tr(H) as an empirical indicator for escaping efficiency.

101 eigenvalue spectrum at iter 3000
100

5 Estimation of projection coefficient
4
3

103 101 10-1

Tr(Htt) Tr(Ht¯ t)

eigenvalue

10 1

2 10-3
10-5
1

10 2
0 50 o10r0der15o0 f e2i0g0 en2v50alu3e0s0 350 400

iteration0 0 2000 4000 6000 8000 10000 12000 14000

10-7 0

2000

4000

6000

8000 10000 12000 14000

iteration

(a) (b) (c)

Figure 3: FashionMNIST experiments. (a) The first 400 eigenvalues of Hessian at G D, the sharp minima

found by GD after 3000 iterations.

(b) The projection coefficient estimation a^

=

,uT1 u1 Tr H 1 Tr 

as

shown

in

Proposition 3.

(c)Tr(Htt) versus Tr(Ht¯ t) during SGD optimization initialized from G D, ¯ t

=

Tr t D

I

denotes the isotropic noise with same expected squared norm as SGD noise.

Behaviors of different dynamics escaping from minima and its generalization effects To com-

pare the different dynamics on escaping behaviors and generalization performance, we run dynamics

ifnoiltlioawlisz.edThfreomhytpheerpsahraarmp emteinr im2afoGr DGLfoDuncdonbsyt

GD. The settings has already been

for each tuned as

compared method are optimal ( = 0.001)

as by

grid search. For GLD leading, we set k = 20 for comprising the computational cost and approxi-

mation accuracy. As for GLD Hessian, to reduce the expensive evaluation of such a huge Hessian in

each iteration, we set k = 20 and update the Hessian every 10 iterations. We adjust t in GLD dynamic, GLD Hessian and GLD 1st eigvec(H) to guarantee that they share the same expected squred

noise norm defined in Eq. (6) as that of SGD. And we measure the expected sharpness of different

minima as EN (0,2I) L( + ) - L(), as defined in ((Neyshabur et al., 2017), Eq.(7)). The results are shown in Figure 4.

As shown in Figure 4, SGD, GLD 1st eigvec(H), GLD leading and GLD Hessian successfully escape from the sharp minima found by GD, while GLD, GLD dynamic and GLD diag are trapped in the minima. This demonstrates that the methods with anisotropic noise "aligned" with loss curvature can help to find flatter minima that generalize well.

We also provide experiments on standard CIFAR-10 with VGG11 in Appendix.

2In fact, in our experiment, we test the equally spacing learning rates in the range [0.01, 0.1], and the final results are consistent with each other.

8

Under review as a conference paper at ICLR 2019

train accuracy (%) test accuracy (%) expected sharpness

100

95

90

GD GLD const

85

GLD dynamic GLD diag

80 GLD leading

GLD Hessian

75 GLD 1st eigven(H)

SGD

iteration70 0 2000 4000 6000 8000 10000 12000 14000

75

70

65 GD

60

GLD const GLD dynamic

55 GLD diag

GLD leading

50 GLD Hessian

45

GLD 1st eigven(H) SGD

iteration40 0 2000 4000 6000 8000 10000 12000 14000

0.030

GD

0.025

GLD const GLD dynamic

0.020

GLD diag GLD leading

0.015

GLD Hessian GLD 1st eigven(H)

0.010 SGD

0.005

iteration0.000 0 2000 4000 6000 8000 10000 12000 14000 16000

(a) (b) (c)

Figure 4: FashionMNIST experiments. Compared dynamics are initialized at G D found by GD, marked by the vertical dashed line in iteration 3000. The learning rate is same for all the compared methods, t = 0.07,
and batch size m = 20. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration. (c) Expected sharpness versus iteration. Expected sharpness is measured as EN (0,2I) L( + ) - L(), and  = 0.01, the expectation is computed by average on 1000 times sampling.

6 CONCLUSION

We theoretically investigate a general optimization dynamics with unbiased noise, which unifies various existing optimization methods, including SGD. We provide some novel results on the behaviors of escaping from minima and its regularization effects. A novel indicator is derived for characterizing the escaping efficiency. Based on this indicator, two conditions are constructed for showing what type of noise structure is superior to isotropic noise in term of escaping. We then analyze the noise structure of SGD in deep learning and find that it indeed satisfies the two conditions, thus explaining the widely know observation that SGD can escape from sharp minima efficiently toward flat minina that generalize well. Various experimental evidence supports our arguments on the behavior of SGD and its effects on generalization. Our study also shows that isotropic noise helps little for escaping from sharp minima, due to the highly anisotropic nature of landscape. This indicates that it is not sufficient to analyze SGD by treating it as an isotropic diffusion over landscape (Zhang et al., 2017; Mou et al., 2017). A better understanding of this out-of-equilibrium behavior (Chaudhari & Soatto, 2017) is on demand.

REFERENCES
Léon Bottou. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nimes, 91(8), 1991.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. arXiv preprint arXiv:1710.11029, 2017.
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with stochastic gradients. arXiv preprint arXiv:1803.05999, 2018.
C Gardiner. Stochastic methods: a handbook for the natural and social sciences 4th ed.(2009).
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1­42, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems 30, pp. 1729­1739. 2017.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.

9

Under review as a conference paper at ICLR 2019

Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In In International Conference on Learning Representations (ICLR), 2017.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic gradient algorithms. In International Conference on Machine Learning, pp. 2101­2110, 2017.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. arXiv preprint arXiv:1704.04289, 2017.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for nonconvex learning: Two theoretical viewpoints. arXiv preprint arXiv:1707.05947, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems 30, pp. 5949­5958. 2017.
Bernt Øksendal. Stochastic differential equations. In Stochastic differential equations, pp. 65­84. Springer, 2003.
Yudi Pawitan. In all likelihood: statistical modelling and inference using likelihood. Oxford University Press, 2001.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.
Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient descent. International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=BJij4yg0Z.
Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient langevin dynamics. arXiv preprint arXiv:1702.05575, 2017.

A PROOFS OF PROPOSITIONS IN MAIN PAPER

A.1 PROOF OF PROPOSITION 1

Proof. The "mild smoothness assumptions" refers that Lt = L(t)  C2. Then the Ito's lemma holds (Øksendal, 2003).
And by Ito's lemma, the SDE of Lt is

dLt =

-LT L

+

1 2

Tr

11
t2 Htt2

dt

+

LT

1
t2

dWt

=

-LT

L

+

1 2

Tr

(Htt)

dt

+

LT

1
t2

dWt.

10

Under review as a conference paper at ICLR 2019

Taking expectation with respect to the distribution of t,

dELt = E

-LT

L

+

1 2

Tr(Htt)

dt,

for the expectation of Brownian motion is zero. Thus the solution of EYt is,

ELt = L0 -

t
E
0

LT L

+

t 0

1 2E

Tr(Htt)

dt.

(16)

A.2 PROOF OF PROPOSITION 2
Proof. Without losing generality, we assume that L0 = 0. For multivariate Ornstein-Uhlenbeck process, when 0 = 0 is an constant, t follows a multivariate Gaussian distribution (Øksendal, 2003). Consider change of variables   (, t) = eHtt. Here, for symmetric matrix A,
eA := U diag(e1 , . . . , en )U,
where 1, . . . , n and U are the eigenvalues and eigenvector matrix of A. Note that with this notation,
deHt = HeHt. dt

Applying Ito's lemma, we have

d(t,

t)

=

eH

t



1 2

dWt,

which we can integrate form 0 to t to get

t

t = 0 +

eH

(s-t)



1 2

dWs

0

The expectation of t is zero. And by Ito's isometry (Øksendal, 2003), the covariance of t is,



t

EttT = E 

eH

(s-t)



1 2

dWs

0

t

eH

(r-t)



1 2

dWr

0

T 

=E

t

eH(s-t)

1 2



1 2

eH (s-t)

ds

0

=E

t
eH(s-t)eH(s-t) ds
0

t
= eH(s-t)eH(s-t) ds. (for H and  are both constant.)
0

11

Under review as a conference paper at ICLR 2019

Thus,

EL(t)

=

1 2

E

Tr

tT Ht

=

1 2

Tr

H Et tT

=

1 2

t
Tr
0

H eH(s-t)eH(s-t)

ds

=

1 2

t
Tr
0

eH(s-t)H eH(s-t)

ds

=

1 2

t
Tr
0

e2H(s-t)H 

ds

=

1 2

Tr

1 2

H

-1

I - e-2Ht H

=

1 4

Tr

I - e-2Ht  .

(for H is symmetric.)

The last approximation is by Taylor's expansion.

A.3 PROOF OF PROPOSITION 3

Proof. Firstly, Tr(H) has the decomposition as Tr(H) =

D i=1

i

uiT

ui

.

Secondly, compute Tr(H), Tr(H¯ ) respectively,

and bound their quotient,

Tr(H )



uT1

u1



a1

Tr  Tr H

,

Tr(H¯ )

=

Tr  D

Tr H,

Tr(H ) Tr(H¯ )



a1D (Tr H)2



a1D k1 + (D - k)D-d1

2

=O

aD2d-1

.

The proof is finished.

(17)

A.4 PROOF OF PROPOSITION 4

Proof. Firstly compute the gradients and Hessian of ,

 f

=

ef 1 + ef

-y

=

2 f2

=

(1

ef + ef

)2

.

ef 1+ef

>0

-

1 1+ef

<0

y = 0, y = 1.

And note the Gauss-Newton decomposition for functions with the form of L =   f ,

H

=

2
E(x,y)

((x,y);) 2

=

2
E(x,y) f 2

f 

fT 

+

E(x,y)

 f

2f 2

.

Since the output layer parameters for f is fixed and the activation functions are piece-wise linear,

f (x; ) is a piece-wise linear function on its parameters .

Therefore

2f 2

= 0,

a.e., and H

=

2
E(x,y) f 2

f 

fT 

.

12

Under review as a conference paper at ICLR 2019

It is easy to check that e-C

 f

2



2 f2



eC

 f

2
. Thus,

H

=

2
E(x,y) f 2

f 

fT 

E(x,y)eC

 f

2 f fT  

= E(x,y)eC

 f f 

 f f 

T = eC F.

H

=

2
E(x,y) f 2

f x

fT x

E(x,y)e-C

 f

2 f fT x x

= E(x,y)e-C

 f f 

 f f 

T = e-C F.

A.5 PROOF OF PROPOSITION 5

Proof. For simplicity, we define g :=  , g0 := L = E .
The gradient covariance and Fisher has the following relationship, F = Eg · gT = E(g0 + )(g0 + )T = g0g0T + E T = g0g0T + .
Applying Taylor's expansion to g0(), g0() = g0() + H()( - ) + o( - ) = H()( - ) + o( - ).

Hence,

g0()

2 2



H

2 2

 - 

2 2

+

o

 - 

2 2

=

H

2 2

 - 

2 2

+

o

 - 

2 2

.



Therefore, with the condition  -  2 

uT H

F
2

u

,

we

have

g0()

2 2



uT

F

u

+

o

||

.

Thus,

uT u Tr 

=

uT F u - uT g0g0T u Tr F - Tr(g0g0T )



uT F u -

g0

2 2

Tr F -

g0

2 2



uT F u - g0 Tr F

2 2

=

uT F u Tr F

1

-

g0

2 2

uT F u

 uT F u Tr F

1 -  - o ||



uT F u Tr F

e-2

,

for  small enough.

On the other hand, Proposition 4 indicates that e-C F H eC F , which means, u, uT (eC F - H)u  0

Thus

uT F u Tr F



uT (e-C H)u Tr(eC H)

.

and Tr(H - e-C F )  0.

Therefore, for , u being a positive eigenvalue and the corresponding unit eigenvector of H, we

have

uT F u Tr F



e-2C

 Tr H

uT u Tr 



uT F u Tr F

e-2



e-2(C+)

 Tr H

.

B ADDITIONAL EXPERIMENTS
B.1 DOMINANCE OF NOISE OVER GRADIENT
Figure 5 shows the comparison of gradient mean and the expected norm of noise during training using SGD. The dataset and model are same as the experiments of FashionMNIST in main paper, or as in Section C.2. From Figure 5, we see that in the later stage of SGD optimization, noise indeed dominates gradient. These experiments are implemented by TensorFlow 1.5.0.
13

Under review as a conference paper at ICLR 2019

100

10 1

10 2
10 3 0

gradient mean norm expected noise norm 2000 40it00eratio60n00

8000

10000

Figure 5: L2 norm of gradient mean, L(t) , and the expected norm of noise

tE[

T t

t]/m dur-

ing the training using SGD. The dataset and model are same as the experiments of FashionMNIST

in main paper, or as in Section C.2

B.2 THE FIRST 50 ITERATIONS OF FASHIONMNIST EXPERIMENTS IN MAIN PAPER
Figure 6 shows the first 50 iterations of FashionMNIST experiments in main paper. We observe that SGD, GLD 1st eigvec(H), GLD Hessian and GLD leading successfully escape from the sharp minima found by GD, while GLD diag, GLD dynamic, GLD const and GD do not.

train accuracy (%) test accuracy (%)

100 GD

GLD const

80

GLD dynamic GLD diag

60

GLD leading GLD Hessian

GLD 1st eigven(H)

40 SGD

20
0 10 i2t0eration30 40 50
(a)

70 GD 60 GLD const
GLD dynamic 50 GLD diag
GLD leading 40 GLD Hessian
GLD 1st eigven(H) 30 SGD
20
10
0 10 i2t0eration30 40 50
(b)

Figure 6: The fisrt 50 iterations of FashionMNIST experiments in main paper. Compared dynamics are initialized at G D found by GD. The learning rate is same for all the compared methods, t = 0.07, and batch size m = 20. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration.

These experiments are implemented by TensorFlow 1.5.0.

B.3 ADDITIONAL EXPERIMENTS ON STANDARD CIFAR-10 AND VGG11 Dataset Standard CIFAR-10 dataset without data augmentation.

Model Standard VGG11 network without any regularizations including dropout, batch normalization, weight decay, etc. The total number of parameters of this network is 9, 750, 922.
Training details Learning rates t = 0.05 are fixed for all optimizers, which is tuned for the best generalization performance of GD. The batch size of SGD is m = 100. The noise std of GLD constant is  = 10-3, which is tuned to best. Due to computational limitation, we only conduct experiments on GD, GLD const, GLD dynamic, GLD diag and SGD.

Estimation of Sharpness The sharpness are estimated by

1 M

M

L( + j) - L(),

j  N (0, 2I),

j=1

with M = 100 and  = 0.01.

Experiments Similar experiments are conducted as in main paper for CIFAR-10 and VGG11, as shown in Figure 7. The observations and conclusions consist with main paper.
These experiments are implemented by PyTorch 0.3.0.

14

Under review as a conference paper at ICLR 2019

train accuracy (%) test accuracy (%) expected sharpness

100

95

90

85 GD

80

GLD const GLD dynamic

75

GLD diag SGD

iteration70 0 2500 5000 7500 10000 12500 15000 17500

(a)

80

75

70

65 GD

60

GLD const GLD dynamic

55 GLD diag

SGD

iteration50 0 2500 5000 7500 10000 12500 15000 17500

(b)

3.0

2.5

2.0

1.5 GD

1.0

GLD const GLD dynamic

0.5 GLD diag

SGD

iteration0.0 0 2500 5000 7500 10000 12500 15000 17500

(c)

Figure 7: CIFAR-10 experiments. Compared dynamics are initialized at G D found by GD, marked by the vertical dashed line in iteration 3000. The learning rate is same for all the compared methods, t = 0.05, and
batch size m = 100. (a) Training accuracy versus iteration. (b) Test accuracy versus iteration. (c) Expected sharpness versus iteration. Expected sharpness is measured as EN (0,2I) L( + ) - L(), and  = 0.01, the expectation is computed by average on 100 times sampling. All observations consist with main paper.

C DETAILED SETUPS FOR EXPERIMENTS IN MAIN PAPER

C.1 TWO-DIMENSIONAL TOY EXAMPLE

Loss Surface where

The loss surface L(w1, w2) is constructed by,

s1 = w1 - 1 - x1,

s2 = w2 - 1 - x2,

(w1, w2; x1, x2) = min{10(s1 cos  - s2 sin )2

+ 100(s1 cos  + s2 sin )2, (w1 - x1 + 1)2 + (w2 - x2 + 1)2},

L(w1, w2)

=

1 N

N

(w1, w2; xk1 , xk2 ),

k=1



=

1 4

,

N = 100,

xk  N (0, ),

=

cos  - sin 

sin  cos 

.

Note that  is the inverse of the Hessian of the quadric form generalizeing the sharp minima. And the 3-dimensional plot of the loss surface is shown in Figure 8.

Hyperparameters All learning rates are equal to 0.005. All dynamics concerned are tuned to share the same expected square norm, 0.01. The number of iteration during one run is 500.
These experiments are implemented by PyTorch 0.3.0.

C.2 FASHIONMNIST WITH CORRUPTED LABELS
Dataset Our training set consists of 1200 examples randomly sampled from original FashionMNIST training set, and we further specify 200 of them with randomly wrong labels. The test set is same as the original FashionMNIST test set.

Model Network architecture:
input  conv1  max_pool  ReLU  conv2  max_pool
 ReLU  fc1  ReLU  fc2  output.
Both two convolutional layers use 5 × 5 kernels with 10 channels and no padding. The number of hidden units between fully connected layers are 50. The total number of parameters of this network are 11, 330.

15

Under review as a conference paper at ICLR 2019

12 10 8 loss 6 4 2

1.5

1.0

0.50.0 w1

0.5

1.0

1.5

1.51.00.05.00w.512.01.5

12 10 8 6 4 2

Figure 8: Constructed 2-dimensional surface in main paper.

Training details
· GD: Learning rate  = 0.1. We tuned the learning rate (in diffusion stage) in a wide range of {0.5, 0.2, 0.15, 0.1, 0.09, 0.08, . . . , 0.01} and no improvement on generalization.
· GLD constant: Learning rate  = 0.07, noise std  = 10-3. We tuned the noise std in range of {10-1, 10-2, 10-3, 10-4, 10-5} and no improvement on generalization.
· GLD dynamic: Learning rate  = 0.07.
· GLD diagnoal: Learning rate  = 0.07.
· GLD leading: Learning rate  = 0.07, number of leading eigenvalues k = 20, batchsize m = 20. We first randomly divide the training set into 60 mini batches containing 20 examples, and then use those minibatches to estimate covariance matrix.
· GLD Hessian: Learning rate  = 0.07, number of leading eigenvalues = 20, update frequence f = 10. Do to the limit of computational resources, we only update Hessian matrix every 10 iterations. But add Hessian generated noise every iteration. And to the same reason, we simplily set the coefficent of Hessian noise to Tr H/m Tr , to avoid extensively tuning of hyperparameter.
· GLD 1st eigvec(H): Learning rate  = 0.07, as for GLD Hessian, and we set the coefficient of noise to 1/m Tr , where 1 is the first eigenvalue of H.
· SGD: Learning rate  = 0.07, batchsize m = 20.

Estimation of Sharpness The sharpness are estimated by

1 M

M

L( + j) - L(),

j  N (0, 2I),

j=1

with M = 1, 000 and  = 0.01.

These experiments are implemented by TensorFlow 1.5.0.

16

