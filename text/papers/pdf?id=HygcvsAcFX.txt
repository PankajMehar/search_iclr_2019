Under review as a conference paper at ICLR 2019
OPTIMAL MARGIN DISTRIBUTION NETWORK
Anonymous authors Paper under double-blind review
ABSTRACT
Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PACBayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN model also outperforms the other three loss models in generalization task through limited training data.
1 INTRODUCTION
In the history of machine learning research, the large margin principle has played an important role in theoretical analysis of generalization ability, meanwhile, it also achieves remarkable practical results for classification (Cortes & Vapnik, 1995) and regression problems (Drucker et al., 1997). More than that, this powerful principle has been used to explain the empirical success of deep neural network. Bartlett et al. (2017) and Neyshabur et al. (2017) present a margin-based multi-class generalization bound for neural networks that scales with their margin-normalized spectral complexity using two different proving tools. Moreover, Arora et al. (2018) proposes a stronger generalization bounds for deep networks via a compression approach, which are orders of magnitude better in practice.
As for margin theory, Schapire et al. (1997) first introduces it to explain the phenomenon that AdaBoost seems resistant to overfitting problem. Two years later, Breiman (1999) indicates that the minimum margin is important to achieve a good performance. However, Reyzin & Schapire (2006) conjectures that the margin distribution, rather than the minimum margin, plays a key role in being empirically resistant to overfitting problem; this has been finally proved by Gao & Zhou (2013). In order to restrict the complexity of hypothesis space suitably, a possible way is to design a classifier to obtain optimal margin distribution.
Gao & Zhou (2013) proves that, to attain the optimal margin distribution, it is crucial to consider not only the margin mean but also the margin variance. Inspired by this idea, Zhang & Zhou (2016) proposes the optimal margin distribution machine (ODM) for binary classification, which optimizes the margin distribution through the first- and second-order statistics, i.e., maximizing the margin mean and minimizing the margin variance simultaneously. To expand this method to the multi-class classification problem, Zhang & Zhou (2017) presents a multi-class version of ODM.
Based on these recent works, we consider the expansion of the optimal margin distribution principle on deep neural networks. In this paper, we propose an optimal margin distribution loss for convolution neural networks, which is not only maximizing the margin mean but also minimizing the margin variance as ODM does. Moreover, we use the PAC-Bayesian framework to derive a novel generalization bound based on margin distribution. Comparing to the spectrally-normalized margin bounds of Bartlett et al. (2017) and Neyshabur et al. (2017), our generalization bound shows that we can restrict the capacity of the model by setting an appropriate ratio between the first-order statistic and the second-order statistic rather than trying to control the whole product of the spectral norms of each layer. And we empirically evaluate our loss function on deep network across different datasets
1

Under review as a conference paper at ICLR 2019

ODN Loss Function
2.0

Input

Activation Function 

Output

1.5

1.0

Fully Connected Network

ODN Loss

0.5

0.0 0r

r+

(a)

1 2

......

(b)



Figure 1: The optimal margin distribution loss function (a) and the structure of fully connected network with optimal margin distribution loss (b).

and model structures. Specifically, we consider the performance of these models in generalization task through limited training data.
Recently, many researchers try to explain the experimental success of deep neural network. One of the research direction is to explain why the deep learning does not have serious overfitting problem. Although several common techniques, such as dropout (Srivastava et al., 2014), batch normalization (Ioffe & Szegedy, 2015), and weight decay (Krogh & Hertz, 1992), do improve the generalization performance of the over-parameterized deep models, these techniques do not have a solid theoretical foundation to explain the corresponding effects. As for our optimal margin distribution loss, it has a generalization bound to prove that we can restrict the complexity of hypothesis space reasonably through searching appropriate statistics dependent on data distribution. In experimental section, we compare our optimal margin distribution loss with the baseline cross-entropy loss under different regularization methods.

2 OPTIMAL MARGIN DISTRIBUTION LOSS

Consider the classification problem with input domain X = {x| x  Rn} and output domain Y = {1, . . . , k}, we denote a labeled sample as z  (X , Y). Suppose we use a network generating a prediction score for the input vector x  X to class i, through a function fi : X  R, for i = 1, . . . , k. The predicted label is chosen by the class with maximal score, i.e. h(x) = arg maxi fi(x).
Define the decision boundary of each class pair {i, j} as:

Di,j := {x|fi(x) = fj(x)}

Constructed on this definition, the margin distance of a sample point x to the decision boundary Di,j is defined by the smallest translation of the sample point to establish the equation as:

|(x, i, j)| = min  2 s.t. fi(x + ) = fj(x + ).


We present an approximation to  by linearizing fi:

|(x, i, j)| =

|fi(x) - fj(x)| xfi(x) - xfj(x)

.
2

Naturally, this pairwise margin distance leads us to the following definition of the margin for a labeled sample z = (x, y):

h(x, y) =

fy(x) - maxi=y fi(x) xfy(x) - x maxi=y fi(x)

.
2

Therefore, the defined classifier h misclassifies (x, y) if and only if the margin is negative. Given a
hypothesis space HS of functions mapping X to Y, which can be learned by the fixed deep neural network through the training set S, our purpose is to find a way to learn a decision function h  HS such that the generalization error R(h) = E(x,y)(X ,Y)[1h(x)=y] is small.

2

Under review as a conference paper at ICLR 2019

In this work, we introduce a type of margin loss, and connect it to deep neural networks. The origin loss function has been specially adapted for accelerating the convergence velocity of networks by us as following definition:

 r--h

 

r-

r,,µ(h) = 0

 

µ(h

-r-)2

(r+)2

h  r -  r -  < h  r + 
h > r + 

(1)

Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance |r -  - h| when the margins of sample points satisfy h  r -  and a square loss increasing progressively with the margin distance | - r - | when the margins satisfy h  r + . Therefore, our margin loss function will enforce the tie which has zero loss to contain the sample points as many as possible. So the parameters of the classifier will be determined not only by the samples that are close to the decision boundary but also by the samples that are away from the decision boundary. In other words, our loss function is aimed at finding a decision boundary which is determined by the whole sample margin distribution, instead of the minority samples that have minimum margins. To verify superiority of the optimal margin distribution network, our paper verifies it both theoretically and empirically.

3 ANALYSIS

To present a new margin bound for our optimal margin distribution loss, some notations are needed.

Consider that the convolution neural networks can be regarded as a special structure of the fully

connected neural networks, we simplify the definition of the deep networks. Let fw(x) : X  Rk

be the function learned by a L-layer feed-forward network for the classification task with parameters

w = (W1, . . . , WL), thus fw(x) = WL(WL-1(. . . (W1x))), here  is the ReLU activation

function. Let fwi denote the output of layer i before activation and  be an upper bound on the

number of output and fwi (x) = Wi

units in each (fwi-1(x)).

layer. Recursively, Let · F , · 1 and

we ·

can redefine 2 denote the

the deep network: fw1 (x) = W1x Frobenius norm, the element-wise

1 norm and the spectral norm respectively.

In order to facilitate the theoretical derivation of our formula, we simplify the definition of the loss function:

Lr,(fw) = Pr
(x,y)(X ,Y)

fw,y(x) - maxi=y fw,i(x) xfw,y(x) - x maxi=y fw,i(x)



r

-

+ Pr

fw,y(x) - maxj=y fw,j (x)  r + 

(x,y)(X ,Y) xfw,y(x) - x maxj=y fw,j (x)

.

Specially, define the L0 as r =  and   , actually equal to the 0-1 loss. And let Lr,(fw) be the empirical estimate of the optimal margin distribution loss. So we will denote the expected risk
and the empirical risk as L0(fw) and L0(fw), which are bounded between 0 and 1.
In the PAC-Bayesian framework, one expresses the prior knowledge by defining a prior distribution over the hypothesis class. Following the Bayesian reasoning approach, the output of the learning algorithm is not necessarily a single hypothesis. Instead, the learning process defines a posterior probability over H, which we denote by Q. In the context of a supervised learning problem, where H contains functions from X to Y, one can think of Q as defining a randomized prediction rule. We consider the distribution Q which is learned from the training data of form fw+u, where u is a random variable whose distribution may also depend on the training data. Let P be a prior distribution over H that is independent of the training data, the PAC-Bayesian theorem states that with possibility at least 1 -  over the choice of an i.i.d. training set S = {z1, ..., zm} sampled according to (X , Y), for all distributions Q over H (even such that depend on S), we have McAllester (2003):

Eu[L0(fw+u)]  Eu[L0(fw+u)] +

DKL(w + u

P

)

+

ln

m 

2(m - 1)

.

(2)

3

Under review as a conference paper at ICLR 2019

Note that the left side of the inequality is based on fw+u. To derive a expected risk bound L0(fw) for a single predictor fw, we have to relate this PAC-Bayesian bound to the expected perturbed loss (Neyshabur et al., 2017). Based on the inequality 2, we introduce a perturbed restriction which is
related to the margin distribution (the margin mean r and margin variance ):

Lemma 1. Let fw(x) : X  Rk be any predictor with parameters w, and P be any distribution

on the parameters that is independent of the training data. Then, for any r >  > 0,  > 0, with

probability at least 1 -  over the training set of size m, for any w, and any random perturbation u

s.t. Pru

maxxX

|fw+u(x) - fw(x)|

<

r- 4



1 2

,

we

have:

L0(fw)  Lr,(fw) +

DKL(w

+u P) m-1

+

ln

3m 

.

In order to bound the change caused by perturbation, we have to bring in three definitions that are used to formalize error-resilience in Arora et al. (2018) as following:

Definition 1. (Layer Cushion). The layer cushion of layer i is defined to be largest number µi such that for any x  S:
µi Wi F (fwi-1(x)) 2  fwi (x) 2.

Intuitively, cushion considers how much smaller the output Wi(fwi-1(x)) is compared to the upper bound Wi 2 fwi-1(x) 2. However, for nonlinear operators the definition of error resilience is less clean. Let's denote M i,j : Rhi  Rhj the operator corresponding to the portion of the deep network from layer i to layer j, and by Ji,j its Jacobian. If infinitesimal noise is injected before level i then M i,j passes it like Ji,j, a linear operator. When the noise is small but not infinitesimal then
one hopes that we can still capture the local linear approximation of the nonlinear operator M by
define Interlayer Cushion:

Definition 2. (Interlayer Cushion). For any two layers i < j, we define the interlayer cushion µi,j, as the largest number such that for any x  S:

µi,j Jfi,wij(x) F (fwi-1(x)) 2  fwj (x) 2.

Furthermore, for any layer i we define the minimal interlayer cushion as µi = minijL µi,j =

min{

1 

,

minijL

µi,j

}.

The next condition qualifies a common appearance: if the input to the activations is well-distributed and the activations do not correlate with the magnitude of the input, then one would expect that on average, the effect of applying activations at any layer is to decrease the norm of the pre-activation vector by at most some small constant factor.

Definition 3. (Activation Contraction). The activation contraction c is defined as the smallest number such that for any layer i and any x  X :

(fwi (x)) 2 

fwi (x) 2 c

To guarantee that the perturbation of the random variable u will not cause a large change on the
output with high possibility, we need a perturbation bound to relate the change of output to the structure of the network and the prior distribution P over H. Fortunately, Neyshabur et al. (2017)
proved a restriction on the change of the output by norms of the parameter weights. In the following lemma, we preset our hyper-parameters r and , s.t. the parameter weights w  H satisfying fw(x) 2  r + , when fixing WL 2 = 1. Thus, we can bound this change in terms of the spectral norm of the layer and the presetting hyper-parameters:

Lemma 2. For any L > 0, let fw : X  Rk be a L-layer network. Then for any w  H satisfying

fw(x) 2  r + , and x  X , and any perturbation u = vec({Ui}iL=1) s.t.

Ui

2

1 L

Wi 2,

the change of the output of the network can be bounded as follows:

|fw+u(x) - fw(x)|2  O

L
c(r + )

Ui 2

i=1 Wi 2µiµi

.

4

Under review as a conference paper at ICLR 2019

The proof of this lemma is given in Appendix A. Eventually, we utilize all above bounding lemmas to derive the following new margin based generalization bound for our Optimal margin Distribution Network.
Theorem 1. (Generalization Bound). For any L, h > 0, let fw : X  Rk be a L-layer feed-forward network with ReLU activations. Then, for any , r,  > 0, with probability  1 -  over a training set of size m, for any w, we have:



L0 (fw )



Lr, (fw )

+

O

 



c2L2 ln(L)(r

+ )2 m(r

L

Wi

2 F

i=1

Wi

2 2

µi

µi

- )2

+ ln

Lm 

. 

The proof of this theorem is given in Appendix A.

Remark. Comparing with the spectral complexity in Bartlett et al. (2017):

Rw :=

L
Wi 2
i=1

L

Wi - Ii

2/3 2,1

3/2
,

i=1

Wi

2/3 2

(3)

which is dominated by the product of spectral norms across all, our margin bound is relevant to

r,  dependent on the margin distribution and µi and µi dependent on the network structure .

The product value in equation 3 is extremely large and is hard to control it, but the parameter in

our generalization bound is easy to restrict. Explicitly, the factor consisted of hyper-parameters

2

r+ r-

=

=1+

 r

1-

 r

2

2

2

1-

 r

-1

is a monotonicity increasing function with regard to the ratio

 r

.

Under

the

assumption

of

separability,

we

can

come

to

the

conclusion

that

smaller



and

larger

r

make the complexity smaller. Searching a suitable value of r and  for the specific data distribution

will lead us to a better generalization performance.

4 EXPERIMENT
In this section, we empirically evaluate the effectiveness of our optimal margin distribution loss on generalization tasks, comparing it with three other loss functions: cross-entropy loss, hinge loss, and soft hinge loss. We first compare them under limited training data situation, using only part of the MNIST dataset (LeCun et al., 1998) to train and evaluate the models deploying the four different losses, with the used data ratio ranging from 0.125% to 100%. Similar experiments are also performed on the legend CIFAR-10 dataset (Krizhevsky & Hinton, 2009). Then we compare them under different regularization situations, investigating the combination of optimal margin distribution loss with dropout and batch normalization. Finally, we visualize and compare the features learned by the deep learning model with the four lose functions as well as the margin distribution from those models.
4.1 EXPERIMENTAL SETUP
Regarding the deep learning model, we use the following combination of datasets and models: a simple deep convolutional network for MNIST, original Alexnet (Krizhevsky et al., 2012) for CIFAR-10. In terms of the implementation of optimal margin distribution loss, as shown in Section 2, there is a gradient term in the loss itself, which can make the computation expensive. To reduce computational cost, in the backpropagation step we considered the gradient term xfy(x) - x maxi=y fi(x) 2 as a constant, so that we recomputed the value of
xfy(x) - x maxi=y fi(x) 2 at every forward propagation step. Furthermore, since the denominator item could be too small, which would cause numerical problem, we added an with small value to the denominator so that clip the loss at some threshold.
For special hyperparameters, including the margin mean parameter and margin variance parameter for the ODN model, and margin parameter for hinge loss model, we performed hyperparameter searching. We held out 5000 samples of the training set as a validation set, and used the remaining samples to train models with different special hyperparameters values, on both the MNIST dataset and the CIFAR-10 dataset. As for the common hyperparameters, such as, learning rate and momentum, we set them as the default commonly used values in Pytorch for all the models. We chose

5

Under review as a conference paper at ICLR 2019

Test Accuracy Test Accuracy

1.00 xent

hinge

0.95

soft hinge ODN

0.90

Generalization

0.85

0.80

0.75

0.70 0.125 0.25 0.5

1 5 10 Fraction of Training Data (%)
(a)

20

0.8

xent hinge

soft hinge

0.7 ODN

Generalization

0.6

0.5

0.4

0.3

100

0.2 0.5

1

5 10 20 Fraction of Training Data (%)
(b)

100

Figure 2: Performance of selected MNIST (a) and CIFAR10 (b) models on generalization tasks.

batch stochastic gradient descent as the optimizer. Evaluated on the testing dataset, the baseline cross-entropy model achieves a test accuracy of 99.09%; the hinge loss model achieves 98.95% on MNIST dataset; the soft-hinge loss model achieves 99.14% and the ODN model achieves 99.16%. On the CIFAR-10 dataset, the baseline cross-entropy model trained on the remaining training samples achieves a test accuracy of 83.51%; the hinge loss model achieves 82.15%; the soft-hinge loss model achieves 81.96% and the ODN model achieves 84.61%.
4.2 LIMITED SMALL SAMPLE LEARNING
It is well-known that deep learning method is very data-hungry, which means that if the training data size decreases, the model's performance can decrease significantly. In reality, this disadvantage of deep learning method can restrict its application seriously since sufficient amount of data is not always available. On the other hand, one of the desirable property of optimal margin distribution loss based models is that it can generalize well even when the training data is insufficient because the optimal margin distribution loss can restrict the complexity of the hypothesis space suitably. To evaluate the performance of optimal margin distribution loss based models under insufficient training data setting, we randomly chose some fraction of the training set, in particular, from 100% of the training samples to 0.125% on the MNIST dataset, and from 100% of the training samples to 0.5% on the CIFAR-10 dataset, and train the models accordingly.
In Fig. 2, we show the test accuracies of cross-entropy, hinge, soft hinge, and optimal margin distribution loss based models trained on different fractions of the MNIST and CIFAR-10 dataset. As shown in the figure, the test accuracies of all these four models increase as the fraction of training samples increases. Obviously, the ODN models proposed by our paper outperform all the other models constantly across different datasets and different fractions. Furthermore, the less training data there are, the larger performance gain the ODN model can have. On the MNIST dataset, the optimal margin distribution loss based model outperforms cross-entropy loss model by around 4.95%, hinge loss model by around 6.84% and soft-hinge loss model by around 3.03% on the smallest training set which contains only 0.125% of the whole training samples. Similarly, The ODN model outperforms cross-entropy loss model by around 9.9%, hinge loss model by around 10.1%, and soft hinge loss model by 13.4% on the smallest CIFAR-10 dataset which contains only 0.5% of the whole training samples.
4.3 REGULARIZATION METHODS
We also compared our optimal margin distribution loss with the baseline cross-entropy loss under different regularization methods and different amounts of training data, whose results are shown in Table. 1. As suggested by Table. 1, our loss can outperform the baseline loss consistently across different situations, no matter whether dropout, batch normalization or all the CIFAR-10 dataset are used or not. Specifically, when the size scale of training samples is small (5% fraction of the CIFAR-10 training set), the advantage of our optimal margin distribution loss is more significant. Moreover, our optimal margin distribution loss can cooperate with batch normalization and dropout,
6

Under review as a conference paper at ICLR 2019

Table 1: Test accuracy of Alexnet on CIFAR-10 with different regularization methods and different fraction of training set.

Accuracy (%)

Batch Normalization

Xent

ODN

Non Batch Normalization

Xent

ODN

ALL DROPOUT

85.782 ± 0.198 87.644 ± 0.151 83.517 ± 0.322 84.643 ± 0.255

ALL NON DROPOUT 81.491 ± 0.143 86.233 ± 0.244 72.223 ± 1.284 76.793 ± 1.279

5% DROPOUT

61.955 ± 1.945 67.636 ± 1.633 50.747 ± 3.735 58.739 ± 1.348

5% NON DROPOUT 57.753 ± 2.228 64.173 ± 1.982 36.293 ± 4.872 47.056 ± 3.927

Hyper-param (r /  / µ)

- 30/0.7/0.1

- 1.2/0.7/0.1

Table 2: Variance decomposition of selected MNIST models on embedding space.

Models
Inter Class V ar Intra Class V ar
ratio

Xent
522 11200 21.45

Training data

Hinge Soft Hinge

529 11092 20.96

466 14128 30.32

ODN
190 16469 86.68

Xent
831 13007 15.65

Test data

Hinge Soft Hinge

811 12986 16.01

854 11362 13.3

ODN
649 13955 21.5

achieving the best performance in Table.1, which is shown in bold red text. Unlike dropout and

batch normalization which are lack of solid theory ground, our optimal margin distribution loss has

the

margin

bound

,

which

guides

us

to

find

the

suitable

ratio

 r

to

restrict

the

capacity

of

models

and

alleviate the overfitting problem efficiently.

4.4 DATA VISUALIZATION
Since the performance of the ODN models is excellent, we hope to see that the distributions of data in the learned feature space (the last hidden layer) are consistent with the generalization results. In this experiment, we use t-SNE method to visualize the data distribution on the last hidden layer for training samples and test samples. Fig. 3 plots the 2-dimension embedding image on limited MNIST and CIFAR-10 dataset, which is only 1% of the whole training samples. t-SNE (Maaten & Hinton, 2008) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.
Consistently, we can find that the result of our ODN model is better than all the others, the distribution of the samples which has the same label is more compact. To quantify the degree of compactness of the distribution, we perform a variance decomposition on the data in the embedding space. By comparing the ratio of the intra-class variance to the inter-class variance in Table. 2 and Table. 3, we can know that our optimal margin distribution loss alway attain the most compact distribution in these four loss functions.
Moreover, the visualization result is consistent with the margin distribution of these four models in Fig. 4, which means getting an optimal margin distribution is helpful to deriving a good learned features space. And that representation features space can further alleviate the overfitting problem of deep learning. Hence, the optimal margin distribution loss function can significantly outperforms the other loss functions in generalization task through limited training data.
7

Under review as a conference paper at ICLR 2019

4 2 0 2 4
64
4 2 0 2 4 6
8

Cross-entropy Loss

20

2

Soft Hinge Loss

4

642

6 0

Hinge Loss 4 2 0 2 4
4 20 2 4 6 ODN Loss
4 2 0 2 4
8 6 4 20 2

15 10 5 0 5 10 15
15
20 15 10 5 0 5 10 15
15

Cross-entropy Loss

10 5 0

5

Soft Hinge Loss

10

10 5 0

5 10

15 Hinge Loss

10

5

0

5

10

15 15 10 5 0 5 10 ODN Loss
20

15

10

5

0

5

10

15 15 10 5 0

5 10 15

(a)

Cross-entropy Loss
4
2
0
2
4 6 4 20 2 4 Soft Hinge Loss
3 2 1 0 1 2 3 4 5
4 20 2 4 6

Hinge Loss
2
0
2
4
4 20 2 4 6 ODN Loss
6 4 2 0 2 4
4 20 2 4

(b)

Cross-entropy Loss 10
5
0
5
10

10 5 0

5 10 15

Soft Hinge Loss

7.5

5.0

2.5

0.0

2.5

5.0

7.5

10.0 15

10

5

0

5 10 15

15 10 5 0 5 10 15
10.0

7.5

5.0

10 5 0 5 10
15

10

Hinge Loss 2.5 0.0 2.5
ODN Loss 50

5.0 7.5 5

10.0 10

(c) (d)

Figure 3: Learned features visualization of selected MNIST models on training set (a); Learned features visualization of selected CIFAR-10 models on training set (b); Learned features visualization of selected MNIST models on testing set (c); Learned features visualization of selected CIFAR-10 models on testing set (d).

PDF

8

Margin distribution

xent

7 hinge

6

soft hinge ODN

5

4

3

2

1

00.0 0.1 0.2 Ma0r.g3ins 0.4 0.5 0.6

Figure 4: Margin distribution of selected MNIST models.

8

Under review as a conference paper at ICLR 2019

Table 3: Variance decomposition of selected CIFAR-10 models on embedding space.

Models
Inter Class V ar Intra Class V ar
ratio

Xent
804 15692 19.52

Training data

Hinge Soft Hinge

713 9466 13.28

637 17546 27.55

ODN
193 13273 68.77

Xent
1993 7260 3.64

Test data

Hinge Soft Hinge

1429 4780 3.34

1917 5810 3.03

ODN
1279 5645 4.41

4.5 MARGIN DISTRIBUTIONS
Fig. 4 plots the kernel density estimates of margin distribution producted by cross-entropy loss, hinge loss, soft hinge loss and ODN models on dataset MNIST. As can be seen, our ODN model derives a large margin mean with a smallest margin variance in all these four models. By calculating the value of ratio between the margin mean and the margin standard deviation, we know that the ratio in our ODN model is 3.20 which is significantly larger than 2.38 in the cross-entropy loss, 2.35 in the hinge loss and 2.63 in the soft hinge loss. The distribution of our model becomes more "sharper", which prevents the instance with small margin, so our method can still perform well as the training data is limited, which is also consistent with the result in Fig. 2.
5 CONCLUSIONS
Recent studies disclose that maximizing the minimum margin for decision boundary does not necessarily lead to better generalization performance, and instead, it is crucial to optimize the margin distribution. However, the influence of margin distribution for deep networks still remains undiscussed. We propose ODN model trying to design a loss function which aims to control the ratio between the margin mean and the margin variance. Moreover, we present a theoretical analysis for our method, which confirms the significance of margin distribution in generalization performance. As for experiments, the results validate the superiority of our method in limited data problem. And our optimal margin distribution loss function can cooperate with batch normalization and dropout, achieving a better generalization performance.
ACKNOWLEDGMENTS
We are grateful to Yu Li and Kangle Zhao for discussions and helpful feedback on the manuscript.
REFERENCES
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Proceedings of the 35th International Conference on Machine Learning, pp. 254­263, 2018.
Peter L. Bartlett, Dylan J. Foster, and Matus J. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241­6250, 2017.
Leo Breiman. Prediction games and Arcing algorithms. Neural Computation, 11(7):1493­1517, 1999.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273­297, 1995.
Harris Drucker, Christopher J. C. Burges, Linda Kaufman, Alex J. Smola, and Vladimir Vapnik. Support vector regression machines. In Advances in Neural Information Processing Systems, pp. 155­161. MIT Press, 1997.
Wei Gao and Zhi-Hua Zhou. On the doubt about margin explanation of boosting. Artificial Intelligence, 203:1­18, 2013.
9

Under review as a conference paper at ICLR 2019
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, pp. 448­456, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 4, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097­1105. 2012.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pp. 950­957, 1992.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov):2579­2605, 2008.
David McAllester. Simplified PAC-Bayesian margin bounds. In Learning Theory and Kernel Machines, pp. 203­215. 2003.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks. CoRR, abs/1707.09564, 2017.
Lev Reyzin and Robert E. Schapire. How boosting the margin can also boost classifier complexity. In Proceedings of the 23rd International Conference on Machine Learning, pp. 753­760. ACM, 2006.
Robert E. Schapire, Yoav Freund, Peter Barlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In Proceedings of the Fourteenth International Conference on Machine Learning, pp. 322­330, 1997.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Teng Zhang and Zhi-Hua Zhou. Optimal margin distribution machine. CoRR, abs/1604.03348, 2016.
Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 4063­4071, 2017.
10

Under review as a conference paper at ICLR 2019

A PROOF OF LEMMA 2 AND THEOREM 1
Proof of Lemma 2.
Let fwL-i(·) = WL(WL-1 . . . Wi+1(·)) and fwi-1(·) = Wi-1(Wi-2(· · · W1(·))), we will write the network as fw = fwL-i(Wi(fwi-1(x))). If we just give ith layer parameter weights Wi a perturbation Ui, we can have following:

i 2 = = 

(Wi + Ui)(fwi-1(x)) - Wi(fwi-1(x)) 2

Ui(fwi-1(x)) 2

Ui 2 · fwi-1(x) 2

fwi (x) 2 ·

Ui 2 Wi 2

(4)

In the last Approximate equation in Equation. 4, we assume that the perturbation Ui is in the linear space span by Wi, therefore, the part of (fwi-1(x)) that is orthogonal to the space of Wi will

not affect the output of perturbation. In other word, we equal the projection on the linear space of

Wi + Ui with the one on the linear space of Wi, ie.

fwi-1(x) 2 =

.fwi (x) 2
Wi 2

M i,i+1(fwi (x) + i) - Jfi,wii+(x1)(fwi (x) + i) 2



Wi+1 2 i 2 

i 2 Wi+1fwi (x) 2 · fwi (x) 2

Wi+1 2 fwi (x) 2 Wi+1fwi (x) 2



i

2

Wi+1fwi (x) fwi (x) 2

2

·

Wi+1 2 fwi (x) 2 µi+1 Wi+1 F fwi (x)

2



i

2

Wi+1fwi (x) fwi (x) 2

2

· c Wi+1 2 µi+1 Wi+1 F

=

i

2

Wi+1fwi (x) fwi (x) 2

2

·c µi+1ri+1



Ui 2 Wi 2

fwi+1(x)

2·

c µi+1ri

Layer Cushion Activation Contraction

where ri+1 is the stable rank of layer i + 1, i.e.

Wi+1 Wi+1

F 2

.

Therefore

by

induction

method

we

have:

M i,j (fwi (x) + i) - Jfi,wij(x)(fwi (x) + i) 2 

Ui 2 Wi 2

fwj (x)

2

·

j k=i

c µk+1rk+1

=

O(

cj-i µj-i

)

11

Under review as a conference paper at ICLR 2019

Obviously, we can know that

M i,j (fwi (x) + i) - Jfi,wj(x)(fwi (x) + i)

2



O(

cd µd

),

when

d=j-i2

fwL-i(fwi (x) + i) - fwL-i(fwi (x))

= M i,L(fwi (x) + i) - M i,L(fwi (x)) 2

= M i,L(fwi (x) + i) - M i,L(fwi (x)) + Jfi,wiL(x)(i) - Jfi,wiL(x)(i) 2

 Jfi,wiL(x)(i) 2 + M i,L(fwi (x) + i) - M i,L(fwi (x)) - Jfi,wiL(x)(i) 2

 Jfi,wiL(x)(i) 2 + (M i,L - Jfi,wiL(x))(fwi (x) + i) 2 + (M i,L - Jfi,wiL(x))(fwi (x)) 2



Jfi,wiL(x)(i)

cL-i 2 + O( µL-i )



Jfi,wiL(x) F

Wi F

fwi-1(x)

cL-i 2 + O( µL-i )

 c Jfi,wiL(x)

F

Wi

F

fwi-1(x)

cL-i 2 + O( µL-i )

Activation Contraction

c µi

Jfi,wiL(x)

F

Wifwi-1(x)

2

+

O(

cL-i µL-i

)

c =
µi

Jfi,wiL(x)

F

fwi (x)

2

+

O(

cL-i µL-i

)



c µiµi

|fw

(x)|2

+

O(

cL-i µL-i

)

Layer Cushion Interlayer Cushion

 O (r + ) Ui 2 c Wi 2 µiµi

Suppose that all the perturbations Ui are independent from each other, so we can just add the influence linearly for union bound:

|fw+u(x) - fw(x)|2  O

L
c(r + )

Ui 2

i=1 Wi 2µiµi

.

Proof. of Theorem 1.

The proof involves chiefly two steps. In the first step we bound the maximum value of perturbation of parameters to satisfied the condition that the change of output restricted by hyper-parameter of margin r, using Lemma 2. In the second step we proof the final margin generalization bound through Lemma 1 with the value of KL term calculated based on the bound in the first step.

Let  =

1

L i=1

Wi 2

L

and consider a network structured by normalized weights Wi

=

 Wi

2 Wi.

Due to the homogeneity of the ReLU, we have that for feedforward networks with ReLU

activations fw = fw, so the empirical and expected loss is the same for w and w. Furthermore, we

can also get that

L i=1

Wi 2 =

L i=1

Wi

2 and

=Wi F
Wi 2

Wi F . Hence, we can just assume
Wi 2

that the spectral norm is equal across the layers, i.e. for any layer i, Wi 2 = .

When we choose the distribution of the prior P to be N (0, I), i.e. u  N (0, I), the problem is that we will set the parameter  according to , which can not depend on the learned predictor w

or its norm. Neyshabur et al. (2017) proposed a method that can avoid this block: they set  based

on an approximation  on a pre-determined grid. By formalizing this method, we can establish the

generalization

bound

for

all

w

for

which

|

- |



1 L

,

and

ensuring

that

each

relevant

value

of



is covered by some  on the grid.

12

Under review as a conference paper at ICLR 2019

Since u  N (0, 2I), we get the following bound for the spectral norm of Ui:

Pr

[

Ui

2



t]



2e-

t2 22

.

UiN (0,2I)

(5)

Taking

the

union

bound

over

layers,

with

probability



1 2

,

the

spectral

norm

of

each

layer

perturba-

tion Ui is bounded by  2 ln(4L). Plugging this into Lemma 2 we have that with probability



1 2

:

max |fw+u(x) - fw(x)|2  c(r + )
xX

Ui 2 

i

 eL(r + )~-1 2 ln(4L)  r -  4

We can derive  = r- from the above inequality. Naturally, we can calculate the
cL(r+)  ln(4L)
KL-diversity in Lemma 1 with the chosen distributions for P .

DKL(w + u

P) 

|w|2 22

O

c2

L2

ln(L)

(r (r

+ -

)2 )2

L

i=1

Wi

2 F

Wi

2 2

µi

µi

Hence,

for

any

~,

with

probability



1

-



and

for

all

w

such

that,

|~

-

|



1 L



,

we

have:



L0 (fw )



Lr, (fw )

+

O

 



c2L2 ln(L)(r

+ )2

L

Wi

2 F

i=1

Wi

2 2

µi

µi

(r - )2m

+ ln

Lm 

. 

This proof method based on PAC-Bayesian framework has been raised by Neyshabur et al. (2017), we use this convenient tool for proofing generalization bound with our loss function which can obtain the optimal margin distribution.

13

