Under review as a conference paper at ICLR 2019
LYAPUNOV-BASED SAFE POLICY OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
In many reinforcement learning applications, it is crucial that the agent interacts with the environment only through safe policies, i.e., policies that do not take the agent to certain undesirable situations. These problems are often formulated as a constrained Markov decision process (CMDP) in which the agent's goal is to optimize its main objective while not violating a number of safety constraints. In this paper, we propose safe policy optimization algorithms that are based on the Lyapunov approach to CMDPs, an approach that has well-established theoretical guarantees in control engineering. We first show how to generate a set of statedependent Lyapunov constraints from the original CMDP safety constraints. We then propose safe policy gradient algorithms that train a neural network policy using DDPG or PPO, while guaranteeing near-constraint satisfaction at every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the linearized Lyapunov constraints. Unlike the existing (safe) constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Furthermore, the action-projection version of our algorithms often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with CPO and the Lagrangian method on several high-dimensional continuous state and action simulated robot locomotion tasks, in which the agent must satisfy certain safety constraints while minimizing its expected cumulative cost.
1 INTRODUCTION
In many real-world reinforcement learning (RL) problems, the agent must satisfy a number of constraints while minimizing its expected cumulative cost. In particular, these constraints might be related to safety, i.e., the policy learned by the agent should not take it to certain undesirable parts of the state and/or action space. In some problems, it is not only important that the policy learned by the agent (that will be deployed) be safe (Amodei et al., 2016), but also crucial that all the policies generated during training (with which the agent interacts with the environment to collect samples) be safe (Achiam et al., 2017). For example, a robot should always avoid taking actions that irrevocably harm its hardware. We often formulate the agent's interaction with the environment in constrained sequential decision-making problems as a constrained Markov decision process (CMDP). CMDPs are extensions of MDPs in which, in addition to the original cost function, there exists a constraint cost function, whose expected cumulative value should remain bounded. The additional constraint cost function gives more flexibility to CMDPs in modeling problems with trajectorybased constraints compared to approaches that artificially re-cast the cost of MDPs to enforce the constraints (Regan & Boutilier, 2009). Under the CMDP framework, we consider a policy to be safe if it satisfies the (expected) cumulative cost constraints.
A common approach to solve CMDPs is to use the Lagrangian method (Altman, 1998; Geibel & Wysotzki, 2005) that augments the original objective function with a penalty on constraint violation and computes the saddle-point of the constrained policy optimization via primal-dual method (Chow et al., 2017). Although safety is ensured when the policy converges asymptotically, a major drawback of this approach is that it makes no guarantee with regards to the safety of the policies generated during training. To address this issue (safety during training), heuristic algorithms for policy search with safety constraints have been proposed (e.g., Uchibe & Doya 2007). While some of them work reasonably well in practice, none is theoretically grounded to generate constraint-satisfying policies. Achiam et al. (2017) recently proposed the constrained policy optimization (CPO) method that extends the trust-region policy optimization (TRPO) algorithm (Schulman et al., 2015a) to handle CMDP constraints. Although there is no theoretical guarantee that the policies generated by CPO are
1

Under review as a conference paper at ICLR 2019

all safe, the empirical results reported in the paper are promising in terms scalability, performance, and constraint satisfaction, both during training and after convergence. Moreover, CPO is based on a principled approach to generate constraint-satisfying policies, and not just on heuristics. However, the CPO methodology is closely connected to TRPO and it is not clear how it can be combined with other on-policy and off-policy RL algorithms.

Another recent approach to solve CMDPs, while remaining safe during training, is by Chow et al. (2018). This work is based on the notion of Lyapunov function that has a long history in control theory to analyze the stability of dynamical systems (Khalil, 1996; Neely, 2010). Using Lyapunov functions in RL was first studied by Perkins & Barto (2002), where they were used to guarantee closed-loop stability of an agent. In a recent work, Berkenkamp et al. (2017) used Lyapunov functions to guarantee that a model-based RL agent can be brought back to a "region of attraction" during exploration. Using the theoretical underpinnings of the Lyapunov approach, Chow et al. (2018) proposed two dynamic programming (DP) algorithms to solve a CMDP and provided theoretical analyses for the feasibility and performance of the resulted policies. The proposed algorithms correspond to the two celebrated DP algorithms: policy and value iteration. They then extended their algorithms to learning (the scenario in which the MDP model is unknown) and proposed RL algorithms that correspond to approximate policy and value iteration. However, since their algorithms are all value-function-based, applying them to continuous action problems is not straightforward.

In this paper, we extend the Lyapunov-based approach to solving CMDPs of Chow et al. (2018) to continuous action problems that play an important role in control and robotics. Our contributions can be summarized as follows: 1) We formulate the problem of safe RL as a CMDP and propose a Lyapunov-function-based policy optimization framework that can handle continuous action CMDPs. 2) By leveraging the theoretical underpinnings of the Lyapunov-based approach to CMDPs in Chow et al. (2018), we present two classes of safe policy optimization algorithms that can work with any standard policy gradient algorithm such as deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) and proximal policy optimization (PPO) (Schulman et al., 2017). The first class of our algorithms is based on constrained optimization w.r.t. policy parameter (similar to what is done in CPO). The second class hinges on the concept of a safety layer introduced by Dalal et al. (2018) and transforms the constrained policy optimization problem into an unconstrained one, by integrating the Lyapunov constraints into the policy network via safety-layer augmentation. 3) We evaluate our algorithms and compare them to two baselines, CPO (Achiam et al., 2017) and the Lagrangian method, on several robot locomotion tasks, in which the agent must satisfy certain safety constraints while minimizing its expected cumulative cost. Our results show that our algorithms outperform the baselines in terms of balancing the performance and constraint satisfaction at every policy update.

2 PRELIMINARIES

We consider the RL problem in which the agent's interaction with the environment is modeled as a Markov decision process (MDP). A MDP is a tuple (X , A, , c, P, x0), where X is the state space; A is the action space;   [0, 1) is the discounting factor; c(x, a)  [0, Cmax] is the immediate cost function (negative reward); P (·|x, a) is the transition probability distribution; and x0  X is the initial state. Our results easily generalize to random initial states and random costs, but for simplicity
we will focus on the case of deterministic initial state and immediate cost. In a more general set-
ting where cumulative constraints are taken into account, we define a constrained Markov decision
process (CMDP), which extends the MDP model by introducing additional costs and associated constraints. A CMDP is defined by (X , A, , c, d, P, x0, d0), where the components X , A, , c, P, x0 are the same as in the unconstrained MDP; d(x)  [0, Dmax] is the immediate constraint cost; and d0  R0 is an upper-bound on the expected cumulative (through time) constraint cost. To formalize the optimization problem associated with CMDPs, let  be the set of Markov stationary policies, i.e., (x) = (·|x) : X  R0s : a (a|x) = 1 for any state x  X . For notational convenience, at each state x  X , we define the generic Bellman operator w.r.t. policy    and generic
cost function h as T,h[V ](x) = a (a|x) h(x, a)+ x X P (x |x, a)V (x ) .

Given a policy    and an initial state x0, the expected cumulative cost is defined as C(x0) :=

E E

 t=0 t=0

 

t t

c(xt, at) | x0, d(xt) | x0, 

, is

and the

the safety constraint is defined as D(x0)  d0, where D(x0) := safety constraint function (expected cumulative constraint cost).

The goal in CMDPs is to solve the constrained optimization problem

  min C(x0) : D(x0)  d0 .


(1)

2

Under review as a conference paper at ICLR 2019

Note that Equation 1 may have no solution. Under the transient CMDP assumption, Theorem 8.1 in Altman (1999)) shows that if the feasibility set is non-empty, then there exists an optimal policy in the class of stationary Markovian policies . To motivate the CMDP formulation, we refer the reader to Chow et al. (2018), which presents two real-world examples in modeling safety using (i) the reachability constraint, and (ii) the constraint that limits the agent's visits to undesirable states.

3 A LYAPUNOV APPROACH FOR SOLVING CMDPS

In this section, we revisit the Lyapunov approach to solving CMDPs proposed by Chow et al.
(2018) and report the mathematical results that are important in developing our safe policy op-
timization algorithms. To start, without loss of generality, we assume that we have access to a
baseline feasible policy of Equation 1, B; i.e. B satisfies DB (x0)  d0. We define a set of Lyapunov functions w.r.t. initial state x0  X and constraint threshold d0 as LB (x0, d0) = {L : X  R0 : TB,d[L](x)  L(x), x  X ; L(x0)  d0}, and call the constraints in this feasibility set Lyapunov constraints. For any arbitrary Lyapunov function L  LB (x0, d0), we denote by FL(x) = (·|x)   : T,d[L](x)  L(x) the set of L-induced Markov stationary policies.
Since T,d is a contraction mapping (Bertsekas, 2005), any L-induced policy  has the property D(x) = limk Tk,d[L](x)  L(x), x  X . Together with the property that L(x0)  d0, they imply that any L-induced policy is a feasible policy of Equation 1. However, in general, the
set FL(x) does not necessarily contain an optimal policy of Equation 1, and thus it is necessary
to design a Lyapunov function (w.r.t. a baseline policy B) that provides this guarantee. In other words, the main goal is to construct a Lyapunov function L  LB (x0, d0) such that

L(x)  T,d[L](x), L(x0)  d0.

(2)

Chow et al. (2018) show in their Theorem 1 that 1) without loss of optimality, the Lyapunov function

can be expressed as L (x) := E

 t=0

t(d(xt)

+

(xt)) | B, x , where

(x)  0 is some auxil-

iary and

constraint cost uniformly upper-bounded by 2) if the baseline policy B satisfies

the(x)con:=ditio2nDmmaxaDxxTVX(

||B (x)

)(x)/(1 -   Dmax

), ·

min{(1 - )(d0 - DB (x0))/Dmax, Dmax - (1 - )D/Dmax + (1 - )D}, where D = maxxX max D(x) is the maximum constraint cost, then the Lyapunov function candidate L  also satisfies the properties of Equation 2, and thus, its induced feasible policy set FL 

contains an optimal policy. Furthermore, suppose that the distance between the baseline and

optimal policies can be estimated effectively. Using the set of L  -induced feasible policies and

noting that the safe Bellman operator T [V ](x) = minFL  (x) T,c[V ](x) is monotonic and contractive, one can show that T [V ](x) = V (x), x  X has a unique fixed point V , such that V (x0) is a solution of Equation 1, and an optimal policy can be constructed via greedification, i.e., (·|x)  arg minFL  (x) T,c[V ](x). This shows that under the above assumption, Equa-

tion 1 can be solved using standard dynamic programming (DP) algorithms. While this result

connects CMDP with Bellman's principle of assumption is challenging when a good estimate issue, Chow et al. (2018) propose to approximate

oofptDwimiTtahVlia(tny,a|u|vxeBirlii)fayriiysnngcootnwsahtvreaatiihlnaetbrcleo.sBtTo,swaatdhidsificrehessissttthhhiiess

largest auxiliary cost satisfying the Lyapunov condition L (x)  TB,d[L ](x), x  X and the safety condition L (x0)  d0. The intuition here is that the larger , the larger the set of policies FL . Thus, by choosing the largest such auxiliary cost, we hope to have a better chance of including the optimal policy  in the set of feasible policies. Specifically, is computed by solving the

following linear programming (LP) problem:

 arg max :X R0

xX

(x) : d0 - DB (x0)  1(x0)

(I - {P (x |x, B)}x,x X )-1

,

(3)

where 1(x0) represents a one-hot vector in which the non-zero element is located at x = x0. When B is a feasible policy, this problem has a non-empty solution. Furthermore, accord-

ing to the derivations in Chow et al. (2018), the maximizer of Equation 3 is an indicator func-

tion x

of the form arg minxX

(x) E

= (d0

 t=0

t

- DB (x0)) 1{xt = x} |

· 1{x = x0, B .

x}/E[

 t=0

t1{xt

They also show that

= x} | x0, B]  0, by further restricting

where (x) to

be a constant function, the maximizer is given by (x) = (1-)·(d0-DB (x0)), x  X . Using the

construction of the Lyapunov function L , Chow et al. (2018) propose the safe policy iteration (SPI)

algorithm (see Algorithm 1 in Appendix A) in which the Lyapunov function is updated via boot-

strapping, i.e., at each iteration L is recomputed using Equation 3 w.r.t. the current baseline policy.

This algorithm has the following properties: 1) Consistent Feasibility, i.e., if the current policy k is

3

Under review as a conference paper at ICLR 2019

feasible, then k+1 is also feasible; 2) Monotonic Policy Improvement, i.e., Ck+1 (x)  Ck (x) for any x  X ; and 3) Asymptotic Convergence. Despite all these nice properties, SPI is still a valuefunction-based algorithm, and thus it is not straightforward to use it in continuous action problems. The main reason is that the greedification step becomes an optimization problem over the continuous set of actions that is not necessarily easy to solve. In Section 4, we show how we use SPI and its nice properties to develop safe policy optimization algorithms that can handle continuous action problems. Our algorithms can be thought as combinations of DDPG or PPO (or any other on-policy or off-policy policy optimization algorithm) with a SPI-inspired critic that evaluates the policy and computes its corresponding Lyapunov function. The computed Lyapunov function is then used to guarantee safe policy update, i.e., the new policy is selected from a restricted set of safe policies defined by the Lyapunov function of the current policy.

4 SAFE POLICY GRADIENT ALGORITHMS WITH LYAPUNOV FUNCTIONS

Policy gradient (PG) algorithms optimize a policy end-to-end by computing sample estimates of
the gradient of the cumulative cost induced by the policy and then updating the policy in the
gradient direction. In general, stochastic policies that give a probability distribution over actions are parameterized by a -dimensional vector , so the space of policies can be written as
(·|x), x  X ,   R . Since in this setting a policy  is uniquely defined by its parameter vector , policy-dependent functions can be written as a function of  or , and they are used
interchangeably in this paper.

Recently there are two PG algorithms which have emerged as generally well-performing, namely
DDPG and PPO. These algorithms are widely used in many continuous control tasks. DDPG (Lil-
licrap et al., 2015) is an off-policy Q-learning style algorithm. A deterministic policy (x) and a Q-value approximator Q(x, a; ) are trained jointly. The Q-value approximator is trained to min-
imize Bellman errors with respect to ; i.e., it is trained to fit the true Q-value function satisfying Q(x, a) = c(x, a) +  x X P (x |x, a)Q(x , (x )). The policy  is then trained to optimize Q(x, (x); ) via chain-rule. The PPO algorithm we use is a penalty form of TRPO (Schulman et al., 2017) with an adaptive rule to tune the DKL penalty weight k. Specifically, PPO trains a Gaussian policy (x) according to the standard policy gradient objective augmented with a penalty on KL-divergence from a previous version of the policy; i.e., the penalty is of the form, DKL(,  ) = E[DKL( (·|xt)||(·|xt))|x0,  ], where  is a previous version of the policy.

In CMDPs, the presence of a constraint D (x0)  d0 may be naively incorporated into the standard forms of DDPG and PPO via the Lagrangian method. That is, one may trans-

form the constrained optimization problem to a penalty form, in which the constraint costs

d(x) are added to the task costs c(x, a). The resulting penalized form of the objective is

min max0 E

 t=0

c(xt

,

at)

+

d(xt

)|x0

,



- d0. In this form, both  and  must be op-

timized jointly to find a saddle-point of the objective. The optimization of  may be performed

by either DDPG or PPO on the augmented cost c(x, a) + d(x). The optimization of  may be

performed by stochastic gradient descent on xt taken from trajectories sampled according to .

Although the Lagrangian approach is easy to implement (see Appendix B for details), in practice it does not lead to safety in training. While the objective encourages finding a solution which is safe, any intermediate step in the optimization may lead to an unsafe policy. In contrast, the Lyapunov approaches we propose are guaranteed to return a safe policy, not only at convergence, but also during training. In the subsections below, we elaborate how to transform DDPG and PPO to their Lyapunov safe counterparts below:

 = arg min C (x0) subject to

((a|x) - B(a|x))QL(x, a)da  (x), x  X . (4)

 aA

where QL(x, a) = d(x) + (x) +  x P (x |x, a)L (x ) is the state-action Lyapunov function.
We will describe two approaches to incorporating Lyapunov constraints in PG: -projection and a-projection. In Section 4.1, we formulate the Lyapunov-based PG using constrained policy optimization (which we call -projection), and in Section 4.2 we show how the Lyapunov constraints can be embedded into the policy network via a safety layer (which we call a-projection). In Section A.1, we also discuss two practical techniques to further enforce safety during policy training.

4

Under review as a conference paper at ICLR 2019

4.1 CONSTRAINED OPTIMIZATION APPROACH TO LYAPUNOV-BASED PG
For demonstration purposes, we hereby show how one can perform constrained policy optimization with PPO and Lyapunov constraints. With almost identical machinery, this procedure can also be applied to DDPG. Consider the following constrained optimization at iteration k with semi-infinite dimensional Lyapunov constraints for policy update:

  arg min

s.t.

( - k), Exµk ,a Qk (x, a) + k ( - k), 2DKL(||k) |=k ·( - k) |=k ( - k), Ea QLk (x, a) |=k  (x), x  X ,

where µk is the -visiting distribution w.r.t. k , and k is the adaptive penalty weight of the DKL(||k) regularizer. Clearly if one updates the policy parameter by solving the above optimization, and if the approximation errors from neural network parameterizations of Qk and QLk and first-order Taylor series expansion are small, then safety may ensure safety during training. However, the presence of infinite-dimensional Lyapunov constraints makes solving the above optimization (in real-time) numerically intractable. To tackle the issue of infinite dimensionality, (without loss of optimality) we re-write the Lyapunov constraint in the following form:
maxxX ( - k), Ea QLk (x, a) |=k - (x)  0. This might still lead to numeri-
cal instability in gradient descent algorithms, because the max-operator in the constraint is nondifferentiable. Similar to the surrogate constraint used in TRPO (to transform the max DKL con-
straint into an average DKL constraint), a more numerically stable way is to approximate the Lyapunov constraint using the following average constraint surrogate:

(

-

k ),

1 M

M
 Ea

QLk (xi, a)

|=k

1 M M

(xi).

i=1

i=1

(5)

where N is the number of on-policy trajectories of k . In practice, if one adopts = (1 - )(d0 - Dk (x0)) from Section 4.1, then the linear term in Equation 5 can be sim-

plified as Ea QLk (xi, a) =  aA (a|x) log (a|x)QD,k (xi, a)da. On the

other hand, by setting M

=

O(1/(1 - )), the constraint threshold becomes

1 M

M i=1

(xi)



d0 - Dk (x0). Collectively, the average constraint surrogate in Equation 5 becomes ( -

k ),

1 M

M i=1



Ea

QD,k (xi, a)

|=k

 d0 - Dk (x0), which is equivalent to the con-

straint used in the CPO algorithm (see Section 6.1 in Achiam et al. (2017)). This draws the connec-

tion between CPO and Lyapunov-based PG with -projection.

The Lyapunov-based algorithms with -projection in constrained policy update is summarized by Algorithm 4 in Appendix A. In the experiment section, we denote the DDPG version and the PPO version of this algorithm by SDDPG and SPPO respectively.

4.2 EMBEDDING LYAPUNOV CONSTRAINTS INTO A SAFETY LAYER
Notice that the main contribution of the Lyapunov approach is to break down a trajectory-based constraint into a sequence of single-step, state dependent constraints. When the state space is infinite/continuous, it is counter-intuitive to directly enforce these Lyapunov constraints (instead of the original trajectory-based constraint) in the optimization w.r.t. policy parameter , because the feasibility set is characterized by infinite dimensional constraints. Rather, leveraging the ideas of a safety layer from Dalal et al. (2018) that was applied to single-step constraints, we propose a novel approach to embed the set of Lyapunov constraints into the policy network. In this way, one reformulates the CMDP problem into an unconstrained one, whose policy parameter  (of the augmented network) can then be optimized by any standard unconstrained PG algorithms. At every given state, the unconstrained action is first computed and is then passed through the safety layer, where a feasible action mapping is constructed by projecting the unconstrained actions onto the feasibility set w.r.t. the corresponding Lyapunov constraint. Therefore, safety during training w.r.t. the original CMDP problem is guaranteed by the Lyapunov theorem.

We hereby demonstrate how one can find a feasible action mapping using the safety layer with DDPG, whose role is to solve the following projection problem at given state x  X :

a(x)  arg min
a

1 2

a - ,unc(x)

2 : (a - B(x))

aQL(x, a) |a=B(x)

(x)

.

(6)

5

Under review as a conference paper at ICLR 2019

In the above optimization problem, ,unc is the unconstrained policy whose policy parameter is updated by standard DDPG, B is the current data-generation policy that is safe, and the left side of the constraint is the first-order Taylor series approximation of aA QL(x, (x)) - QL(x, B(x))da over actions, w.r.t. the action B(x) induced by the baseline policy. As in Section 4.1, since the auxiliary cost is state-dependent, one can readily find aQL(x, a) |a=B(x) by computing the gradient of the constraint action value function aQD(x, a) |a=B(x). Generally, the safety layer perturbs the unconstrained action as little as possible in the Euclidean norm in order to satisfy the
Lyapunov constraints. Notice that the objective function is positive-definite and quadratic and the
constraint approximation is linear. Therefore, we can find the global solution to this convex problem
effectively via an in-graph iterative QP-solver, such as the one from Amos & Kolter (2017). Further-
more, since the optimization in Equation 6 only has a single Lyapunov constraint, one can express a(x) using the following analytical solution.
Proposition 1. At any given state x  X , the solution to the optimization problem in Equation 6 has the following form: a(x) = ,unc(x) + (x)aQL(x, a) |a=B(x), where

(x) =

aQL(x, a) |a=B (x)

,unc(x) - (x) / aQL(x, a) |a=B (x)

aQL(x, a) |a=B (x)

.

+

The closed-form solution is essentially a linear projection of the unconstrained action ,unc(x) to the safe hyperplane characterized with slope aQL(x, a) |a=B(x) and intercept (x) = (1 - )(d0 - DB (x0)). Implementing a(x) is very simple; it only consists of several arithmetic operations such as matrix products and ReLU. Extending this closed-form solution to handle multiple constraints is possible, under the assumption of having at most one constraint active at a time.
In general, the safety layer approach can also be applied to policy gradient algorithms, such as PPO, that learn a non-deterministic policy. For example in the PPO case when the policy is parameterized with a Gaussian distribution, then one simply need to project both the mean and the standarddeviation vector, in order to obtain a feasible action probability. The Lyapunov-based algorithms with a-projection in safety layer is summarized by Algorithm 5 in Appendix A. In the experiment section, we denote the DDPG version and the PPO version of this algorithm by SDDPG-modular and SPPO-modular respectively.
5 EXPERIMENTS
We empirically validate the Lyapunov-based PG algorithms on several robot locomotion continuous control tasks. In these experiments we aim to address the following questions about our proposed algorithm: (i) How is the performance (in terms of cost and safety during training) of Lyapunovbased PG compared to other baseline methods such as CPO and the naive Lagrangian approach? (ii) In the presence of approximation errors in value functions and policies, how robust is Lyapunovbased PG algorithm with respect to constraint violations?
To understand the performance of these algorithms in terms of both cost and safety guarantees, we designed several interpretable experiments in simulated robot locomotion continuous control tasks, whose notions of safety are motivated by physical constraints. We consider three domains using the MuJoCo simulator (Todorov et al., 2012) with a variety of different agents: (i)HalfCheetah-Safe: The HalfCheetah agent is rewarded for running, but its speed is limited for stability and safety; (ii) Point-Circle: The Point agent is rewarded for running in a wide circle, but is constrained to stay within a safe region defined by |x|  xlim (Achiam et al., 2017); (iii) Point-Gather & Ant-Gather: The agent, which is either a Point or an Ant, is rewarded for collecting target objects in a terrain map, while being constrained to avoid bombs (Achiam et al., 2017).
Visualizations of these tasks as well as more detailed descriptions (immediate cost and constraint cost functions, constraint thresholds) are given in Appendix C. In these experiments there are three different agents: (1) a point-mass (X  R9, A  R2); an ant quadruped robot (X  R32, A  R8); (3) a half-cheetah (X  R18, A  R6). For all experiments, we use two neural networks with two hidden layers of size (100, 50) and ReLU activations to model the mean and log-variance of the Gaussian actor policy, and two neural networks with two hidden layers of size (200, 50) and tanh activations to model the critic and constraint critic. To build a low variance sample gradient estimate, we use GAE- (Schulman et al., 2015b) to estimate the advantage and constraint advantage functions, with a hyper-parameter   (0, 1) optimized by grid-search.

6

Under review as a conference paper at ICLR 2019

5.1 COMPARISON WITH UNCONSTRAINED PG, LAGRANGIAN APPROACH, AND CPO
For comparison purposes, we implement the naive Lagrangian approach. Details of this algorithm are available in Appendix B. For fair comparisons, we also optimize the Lagrangian using a natural policy gradient when needed. To understand the optimal unconstrained performance on these environments, we also include the learning performance of two state-of-the-art unconstrained reinforcement learning algorithms, namely DDPG (Lillicrap et al., 2015) and PPO (Schulman et al., 2017). In the PPO case, we also compare the performance of CPO (which is equivalent to Lyapunovbased PG with -projection) with Lyapunov-based PG (with a-projection) . Notice that the original implementation of CPO in Achiam et al. (2017) is based on TRPO (Schulman et al., 2015a). However since backtracking line-search in TRPO can be computationally expensive, and it may lead to conservative policy updates, without loss of generality we adopt the original construction of CPO to create a PPO counterpart of CPO (which coincides with SPPO) and use that as our baseline.

HalfCheetah-Safe, Return
120 100 80 60 40 20
0 20 40
0 10 20 30 40 50

HalfCheetah-Safe, Constraint 100 80 60 40 20
0 0 10 20 30 40 50

Point-Gather, Return 20 15 10 5 0 5 0 10 20 30 40

50

Point-Gather, Constraint 10 8 6 4 2 0 0 10 20 30 40 50

Figure 1: Results of various DDPG algorithms on safe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from DDPG (red), DDPG-Lagrangian (purple), SDDPG (blue), SDDPG-modular (green) on HalfCheetah-Safe and Point-Gather. We discover that the Lyapunovbased approaches can perform safe learning, despite the fact that the environment dynamics model and cost functions are not known, control actions are continuous, and deep function approximations are necessary.

HalfCheetah-Safe, Return
120 100 80 60 40 20
0 20 40
0 20 40 60 80

HalfCheetah-Safe, Constraint 100 80 60 40 20
0 0 20 40 60 80

Point-Gather, Return 16 14 12 10 8 6 4 2 0 2 0 20 40 60 80

Point-Gather, Constraint 10 8 6 4 2 0 0 20 40 60 80

Figure 2: Results of various PPO algorithms on safe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from PPO (red), PPO-Lagrangian (purple), SPPO (blue), SPPOmodular (green) on HalfCheetah-Safe and Point-Gather. Similar to Figure 1, the Lyapunov-based approaches can perform safe learning in the control tasks when function approximations on policies and value functions are necessary.

Learning curves for unconstrained DDPG, Lagrangian DDPG, SDDPG and SDDPG-modular are shown in Figure 1, and the learning curves for unconstrained PPO, Lagrangian PPO, SPPO and SPPO-modular are shown in Figure 6. Due to space restrictions, more results are included in Appendix C. From the comparison plots, one can clearly see that the unconstrained DDPG and PPO agents are constraint-violating in these environments. Although the Lagrangian approaches in DDPG and PPO converge to feasible policies with reasonably good performance, in accord with our earlier claims these algorithms cannot guarantee constraint satisfaction during training. Furthermore it is worth-noting that the Lagrangian approach can be sensitive to the initialization of the Lagrange multiplier 0. If 0 is too large, it would make policy updates overly conservative, while if 0 is too small then constraint violation will be more pronounced. By default, we initialize 0 = 10, which assumes no knowledge about the environment.
Generally in both DDPG and PPO experiments, the Lyapunov-based PG algorithms lead to more stable learning and constraint satisfaction than the Lagrangian approach. The Lyapunov approaches quickly stabilize the constraint cost to be below the threshold, while the constraint costs from the Lagrangian approach tend to jiggle around the threshold. In many cases the a-projection Lyapunovbased PG (SDDPG-modular, SPPO-modular) converges faster than the -projection counterpart (SDDPG, SPPO). This corroborates with the hypothesis that the a-projection approach is less conservative during policy updates than the -projection approach (which is what CPO is based on).

7

Under review as a conference paper at ICLR 2019

Finally, in most experiments (HalfCheetah, PointGather, and AntGather) the DDPG class of algorithms tends to have faster learning than the PPO counterpart. This is potentially due to the improved data-efficiency when using off-policy samples in PG updates. Although this benefit is not directly related to the addition of Lyapunov constraints, this also supports our claim that some Lyapunovbased safe PG algorithms (SDDPG, SDDPG-modular) can learn more effectively than CPO (which is analogous to SPPO).

5.2 CONSTRAINT VIOLATION DURING TRAINING
Due to function approximation errors in policies and value functions, in practice most safe learning algorithms including the Lyapunov-based PG methods may still take a bad step and lead to constraint violation. While methods like safeguard policy update and constraint tightening from Section A.1 might help to remedy this issue, it is still unclear how robust each algorithm is regarding constraint satisfaction during training. To study the degree of constraint violation in different PG algorithms, we show their constraint violation plots in Figure 3 for DDPG-based and PPO-based algorithms. To make the comparison fair, we apply both safeguard policy update and the constraint tightening to all safe PG algorithms. From the constraint violation plots, it is clear that the Lagrangian methods (both DDPG and PPO) violate safety constraints more often than the Lyapunov-based counterparts. In many cases the Lyapunov-based PG algorithms with a-projection have lower constraint violation than its -projection counterparts. We speculate this is due to the fact that the safety layer generates smoother gradient updates during end-to-end training.

HalfCheetah-Safe, DDPG 100 80 60 40 20
0 0 10 20 30 40 50

HalfCheetah-Safe, PPO 100 80 60 40 20
0 0 20 40 60 80

Point-Gather, DDPG
400 350 300 250 200 150 100 50
0 0 10 20 30 40

50

Point-Gather, PPO
400 350 300 250 200 150 100 50
0 0 20 40 60 80

Figure 3: Cumulative constraint violations of various PG algorithms on safe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from DDPG (red), DDPG-Lagrangian (purple), SDDPG (blue), SDDPG-modular (green), PPO (red), PPO-Lagrangian (purple), SPPO (blue), SPPO-modular (green) on HalfCheetah-Safe and Point-Gather. Compared with Lagrangian approach, we discover that the Lyapunov-based approaches generally have more stable and safe learning, which lead to lower cumulative constraint violations.
6 CONCLUSIONS
In this paper, we formulated the problem of safe RL as a CMDP and used the notion of Lyapunov function to develop policy optimization algorithms that learn policies that are both safe and have low expected cumulative cost. Our algorithms extend the Lyapunov-based approach to solving CMDPs of Chow et al. (2018) to continuous action problems. Our algorithms combine DDPG or PPO (or any other on-policy or off-policy policy optimization algorithm) with a critic that is inspired by the safe policy iteration (SPI) algorithm of Chow et al. (2018) and both evaluates the policy and computes its corresponding Lyapunov function. The computed Lyapunov function is then used to guarantee safe policy update. We can categorize our algorithms into two classes in terms of the way they perform safe policy update. The first class is based on constrained optimization w.r.t. policy parameter, similar to what is done in CPO. The second class relies on the safety layer concept (Dalal et al., 2018) that integrates the Lyapunov constraints into the policy network by adding an action projection layer to it. We evaluated our algorithms on four high-dimensional simulated robot locomotion tasks and compared them with CPO (Achiam et al., 2017) and the Lagrangian method in terms of minimizing the expected cumulative return and constraint violation during training. Our results indicate that our Lyapunov-based algorithms 1) achieve safe learning, 2) have better data-efficiency, and 3) can be more naturally integrated within the standard end-to-end differentiable policy gradient training pipeline. In general, our work is a step forward in deploying RL to real-world problems in which safety guarantees are of paramount importance.
Future work includes 1) developing more stable and safe learning algorithms by further exploiting the properties of Lyapunov functions, 2) exploring more efficient ways to include Lyapunov constraints in constrained policy optimization, and 3) applying the Lyapunov-based PG algorithms to real-world continuous control problems, particularly in robotics.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv preprint arXiv:1705.10528, 2017.
E. Altman. Constrained Markov decision processes with total cost criteria: Lagrangian approach and dual linear program. Mathematical methods of operations research, 48(3):387­417, 1998.
E. Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mane´. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
B. Amos and Z. Kolter. Optnet: Differentiable optimization as a layer in neural networks. arXiv preprint arXiv:1703.00443, 2017.
F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause. Safe model-based reinforcement learning with stability guarantees. In Advances in Neural Information Processing Systems, pp. 908­ 918, 2017.
D. Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
D. Bertsekas. Dynamic programming and optimal control, volume 1-2. Athena scientific Belmont, MA, 2005.
Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070­6120, 2017.
Y. Chow, O. Nachum, M. Ghavamzadeh, and E. Duenez-Guzman. A Lyapunov-based approach to safe reinforcement learning. In Accepted at NIPS, 2018.
G. Dalal, K. Dvijotham, M. Vecerik, T. Hester, C. Paduraru, and Y. Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
P. Geibel and F. Wysotzki. Risk-sensitive reinforcement learning applied to control under constraints. Journal of Artificial Intelligence Research, 24:81­108, 2005.
H. Khalil. Noninear systems. Prentice-Hall, New Jersey, 2(5):5­1, 1996.
T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
M. Neely. Stochastic network optimization with application to communication and queueing systems. Synthesis Lectures on Communication Networks, 3(1):1­211, 2010.
T. Perkins and A. Barto. Lyapunov design for safe reinforcement learning. Journal of Machine Learning Research, 3(Dec):803­832, 2002.
K. Regan and C. Boutilier. Regret-based reward elicitation for markov decision processes. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 444­ 451. AUAI Press, 2009.
T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015a.
J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
9

Under review as a conference paper at ICLR 2019 R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Proceedings of Advances in Neural Information Processing Systems 12, pp. 1057­1063, 2000. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012. E. Uchibe and K. Doya. Constrained reinforcement learning from intrinsic and extrinsic rewards. In International Conference on Development and Learning, pp. 163­168. IEEE, 2007.
10

Under review as a conference paper at ICLR 2019

A DETAILS OF THE SAFE POLICY GRADIENT ALGORITHMS
Algorithm 1 Safe Policy Iteration (SPI)
Input: Initial feasible policy 0; for k = 0, 1, 2, . . . do
Step 0: With b = k, evaluate the Lyapunov function L k , where k is a solution of Equation 3 Step 1: Evaluate the cost value function Vk (x) = Ck (x); Then update the policy by solving the following problem: k+1(·|x)  argminFL k (x) T,c[Vk ](x), x  X end for Return Final policy k

Algorithm 2 Trajectory-based Policy Gradient Algorithm for CMDP

Input: parameterized policy (·|·; )

Initialization: policy parameter  = 0, and the Lagrangian parameter  = 0

while TRUE do

for i = 0, 1, 2, . . . do

for j = 1, 2, . . . do

Generate N trajectories {j,i}jN=1 by starting at x0 and following the policy i.

end for  Update:

i+1

=

i

-

2,i

1 N

N
 log P(j,i)|=i

C(j,i) + iD(j,i)

j=1

 Update: i+1 =  i + 1,i
end for if {i} converges to max then
Set max  2max. else
return parameters , ,  and break end if end while

-

d0

+

1 N

N
D(j,i)

j=1

Algorithm 3 Actor-Critic Algorithms for CMDP
Input: Parameterized policy (·|·; ) and value function feature vector (·) Initialization: policy parameters  = 0; Lagrangian parameter  = 0; value function weight v = v0 while TRUE do
for k = 0, 1, 2, . . . do Sample ak  (·|xk; k); Ck (xk, ak) = C(xk, ak) + kD(xk, ak); xk+1  P (·|xk, ak); // AC Algorithm:

TD Error: Critic Update:
 Update:

k(vk) = Ck (xk, ak) + Vk (xk+1) - Vk (xk) vk+1 = vk + 3(k)k(vk)(xk) k+1 = k - 2(k) log (ak|xk) · k(vk)/1 - 

 Update: // NAC Algorithm:

k+1 = 

k + 1(k)

-

d0

+

1 N

N
D(j,i)

j=1

Critic Update: wk+1 = I - 3(k) log (ak|xk)|=k  log (ak|xk)|=k

(7) (8) (9)
(10)
wk

 Update: Other Updates:

+ 3(k)k(vk) log (ak|xk)|=k k+1 = k - 2(k)wk/1 - 
Follow from Eqs. 7, 8, and 10.

(11) (12)

end for end while

A.1 PRACTICAL IMPLEMENTATIONS OF SAFE PG
Due to function approximation errors, even with the Lyapunov constraints in practice the safe PG algorithm may take a bad step and produce an infeasible policy update and cannot auto-

11

Under review as a conference paper at ICLR 2019

Algorithm 4 Lyapunov-based Policy Gradient with -projection (SDDPG and SPPO)

Input: Initial feasible policy 0;
for k = 0, 1, 2, . . . do Step 0: With b = k , generate N trajectories {j,k}jN=1 of T steps by starting at x0 and following the policy k Step 1: Using the trajectories {j,k}jN=1, estimate the critic Q(x, a) and the constraint critic QD,(x, a); · For DDPG, these functions are trained by minimizing the MSE of Bellman residual, and one
can also use off-policy samples from replay buffer (Schaul et al., 2015); · For PPO these functions can be estimated by the generalized advantage function technique from
Schulman et al. (2015b)
Step 2: Based on the closed form solution of a QP problem with an LP constraint in Section 10.2 of Achiam et al. (2017), calculate k with the following formula:



k

=

-k 

-

Q(x¯, a¯) |=k

H(k)-1QD,(x¯, a¯) |=k 

,

QD,(x¯, a¯) |=k H(k)-1QD,(x¯, a¯) |=k

+

where

Q(x¯, a¯)

=

1 N

T -1
t log (a|x)Q(x, a),

x,aj,k,1jN t=0

QD,(x¯, a¯)

=

1 N

T -1
t log (a|x)Q(x, a),

x,aj,k,1jN t=0

k is the adaptive penalty weight of the DKL(||k ) regularizer, and H(k) = 2DKL(||) |=k is the Hessian of this term
Step 3: Update the policy parameter by following the objective gradient; · For DDPG

k+1



k

-k

·

N

1 ·

T

(x) |=k ·(aQk (x, a)+kaQD,k (x, a)) |a=k (x)

xj,k ,1jN

· For PPO,

k+1



k

-

k N k

H(k) -1

T -1
t ·  log (aj,t|xj,t) |=k ·

xj,t,aj,tj,k ,1jN t=0

(Qk (xj,t,aj,t) + k QD,k (xj,t,aj,t))

Step 4: At any given state x  X , compute the feasible action probability a(x) via action projection in the safety layer, that takes inputs aQL(x, a) = aQD,k (x, a) and (x) = (1 - )(d0 - QD,k (x0, k(x0))), for any a  A. end for
Return Final policy k ,

12

Under review as a conference paper at ICLR 2019

Algorithm 5 Lyapunov-based Policy Gradient with a-projection (SDDPG-modular and SPPO-
modular)
Input: Initial feasible policy 0; for k = 0, 1, 2, . . . do
Step 0: With b = k , generate N trajectories {j,k}jN=1 of T steps by starting at x0 and following the policy k Step 1: Using the trajectories {j,k}Nj=1, estimate the critic Q(x, a) and the constraint critic QD,(x, a);
· For DDPG, these functions are trained by minimizing the MSE of Bellman residual, and one
can also use off-policy samples from replay buffer (Schaul et al., 2015); · For PPO these functions can be estimated by the generalized advantage function technique from
Schulman et al. (2015b)
Step 2: Update the policy parameter by following the objective gradient; · For DDPG

k+1



k

-

k

·

1 N·

T

(x) |=k ·aQk (x, a) |a=k (x);

xj,k ,1jN

· For PPO,

k+1



k

-

k N k

H(k) -1

T -1
t· log (aj,t|xj,t) |=k ·Qk (xj,t,aj,t)

xj,t,aj,tj,k ,1jN t=0

where k is the adaptive penalty weight of the DKL(||k ) regularizer, and H(k) =

Step 3:

2DKL(||) |=k is At any given state x 

the X,

Hessian of this term compute the feasible

action

probability

a(x)

via

action

projec-

tion in the safety layer, that takes inputs aQL(x, a) = aQD,k (x, a) and (x) = (1 - )(d0 - QD,k (x0, k(x0))), for any a  A. end for

Return Final policy k ,

matically recover from such a bad step. To tackle this issue, similar to Achiam et al. (2017)
we propose the following safeguard policy update rule to purely decrease the constraint cost: k+1 = k - sg,kD (x0)=k , where sg,k is the learning rate for safeguard update. If sg,k >> k (learning rate of PG), then with the safeguard update  will quickly recover from the bad step but it might be overly conservative. This approach is principled because as soon as k is unsafe/infeasible w.r.t. CMDP, the algorithm uses a limiting search direction. One can directly
extend this safeguard update to the multiple-constraint scenario by doing gradient descent over the
constraint that has the worst violation. Another remedy to reduce the chance of constraint violation
is to do constraint tightening on the constraint cost threshold. Specifically, instead of d0, one may pose the constraint based on d0 · (1 - ), where   (0, 1) is the factor of safety for providing additional buffer to constraint violation. Additional techniques in cost-shaping have been proposed
in Achiam et al. (2017) to smooth out the sparse constraint costs. While these techniques can fur-
ther ensure safety, construction of the cost-shaping term requires knowledge from the environment,
which makes the safe PG algorithms more complicated.

13

Under review as a conference paper at ICLR 2019

B LAGRANGIAN APPROACH TO SAFE RL

There are a number of mild technical and notational assumptions which we will make throughout this section, so we state them here:
Assumption 1 (Differentiability). For any state-action pair (x, a), (a|x) is continuously differentiable in  and (a|x) is a Lipschitz function in  for every a  A and x  X .
Assumption 2 (Strict Feasibility). There exists a transient policy (·|x) such that D (x0) < d0 in the constrained problem.
Assumption 3 (Step Sizes). The step size schedules {3,k}, {2,k}, and {1,k} satisfy

1,k = 2,k = 3,k = ,

kkk

12,k ,
k

22,k ,
k

32,k < ,
k

1,k = o 2,k , 2(i) = o 3,k .

(13) (14) (15)

Assumption 1 imposes smoothness on the optimal policy. Assumption 2 guarantees the existence of a local saddle point in the Lagrangian analysis introduced in the next subsection. Assumption 3 refers to step sizes corresponding to policy updates that will be introduced for the algorithms in this paper, and indicates that the update corresponding to {3,k} is on the fastest time-scale, the updates corresponding to {2,k} is on the intermediate time-scale, and the update corresponding to {1,k} is on the slowest time-scale. As this assumption refer to user-defined parameters, they can always be chosen to be satisfied.
To solve the CMDP, we employ the Lagrangian relaxation procedure (Bertsekas, 1999) to convert it to the following unconstrained problem:

max min
0 

L(, ) = C (x0) +  D (x0) - d0

,

(16)

where  is the Lagrange multiplier. Notice that L(, ) is a linear function in . Then there exists a local saddle point (, ) for the minimax optimization problem max0 min L(, ), such that for some r > 0,   R  B (r) and   [0, max], we have

L(, )  L(, )  L(, ),

(17)

where B (r) is a hyper-dimensional ball centered at  with radius r > 0.

In the following, we present a policy gradient (PG) algorithm and an actor-critic (AC) algorithm. While the PG algorithm updates its parameters after observing several trajectories, the AC algorithms are incremental and update their parameters at each time-step.

We now present a policy gradient algorithm to solve the optimization problem Equation 16. The idea of the algorithm is to descend in  and ascend in  using the gradients of L(, ) w.r.t.  and , i.e.,

L(, ) =  C (x0) + D (x0) , L(, ) = D (x0) - d0.

(18)

The unit of observation in this algorithm is a system trajectory generated by following policy k . At each iteration, the algorithm generates N trajectories by following the current policy, uses them
to estimate the gradients in Equation 18, and then uses these estimates to update the parameters , .

Let  = {x0, a0, c0, x1, a1, c1, . . . , xT -1, aT -1, cT -1, xT } be a trajectory generated by following

the policy , where xT = xTar is the target state of the system and T is the (random) stopping

time. The cost, constraint cost, and probability of  are defined as C() =

T -1 k=0



k

C

(xk

,

ak

),

D() =

T -1 k=0

kD(xk, ak),

and

P ( )

=

P0(x0)

T -1 k=0

 (ak |xk )P

(xk+1 |xk ,

ak ),

respectively.

Based on the definition of P(), one obtains  log P() =

T -1 k=0



log



(ak |xk

).

Algorithm 2 contains the pseudo-code of our proposed policy gradient algorithm. What appears inside the parentheses on the right-hand-side of the update equations are the estimates of the gradients of L(, ) w.r.t. ,  (estimates of the expressions in 18). Gradient estimates of the Lagrangian

14

Under review as a conference paper at ICLR 2019

function are given by

L(, ) = P() ·  log P() C () + D () ,


L(, ) = -d0 + P() · D(),


where the likelihood gradient is



T -1



 log P() =

log P (xk+1|xk, ak) + log (ak|xk) + log 1{x0 = x0}

 k=0



=

T -1 k=0



log

 (ak |xk )

=

T -1 k=0



1 (ak

|xk

)





(ak

|xk

).

In the algorithm,  is a projection operator to [0, max], i.e., () = arg min^[0,max]  - ^ 22, which ensures the convergence of the algorithm. Recall from Assumption 3 that the stepsize schedules satisfy the standard conditions for stochastic approximation algorithms, and ensure that the policy parameter  update is on the fast time-scale 2,i , and the Lagrange multiplier 
update is on the slow time-scale 1,i . This results in a two time-scale stochastic approximation
algorithm, which has shown to converge to a (local) saddle point of the objective function L(, ). This convergence proof makes use of standard in many stochastic approximation theory, because in the limit when the step-size is sufficiently small, analyzing the convergence of PG is equivalent to analyzing the stability of an ordinary differential equation (ODE) w.r.t. its equilibrium point.

In policy gradient, the unit of observation is a system trajectory. This may result in high variance for the gradient estimates, especially when the length of the trajectories is long. To address this issue, we propose two actor-critic algorithms that use value function approximation in the gradient estimates and update the parameters incrementally (after each state-action transition). We present two actor-critic algorithms for optimizing Equation 16. These algorithms are still based on the above gradient estimates. Algorithm 3 contains the pseudo-code of these algorithms. The projection operator  is necessary to ensure the convergence of the algorithms. Recall from Assumption 3 that the step-size schedules satisfy the standard conditions for stochastic approximation algorithms, and ensure that the critic update is on the fastest time-scale 3,k , the policy and -update 2,k is on the intermediate timescale, and finally the Lagrange multiplier update is on the slowest time-scale
1,k . This results in three time-scale stochastic approximation algorithms.

Using the policy gradient theorem from Sutton et al. (2000), one can show that

L(, )

=

 V (x0 )

=

1 1-

µ(x, a|x0)  log (a|x) Q(x, a),

x,a

(19)

where µ is the discounted visiting distribution and Q is the action-value function of policy . We

can

show

that

1 1-



log

 (ak |xk )

·

k

is

an

unbiased

estimate

of

L(, ),

where

k = C¯(xk, ak) + V (xk+1) - V (xk)

is the temporal-difference (TD) error, and V is the value estimator of V.
Traditionally, for convergence guarantees in actor-critic algorithms, the critic uses linear approximation for the value function V(x)   (x) = V,(x), where the feature vector (·) belongs to a low-dimensional space R2 . The linear approximation V, belongs to a low-dimensional subspace SV = |  R2 , where  is a short-hand notation for the set of features, i.e., (x) =  (x). Recently with the advances of deep neural networks, it has become increasingly popular to model the critic with a deep neural network architecture, based on the objective function of minimizing the MSE of Bellman residual w.r.t. V or Q (Mnih et al., 2013).

15

Under review as a conference paper at ICLR 2019

C EXPERIMENTAL SETUP
Our experiments are performed on safety-augmented versions of standard MuJoCo domains (Todorov et al., 2012).
HalfCheetah-Safe. The agent is a the standard HalfCheetah (a 2-legged simulated robot rewarded for running at high speed) augmented with safety constraints. We choose the safety constraints to be defined on the speed limit. We constrain the speed to be less than 1, i.e., constraint cost is thus 1[|v| > 1] . Episodes are of length 200. The constraint threshold is 50.
Point Circle. This environment is taken from (Achiam et al., 2017). The agent is a point mass (controlled via a pivot). The agent is initialized at (0, 0) and rewarded for moving counter-clockwise along a circle of radius 15 according to the reward -dx·y+dy·x , for position x, y and velocity
1+| x2+y2-15|
dx, dy. The safety constraint is defined as the agent staying in a position satisfying |x|  2.5. The constraint cost is thus 1[|x| > 2.5]. Episodes are of length 65. The constraint threshold is 7.
Point Gather. This environment is taken from (Achiam et al., 2017). The agent is a point mass (controlled via a pivot) and the environment includes randomly positioned apples (2 apples) and bombs (8 bombs). The agent given a reward of 10 for each apple collected and a penalty of -10 for each bomb. The safety constraint is defined as number of bombs collected during the episode. Episodes are of length 15. The constraint threshold is 1 for DDPG algorithms and 2 for PPO algorithms.
Ant Gather. This environment is is the same as Point Circle, only with an ant agent (quadrapedal simulated robot). Each episode is initialized with 8 apples and 8 bombs. The agent given a reward of 10 for each apple collected, and a penalty a reward of -20 is incurred if the episode terminates prematurely (because the ant falls). Episodes are of length at most 500. The constraint threshold is 4 for DDPG algorithms and 8 PPO algorithms.
Figure 4 shows the visualization of the above domains used in our experiments.

HalfCheetah-Safe

Point-Circle

Ant-Gather

Point-Gather

Figure 4: The Robot Locomotion Control Tasks
C.1 ADDITIONAL EXPERIMENTAL RESULTS In this section, we include more experimental results of various PG algorithms on safe robot locomotion tasks.

16

Under review as a conference paper at ICLR 2019

Point-Circle, PPO
400 350 300 250 200 150 100 50
0 0 20 40 60 80

Ant-Gather, DDPG
100 80 60 40 20 0 0 10 20 30 40

50

Ant-Gather, PPO
100 80 60 40 20 0 0 20 40 60 80

Figure 5: Cumulative constraint violations of various PG algorithms on safe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from DDPG (red), DDPG-Lagrangian (purple), SDDPG (blue), SDDPG-modular (green), PPO (red), PPO-Lagrangian (purple), SPPO (blue), SPPO-modular (green) on Ant-Gather and Point-Circle.

Return

Ant-Gather
12
10
8
6
4
2
0 0 20 40 60 80
6 4 2 0 2 4 6 8 10 0 20 40 60 80

1400 1200 1000 800 600 400 200
0 0
45 40 35 30 25 20 15 10 5 00

Point-Circle
20 40 60 80 20 40 60 80

Constraint

Figure 6: Results of various PG algorithms on safe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from PPO (red), PPO-Lagrangian (purple), SPPO (blue), SPPOmodular (green) on Ant-Gather and Point-Circle.

17

Under review as a conference paper at ICLR 2019

Ant-Gather, Return
15 10 5 0 5 10 0 10 20 30 40 50

Ant-Gather, Constraint
30 25 20 15 10 5 0 0 10 20 30 40 50

Figure 7: Results of various PG algorithms on safe robot locomotion tasks, with x-axis in thousands of episodes. We include runs from DDPG (red), DDPG-Lagrangian (purple), SDDPG (blue), SDDPG-modular (green) on Ant-Gather.

18

