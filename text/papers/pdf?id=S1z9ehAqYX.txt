Under review as a conference paper at ICLR 2019
SHRINKAGE-BASED BIAS-VARIANCE TRADE-OFF FOR DEEP REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Deep reinforcement learning has achieved remarkable successes in solving various challenging artificial intelligence tasks. A variety of different algorithms have been introduced and improved towards human-level performance. Although technical advances have been developed for each individual algorithms, there has been strong evidence showing that further substantial improvements can be achieved by properly combining multiple approaches with difference biases and variances. In this work, we propose to use the James-Stein (JS) shrinkage estimator to combine on-policy policy gradient estimators which have low bias but high variance, with low-variance high-bias gradient estimates such as those constructed based on model-based methods or temporally smoothed averaging of historical gradients. Empirical results show that our simple shrinkage approach is very effective in practice and substantially improve the sample efficiency of the state-of-the-art on-policy methods on various continuous control tasks.
1 INTRODUCTION
Deep reinforcement learning (RL) has achieved remarkable successes recently, as witnessed by the human/super-human level performance achieved in various challenging artificial intelligence tasks; notable examples include AlphaGo (Silver et al., 2017; 2016), Atari games (Mnih et al., 2013), robotic learning (Levine et al., 2016; Andrychowicz et al., 2017), among many others. A remarkable nature of RL is that multiple types of algorithms have been developed, based on different principles, highlighting different features and trade-offs. This includes model-based approaches which derive optimal policies from estimated dynamic models of the unknown environment (Deisenroth & Rasmussen, 2011; Nagabandi et al., 2017), model-free off-policy methods such as Q-learning (Mnih et al., 2013), as well as model-free on policy methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Trust Region Policy Optimization (TRPO) (Schulman et al., 2015). These methods exploit different properties of the Markov decision process, and have different pros and cons. Model-based and off-policy methods are known to be more sample efficient when implemented successfully, while on-policy methods are less sample efficient, but are simpler and easier to train, especially for problems with continuous or high dimensional action spaces. Although there have been emerging advances in improving each individual algorithms, more substantial improvements are likely achieved by adaptively integrating multiple strategies into an overall intelligence system. This is evident from human brain, which has been discovered to contain multiple distinct or even competing pathways for learning from reward (Daw et al., 2005; Rangel et al., 2008), including both model-free strategies consisting of error-and-trial learning to repeat rewarded actions, and model-based learning that learns models of the environment, and use it to evaluate candidate actions by mental simulations of their consequences. We would expect integrating different methods in RL will improve the performance, especially when integrated into an adaptive fashion. Algorithmically, a key challenge for an adaptive combination is to evaluate the reliability of each individual strategy, without knowing the ground truth answer while performing the task.
In this work, we focus on the problem of adaptively combining policy gradient estimators from various sources. We leverage the fact that different strategies yield different gradient estimators with different bias-variance trade-off, which can be exploited by statistical shrinkage estimators to obtain optimal combinations that have better mean square error than any individual estimators. In particular, policy gradient estimators provide (nearly) unbiased estimators of the true gradient of the expected reward, but often suffers from high variance and is sample inefficient. Model-based and on-policy
1

Under review as a conference paper at ICLR 2019
methods, on the other hand, provide more biased estimates but can have much smaller variance by taking the advantage of the large amount of imaginary data efficiently simulated from the model, or off-policy data stored in the replay buffer.
In particular, we propose to use James-Stein (JS) shrinkage estimator (Stein, 1956; James & Stein, 1961; Efron & Morris, 1975) to combine the unbiased on-policy gradient estimator with biased but low variance estimators constructed based on models or off-policy data. The basic idea of JS estimator is to access the quality of the biased estimator by inspecting its consistency with unbiased estimators, and use it as a clue for constructing an optimally weighted combination to trade-off bias and variance. JS estimator admits strong theoretical guarantees for standard Gaussian cases, and more importantly, also provides a simple practical heuristic for more complex practical non-Gaussian cases. It can potentially provide an ideal tool for solving the very challenge of bias-variance trade-off in various components of deep reinforcement learning, but seems to be still largely under-explored in the literature. In this work, we explore the practical application of JS estimators in deep RL, illustrated by two examples:
Shrinking Towards Model-based Gradient We develop a hybrid algorithm which adaptively integrates on-policy policy gradient with Dyna-style model-based learning agent (Sutton, 1991). We train a transition model using the existing rollout data in parallel to the on-policy gradient descent; imaginary rollouts are simulated from the model, providing a biased, lower variance gradient estimator. This model-based gradient estimator compared against the unbiased, high variance on-policy gradient, and a weighted combination is constructed using JS estimator based on the consistency of the two estimators. Our method improves the sample efficiency by exploiting the data more efficiently using model learning while avoiding significant bias introduced by the model using adaptive JS estimator.
Shrinking Towards Temporally Smoothed Gradient It has been observed heuristically that proper bias is helpful for training (Sutskever et al., 2013; Nesterov, 2013). We present a simple method that explicitly constructs an optimal bias-variance trade-off by shrinking the on-policy gradient estimator towards a smoothed gradient estimator obtained by a weighted sum over previous iterations. The smoothed gradient provides a reasonable guess of the true gradient of the current estimation and forms a biased, low variance estimator. By shrinking towards the smoothed gradient, we significantly decrease the variance of the on-policy gradient, at the cost of introducing a small bias. This effectively creates a statistical momentum, which shares similarity with the traditional momentum methods derived from optimization perspectives, but with an adaptive coefficient determined by the JS estimator. It can also be combined with traditional momentum to yield gains on both statistical and optimization sides. This simple algorithm effectively leverages the historic (off-policy) data via the temporally smoothed gradient, yielding higher sample efficiency than standard policy gradient.
We perform empirical studies to test the performance of both above examples, showing our adaptive combination strategies can yield better results than any fixed-weight combination, significantly improving the sample efficiency on standard on-policy methods. This illustrates the practical power of JS estimators, which we expect to be broadly applicable to more settings in deep RL in future works.
2 BACKGROUND
We first introduce the basics of reinforcement learning and model-free policy gradient, and set up the notation that we will use in the paper. We then introduce a model-based policy gradient method.
2.1 REINFORCEMENT LEARNING
Reinforcement learning considers the problem of finding an optimal policy for an agent which interacts with an uncertain environment and collects reward per action. The goal of the agent is to maximize the long-term cumulative reward. Formally, this problem can be formulated as a Markov decision process over the environment states s  S and agent actions a  A, under an unknown environmental dynamic defined by a transition probability T (s |s, a) and a reward signal r(s, a) immediately following the action a performed at state s. The agent's action a is selected by a conditional probability distribution (a|s) called policy. In policy optimization, we consider a set of candidate policies (a|s) parameterized by  and obtain the optimal policy by maximizing the
2

Under review as a conference paper at ICLR 2019

expected cumulative reward or return

J () = Es,a(a|s) [r(s, a)] ,

where (s) =

 t=1

t-1Pr(st

=

s)

is

the

normalized

discounted

state

visitation

distribution

with

discount factor   [0, 1). To simplify the notation, we denote Es,a(a|s)[·] by simply E[·] in

the rest of paper.

2.2 ON-POLICY POLICY GRADIENT

According to the policy gradient theorem (Sutton & Barto, 1998), the gradient of J() equals

J () = E [ log (a|s)Q(s, a)] ,

(1)

where Q(s, a) = E

 t=1

t-1r(st,

at)|s1

=

s, a1

=

a

denotes the expected return under policy

 starting from state s and action a.

Different policy gradient methods are based on different stochastic estimations of the expected
gradient in Eq (1). Perhaps the most straightforward way is to simulate the environment with the current policy  to obtain a trajectory {(st, at, rt)}nt=1 and estimate J() using the Monte Carlo estimation:

^ J ()

=

1 n

n

t-1 log (at|st)Q^(st, at),

t=1

(2)

where Q^(st, at) is an empirical estimate of Q(st, at), e.g., Q^(st, at) = jt j-trj. This is known as an on-policy gradient estimator because it only uses the data from the current policy.

Proximal Policy Optimization (PPO) (Schulman et al., 2017; Heess et al., 2017) is one of the state-theart model-free policy gradient methods for policy optimization. It uses a proximal Kullback-Leibler (KL) divergence penalty to regularize and stabilize the policy gradient update. Given an existing policy old, PPO obtains a new policy by maximizing the following surrogate loss function

Jppo() = Eold

 (a|s) old(a|s)

Q

(s,

a)

-

KL

[old(·|s)

||

 (·|s)]

,

where the first term is an approximation of the expected reward, and the second term enforces the the
updated policy to be close to the previous policy under KL divergence. The gradient of Jppo() can be rewritten as

Jppo() = Eold w(s, a) log (a|s)Q(s, a)

where w(s, a) := (a|s)/old(a|s) is the density ratio of the two polices, Q(s, a) + w(s, a)-1 where the second term comes from the KL penalty.

and

Q(s, a)

:=

The advantage of using on-policy estimation is that it provides (nearly) unbiased estimation of the gradient of expected reward. A well-known challenge, however, is that it has large variance, given that it is only possible to use a small number of data from the current policy at each iteration because the cost of simulation from the real environment is high. In addition, it does not take advantage of the off-policy data collected previously. Using off-policy data allows us to obtain low variance but potentially highly biased estimates, which can be combined with on-policy gradient with James-Stein estimator as we propose in this work. In the sequel, we introduce model-based policy gradient as an important way for leveraging off-policy data.

2.3 MODEL-BASED POLICY GRADIENT

The model-based methods derive optimal polices by learning a transition model of the underlying

dynamic T (s |s, a) of the Markov Decision Process. Assume the states s is continuous, it is common

to assume s = s+f(s, a)+ where f is a parametric function (e.g., neural network) with learnable parameter , and  is a zero-mean noise. The model parameter  is often trained by minimizing the

L2 one-step prediction loss,

n

min


st+1 - st - f(st, at) 22,

t=1

(3)

3

Under review as a conference paper at ICLR 2019

where D := (st, at, st+1)nt=1 is a set of transition pairs collected from previous rollouts. See Deisenroth & Rasmussen (2011); Fu et al. (2016); Nagabandi et al. (2017); Thanard Kurutach (2018);
Depeweg et al. (2017) for variety of the recent variants of model-based learning methods.

With a learned model, the problem of estimating the optimal policy becomes a planning problem, which can be solved either using various classical planning and control methods, typically based on variants of dynamic programming (see e.g., Sutton & Barto, 1998), or sample-based methods that generate fictitious data by simulating the model forward and obtain optimal policies by applying standard RL algorithm on the fictitious data.

In this work, we will mainly consider a simple sample-based policy gradient method whose main idea is rooted from Dyna (Sutton, 1991). The idea is simply generating a fictitious data (s~t, a~t, r~t, s~t+1) by simulating the model, and plug it into the policy gradient formula (2),

n
^ m odelJ () = t-1 log (a~t|s~t)Q^(s~t, a~t).
t=1

(4)

By simulating a large number of fictitious data from the model, which is much less expensive than
simulating from the real environment, we can make the variance of ^ m odelJ() significantly smaller than the variance of the on-policy gradient in (2).

The idea of using ^ m odelJ() can be found in (Gu et al., 2016; Thanard Kurutach, 2018). The problem, however, is that the transition model can deviate significantly from the true model, introducing a potentially large bias that makes the training problem unstable. Thanard Kurutach (2018) found that by training an ensemble of models can help stabilize the training. As we discuss in the sequel, one of our key idea is to calculate on-policy and model-based gradient simultaneously, and use the unbiased information of the on-policy gradient to evaluate the biasness of model-based gradient, and weigh its importance accordingly to ensure the stability while exploiting the model-based information.

3 GRADIENT ESTIMATION WITH SHRINKAGE

We introduce our main method which uses shrinkage estimator to trade-off the bias and variance to adaptively combine multiple gradient estimators. We start with reviewing James-Stein Shrinkage estimator in Section 3.1, and then discuss its two applications in reinforcement learning, including shrinkage estimator towards model-based policy gradient in Section 3.2, and temporally smoothed gradients in Section 3.3.

3.1 JAMES-STEIN SHRINKAGE ESTIMATOR

James-Stein estimator (Stein, 1956; James & Stein, 1961) is an estimator of the mean of a multivariate normal distribution that achieves a smaller mean square error (MSE) than the maximum likelihood estimator (MLE) in dimensions more than three. This estimator shrinks the standard maximum likelihood estimator towards a specified target by an adaptive shrinkage factor, which introduces bias whilst obtains lower variance and MSE.

Consider the problem of estimating the mean of a p-dimensional multivariate normal distribution N (, Q), where   Rd is the mean and Q  Rp×p is the covariance matrix. Given a sample x drawn from N (, Q), a natural estimator of the maximum likelihood estimation (MLE) which predicts  using   x. This estimator is unbiased, and gives the minimum variance among all the possible
unbiased estimators.

MLE was used to be believed to be admissible, that is, no other estimators can always achieve lower MSE than MLE. However, this was proved to be false by James and Stein, who constructed an estimator, now known as James-Stein estimator, that always dominants MLE when p > 2:

^JS = ¯ + (x - ¯),

(5)

where ¯  Rp is any fixed constant relative to x, and  is a combination coefficient defined as

 = 1 - (p~ - 2)/(x - ¯) Q-1(x - ¯),

(6)

4

Under review as a conference paper at ICLR 2019

Algorithm 1 PPO-MBS: PPO with model-based gradient shrinkage
Initialize policy (a|s), model-based dynamic f(s, a), and replay buffer D  . repeat
Collect real trajectories using  and add them to replay buffer D. for K iterations do
Update  with data from replay buffer D using (3). end for Sample imaginary trajectories {i}in=1 with max time-step Tp using  and f. for M iterations do
Calculate model free PPO gradient gc w.r.t  with real samples. Calculate model-based PPO gradient gmb w.r.t  with imaginary trajectories {i}in=1. Get JSE g^JS+ using (8) and update  with g^JS+ . end for
until Convergence

where p~ = trace(Q)/max(Q) is considered as an effective dimension of covariance matrix Q, which equals p if Q is an identity matrix. The intuition is that := (x - ¯) Q-1(x - ¯) the bias of ¯ in respect to the variation of x. If ¯ is close to the true mean ,  is small and ¯ is highly weighted; if ¯ is far away from the true mean,  is large and the unbiased estimator x is highly weighted. Overall, the adaptive strategy ensures the bias and variance is always traded off optimally.
More generally, we can relax c = p~ - 2 in equation (5) to 0 < c < 2(p~ - 2), which still dominates the MLE when p~ > 2. Baranchik (1964) also suggests that JS estimator can be further improved by limiting the combination coefficient to be positive. This gives a general positive James-Stein estimator: (JSE+) as

^JS+ = ¯ + +(x - ^),

+ = max 0, 1 - c/(x - ¯) Q-1(x - ¯) .

(7)

Although the standard theoretical results of JS estimators are established for normal distribution,
which may not hold for settings in deep RL, however, JS still provides a simple yet highly useful
practical strategy for adaptive combination. In practice, when the covariance matrix Q is not known, it can be replaced by an empirical estimation Q^, which is estimated over multiple trajectories. In fact, in our subsequent experiments, we approximate Q^ with its diagonal matrix to avoid matrix inverse. This introduces error in estimating +, but can be practically compensated choosing a proper
parameter c using trial tests. This work focuses on demonstrating a novel empirical application of JS
estimators for deep RL, and defers further theoretical work to future works.

3.2 SHRINKING TOWARDS MODEL-BASED GRADIENT ESTIMATOR

We describe our method that adaptively combines model-based and on-policy strategies using shrinkage estimator. For notation, we denote by gc := ^ J() the on-policy gradient in (2) obtained from the "real" trajectories, and gmb := ^ m odelJ() the model-based gradient obtained from "imaginary" trajectories rolled out from learned dynamics. With these two gradient estimators, we
can obtain a new estimator as

g^JS+ = gmb + +(gc - gmb).

(8)

The only difference between these two estimators gmb and gc is the data used for gradient calculation. Here + serves as a measurement of the quality of dynamic model training. A good dynamic gives imaginary trajectories that are similar to real trajectories, making gmb are close to gc and hence a small + according to (6).
One subtle difficulty is that when simulating long horizon trajectories, the error in the model is amplified, causing significantly bad model-based estimator (yielding   1 in (8)). To address this problem, we simulate only short horizon imaginary trajectories initialized with different states randomly drawn from true samples. This trick allows us to obtain higher quality model-based estimators. We integrate our method with Proximal Policy Optimization (PPO)(Schulman et al., 2017; Heess et al., 2017), which we refer as PPO-MBS, and is summarized in algorithm 1.

5

Under review as a conference paper at ICLR 2019

Algorithm 2 PPO-STS: PPO with temporally smoothed gradient shrinkage
Initialize policy (a|s) and historic gradient set   . repeat
Collect real trajectories using . for M iterations do
Calculate model free PPO gradient gc w.r.t . Calculate weighted average historic gradient gw. Get JSE g^JS+ using (10) and update  with g^JS+ . Add gw to the gradient set . end for until Convergence

3.3 SHRINKING TOWARDS TEMPORALLY SMOOTHED GRADIENT
In addition to the model-based method, we present another simpler method that constructs shrinkage estimator based on a smoothed gradient obtained by averaging previously gradients. This simple strategy is significantly simpler and computationally efficient since no additional model learning or other expensive calculation is needed. It turns out this strategy works surprisingly well.
Denote by gt the on-policy gradient at iteration t, and gc := gT the gradient at the current iteration (T ). We calculate a smoothed gradient from the previous iterations by a weighted average:

gw =

tgt

t<T

t .
t<T

(9)

Assume the parameters do not change much over the iterations, gw provides a reasonable guess of the current gradient. Therefore, we can shrink the current noisy gradient towards the gw:

g^JS+ = gw + +(gc - gw).

(10)

We refer our method as PPO-STS and summarize it in algorithm 2. Interestingly, one can view the update in (10) as a type of momentum method, because it forces the parameters to update along the previous gradient directions. Compared with the standard momentum methods developed from the optimization perspective (referred as optimization momentum) (Sutskever et al., 2013; Nesterov, 2013; Kingma & Ba, 2014; Duchi et al., 2011) , our method is derived for the statistical purpose of obtaining a better estimator of gradient (and hence forms statistical momentum). A unique feature of the statistical momentum is that its magnitude changes adaptively with the consistency between the momentum and the current gradient estimator, reducing to zero if the gradient estimator is deterministic (zero variance). As we do in our experiments, our statistical momentum can be directly applied on the top of classical optimization momentum to combine the benefits of both.

4 EXPERIMENTS

We evaluate PPO-MBS and PPO-STS on continuous control tasks (Brockman et al., 2016) using the MuJoCo physics simulator (Todorov et al., 2012). We report the variance, bias, and MSE of gradient estimation with and without PPO-STS, PPO-MBS in Section 4.1, and then we show the results of extensive experiments in Section 4.2. We choose Proximal Policy Gradient (PPO) as the baseline, which is one of the state-of-the-art policy gradient methods on various continuous control tasks. Implementation details1and hyperparameters values are provided in Appendix A and Table 1.
Extensive experiments show that PPO-STS and PPO-MBS can reduce the MSE and the variance of the gradient estimation while we can obtain significantly better performance comparing standard on-policy methods.

4.1 EVALUATION OF PPO-STS AND PPO-MBS
We start with evaluating the effect of PPO-STS and PPO-MBS on gradient estimation, namely, we demonstrate the bias variance decomposition of gradient estimations, and also compare the average reward of adaptive combination with different fixed combinations.

6

Under review as a conference paper at ICLR 2019

Value of 

Variance

Bias Square

1.00 0.3 0.3
0.75 0.2 0.2
0.50
0.1 0.1 0.25

MSE

0.0 0

5M 0.0 0

5M 0.00 0

5M

4000 0.3
3000 0.2
2000
0.1 1000

Average Reward

PPO-MBS = 0.1 = 0.5 = 0.9 =0
PPO

0.0 0 5M

5M

Figure 1: Comparison of MSE, Variance, and Bias of PPO and PPO-MBS gradient estimation on Walker2d.
0.3 0.3 1.00

Value of 

Variance

Bias Square

0.2 0.2 0.75
0.50 0.1 0.1
0.25

MSE

0.0 0

5M 0.0 0

5M 0.00 0

5M

0.3 4500
0.2 3500 2500
0.1 1500

Average Reward

PPO-STS = 0.1 = 0.5 = 0.9
PPO

0.0 0

500 5M

5M

Figure 2: Comparison of MSE, Variance, and Bias of PPO and PPO-STS gradient estimation on Walker2d.

We choose the Walker2d as the evaluation environment, and calculate the MSE, variance, and bias of gradient estimation in PPO-MBS (Algo 1), PPO-STS (Algo 2), and PPO during the training process. Figure 1 shows the results on Walker2d. Similarly, Figure 2 shows the results of PPO-MBS on Walker2d-v1. The empirical results show that our methods obtain significantly lower MSE, and variance than the typical model-free policy gradient method. It also shows that adaptive combination (adaptively to learn value of ) is essential to get a better estimation of gradient, and can lead to higher reward in policy optimization. The value of James-Stein coefficient  can also indicate the difference between temporal gradient and current gradient, or model-free policy gradient and model-based policy gradient estimates.
4.2 COMPARISON PPO-STS, PPO-MBS WITH STATE-OF-THE-ART METHODS
Finally, we evaluate PPO-STS, PPO-MBS on a more extensive list of tasks shown in Figure 3. It shows that PPO-STS and PPO-MBS have superior performance over baseline method PPO. Empirically, we find that PPO-STS has better performance on high dimensional tasks, such as Walker2d while PPO-MBS performs better on low dimensional environments such as Reacher, Swimmer or Hopper. Ww further investigate PPO-STS on more higher dimensional tasks such as Humanoid and Ant, the results of which are in Appendix B.
5 RELATED WORK
As a classical method, James-Stein (JS) estimator has been extensively studied and used in statistics and machine learning. See e.g., Gruber (2017); Young & Smith (2005) for overviews. However, we are not aware of previous works on applying JS estimators for gradient estimation in RL.
The idea of combining multiple algorithms to achieve better results have been widely considered in reinforcement learning. For example, Wiering & Van Hasselt (2008) developed an ensemble method
7

Under review as a conference paper at ICLR 2019

Average Reward

Hopper
3000 2500
1500
500 1.00

120 100 80 60 40 5M 1.0

Swimmer

5
10
15
5M 20 1.0

Reacher

4500 3500 2500 1500 500k 500
1.00

Walker2d

5M

0.75 0.9

0.9

0.75

0.50 0.8 0.25 0.7 0.00 5M 0.6

0.8 0.7 5M 0.6

0.50 0.25 500k 0.00

PPO PPO-MBS PPO-STS
5M

Figure 3: Comparison of PPO-STS, PPO-MBS, and PPO on various continuous control task.

Value of 

by combining the policies derived from the value functions of the different RL algorithms. Hessel et al. (2017) combines several extensions of DQN to improve Atari games. Closely related to our work is Gu et al. (2017), which uses a linear combination of on-policy and off-policy gradients, but with a fixed, user-specified combination coefficient. O'Donoghue et al. (2017) unifies policy gradient and Q-Learning and empirically observes better data efficiency and stability.
Another large body of works have explored the model-based/ model-free integration. The classic Dyna framework (Sutton, 1991) has primarily been used with small and discrete systems, although its extensions for high dimensional continuous control tasks exist. (Gu et al., 2016; Feinberg et al., 2018). There is also a line of research using imaginary trajectories for training policy (Kalweit & Boedecker, 2017; Thanard Kurutach, 2018). Heess et al. (2015) uses models to improve the accuracy of model-free value function backups. Model-based training has also been used to produce a good initialization for the model-free training (Farshidian et al., 2014; Nagabandi et al., 2017), most recently, Clavera et al. (2018) proposes to meta learn a policy to adapt to an ensemble of dynamic models.
Our work provides a unified adaptive framework for various gradient estimators, with the purpose of utilizing advantages of each component, which results in lower variance and MSE over onpolicy model free optimization. Two illustrated examples in our experiments show that our adaptive combination strategy achieves better results than on-policy methods or fixed-weighted combinations.

6 CONCLUSION
We propose to use James-Stein shrinkage estimator to combine the unbiased on-policy gradient estimator with biased but low variance estimators constructed based on models-based methods or historic temporally smoothed gradients. Empirical experiments show that our adaptive combination strategy can reduce variance and MSE, yielding significant improvement over standard on-policy methods. Future work includes investigating more powerful shrinkage estimators and alternative biased estimators for combination in policy optimization.

REFERENCES
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048­5058, 2017.
Alvin John Baranchik. Multiple regression and estimation of the mean of a multivariate normal distribution. PhD thesis, Department of Statistics, Stanford University Stanford, California, 1964.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.

8

Under review as a conference paper at ICLR 2019
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization. arXiv preprint arXiv:1809.05214, 2018.
Nathaniel D Daw, Yael Niv, and Peter Dayan. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nature neuroscience, 8(12):1704, 2005.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp. 465­472, 2011.
Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, and Steffen Udluft. Learning and policy search in stochastic dynamical systems with bayesian neural networks. International Conference on Learning Representations (ICLR), 2017.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https: //github.com/openai/baselines, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Bradley Efron and Carl Morris. Data analysis using stein's estimator and its generalizations. Journal of the American Statistical Association, 70(350):311­319, 1975.
Farbod Farshidian, Michael Neunert, and Jonas Buchli. Learning of closed-loop motion control. In Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pp. 1441­1446. IEEE, 2014.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine. Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018.
Justin Fu, Sergey Levine, and Pieter Abbeel. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, pp. 4019­4026. IEEE, 2016.
Marvin Gruber. Improving Efficiency by Shrinkage: The James­Stein and Ridge Regression Estimators. Routledge, 2017.
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pp. 2829­2838, 2016.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, Bernhard Schölkopf, and Sergey Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. Advances in Neural Information Processing Systems, 2017.
Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944­2952, 2015.
Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
William James and Charles Stein. Estimation with quadratic loss. In Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1, pp. 361­379, 1961.
9

Under review as a conference paper at ICLR 2019
Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous deep reinforcement learning. In Conference on Robot Learning, pp. 195­206, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2014.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334­1373, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. IEEE International Conference on Robotics and Automation(ICRA), 2017.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.
Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Pgq: Combining policy gradient and q-learning. Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2017.
Antonio Rangel, Colin Camerer, and P Read Montague. A framework for studying the neurobiology of value-based decision making. Nature reviews neuroscience, 9(7):545, 2008.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. In International Conference on Machine Learning, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. International Conference of Learning Representations (ICLR), 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. Advances in Neural Information Processing Systems, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550(7676):354­359, Oct 2017. ISSN 0028-0836.
Charles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. Technical report, Stanford University, United States, 1956.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139­1147, 2013.
Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin, 2(4):160­163, 1991.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Yan Duan Aviv Tamar Pieter Abbeel Thanard Kurutach, Ignasi Clavera. Model-ensemble trustregion policy optimization. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=SJJinbWRZ.
10

Under review as a conference paper at ICLR 2019 Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012. Marco A Wiering and Hado Van Hasselt. Ensemble algorithms in reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 38(4):930­936, 2008. G Alastair Young and Richard L Smith. Essentials of statistical inference, volume 16. Cambridge University Press, 2005.
11

Under review as a conference paper at ICLR 2019

A EXPERIMENT DETAILS
A.1 POLICY AND VALUE FUNCTION ARCHITECTURES
The network structure largely depends on the OpenAI baselines (Dhariwal et al., 2017). The stochastic policy (at|st) is parameterized with a diagonal Gaussian N (µ(st), ) with a local state-dependent mean and a global diagonal variance matrix. µ(st) is a two-layer MLP with hidden sizes 64 × 64. The value function v^(st) is parameterized with 64 × 64 MLP separately and we use GAE (Schulman et al., 2016) to estimate advantage function. We use tanh as the activation function for both networks.

A.2 TRAINING DETAILS
For all training methods, we use Adam (Kingma & Ba, 2014) as the gradient descent optimizer. For every 10 training iteration, we evaluate the policy (a|s) with its mean µ(s) using 25,000 rollout samples. The key parameters and hyperparameter search values of the training algorithms are summarized in Table 1. The optimal performing hyperparameter results are reported over 5 different seeds.

Parameter

PPO

PPO-MBS

PPO-STS

batchsize

5,000 (500 for Reacher)

learning rate of v^(st)

0.001

GAE  and 

 = 0.95,  = 0.99

learning rate of (a|s) {1e-3, 3e-4, 1e-4}

0.0003

inner iteration M
weight  in PPO-STS Imaginary horizon Tp in PPO-MBS

{20, 30 ,40} ­

30 {0.97, 0.98, 0.99}

­ {30, 50 , 80} ­

Table 1: Hyperparameters in the training process

A.3 DETAILS OF MODEL-BASED LEARNING IN PPO-MBS
A.3.1 DYNAMIC MODEL LEARNING
We represent the dynamic model f(a, s) with a 2-hidden-layer MLP with hidden sizes 512 × 512 and ReLU as the activation function. We train the model with Adam (Kingma & Ba, 2014) optimizer with learning rate 0.0003 using a batch size of 512 for every 5 policy optimizing iterations. The model is trained over a subset randomly drawn from the training dataset with size 25,000 for 10 passes and we use validation dataset for early stopping (We evaluate the validation loss for each model learning pass). Empirically we find resetting the model parameter  every 15 policy optimizing iterations helps improve to the performance.
A.3.2 DATA PROCESSING
We obtain a replay buffer D which stores the input and the output pair {(sti, ait), stk+1 - stk}Nk=1 of the dynamic f(at, st) with most recent N = 50, 000 samples which has been used by policy learning. Further, we subtract the mean of the data and divide by the standard deviation of the data to ensure the loss function weights the different parts of the states. Finally, we split the collected data using a 3-to-1 ratio for training and validation datasets.
12

Under review as a conference paper at ICLR 2019

A.3.3 IMAGINARY TRAJECTORY PLANNING
We use the dynamic model f(st, at) and the policy (a|s) to collect the imaginary trajectories with a max horizon Tp  {30, 50, 80}. To estimate gmb, we sample a total batch size of 25,000 imaginary trajectories. To make the imaginary trajectories diverse, we randomly use the first Tmax -Tp time step of states in the real trajectory samples as initialization s0 for planning, where Tmax is the maximum horizon of the environment. For Reacher, we only use the initial states from real trajectories s0 as the initialization since its maximum horizon is Tmax = 50.
A.4 DETAILS OF ENVIRONMENTS
Here we list the reward function used in model-based dynamics learning. The environments we use are based on the top of OpenAI Gym(Brockman et al., 2016). We modified the environment by adding the observations of Mujoco(Todorov et al., 2012) environments which are expected by gym but are requisite for reward calculation.
The reward functions r(st, at, st+1) and the optimization horizon Tmax are described below:

Environments Swimmer Walker2d Hopper Pusher
Reacher

r(st, at, st+1)
stxvel - 0.0001||at||22
stxvel - 0.001||at||22 + 1
sxt vel - 0.001||at||22 + 1 -0.5||xbody - xarm||22+ ||xbody - xgoal||22 - 0.1||at||22 -||xfinger - xgoal||22 - ||at||22

Tmax 1000 1000 1000 100
50

Table 2: Detail of the environments used in our experiments

stxvel denotes the x-axis velocity at time t, which is calculated by stxvel

=

,sxt+po1s-sxt pos
dt

where

dt = 0.02 in the Mujoco (Todorov et al., 2012) simulator. xbody, xarm, xfinger denote the position of

the body, the arm and the fingertip of the object separately, and xgoal denotes the position of the goal.

B PPO-STS ON HUMANOID AND ANT ENVIRONMENTS

We compare PPO-STS with PPO on higher dimensional tasks like Humanoid and Ant, and show the results in Figure 4. It shows that PPO-STS can be consistently better than PPO, which makes PPO-STS more valuable since it does not introduce additional computational costs.

Average Reward
5000 4000 3000 2000 1000

1.0 0.9 0.8 0.7 8M 0.6

Value of 

5000 4000 3000 2000 1000 8M

Average Reward

1.00 0.75 0.50 0.25 18M 0.00

Value of 
PPO PPO-STS
18M

Figure 4: Comparison of PPO-STS with PPO on Humanoid and Ant.

13

