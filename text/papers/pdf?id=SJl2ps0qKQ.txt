Under review as a conference paper at ICLR 2019
LEARNING TO DECOMPOSE COMPOUND QUESTIONS WITH REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
As for knowledge-based question answering, a fundamental problem is to relax the assumption of answerable questions from simple questions to compound questions. Traditional approaches firstly detect topic entity mentioned in questions, then traverse the knowledge graph to find relations as a multi-hop path to answers, while we propose a novel approach to leverage simple-question answerer to answer compound questions. Our model consists of two components: (i) a novel learning-to-decompose agent that learns a policy to decompose a compound question into simple questions and (ii) a simple-question answerer that classifies the corresponding relation to answers. Experiments demonstrate that our model learns complex rules of compositionality as policy, which benefits a simple neural network to achieve state-of-the-art results on the most challenging research dataset. We analyze the interpretable decomposition process as well as generated partitions.
1 INTRODUCTION
Knowledge-Based Question Answering (KBQA) is one of the most interesting approaches of answering a question, which bridges a curated knowledge base of tremendous facts to answerable questions. With question answering as a user-friendly interface, users can easily query a knowledge base through natural language, i.e., in their own words. In the past few years, many systems (Berant et al., 2013; Bao et al., 2014; Yih et al., 2015; Dong et al., 2015; Zhang et al., 2017; Hao et al., 2017) have achieved remarkable improvements in various datasets, such as WebQuestions (Berant et al., 2013), SimpleQuestions (Bordes et al., 2015) and MetaQA (Zhang et al., 2017).
However, most of them (Yih et al., 2014; Bordes et al., 2015; Dai et al., 2016; Yin et al., 2016; Yu et al., 2017) assume that only simple questions are answerable. Simple questions are questions that have only one relation from the topic entity to unknown tail entities (answers, usually substituted by an interrogative word) while compound questions are questions that have multiple1 relations. For example, "Who are the daughters of Barack Obama?" is a simple question and "Who is the mother of the daughters of Barack Obama?" is a compound question which can be decomposed into two simple questions.
In this paper, we aim to relax the assumption of answerable questions from simple questions to compound questions. Figure 1 illustrates the process of answering compound questions. Intuitively, to answer a compound question, traditional approaches firstly detect topic entity mentioned in the question, as the starting point for traversing the knowledge graph, then find a chain of multiple ( 3) relations as a multi-hop2 path to golden answers.
We propose a learning-to-decompose agent which assists simple-question answerer to solve compound questions directly. Our agent learns a policy for decomposing compound question into simple ones in a meaningful way, guided by the feedback from the downstream simple-question answerer. The goal of the agent is to produce partitions and compute the compositional structure of questions with minimum information loss. The intuition is that encouraging the model to learn structural compositions of compound questions will bias the model toward better generalizations about how
1We assume that the number of corresponding relations is at most three. 2We are aware of the term multi-hop question in the literature. We argue that compound question is a better fit for the context of KBQA since multi-hop characterizes a path, not a question.
1

Under review as a conference paper at ICLR 2019
Figure 1: An example of answering compound questions. Given a question Q, we first identify the topic entity e with entity linking. By relation detection, a movie-to-actor relation f1, an actor-tomovie relation f2 and a movie-to-writer relation f3 forms a path to the answers Wi. Note that each relation fi corresponds to a part of the question. If we decomposes the question in a different way, we may find a movie-to-movie relation g as a shortcut, and g(e) = f2(f1(e)) = (f2  f1)(e) holds. Our model discovered such composite rules. See section 4 for further discussion.
the meaning of a question is encoded in terms of compositional structures on sequences of words, leading to better performance on downstream question answering tasks. We demonstrate that our agent captures the semantics of compound questions and generate interpretable decomposition. Experimental results show that our novel approach achieves state-of-theart performance in various challenging datasets, without re-designing complex neural networks to answer compound questions.
2 RELATED WORK
2.1 KNOWLEDGE-BASED QUESTION ANSWERING For combinational generalization (Battaglia et al., 2018) on the search space of knowledge graph, many approaches (Yih et al., 2014; Yin et al., 2016; Zhang et al., 2017) tackle KBQA in a tandem manner, i.e., topic entity linking followed by relation detection. An important line of research focused on directly parsing the semantics of natural language questions to structured queries (Cai & Yates, 2013; Kwiatkowski et al., 2013; Yao & Van Durme, 2014; Yao et al., 2014; Bao et al., 2014; Yih et al., 2014; 2015). An intermediate meaning representation or logical form is generated for query construction. It often requires pre-defined rules or grammars (Berant et al., 2013) based on hand-crafted features. By contrast, another line of research puts more emphasis on representanting natural language questions in stead of constructing knowledge graph queries. Employing CNNs (Dong et al., 2015; Yin et al., 2016) or RNNs (Dai et al., 2016; Yu et al., 2017), variable-length questions are compressed into their corresponding fix-length vector. Most approaches in this line focus on solving simple questions because of the limited expression power of fix-length vector, consistent with observations (Sutskever et al., 2014; Bahdanau et al., 2015) in Seq2Seq task such as Neural Machine Translation. Closely related to the second line of research. our proposed model learns to decompose compound question into simple questions, which eases the burden of learning vector representations for compound question. Once the decomposition process is completed, a simple-question answerer directly decodes the vector representation of simple questions to a inference chain of relations with the desired order, which resolves the bottleneck of KBQA.
2.2 REINFORCEMENT LEARNING FOR NATURAL LANGUAGE UNDERSTANDING Many reinforcement learning approaches learn sentence representations in a bottom-up manner. Yogatama et al. (2017) learn tree structures for the order of composing words into sentences using reinforcement learning with Tree-LSTM (Tai et al., 2015; Zhu et al., 2015), while Zhang et al. (2018)
2

Under review as a conference paper at ICLR 2019

Figure 2: An overview of our model and the flow of data. Two orange rounded rectangles correspond to components of our model, a learning-to-decompose agent and a simple-question answerer. The blue rectangles represent data in different forms. The solid line indicates the process of transforming data points, while the dashed line indicates the feedback (loss or reward) received by our model.

employs REINFORCE (Williams, 1992) to select useful words sequentially. Either in tree structure or sequence, the vector representation is built up from the words, which benefits the downstream natural language processing task such as text classification (Socher et al., 2013) and natural language inference (Bowman et al., 2015).
By contrast, from the top down, our proposed model learns to decompose compound questions into simple questions, which helps to tackle the bottleneck of KBQA piece by piece. See section 3 for more details.

3 MODEL
Figure 2 illustrates an overview of our model and the data flow. Our model consists of two components: a learning-to-decompose agent that decomposes each input question into at most three partitions and a simple-question answerer that maps each partition to its corresponding relation. We refer to the learning-to-decompose agent as the agent and the simple-question answerer as the answerer in the rest of our paper for simplicity.

3.1 LEARNING-TO-DECOMPOSE AGENT
Our main idea is to best divide an input question into at most three partitions which each partition contains the necessary information for the downstream simple-question answerer. Given an input question of N words3 x = {x1, x2, . . . , xN }, we assume that a sequence of words is essentially a partially observable environment and we can only observe the corresponding vector representation ot = xt  RD at time step t. Figure 3 summarizes the process for generating decision of compound question decomposition.
Memory Unit The agent has a Long Short-Term Memory (LSTM; Hochreiter & Schmidhuber (1997)) cell unrolling for each time step to memorize input history.

it = (Wi[xt, ht-1] + bi) gt = tanh(Wg[xt, ht-1] + bg) ct = ft ct-1 + it gt

ft = (Wf [xt, ht-1] + bf ) ot = (Wo[xt, ht-1] + bo) ht = ot tanh(ct)

(1)

where Wi, Wf , Wg, Wo  RH×(D+H), bi, bf , bg, bo  RH , and [·, ·] denotes the concatenation of two vectors. (·) is the element-wise sigmoid activation function.

3Note that the length of sequences may vary from questions to questions. We handle such dynamic directly without padding zeros. The similar situation exists in a game environment that allows early stopping.

3

Under review as a conference paper at ICLR 2019

Figure 3: A zoom-in version of the lower half of figure 2. Our agent consists of two components: a Memory Unit and an Action Unit. The Memory Unit observes current word at each time step t and updates the state of its own memory. We use a feedforward neural network as policy network for the Action Unit.

The state st  R2H of the agent is defined as

st = ct  ht

(2)

which maintained by the above memory cell (Eq. 1) unrolling for each time step.
Action Unit The agent also has a stochastic policy network (|s; W) where W is the parameter of the network. Specifically, we use a two-layer feedforward network that takes the agent's state s as its input:

(|s; W)  exp(W(2)(ReLU(W(1)s + b(1))) + b(2))

(3)

where W(1)  RH×2H , b(1)  RH , W(2)  R3×H and b(2)  R3.
Following the learned policy, the agent decomposes a question of length N by generating a sequence of actions t  {1st, 2nd, 3rd}, t = 1, 2, . . . , N . Words under the same decision (e.g. 1st) will be appended into the same sub-sequence (e.g. the first partition).
Formally, x(k) = {x1(k), x(2k), . . . , xt(kk)}, k = 1, 2, 3 denotes the partitions of a question. Note that in a partition, words are not necessarily consecutive4. The relative position of two words in original question is preserved. t1 + t2 + t3 = N holds for every question.
Reward The episodic reward R will be +1 if the agent helps the answerer to get the golden answers after each episode, or -1 otherwise. There is another reward function R =  log P (Y  | X) that is widely used in the literature of using reinforcement learning for natural language processing task (Bahdanau et al., 2017; Zhang et al., 2018). We choose the former as reward function for lower variance.
Each unique rollout (sequence of actions) corresponds to unique compound question decomposition. We do not assume that any question should be divided into exactly three parts. The decomposition of compound question structure is leave to be discovered by our agent. See section 4 for case study. The goal of our agent is to learn partition strategies that benefits the answerer the most.

4The subscripts of each partition are re-ordered for simplicity.

4

Under review as a conference paper at ICLR 2019

3.2 SIMPLE-QUESTION ANSWERER
With the help of the learning-to-decompose agent, a simple-question answerer can answer compound questions. Once the question is decomposed into partitions as simple questions, the answerer takes each partition x(k) = {x(1k), x2(k), . . . , x(tkk)} as input and classifies it as the corresponding relation in knowledge graph.
For each partition x(k), we use another LSTM network to construct simple-question representation directly from the vector of words in each partition, denoted by x(k)  RH . We again use a two-layer feedforward neural network to make prediction, i.e. estimate the probability of golden relation r.

P (y^ = r | x(k); Wp)  exp(Wp(2)(ReLU(Wp(1)x(k) + b(p1))) + b(p2))

(4)

where Wp(1)  RH×2H , b(p1)  RH , Wp(2)  R3×H and b(p2)  RC . C is the number of classes.
All partitions share the same simple-question answerer which is a relatively simple neural network without architectures usually appeared in the current state-of-the-art model such as attention mechanism (Sutskever et al., 2014).
Note that we use a classification network for sequential inputs that is as simple as possible. In addition to facilitating the subsequent theoretical analysis, the simple-question answerer we proposed is much simpler than good baselines for simple question answering over knowledge graph, without modern architecture features such as bi-directional process, read-write memory (Bordes et al., 2015), attention mechanism (Yin et al., 2016) or residual connection (Yu et al., 2017).
The main reason is that our agent learns to decompose input compound questions to the simplified version which is answerable for such a simple classifier. This can be a strong evidence for validating the agent's ability on compound question decomposition.

3.3 TRAINING OUR MODEL
The agent and the answerer share the same embeddings. The agent can only observe word embeddings while the answerer is allowed to update them in the backward pass.
We do not use the pre-train trick for all the experiments since we have already observed consistent convergence on different task settings. We reduce the variance of Monte-Carlo Policy Gradient estimator by taking multiple ( 5) rollouts for each question and subtracting a baseline that estimates the expected future reward given the observation at each time step.
The Baseline We follow Ranzato et al. (2016) which uses a linear regressor which takes the agent's memory state st as input and minimizes the mean squared loss for training. Such a loss signal is used for updating the parameters of baseline only. The regressor is an unbiased estimator of expected future rewards since it only depends on the agent's memory states.
Our agent learns a optimal policy to decompose compound questions into simple ones using MonteCarlo Policy Gradient (MCPG) method. The partitions of question is then feeded to a simplequestion answerer for policy evaluation. The agent takes the final episodic reward in return.

4 EXPERIMENTS
The goal of our experiments is to evaluate our hypothesis that our model discovers useful question partitions and composition orders that benefits a simple-question answerer to tackle compound question answering. Our experiments are three-fold. First, we trained the proposed model to master the order of arithmetic operators (e.g. + - ×÷) on an artificial dataset. Second, we evaluate our method on the standard benchmark dataset MetaQA (Zhang et al., 2017). Finally, we discuss some interesting properties of our agent by case study.
5

Under review as a conference paper at ICLR 2019

4.1 MASTERING ARITHMETIC SKILLS
The agent's ability of compound question decomposition can be viewed as the ability of priority assignment. To validate the decomposition ability of our proposed model, we train our model to master the order of arithmetic operations. We generate an artificial dataset of complex albebraic expressions. (e.g. 1 + 2 - 3 × 4 ÷ 5 =? or 1 + (2 - 3) × 4 ÷ 5). The algebraic expression is essentially a question in math language which the corresponding answer is simply a real number.
Specifically, the complex albebraic expression is a sequence of arithmetic operators including +, -, ×, ÷, ( and ). We randomly sample a symbol sequence of length N , with restriction of the legality of parentheses. The number of parentheses is P ( 2). The number of symbols surrounding by parentheses is Q. The position of parentheses is randomly selected in order to increase the diversity of expression patterns. For example, (+×) + (÷) and + × (+×) - ÷ are data points (1 + 2 × 3) + (4 ÷ 5) and 1 + 2 × (3 + 4 × 5) - 6 ÷ 7 with N = 8.
This task aims to test the learning-to-decompos agent whether it can assign a feasible order of arithemetric operations. We requires the agent to assign higher priority for operations surrounding by parentheses and lower priority for the rest of operations. We also require that our agent is able to learn a policy from short expressions (N  8), which generalizes to long ones (13  N  16).
We use 100-dimensional (D = 100) embeddings for symbols with Glorot initialization (Glorot & Bengio, 2010). The dimension of hidden state and cell state of memory unit H is 128. We use the RMSProp optimizer (Tieleman & Hinton, 2012) to train all the networks with the parameters recommended in the original paper, with the exception of learning rate . he learning rate for the agent and the answerer is 0.00001 while the learning rate for the baseline is 0.0001. We test the performance in different settings. Table 1 summarizes the experiment results.

Table 1: Agent Performance under Different Settings.

Train

Test Test ACC

N = 5, P = 0, Q = 0 N = 20, P = 0, Q = 0 99.21

N = 8, P = 1, Q = 3 N = 13, P = 1, Q = 3 93.37

N = 8, P = 1, Q = 3 N = 13, P = 1, Q = 7 66.42

The first line indicates that our agent learns an arithmetic skill that multiplication and division have higher priority than addition and subtraction. The second line indicates that our agent learns to discover the higher-priority expression between parentheses. The third line, compared to the second line, indicates that increasing the distance between two parentheses could harm the performance. We argue that this is because of the Long Short-Term Memory Unit of our agent suffers when carrying the information of left parenthesis for such a long distance.

4.2 KBQA BENCHMARK
We evaluate our proposed model on the test set of the most challenging KBQA research dataset, i.e. MetaQA (Zhang et al., 2017). Each question in this dataset is labeled with the golden topic entity and the inference chain of relations. The statistics of MetaQA dataset is shown in table 2. The number of compound questions is roughly twice that of simple questions. The max length of training questions is 16. The size of vocabulary in questions is 39,568.

Table 2: Data Statistics on MetaQA Dataset. 1-hop 2-hop 3-hop Total
Train 96,106 118,980 114,196 329,282 Dev 9,992 14,872 14,274 39,138 Test 9,947 14,872 14,274 39,093
The coupled knowledge graph contains 43,234 entities and 9 relations. We also augmented the relation set with the inversed relations if exists a proper meaning, as well as a "NO OP" relation as placeholder. The total number of relations we used is 14.
6

Under review as a conference paper at ICLR 2019

Figure 4: A continuous example of figure 1. The hollow circle indicates the corresponding action the agent takes for each time step.

One can be either assume topic entity of each question is linked, or use a simple string matching heuristic like character trigrams matching to link topic entity to knowledge graph directly. We use the former setting while the performance of the latter is reasonable good. We tend to evaluate the relation detection performance directly.
We use 100-dimensional (D = 100) word embeddings with Glorot initialization (Glorot & Bengio, 2010). The dimension of hidden state and cell state of memory unit H is 128. We use the RMSProp optimizer (Tieleman & Hinton, 2012) to train all the networks with the parameters recommended in the original paper, with the exception of the learning rate . The learning rate for the agent and the answerer is 0.00001 while the learning rate for the baseline is 0.0001. We use three samples for Monte-Carlo Policy Gradient estimator of REINFORCE. The metric for relation detection is overall accuracy that only cumulates itself if all relations of a compound question are correct.
Table 3 presents our results. The last column for total accuracy is the most representative for our model's performance since the only assumption we made about input questions is the corresponding relations are at most three.

Table 3: Accuracy on MetaQA test set.

Model

1-hop 2-hop

KV-MemNN

95.8 25.1

Bordes, Chopra, and Weston's QA system 95.7 81.8

VRN (Zhang et al., 2017)

97.5 89.9

Ours 96.1 86.2

3-hop 10.1 28.4 62.5 82.3

Total 37.6 65.8 81.8 86.9

4.3 CASE STUDY
Figure 4 illustrates a continuous example of figure 1 for case study, which is actually generated by our learning-to-decompose agent. Assuming the topic entity e is detected and replaced by a placeholder, the agent may discover two different structures of the question that is consistent with human intuition. Since the knowledge graph does not have a movie-to-movie relation named "share actor with", the lower partition can not help the answerer classify relations correctly. However, the upper partition will be rewarded. As a result, our agent optimizes its strategies such that it can decompose the original question in the way that benefits the downstream answerer the most.
We observe the fact that our model understands the concept of "share" as the behavior "take the inversed relation". That is, "share actors" in a question is decomposed to "share" and "actors" in two partitions. The corresponding formulation is g(e) = f2(f1(e)) = (f2  f1)(e). We observe the same phenomenon on "share directors". We believe it is a set of strong evidences for supporting our main claims.
5 DISCUSSION
Understanding compound questions, in terms of The Principle of Semantic Compositionality (Pelletier, 1994), require one to decompose the meaning of a whole into the meaning of parts. While
7

Under review as a conference paper at ICLR 2019
previous works focus on leveraging knowledge graph for generating a feasible path to answers, we propose a novel approach making full use of question semantics efficiently, in terms of the Principle of Semantic Compositionality. In other words, it is counterintuitive that compressing the whole meaning of variable-length sentence to a fixed-length vector, which leaves the burden to the downstream relation classifier. In contrast, we assume that a compound question can be decomposed into three simple questions at most. Our model generates partitions by a learned policy given a question. The vector representations of each partition are then fed into the downstream relation classifier. While previous works focus on leveraging knowledge graph for generating a feasible path to answers, we propose a novel approach making full use of question semantics efficiently, in terms of the Principle of Semantic Compositionality. Our learning-to-decompose agent can also serve as a plug-and-play module for other question answering task that requires to understand compound question. This paper is an example of how to help a simple-question answerer to understand compound question. The answerable question assumption must be relaxed in order to generalizing question answering. ACKNOWLEDGMENTS This work is funded by NSFC 61473260/61673338, and Supported by Alibaba-Zhejiang University Joint Institute of Frontier Technologies.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao. Knowledge-based question answering as machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 967­976, 2014.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1533­1544, 2013.
Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.
Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 632­642, 2015.
Qingqing Cai and Alexander Yates. Large-scale semantic parsing via schema matching and lexicon extension. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 423­433, 2013.
Zihang Dai, Lei Li, and Wei Xu. Cfo: Conditional focused neural question answering with largescale knowledge bases. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 800­810, 2016.
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. Question answering over freebase with multi-column convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pp. 260­269, 2015.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010.
Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, Hua Wu, and Jun Zhao. An endto-end model for question answering over knowledge base with cross-attention combining global knowledge. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 221­231, 2017.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1545­1556, 2013.
Francis Jeffry Pelletier. The principle of semantic compositionality. Topoi, 13(1):11­24, 1994.
Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In Proceedings of the 4th International Conference on Learning Representations (ICLR), 2016.
9

Under review as a conference paper at ICLR 2019
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631­1642, 2013.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. ACL, 2015.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report, 2012.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Xuchen Yao and Benjamin Van Durme. Information extraction over structured data: Question answering with freebase. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 956­966, 2014.
Xuchen Yao, Jonathan Berant, and Benjamin Van Durme. Freebase qa: Information extraction or semantic parsing? In Proceedings of the ACL 2014 Workshop on Semantic Parsing, pp. 82­86, 2014.
Wen-tau Yih, Xiaodong He, and Christopher Meek. Semantic parsing for single-relation question answering. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 643­648, 2014.
Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pp. 1321­1331, 2015.
Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and Hinrich Schu¨tze. Simple question answering by attentive convolutional neural network. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pp. 1746­1756, 2016.
Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to compose words into sentences with reinforcement learning. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.
Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. Improved neural relation detection for knowledge base question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 571­581, 2017.
Tianyang Zhang, Minlie Huang, and Li Zhao. Learning structured representation for text classification via reinforcement learning. AAAI, 2018.
Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. Variational reasoning for question answering with knowledge graph. arXiv preprint arXiv:1709.04071, 2017.
Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo. Long short-term memory over recursive structures. pp. 1604­1612, 2015.
10

