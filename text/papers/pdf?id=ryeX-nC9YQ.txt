Under review as a conference paper at ICLR 2019

DIMENSION-FREE BOUNDS FOR LOW-PRECISION TRAINING

Anonymous authors Paper under double-blind review

ABSTRACT

Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models. Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates. These bounds tend to depend on the dimension of the model d in that the number of bits needed to achieve a particular error bound increases as d increases. This is undesirable because a motivating application for low-precision training is large-scale models, such as deep learning, where d can be huge. In this paper, we prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic, which lets us better understand what affects the convergence of these algorithms as parameters scale. Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as lowprecision floating-point computation and logarithmic quantization.

1 INTRODUCTION

As machine learning models continue to scale to target larger problems on bigger data, the task of training these models quickly and efficiently becomes an ever-more-important problem. One promising technique for doing this is low-precision computation, which replaces the 32-bit or 64-bit floating point numbers that are usually used in ML computations with smaller numbers, often 8-bit or 16-bit fixed point numbers. Low-precision computation is a broadly applicable technique that has received a lot of attention, especially for deep learning, and specialized hardware accelerators have been developed to support it (Jouppi et al., 2017; Burger, 2017; Caulfield et al., 2017).

A major application for low-precision computation is the training of ML models using empirical

risk minimization. This training is usually done using stochastic gradient descent (SGD), and most

research in low-precision training has focused on low-precision versions of SGD. While most of

this work is empirical (Wu et al., 2018; Das et al., 2018; Zhu et al., 2016; Ko¨ster et al., 2017;

Lee et al., 2017; Hubara et al., 2016; Rastegari et al., 2016; Zhou et al., 2016; Gupta et al., 2015;

Courbariaux et al., 2014; 2015; De Sa et al., 2017), significant research has also been done in the

theoretical analysis of low-precision training. This theoretical work has succeeded in proving bounds

on the convergence rate of low-precision SGD and related low-precision methods in various settings,

including for convex (De Sa et al., 2018; Zhang et al., 2017) and non-convex objectives (De Sa et al.,

2015; Li et al., 2017; Alistarh et al., 2017). One common characteristic of these results is that the

bounds tend to depend on the dimension d of the model being learned (equivalently, d is the number

of parameters). For example, (Li et al., 2017) gives the convergence bound 

E[f (w¯T )

- f (w)]



(1 +

log(T + 1))m2 ax 2µT

+

max 2

d ,

(1)

where the objective f is strongly convex with parameter µ, low-precision SGD outputs w¯T after T iterations, w is the true global minimizer of the objective, m2 ax is an upper bound on the second moment of the stochastic gradient samples E[ f~(w) 22]  m2 ax, and  is the quantization step, the difference between adjacent numbers in the low-precision format. Notice that, as T  , this

bound shows convergence down to a level of error that increases with the dimension d. Equivalent-

ly, in order to achieve the same level of error as d increases, we would need to use more bits of

quantization to make  smaller. Similar dimension-dependent results, where either the error or the

number of bits needed increases with d, can also be seen in other work on low-precision training

algorithms (Alistarh et al., 2017; Zhang et al., 2017; De Sa et al., 2018). This dependence on d is

unsatisfying because the motivation for low-precision training is to tackle large-scale problems on big data, where d can range up to 108 or more for commonly used models (Simonyan and Zisserman,

1

Under review as a conference paper at ICLR 2019

Table 1: Summary of our dimension-free results compared with prior work. The values report
the number of bits needed, according to the theoretical bound, for the LP-SGD (Li et al., 2017) algorithm to achieve an expected objective gap (f (w) - f (w)) of when we let step size   0, epoch length T  . Here we let R denote the radius of the range of numbers representable in the low-precision format and assume w 2 = (R). The rest of the parameters can be found in the assumptions to be introduced later.

NUMBER OF BITS NEEDED FOR E[f (w) - f (w)] 

PRIOR DIMENSION-DEPENDENT BOUND

 log2 O Rmax d/

OUR DIMENSION-FREE BOUND

log2 O R1/

DIMENSION-FREE WITH LOGARITHMIC QUANTIZATION log2 O (R/) · log 1 + 1/

2014). For example, to compensate for a factor of d = 108 in (1), we could add bits to decrease the quantization step  by a factor of d, but this would require adding log2(104)  13 bits, which is significant compared to the 8 or 16 bits that are commonly used in low-precision training.

In this paper, we address this problem by proving dimension-free bounds on the convergence of LP-SGD Li et al. (2017). Our main technique for doing so is a tight dimension-independent bound on the expected quantization error of the low-precision stochastic gradients in terms of the 1-norm. Our results are summarized in Table 1, and we make the following contributions:
· We describe conditions under which we can prove a dimension-free bound on the convergence of SGD with fixed-point, quantized iterates on strongly convex problems.
· We study non-linear quantization schemes, in which the representable low-precision numbers are distributed non-uniformly. We prove dimension-free convergence bounds for SGD using logarithmic quantization (Lee et al., 2017), and we show that using logarithmic quantization can reduce the number of bits needed for HALP to provably converge.
· We study quantization using low-precision floating-point numbers, and we present theoretical analyis that suggests how to assign a given number of bits to exponent and mantissa to optimize the accuracy of training algorithms. We validate our results experimentally.

2 RELATED WORK
Motivated by the practical implications of faster machine learning, much work has been done on low-precision training. This work can be roughly divided into two groups. The first focuses on training deep models with low-precision weights, to be later used for faster inference. For some applications, methods of this type have achieved good results with very low-precision models: for example, binarized (Courbariaux et al., 2015; Hubara et al., 2016; Rastegari et al., 2016) and ternary networks (Zhu et al., 2016) have been observed to be effective (although as is usual for deep learning they lack theoretical convergence results). However, these approaches are still typically trained with full-precision iterates: the goal is faster inference, not faster training (although faster training is often achieved as a bonus side-effect).
A second line of work on low-precision training, which is applied to both DNN training and nondeep-learning tasks, focuses on making various aspects of SGD low-precision, while still trying to solve the same optimization problem as the full-precision version. The most common way to do this is to make the iterates of SGD (the wt in the SGD update step wt+1 = wt - tft(wt)) stored and computed in low-precision arithmetic (Courbariaux et al., 2014; Gupta et al., 2015; De Sa et al., 2018; 2015; Li et al., 2017). This is the setting we will focus on most in this paper, because it has substantial theoretical prior work which exhibits the dimension-dependence we set out to study (Li et al., 2017; Zhang et al., 2017; Alistarh et al., 2017; De Sa et al., 2018). The only paper we found with a bound that was not dimension-dependent was De Sa et al. (2015), but in that paper the authors required that the gradient samples be 1-sparse (have only one nonzero entry), which is not a realistic assumption for most ML training tasks. In addition to quantizing the iterates, other work has studied quantizing the training set (Zhang et al., 2017) and numbers used to communicate among parallel workers (Alistarh et al., 2017). We expect that our results on dimension-free bounds will be complementary with these existing theoretical approaches, and we hope that they can help to explain the success of the exciting empirical work in this area.

3 DIMENSION-FREE BOUNDS FOR SGD
In this section, we analyze the performance of stochastic gradient descent (SGD) using low-precision training. Though there are numerous variants of this algorithm, SGD remains the de facto algorithm

2

Under review as a conference paper at ICLR 2019

used most for machine learning. We will start by describing SGD and how it can be made low-

precision. Suppose we are trying to solve the problem

1 minimize: f (w) =
n

n

f~i(w)

i=1

over: w  Rd.

(2)

SGD solves this problem iteratively by repeatedly running the update step

wt+1 = wt - f~it (wt)

(3)

where  is the step size1 or learning rate, and it is the index of a component function chosen ran-

domly and uniformly at each iteration from {1, . . . , n}. To make this algorithm low-precision, we

quantize the iterates (the vectors wt) and store them in a low-precision format. The standard format to use lets us represent numbers in a set

dom(, b) = {- · 2b-1, · · · , -, 0, , · · · ,  · (2b-1 - 1)}

with  > 0 being the quantization gap, the distance between adjacent representable numbers, and b  N being the number of bits we use (De Sa et al., 2018). Usually,  is a power of 2, and this scheme is called fixed-point arithmetic. It is straightforward to encode numbers in this set as b-bit signed integers, by just multiplying or dividing by  to convert to or from the encoded format--
and we can even do many arithmetic computations on these numbers directly as integers. This
is sometimes called linear quantization because the representable points are distributed uniformly
throughout their range. However, as the gradient samples will produce numbers outside this set dur-
ing iteration, we need some way to map these numbers to the set of numbers that we can represent. The standard way to do this is with a quantization function Q(,b)(x) : R  dom(, b). While many quantization functions have been proposed, the one typically used in theoretical analysis (which we
will continue to use here) is randomized rounding. Randomized rounding, also known as unbiased rounding or stochastic rounding, rounds up or down at random such that E[Q(,b)(x)] = x whenever x is within the range of representable numbers (i.e. when - · 2b-1  x   · (2b-1 - 1)). When x is outside that range, we quantize it to the closest representable point. When we apply Q(,b) to a vector argument, it quantizes each of its components independently.

Using this quantization function, we can write the update step for low-precision SGD (LP-SGD),

which is a simple quantization of (3),

wt+1 = Q(,b) wt - f~it (wt)

(4)

As mentioned before, one common feature of prior bounds on the convergence of LP-SGD is that

they depend on the number of dimensions d, whereas bounds on full precision SGD under the same

conditions do not do so. This difference is due to the fact that, when we use a quantization function

Q to quantize a number w, it increases its variance by E[(Q(w) - w)2]  2/4. Observe that this

inequality is tight since it holds as an equality when w is in the middle of two quantization points, e.g. w = /2, as illustrated in Figure 1(a). When quantizing a vector w  Rd, the squared error can

be increased by

E[

Q(w) - w

2 2

]

=

d

E[(Q(wk)

-

wk )2 ]



2d ,
4

k=1

(5)

and this bound is again tight. This variance inequality is the source of the d term in analyses of

LP-SGD, and the tightness of the bound leads to the natural belief that the d term is inherent, and

that low-precision results are inevitably dimension-dependent.

However, we propose that if we can instead bound the variance in (5) with some properties of the problem itself that do not change as d changes, we can achieve a result that is dimension-

independent. One way to do this is to look at the variance graphically. Figure 3 plots the quantization error as a function of w along with the bound in (5). Notice that the squared error looks like a series

of parabolas, and the bound in (5) is tight at the top of those parabolas, but loose elsewhere. Instead,
suppose we want to do the opposite and produce a bound that is tight when the error is zero (at points in dom(, b)). To do this, we observe that E[(Q(w) - w)2]  |w - z| for any z  dom(, b). This

bound is also tight when z is adjacent to w, and we plot it in Figure 3 as well. The natural vector

analog of this is

E[ Q(w) - w 22] 

d k=1

|wk

-

zk |

=



w-z

1 , z  dom(, b)d

(6)

1Usually in SGD the step size is decreased over time, but here for simplicity we consider a constant learning rate schedule.

3

Under review as a conference paper at ICLR 2019

Variance and Bounds loss gap

Variance and Bounds for Linear Quantization
1.0 actual variance 2/4 bound
0.8 our l1-norm bound 0.6
0.4
0.2
0.0
0.2 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 quantized value

0.200 0.175

Convergence of Low-Precision SGD
d = 128, fp d = 1024, fp

0.150 0.125

d = 8192, fp d = 128, b = 8 d = 1024, b = 8

0.100

d = 8192, b = 8 d = 128, b = 6

0.075 d = 1024, b = 6

0.050 d = 8192, b = 6

0.025

0.000 0

200000 400000 600000 800000 1000000 iterations

(a) The squared quantization error E[(Q(w) - w)2] and two possible tight upper bounds in the
one-dimensional case.

(b) The convergence of training loss gap f (w) - f (w) as we train low-precision SGD, showing the effect of different model sizes and precision.

Figure 1: (a) Quantization error bounds; (b) Convergence of full-precision (fp) SGD and LP-SGD.

where · 1 denotes the L1 norm. This is a dimension-independent bound we can use to replace (5) to bound the convergence of LP-SGD and other algorithms. However, this replacement is nontrivial as our bound is now non-constant: it depends on w, which is a variable updated each iteration. Also, in order to bound this new L1 norm term, we will need some new assumptions about the problem. Next, we will state these assumptions, along with the standard assumptions used in the analysis of SGD for convex objectives, and then we will use them to present our dimension-free bound on the convergence of SGD.

Assumption 1 All the loss functions f~i are differentiable, and their gradients are L-Lipschitz continuous in the sense of 2-norm, that is,

i  {1, 2, · · · , n}, x, y  Rd, f~i(x) - f~i(y) 2  L x - y 2

Assumption 2 All the gradients of the loss functions f~i are L1-Lipschitz continuous in the sense of 1-norm to 2-norm, that is,
i  {1, 2, · · · , n}, x, y  Rd, f~i(x) - f~i(y) 1  L1 x - y 2

These two assumptions are simply expressing of Lipschitz continuity in different norms. Assumption 1 is a standard assumption in the analysis of SGD on convex objectives, and has been applied in the low-precision case as well in prior work (De Sa et al., 2018). Assumption 2 is analogous to 1, except we are bounding the L1 norm instead of the L2 norm. This holds naturally (with a reasonable value of L1) for many problems, in particular problems for which the gradient samples are sparse.

Assumption 3 The gradient of the total loss function f is µ-strongly convex for some µ > 0:

w, v,

f (w) - f (v) - µ 2

w-v

2 2

 (w - v)T f (v)

Assumption 3 is a standard assumption that bounds the curvature of the loss function f , and is satisfied for many classes of convex objectives. For example, any convex loss with L2 regularization will always be strongly convex. When an objective is strongly convex and Lipschitz continuous, it is standard to say it has condition number  = L/µ, and here we extend this to say it has L1 condition number 1 = L1/µ.

Assumption 4 The gradient of each loss function is bounded by some constant  near the optimal

point in the sense of l1 and l2 norm, that is,

E[

f~i(w)

2 2

]



2,

E[ f~i(w) 1]  1

This assumption constrains the gradient for each loss function at the optimal point. We know

f (w)

=

1 n

i ~ fi(w) = 0, so it is intuitive that each f~i(w) can be bounded by some

value. Therefore this is a natural assumption to make and it has been used in a lot of other work in

this area. Note that this assumption only needs to hold under the expectation over all f~i. With these

assumptions, we proved the following theorem for low-precision SGD:

Theorem 1 Suppose that we run LP-SGD on an objective that satisfies Assumptions 1­4, and with step size  < 1/(22µ). After T LP-SGD update steps (4), select w¯T uniformly at random from

4

Under review as a conference paper at ICLR 2019

{w0, w1, . . . , wT -1}: i.e. w¯T is a random model from the trajectory of LP-SVRG so far. Then, the

expected objective gap of w¯T is bounded by

E[f (w¯T ) - f (w)] 

1 2T

w0 - w

2 2

+

2 + 1 2

+

212µ 4

This theorem shows a bound of the expected distance between the result we get at K-th iteration

and the optimal value. By choosing an appropriate step size we can achieve convergence at a 1/T

rate, while the limit we converge to is only dependent on dimension-free factors. Meanwhile, as

mentioned in the first section, previous work gives a dimension-dependent bound (1) for the prob-

lem, which also converges at a 1/T rate.2 Therefore our result guarantees a dimension-independent

convergence limit without weakening the convergence rate.

It is important to note that, because the dimension-dependent bound in (5) was tight, we should not expect our new result to improve upon the previous theory in all cases. In the worst case, 1 = d· and similarly 1 = d· ; this follows from the fact that for vectors in Rd, the norms are related by the inequality x 1  d · x 2. Substituting this into our result produces a dimension-dependent bound again. This illustrates the importance of introducing the new parameters 1 and 1 and requiring that they be bounded; if we could not express our bound in terms of these parameters, the
best we could do here is recover a dimension-dependent bound.

Experiments Next, we validate our theoretical results experimentally. To do this, we analyzed

how the size of the noise floor of convergence of SGD and LP-SGD varies as the dimension is

changed for a class of synthetic problems. Importantly, we needed to pick a class of problems for

which the parameters L, L1, µ, , and 1, did not change as we changed the dimension d. To do this,

we chose a class of synthetic linear regression models with loss components sampled independently

and identically as

f~i(w)

=

1 (x~T w 2

-

y~)2

where x~ is a sparse vector sampled to have s nonzero entries each of which is sampled uniformly from {-1, 1}, and y~ is sampled from N (x~T w, 2) for some variance parameter . Importantly, the

nonzero entries of x~ were chosen non-uniformly such that Pr[x~i = 0] = pi for some probabilities pi which decrease as i increases; this lets us ensure that µ remains constant as d is increased. For sim-

plicity, we sampled a fresh loss component of this form at each SGD iteration, which is sometimes

called

the

online µ

setting. = pd

It

is L

=strasightfoLrw1 a=rdstosderivet2ha=t for2sthis

problem 1 =

2s/.

We set  = 0.01,  = 0.2, p1 = 0.9, pd = 0.001, and s = 16, we chose each entry of w uniformly from [-1/2, 1/2], and we set  such that the low-precision numbers would range from -1 to 1.

Figure 1(b) shows the convergence of SGD and LP-SGD as the dimension d is changed, for both

8-bit and 6-bit quantization. Notice that while changing d has an effect on the initial convergence

rate for both SGD and LP-SGD, it has no effect on the noise ball size, the eventual loss gap that the

algorithm converges to. Figure 2(a) measures this noise ball size more explicitly as the dimension

is changed: it reports the loss gap averaged across the second half of the iterates. Notice that as

the dimension d is changed, the average loss gap is almost unchanged, even for very low-precision

methods for which the precision does significantly affect the size of the noise ball. This validates

our dimension-free bounds, and shows that they can describe the actual dependence on d in at least

one case.

Figure 2(b) validates our results in the opposite way: it looks at how this gap changes as our new

parameters 1 and changed s across a

L1 change while d, µ, and range, setting  = 0.8/ s,

 are kept fixed. To do this, we fixed d = 1024 and which keeps 2 constant as s is changed: this has the

effect of changing 1 (and, as a side effect, L1 and L). We can see from figure 2(b) that changing 1

in this way has a much greater effect on LP-SGD than it does on SGD. This validates our theoretical

results, and suggests that 1 and L1 can effectively determine the effect of low-precision compute

on SGD.

4 NON-LINEAR QUANTIZATION
Up till now, most theoretical work in the area of low-precision machine learning has been on linear quantization, where the distance between adjacent quantization points is a constant value . Another
2Previous work (1) used a decaying step size while ours uses a constant step size to achieve a better result.

5

Under review as a conference paper at ICLR 2019

asymptotic average loss gap asymptotic average loss gap

0.1D0 imension

vs.

Noise

Ball

Size

of

Low-Precision SGD
full precision

0.08

8-bit 6-bit

0.06

0.04

0.02

0.00 64 128 256 512 1024 2048 4096 8192 model size

0.200 0.175

1 vs. Noise Ball Size of Low-Precision SGD
full precision 8-bit

0.150 6-bit

0.125

0.100

0.075

0.050

0.025

0.000 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 L1 gradient noise at optimum 1

(a) The size of the noise ball is not significantly affected by model size d.

(b) The size of the noise ball does depend on 1, especially when precision is low.

Figure 2: Plots of the asymptotic loss gap from Figure 1(b) as a function of model size d and 1.

option is non-linear quantization (NLQ), in which we quantize to a set of points that are nonuniformly distributed. This approach has been shown to be effective for accelerating deep learning in some settings (Lee et al., 2017). In general, we can quantize to a set of points

D = {-qn, · · · , -q1, q0, q1, · · · , qn-1},

and, just like with linear quantization, we can still use a quantization function Q(w) with randomized

rounding that rounds up or down to a number in D in such a way that E[Q(w)] = w for w 

[-qn, qn-1]. When we consider the quantization variance here, the natural dimension-dependent

bound would be

E[

Q(w) - w

2 2

]



d 4

max(qi
i

-

qi-1)2.

This is still a tight bound since it holds with equality for a number in the middle of two most distant

quantization points. However, when applied in the analysis of LP-SGD, this bound induces poor

performance and often under-represents the actual result.

Here we discuss a specific NLQ method and use it to introduce a tight bound on the quantization variance. This method has been previously studied as logarithmic quantization or µ-law quantiza-
tion, and is defined recursively by

q0 = 0,

qi+1 - qi =  + qi

(7)

where  > 0 and  > 0 are fixed parameters. Note that this includes linear quantization as a special case by setting  = 0. It turns out that we can prove a tight dimension-independent bound on the quantization variance of this scheme. First, we introduce the following definition.

Definition 1 An unbiased quantization function Q satisfies the dimension-free variance bound with parameters , , and  if for all w  [-qn, qn-1] and all z  D,

E[

Q(w) - w

2 2

]





w-z

1+

z

2·

w-z 2+

w-z

2 2

.

We can prove that our logarithmic quantization scheme satisfies this bound.

Lemma 1 The logarithmic quantization scheme (7) satisfies the dimension-free variance bound

with

parameters

,

,

and



=

2 4(+1)

<

 4

.

Notice that this bound becomes identical to the linear quantization bound (6) when  = 0, so this result is a strict generalization of our results from the linear quantization case. With this setup, we can apply NLQ to the low-precision training algorithms we have studied earlier in this paper.

Theorem 2 Suppose that we run LP-SGD on an objective that satisfies Assumptions 1­4, and using

a

quantization

scheme

that

satisfies

the

dimension-free

variance

bound.

If



<

1 

,

then

E[(f (w~)-f (w))] 

w0 - w

2 2

+

(1

+

)2

+

1

+



w

2 + (L1 + L

w

2 + )2

2T 2

4µ

6

Under review as a conference paper at ICLR 2019

Variance and Bounds Variance and Bound

Variance and Bounds for Non-Linear Quantization

14000 12000

actual variance our bound

10000

8000

6000

4000

2000

0 0 200 400 qu6a0n0tized8v0a0lue1000 1200 1400

Variance and Bound for de-normal FPQ

200

actual variance our bound

150

100

50

0 0 50 100 150 200 250 quantized value

(a) variance and bound for µ­law quantization

(b) variance and bound for FPQ

Figure 3:

A figure showing the actual quantization variance E[

Q(w) - w

2 2

]

and

the

tight

upper

bound that we introduced in one dimension. Similarly to 1(a) we plot this bound when taking the

minimum over all possible z.

This theorem is consistent with Theorem 1 in that, if we set  =  = 0, which makes logarithmic quantization linear, they would have an identical result. If we fix the representable range R (the
largest-magnitude values representable in the low-precision format) and choose our quantization
parameters optimally, we get the result that the number of bits we need to achieve objective gap is log2 O (R/)·log 1+1/ . This bound is notable because even in the worst case wherewe do not have a bound on 1 and must use 1  d·, this bound gives us log2 O (R/)·log 1+ d . That is, while a dimension-dependent factor still remains, it is now "hidden" within a log term. This
greatly decreases the effect of the dimension, and suggests that NLQ may be a promising technique
to use for low-precision training at scale.

Floating point. Next, we look at another type of non-linear quantization that is of great practical use: floating-point quantization (FPQ). Here, the quantization points are simply floating-point numbers with some fixed number of exponential bits be and mantissa bits bm. Floating-point numbers are represented in the form

(-1)sign bit · 2exponent-bias · (1.m1m2m3 . . . mbm )

(8)

where "exponent" is a be-bit unsigned number, the mi are the bm bits of the mantissa, and "bias" is a term that sets the range of the representable numbers by determining the range of the exponent. In standard floating point numbers, the exponent ranges from [-2be-1 + 2, 2be-1 - 1], which corresponds to a bias of 2be-1 - 1. To make our results more general, we also consider non-standard bias by defining a scaling factor s = 2-(bias-standard bias); the standard bias setting corresponds to s = 1.
We also consider the case of denormal floating point numbers, which tries to address underflow by replacing the 1 in (8) with a 0 for the smallest exponent value. Under these conditions, we can prove
that floating-point quantization satisfies the bound in Definition 1.

Lemma 2 The FPQ scheme using randomized rounding satisfies the dimension-free variance bound

with parameters normal, , and  for normal FPQ and denormal, , and  for denormal FPQ where

normal

=

4s 22be

,

denormal

=

,8s
22be +bm

 = 2-bm ,



=

2 4(+1)

.

This bound can be immediately combined with Theorem 2 to produce dimension-independent bounds on the convergence rate of low-precision floating-point SGD. If we are given a fixed number of total bits b = be + bm, we can minimize this upper bound on the objective gap to try to predict the best way to allocate our bits between the exponent and the mantissa. Unfortunately, there is no analytical expression for this optimal choice of be. To give a sense of the asymptotic behavior of this optimal allocation, we present upper and lower bounds on it.

Theorem 3 When using FPQ without denormal numbers, given b total bits, the optimal number of exponential bits be such that the asymptotic upper bound on the objective gap given by Theorem 2 is minimized is in the interval between:

log2 2 log2

2(ln 2)s1  w 2

+ 2b

and

log2 2 log2

2(ln 2)sL1 L w 2+

+ 2b .

7

Under review as a conference paper at ICLR 2019

noise ball size noise ball size theoretical noise ball

SGD noise ball size under FPQ with 16 bits
106 empirical result 105 theory bound 104 103 102 101 100 10 1 10 2
2345678 number of exponential bits

16 bits noise ball of FPQ on MNIST

empirical results

theory bound

106

optimal interval

100

105

10 1 104

10 2 2 3 4 5 6 7 8 9 103 number of exponential bits

(a) Training noise ball size of SGD using 20 bits normal FPQ on synthetic data set

(b) Training noise ball size of SGD using 16 bits normal FPQ on MNIST

Figure 4: Plots of noise ball size vs. be when running SGD with 20 and 16 bits FPQ on synthetic data set and MNIST. Note the use of two y-axes in Figure 4(b) to make the series fit in one figure.

Theorem 4 When using denormal FPQ, given b total bits, the optimal number of exponential bits

be such that the asymptotic upper bound on the objective gap, as T   and   0, given by

Theorem 2 is minimized is in the interval between:

log2

1

-

2 ln 2

W

e w 2 8s1

and

log2

1

-

2 ln 2

W

e(L w 2+) 8sL1

where e denotes the base of the natural logarithm and W stands for the Lambert W function. In cases where neither of these two values exists, the noise ball size increases as be, thus be = 2 would be the optimal setting, which is equivalent to linear quantization.

These theorems give us an idea of where the optimal setting of be lies such that the theoretical asymptotic error is minimized. When using normal FPQ, this optimal assignment of be is O(log(b)), and for denormal FPQ the result is independent of b. This suggests that once the total number of bits
grows past a threshold, we should assign most of or all the extra bits to the mantissa.

Experiments For FPQ, we ran experiments on two different data sets. First, we ran LP-SGD on the same synthetic data set that we used for linear regression. Here we used normal FPQ with 20 bits in total, and we get the result in Figure 4(a). In this diagram, we plotted the empirical noise ball size, its theoretical upper bound, and the optimal interval for be as Theorem 3 predicts. As the figure shows, our theorem accurately predicts the optimal setting of exponential bits, which is 5 in this case, to minimize both the theoretical upper bound and the actual empirical result of the noise ball size, despite the theoretical upper bound being loose.
Second, we ran LP-SGD on the MNIST dataset (Deng, 2012). To set up the experiment, we normalized the MNIST data to be in [0, 1] by dividing by 255, then subtracted out the mean for each features. We ran multiclass logistic regression using an L2 regularization constant of 10-4 and a step size of  = 10-4, running for 500 total epochs (passes through the dataset) to be sure we converged. For this task, our (measured) problem parameters were L = 37.41, L1 = 685.27,  = 2.38, 1 = 29.11, and d = 784. In Figure 4(b), we plotted the observed loss gap, averaged across the last ten epochs, for LP-SGD using various 16-bit floating point formats. We also plot our theoretical bound on the loss gap, and the predicted optimal number of exponential bits to use based on that bound. Our results show that even though our bound is very loose for this task, it still predicts the right number of bits to use with reasonable accuracy. This experiment also validates the use of IEEE standard half-precision floating-point numbers, which have 5 exponential bits, for this sort of task.

5 CONCLUSION
In this paper, we presented dimension-independent bounds on the convergence of SGD when applied to low-precision training. We pointed out the conditions under which such bounds hold. We further extended our results to non-linear methods of quantization: logarithmic quantization and floating point quantization. We analyzed the performance of SGD under logarithmic quantization and demonstrated that NLQ is a promising method for reducing the number of bits required in lowprecision training. We also presented ways in which our theory could be used to suggest how to allocate bits between exponent and mantissa when FPQ is used. We hope that our work will encourage further investigation of non-linear quantization techniques.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1707­1718, 2017.
Doug Burger. Microsoft unveils project brainwave for real-time ai. Microsoft Research, Microsoft, 22, 2017.
Adrian M Caulfield, Eric S Chung, Andrew Putnam, Hari Angepat, Daniel Firestone, Jeremy Fowers, Michael Haselman, Stephen Heil, Matt Humphrey, Puneet Kaur, et al. Configurable clouds. IEEE Micro, 37(3):52­61, 2017.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123­3131, 2015.
Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas, et al. Mixed precision training of convolutional neural networks using integer operations. arXiv preprint arXiv:1802.00930, 2018.
Christopher De Sa, Matthew Feldman, Christopher Re´, and Kunle Olukotun. Understanding and optimizing asynchronous low-precision stochastic gradient descent. In Proceedings of the 44th Annual International Symposium on Computer Architecture, pages 561­574. ACM, 2017.
Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger, Kunle Olukotun, and Christopher Re´. High-accuracy low-precision training. arXiv preprint arXiv:1803.03383, 2018.
Christopher M De Sa, Ce Zhang, Kunle Olukotun, and Christopher Re´. Taming the wild: A unified analysis of hogwild-style algorithms. In Advances in neural information processing systems, pages 2674­2682, 2015.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141­142, 2012.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pages 1737­ 1746, 2015.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In Advances in neural information processing systems, pages 4107­4115, 2016.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In International Conference on Neural Information Processing Systems, pages 315­ 323, 2013.
Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture, pages 1­12. ACM, 2017.
Urs Ko¨ster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K Bansal, William Constable, Oguz Elibol, Scott Gray, Stewart Hall, Luke Hornof, et al. Flexpoint: An adaptive numerical format for efficient training of deep neural networks. In Advances in Neural Information Processing Systems, pages 1742­1752, 2017.
Edward H Lee, Daisuke Miyashita, Elaina Chai, Boris Murmann, and S Simon Wong. Lognet: Energy-efficient neural networks using logarithmic computation. In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pages 5900­5904. IEEE, 2017.
9

Under review as a conference paper at ICLR 2019
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems, pages 5813­5823, 2017.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pages 525­542. Springer, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. arXiv preprint arXiv:1802.04680, 2018.
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. Zipml: Training linear models with end-to-end low precision, and a little bit of deep learning. In International Conference on Machine Learning, pages 4035­4043, 2017.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064, 2016.
10

Under review as a conference paper at ICLR 2019

A ALGORITHMS
In our work, we presented dimension-free bounds on the performance of low-precision SGD. Later in this appendix we will demostrate the broad applicability of our techniques by extending our results to two other low-precision machine learning algorithms that have been proposed in previous work (De Sa et al., 2018). Here we present all of these algorithms in detail for completeness.
Algorithm 1 LP-SGD: Low-Precision Stochastic Gradient Descent given: n loss functions f~i, number of epochs T , step size , and initial iterate w0. given: low-precision representation (, b) for t = 1 to T do sample it uniformly from {1, 2, · · · , n}, quantize wt+1  Q(,b) wt - f~it (wt) end for return wT

Algorithm 2 LP-SVRG: Low-Precision SVRG

given: n loss functions fi, number of epochs K, epoch length T , step size , and initial iterate w~0.

given: low-precision representation (, b)

for k = 1 to K do

f (w~k)

=

1 n

N i=1

fi

(w~k

)

wk,0  w~k

for t = 1 to T do

sample i uniformly from {1, . . . , n}

uk,t  wk,t-1 -  (fi(wk,t-1) - fi(w~k) + f (w~k)) quantize wk,t  Q(,b) (uk,t)

end for

option I: set w~T  wk,T option II: sample t uniformly from {0, . . . , T - 1}, then set w~T  wk,t

end for

return w~T

Algorithm 3 HALP: High-Accuracy Low-Precision SGD

given: n loss functions fi, number of epochs K, epoch length T , step size , and initial iterate w~0.

given: number of low-precision-representation bits b.

for k = 1 to K do

f (w~k)

=

1 n

N i=1

fi

(w~k

)

re-scale:

k



g~k 2 µ(2b-1 -1)

zk,0  Q(k,b)(0) for t = 1 to T do

sample i uniformly from {1, . . . , n}

uk,t  zk,t-1 -  fi(w~k + zk,t-1) - fi(w~k) + f (w~k)
quantize: zk,t  Q(k,b) (uk,t) end for

option I: set w~T  w~k + zk,T option II: sample t uniformly from {0, . . . , T - 1}, then set w~T  w~k + zk,t

end for

return w~T

In both LP-SVRG and HALP we use option II to assign the next outer iterate, as was done in the prior work. And when applying HALP under logarithmic quantization, we set the logarithmic quantization parameter  and k in the re-scale step to be:

 = ,
4( + 2)

k = µ

 g~k 2 (1 + )2b-1 - 1

.

This assignment of k is natural because it leads to low-precision numbers that have the same range as the linear-quantization numbers in the original HALP algorithm.

11

Under review as a conference paper at ICLR 2019

B BOUNDS FOR SGD VARIANTS

In this section, we are going to show that the dimension-free bound of low-precision machine learning applies not only to SGD, but also to some variants: low-precision stochastic variance-reduced gradient (LP-SVRG) and high-accuracy low-precision (HALP) (De Sa et al., 2018). This result will further indicate the breadth of the technicality of our work. Since the algorithms are long and have both been described and analyzed theoretically in previous work, we will not restate them here, but we have included them in the supplementary material for completeness. Both these algorithms are similar to LP-SGD in that they run a quantized update rule similar to (4) with some step size . However, unlike LP-SGD, they run this low-precision update rule in an inner loop for some number of iterations, the epoch length T , and then supplement it in an outer loop with some amount of fullprecision computation. That is, HALP and LP-SVRG supplement their low-precision inner loop with infrequent full-precision computation, and the parameter T controls how often this is done. Here, we will state dimension-free bounds on the convergence of these algorithms, and show that we do not need to change  or T significantly to do so, compared with the previous work (De Sa et al., 2018).
For HALP and LP-SVRG, we no longer need the bound on the gradient of loss functions in Assumption 4 since the algorithms use the technique of variance reduction which eliminates the asymptotic dependence on the variance of the gradient samples. We still require the similar assumptions on Lipschitz continuity and strong convexity (Assumptions 1­3) for our theorem to hold. Note that, as was the case for LP-SGD, the prior dimension-dependent analysis for LP-SVRG and HALP requires only Assumptions 1 and 3, but not Assumption 2. Now we present dimension-free bounds with results in terms of the condition numbers  = L/µ and 1 = L1/µ. Firstly, for LP-SVRG (Low-Precision Stochastic Variance-Reduced Gradient), we proved the following.

Theorem 5 Suppose that we run LP-SVRG algorithm under Assumptions 1­3 with initial weights w~0. For any constant 0 <  < 1 (a parameter which lets us trade off the convergence rate with the epoch length), if we set the step size and epoch length to be

 =
4L(1 + )

T



8(1 + ) 2

then the output of LP-SVRG after T epochs, w~T , will have an objective gap bounded by

E[f (w~T

)

-

f (w)]



2T (f (w~0)

-

f (w))

+

36212µ (1 - )2

.

Meanwhile, previous work (De Sa et al., 2018) uses the same step size and epoch length to achieve

the following bound on the objective gap:

E[f (w~T )

-

f (w)]



T (f (w~0)

-

f (w))

+

2d2µ (1 - )

this result gives a convergence rate that is only a constant factor smaller than ours, and a convergence limit that will increase rapidly as the problem scales up and d becomes large. Our result is dimension-free and so remains the same as d changes, thus outperforming the previous rate when dealing with problems that have a large scale. Next, we look at a related SGD variant, HALP, and we proved the following.

Theorem 6 Suppose that we run HALP under Assumptions 1­3 with initial weights w~0. For any constant 0 <  < 1, if we set the number of bits, step size and epoch lengths to be:

b > 1 + log2



1

+

31 



,



=

-31

(


2b-1

4L(1+)

-1)-1

,

T

=

(

-31

8(1+) (2b-1 -1)-1

)2

,

then the output of HALP after T epochs, w~T , will have an objective gap bounded by E[f (w~T ) - f (w)]  T (f (w~0) - f (w)) .

Note that this theorem shows that HALP can achieve a linear asymptotic convergence rate even with constant-bit-width low-precision computation. This high accuracy property, which was shown to hold in the paper that originally proposed HALP (De Sa et al., 2018), is maintained in our bound.

12

Under review as a conference paper at ICLR 2019

Also, compared with this previous work, which achieves the same linear asymptotic convergence rate while requiring b, , and T to be

  8(1 + )

b > 1 + log2

1+ 

2d(1 + )

,

= , 4L(1 + )

T = 2 - 22d(1 + )(2b-1 - 1)-2

our work eliminated the effect of d on the number of bits that we need, which is a promising result that suggests that this method, too, can be applied when scaling up to very large model sizes.

C NON-LINEAR QUANTIZATION FOR SGD VARIANTS

Similarly, we also have our theories for LP-SVRG and HALP when non-linear quantization is applied.
Firstly, for logarithmic quantization, we proved the following:

Theorem 7 Suppose that we run LP-SVRG on an objective that satisfies Assumptions 1­3, using
a quantization scheme that satisfies the dimension-free variance bound, and with initial iterate w~0. For any constant 0 <  < 1, if the quantization parameters, step size, and epoch length satisfy

 < ,
2( + 2)

(1 - 2) -  = ,
4L(1 + )(1 + )

T



8(1 + )(1 + ) ((1 - 2) - )2

then the output of LP-SVRG after T epochs, w~T , will have an objective gap bounded by

E[f (w~T )

-

f (w)]



2T

E[f (w~0)

-

f (w)]

+

72 (1 - )2

221µ + 2

w

2 2

2µ

Observe that when   0, the non-linear quantization will become linear, and the results of the previous two theorems hold either identically to the result for linear quantization or only with a loss of a constant factor. HALP is slightly more difficult to analyze, as it dynamically adjusts the range of its low-precision representation (using linear quantization) across outer iterations of the algorithm. Fortunately, it is straightforward to do the same dynamic adjustment with logarithmic quantization (although for lack of space, and because our main claims in this paper are not about the details of HALP, we defer the details to the supplementary material). Doing this lets us prove the following.

Theorem 8 Suppose that we run HALP on an objective that satisfies Assumptions 1­3, using logarithmic quantization (7), and with initial iterate w~0. For any constant 0 <  < 1, if the quantization parameters satisfy

 = ,
4( + 2)

4( + 2) b > 1 + log2 1 +  log

1 + 31  2( + 2) 

and the epoch lengths are set to be



=



-

61  2b-1 -1

,

8(1 + )L(1 + )

32(1 + )(1 + )

T=





-

61  2b-1 -1

2

then the output of HALP after T epochs, w~T , will have an objective gap bounded by E[f (w~T ) - f (w)]  T (f (w~0) - f (w)) .

Compared with the result for HALP when using linear quantization, the number of bits is substantially reduced while the same convergence still holds. This, along with the other comparisons reported in Table 1, suggests that NLQ may be a promising technique to use for low-precision training at scale.
Next, for Floating Point Quantization, for simplicity we denote nm = 2bm , ne = 2be , then we proved the following theorems.
First, for LP-SVRG algorithm:

13

Under review as a conference paper at ICLR 2019

Theorem 9 When using normal floating-point quantization and low-precision SVRG algorithm, if

b is the total number of bits given, the optimal setting of exponential bits be such that the noise ball

size is minimized is:

be = log2

2b + 2 log2

2(ln 2)s1 w 2 

Theorem 10 When using denormal floating-point quantization and low-precision SVRG algorithm,

if b is the total number of bits given, the optimal setting of exponential bits be such that the noise

ball size is minimized is :

be = log2

1- 2 W ln 2

w 2  8s1

And in cases where the value does not exist, the noise ball size increases as be, thus be = 2 would be the optimal setting, which is equivalent to linear quantization.

Similar to Theorem 3 and Theorem 4, the theorems show that, when using normal FPQ to run algorithm LP-SVRG, the optimal assignment of be is O(log(b)), and for denormal FPQ the result is independent of b. This suggests that once the total number of bits is past a threshold, we should assign most of or all the extra bits to mantissa.
Second, for HALP algorithm:

Theorem 11 When using normal floating-point quantization and HALP algorithm, for any constant   (0, 1), if we the number of mantissa bits and exponential bits satisfy:

 

4( + 2)

nm =

, 

ne

>

log2

 

481 2-


1 nm



and we set the step size and epoch length to be:



=



-

481 

( )2ne

2-

1 nm

(1 + )L

·

nm(nm + 1) 2(2nm + 1)2

,

T=

8(2nm + 1)2L(1 + )


2

µnm(nm + 1)



-

481 

( )2ne

2-

1 nm

then the output of HALP after T epochs, w~T , will have an objective gap bounded by E[f (w~T ) - f (w)]  T (f (w~0) - f (w)) .

Theorem 12 When using denormal floating-point quantization and HALP algorithm, for any con-

stant   (0, 1), if we the number of mantissa bits and exponential bits satisfy:



4( + 2)

nm =

, 

ne > log2

961  (2nm - 1)

and we set the step size and epoch length to be:



=



-

961  2ne (2nm-1)

(1 + )L

·

nm(nm + 1) 2(2nm + 1)2

,

T=

8(2nm + 1)2L(1 + )


µnm(nm + 1)



-

961  2ne (2nm-1)

2

then the output of HALP after T epochs, w~T , will have an objective gap bounded by E[f (w~T ) - f (w)]  T (f (w~0) - f (w)) .

Unlike LP-SGD and LP-SVRG, the HALP algorithm achieves linear convergence and does not have a noise ball. In this case, the theorems show that we need a fixed number of mantissa bits, while the number of exponential bits is not upper bounded, and the more bits for exponential, the smaller epoch length we need to achieve linear convergence.

14

Under review as a conference paper at ICLR 2019

Table 2: Summary of our dimension-free results compared with prior work. The values report the number of bits needed, according to the theoretical bound, for the algorithm to achieve an expected objective gap (f (w) - f (w)) of when we let step size   0, epoch length T  . Note that the expressions for HALP do not depend on because HALP achieves solutions with arbitrary accuracy using a fixed number of bits. Here we denote the radius of the representable range as R and assume R = w 2. The rest of the parameters can be found in the assumptions to be introduced later.

ALGORITHM
LP-SGD (LI ET AL., 2017) LP-SVRG (DE SA ET AL., 2018) HALP (DE SA ET AL., 2018)

NUMBER OF BITS NEEDED FOR E[f (w) - f (w)] 

PRIOR DIMENSION-
DEPENDENT BOUND



log2 O

Rmax 

d

OUR DIMENSION-
FREE BOUND

log2 O

R1 

DIMENSION-FREE WITH LOG-QUANTIZATION

log2 O

R 

log

1+

1 

log2 O R

dL 



log2 O  d

log2 O 1R

µ 

 log2 O 1 

log2 O R

µ 

log

1

+

1 

log2 O

 log

1

+

1 

D A TABLE SUMMARY OF THE RESULTS ON DIFFERENT ALGORITHMS

E PROOF FOR RESULTS IN TABLE 2

As mentioned in the caption of Table 2, here only we consider the convergence limit, that is, we

assume   0, T  , and we compute the minimum number of bits b we would require in

order for the limit to be less than some small positive . Meanwhile, we denote the radius of the

representable range by case for all our bounds

R and we assume R that depend on w

= 2.

w Then

2 without loss of generality, as this in linear quantization, we have:

is

the

worst

q2b-1-1 =  · 2b-1 - 1  R

and in non-linear quantization, we need:

 q2b-1-1 = 

(1 + )(2b-1-1) - 1

R

(9)

In the following proof we'll take the equality for these two inequalities. And we need a simple lemma here:

Lemma 3 For positive A, B and small positive number ,

Ax2 + Bx  

the positive solution x satisfies x = O

 B

.

The proof of this lemma is trivial.

E.1 LP-SGD IN PREVIOUS WORK

In previous work Li et al. (2017), we have



f (w¯T ) - f (w) 

(1 + log(T + 1))G2 2µT

G +
2

d

here we re-denote G as max for concordance with our result. Here m2 ax is an upper bound on the

second moment of the stochastic gradient samples E[

f~(w)

22]



m2 ax.

Substitute 

with

R 2b-1 -1

and set the limit to be  , then we have:

 

maxR d 2 (2b-1 - 1)





b  log2

2 + maxR d 

= log2 O

maxR d 

15

Under review as a conference paper at ICLR 2019

E.2 LP-SGD IN OUR WORK

In Theorem 1, we know that

E[f (w~) - f (w)]  1 2T

w0 - w

2 2

+

2

+ 2

1

+ 221µ 4

Set the limit to be  , then by the lemma we have:

1 + 212µ   24



R 2b-1 -

1

=



=

O

2 1

 b  log2 O

1R 

E.3 LP-SGD IN OUR WORK USING NLQ

In Theorem 2, we know that

E[(f (w~)-f (w))]  1 2T

w0 - w

2 2

+

(1

+

)2

+

1 2

+



w

2 + (L1 + L w 4µ

2 + )2

Set the limit to be   and replace w 2 with R, then we know that:

1 + R = O () 2

then we can set

1 = R = O ()

then by formula 9 we get:

(1 + )(2b-1-1) - 1  R ·  = R · 1 = 1  R 

 2b-1 - 1  log(1+)

1 + 1 

1 = log

1 + 1



 b  log2 O

R log


1 + 1 

=O

R log

1 + 1



E.4 LP-SVRG IN PREVIOUS WORK

In previous work (De Sa et al., 2018), the result is:

E[f (w~T )

-

f (w)]



T (f (w~0)

-

f (w))

+

2d2L (1 - )

Substitute



with

R 2b-1 -1

and

set

the

limit

to

be



,

then

we

have:

2dR2L

8dR2L

dL

(1 - ) (2b-1 - 1)2    b  log2 2 + (1 - ) = log2 O R 

E.5 LP-SVRG IN OUR WORK

In Theorem 5, we know that

E[f (w~T

)

-

f (w)]



2T

(f (w~0)

-

f (w))

+

36212µ (1 - )2

Substitute



with

R 2b-1 -1

and

set

the

limit

to

be



,

then

we

have:

36R221µ (1 - )2 (2b-1 - 1)2





b  log2

2+

36R221µ (1 - )2

µ = log2 O 1R 

16

Under review as a conference paper at ICLR 2019

E.6 LP-SVRG IN OUR WORK USING NLQ

In Theorem 7, we know that

72 E[f (w~T ) - f (w)]  2T E[f (w~0) - f (w)] +

212µ + 2

w

2 2

2

µ

(1 - )2

Set the limit to be   and replace w 2 with R, then we can let:

(1

72 -

)2

212µ

=

 ,
2



(1

72 -

)2



2R2

2µ

=

 2

then

(1 + )(2b-1-1) - 1  R ·  = R · 

21µ R22µ

=

1 

 2b-1 - 1  log(1+)

1 + 1 

1 = log

1 + 1



= O R

 b  log2 O R

µ log

1 + 1



µ log

1 + 1



E.7 RESULTS FOR HALP
The number of bits required in HALP are stated in detail in the theorems both in our work and in previous work. To convert them to the bounds written in the table is a straightforward application of big-O analysis.

F PROOF FOR THEOREMS

Before we prove the main theorems presented in the paper, we will prove the following lemmas that will be useful later, as well as the lemmas we presented before.
The proof of lemma 1 can be extracted from the proof of lemma 6 that we will show later.

Proof F.1 (Proof of Lemma 2) Here we consider the positive case first, then symmetrically the negative case also holds. First, for normal FPQ, the set of quantization points are:

D = {0} 

s·

x 1+
nm

· 2y

|

x = 0, 1, · · · , nm - 1,

y = - ne + 2, · · · , ne - 1 22

and we set the parameters for the nonlinear quantization bound to be:



=

s

·

2-

ne 2

+2

=

4s 1

 2

ne ,

= , nm

2 1 = =
4(1 + ) 4nm(nm + 1)

For any w within representable range, we can assume it is in [qi, qi+1), then

E[[Q(w) - w]2] =

qi+1 qi+1

-w - qi

· (w

- qi)2

+

w - qi qi+1 - qi

· (qi+1

-

w)2

= (w - qi)(qi+1 - w)

So now we only need to prove that

v  D, (w - qi)(qi+1 - w)   · |w - v| +  · |v| · |w - v| +  · |w - v|2

First,

we

consider

a

special

case

where

qi

=

0.

In

this

case,

qi+1

=

s

·

1

·

2-

ne 2

+2

=

.

If

v

=

0,

it

is obvious that

LHS = (w - qi)(qi+1 - w) = w( - w)  w  RHS

17

Under review as a conference paper at ICLR 2019

and similarly for v = ,

LHS = (w - qi)(qi+1 - w) = w( - w)  ( - w)  RHS

and for v > ,

RHS  (v - w)  ( - w)  w( - w) = LHS

Next, we consider the case where qi = 0. In this case, we can assume qi = s ·

qi+1

- qi

=

s · 2y



1 nm

qi

=

 qi .

If v  qi+1, denote y = qi+1 - w, then

1

+

x nm

LHS = (w - qi)(qi+1 - w) = y · (qi+1 - qi - y) = y · (qi - y) RHS   · qi+1 · (qi+1 - w) = qi+1y  qiy  LHS

Secondly, if 0  v  qi, denote y = w - qi, then

LHS = (w - qi)(qi+1 - w) = y · (qi+1 - qi - y) = y · (qi - y) RHS =  · (w - v) +  · v · (w - v) +  · (w - v)2

= - ( - ) · v2 + (- + w - 2w) · v + w - 2

· 2y, then

observe that  -  > 0, so the right hand side is a concave function of v, thus it achieves minimum at either v = 0 or v = qi. At v = qi:

RHS = y + qiy + y2  qiy  LHS

and at v = 0, since qi+1  (1 + )qi and qi  w,

RHS - LHS =  · w +  · 0 · w +  · w2 - (w - qi)(qi+1 - w) = (1 + )w2 + ( - qi - qi+1)w + qiqi+1  (1 + )w2 - (qi + qi+1) · w + qiqi+1 = (1 + )w2 - [(2 + )qiw + (qi+1 - (1 + )qi)w] + [(1 + )qi2 + (qi+1 - (1 + )qi)qi] = (1 + )w2 - (2 + )qi · w + (1 + )qi2 + (qi+1 - (1 + )qi)(qi - w)  (1 + )w2 - (2 + )qi · w + (1 + )qi2

which is a positive parabola.

Recall that 

=

2 4(+1)

=

(+2)2 4(+1)

- 1, thus the determinant is (2 +

)2qi2 - 4(1 + )(1 + )qi2 = 0, therefore RHS - LHS  0.

Now we extend this conclusion to the case where v  0. In this case,

RHS =  · (w - v) +  · (-v) · (w - v) +  · (w - v)2

since w, , ,  are all positive, this is apparently a decreasing function of v, thus it achieves minimum at v = 0, which is what we have already proven.

So far, we've proven the lemma in the case of w  0, v  0 and w  0, v  0, and symmetrically it holds for w  0, v  0 and w  0, v  0, which indicates that we can extend D to be a set
containing both positive and negative numbers.

In the de-normal FPQ case, the set of quantization points are:

D=

s· x nm

·

2-

ne 2

+3

|

x = 0, 1, · · · , nm - 1



s·

x 1+
nm

· 2y

|

x = 0, 1, · · · , nm - 1,

y = - ne + 3, · · · , ne - 1 22

and we set the parameters for the nonlinear quantization bound to be:

 = s·

1

·

2-

ne 2

+3

=

8

·

nm C

s2nene ,

1 = ,
nm

2 1 = =
4(1 + ) 4nm(nm + 1)

The proof for this case follows the exact same structure as the normal FPQ case.

18

Under review as a conference paper at ICLR 2019

Lemma 4 Under condition of linear quantization when using low-precision representation (, b), for any w, v  Rd where Q(,b)(w) = w,

E[ Q(,b)(w + v) - w 22] 

(w + v) - w

2 2

+



v 1.

where Q is the linear quantization function.

Proof F.2 (Proof of Lemma 4) (This proof follows the same structure as the proof for lemma 1 in (De Sa et al., 2018)) First, observe that this lemma holds if it holds for each dimension, so we only need to prove that for any w, v  R where Q(,b)(w) = w, i.e. w  dom(, b),
E[(Q(,b)(w + v) - w)2]  (w + v - w)2 + |v|
then we can sum up all the dimensions to get the result.

Now we consider the problem in two situations. First, if w + v is within the range representable by (, b), then E[Q(,b)(w + v)] = w + v. In this case,

E[(Q(,b)(w + v) - w)2] = E[[(Q(,b)(w + v) - (w + v)) - ((w + v) - w)]2] = E[[Q(,b)(w + v) - (w + v)]2 - 2[Q(,b)(w + v) - (w + v)][(w + v) - w]]
+ [(w + v) - w]2

= E[[Q(,b)(w + v) - (w + v)]2] - 2[(w + v) - (w + v)][(w + v) - w] + [(w + v) - w]2

= [(w + v) - w]2 + E[[Q(,b)(w + v) - (w + v)]2]

Since (w + v) is within representable range, E[[Q(,b)(w + v) - (w + v)]2] is equivalent to E[[Q(,)(v) + w - (w + v)]2], which equals E[[Q(,)(v) - v]2] since Q(,b)(w) = w.

Now we only need to prove that E[[Q(,)(v) - v]2]  |v|. Observe that this trivially holds for

v = 0, and is symmetrical for positive and negative v. Without loss of generality we assume v > 0,

let z be the rounded-down quantization of v, then we have z  0. Then Q(,b)(v) will round to

z

+



(the

rounded-up

quantization

of

v)

with

probability

v-z 

,

and

it

will

round

to

z

with

probability

z+-v 

.

This

quantization

is

unbiased

because

v-z

z +  - v vz - z2 + v - z z2 + z - vz

E[Q(,)(w)] =

(z + ) + 



z=



+ = v. 

Thus, its variance will be

E[(Q(,)(v) - v)2]

=

v - z (z +  - v)2 + 

z +  - v (z - v)2 

= (v - z)(z +  - v)

z+-v v-z +



= (v - z) - (v - z)2

 (v - z)  v.

therefore

E[(Q(,b)(w + v) - w)2]  (w + v - w)2 + |v|

In the other case, when w +v is on the exterior of the representable region, the quantization function Q(,b) just maps it to the nearest representable value. Since w is in the interior of the representable region, this operation will make w + v closer to w. Thus,

(Q(,b)(w + v) - w)2  (w + v - w)2,

and so it will certainly be the case that

E[(Q(,b)(w + v) - w)2]  (w + v - w)2 + |v|.

Now that we've proven the inequality for one dimension, we can sum up all d dimensions and get

E[ Q(,b)(w + v) - w 22] 

(w + v) - w

2 2

+



v 1.

19

Under review as a conference paper at ICLR 2019

For completeness, we also re-state the proof of following lemma, which was presented as equation (8) in (Johnson and Zhang, 2013), and here we present the proof for this lemma used in (De Sa et al., 2018).

Lemma 5 Under the standard condition of Lipschitz continuity, if i is sampled uniformly at random from {1, . . . , N }, then for any w,

E[

fi(w) - fi(w)

2 2

]



2L

(f

(w)

-

f

(w))

.

Proof F.3 (Proof of Lemma 5) For any i, define

gi(w) = fi(w) - fi(w) - (w - w)T fi(w).

Clearly, if i is sampled randomly as in the lemma statement, E[gi(w)] = f (w). But also, w must be the minimizer of gi, so for any w

gi(w) 

min


gi

(w

-

gi(w))

 min


gi(w) - 

gi(w)

2 2

+

2L 2

gi(w)

2 2

=

gi(w)

-

1 2L

gi(w)

2 2

.

where the second inequality follows from the Lipschitz continuity property. Re-writing this in terms of fi and averaging over all the i now proves the lemma statement.

Lemma 6 Under the condition of logarithmic quantization, for any w, v  Rd where v  Dd,

E[ Q(w) - w 22] 

w - w

2 2

+



w-v 1+

v2

w-v 2+

w-v

2 2

where Q is the non-linear quantization function.

Note that the proof this lemma naturally extends to lemma 1.

Proof F.4 (Proof of Lemma 6) Here we only consider the positive case first, where
D = {q0, q1, · · · , qn-1}
with [0, qn-1] being the representable range of D. As for the negative case, we will show later that it holds symmetrically.
Observe that this lemma holds if it holds for each dimension, so we only need to prove that for any w, v  R where v  D,
E[[Q(w) - w]2]  |w - w|2 +  · |w - v| +  · |v| · |w - v| +  · |w - v|2
then we can sum up all the dimensions and use Cauchy-Schwarz inequality to get the result.
Now we consider the problem in two situations.
First, if w is outside the representable range, the quantization function Q just maps it to the nearest representable value. Since w is in the interior of the representable range, this operation will make w closer to w. Thus,
[Q(w) - w]2  (w - w)2, and so it will certainly be the case that
E[[Q(w) - w]2]  |w - w|2 +  · |w - v| +  · |v| · |w - v| +  · |w - v|2

Second, if w is within the representable range, then E[Q(w)] = w. In this case,
E[[Q(w) - w]2] = E[[(Q(w) - w) - (w - w)]2] = E[[Q(w) - w]2 - 2[Q(w) - w](w - w)] + (w - w)2 = E[[Q(w) - w]2] - 2(w - w)(w - w) + (w - w)2 = (w - w)2 + E[[Q(w) - w]2]

20

Under review as a conference paper at ICLR 2019

Since w is within representable range, we can assume it is in [qi, qi+1), then

E[[Q(w) - w]2] =

qi+1 qi+1

-w - qi

· (w

- qi)2

+

w - qi qi+1 - qi

· (qi+1

-

w)2

= (w - qi)(qi+1 - w)

So now we only need to prove that
(w - qi)(qi+1 - w)   · |w - v| +  · |v| · |w - v| +  · |w - v|2
Note that v  D, so it is either v  qi+1 or v  qi. Firstly, if v  qi+1, denote y = qi+1 - w, then
LHS = (w - qi)(qi+1 - w) = y · (qi+1 - qi - y) = y · ( + qi - y) RHS =  · (v - w) +  · v · (v - w) +  · (v - w)2
  · (qi+1 - w) +  · qi+1 · (qi+1 - w) +  · (qi+1 - w)2 = y + qi+1y + y2  y + qiy - y2 = LHS
Secondly, if 0  v  qi, denote y = w - qi, then
LHS = (w - qi)(qi+1 - w) = y · (qi+1 - qi - y) = y · ( + qi - y) RHS =  · (w - v) +  · v · (w - v) +  · (w - v)2
= - ( - ) · v2 + (- + w - 2w) · v + w - 2

observe that  -  > 0, so the right hand side is a concave function of v, thus it achieves minimum at either v = 0 or v = qi. At v = qi:

RHS = y + qiy + y2  y + qiy - y2 = LHS

and at v = 0:

RHS - LHS =  · w +  · 0 · w +  · w2 - (w - qi)(qi+1 - w) = (1 + )w2 + ( - qi - qi+1)w + qiqi+1 = (1 + )w2 - (2 + )qi · w + qiqi+1  (1 + )w2 - (2 + )qi · w + (1 + )qi2

which is a positive parabola.

Recall that 

=

2 4(+1)

=

(+2)2 4(+1)

- 1, thus the determinant is (2 +

)2qi2 - 4(1 + )(1 + )qi2 = 0, therefore RHS - LHS  0.

Now we extend this conclusion to the case where v  0. In this case,

RHS =  · (w - v) +  · (-v) · (w - v) +  · (w - v)2

since w, , ,  are all positive, this is apparently a decreasing function of v, thus it achieves minimum at v = 0, which is what we have already proven.
So far, we've proven the lemma in the case of w  0, v  0 and w  0, v  0, and symmetrically it holds for w  0, v  0 and w  0, v  0, which indicates that we can extend D to be a set containing both positive and negative numbers, and we can reset D to be

D = {-qn, · · · , -q1, q0, q1, · · · , qn-1}

where

q0 = 0, qi+1 - qi =  + qi

Now we have proven all the lemmas we need. Next, we make some small modifications to the assumptions (weakening them) so that our theorems are shown in a more general sense. For assumption 2, we change it to:

21

Under review as a conference paper at ICLR 2019

Assumption 5 All the gradients of the loss functions fi are L1-Lipschitz continuous in the sense of 1-norm to p-norm, that is,
i  {1, 2, · · · n}, x, y, fi(x) - fi(y) 1  L1 ||x - y||p

While in the body of the paper and in our experiments we choose p = 2 for simplicity, here we are going to prove that a generalization of Theorem 1 holds for all real numbers p. We also need a similar generalization of Assumption 3.

Assumption 6

The average of the loss functions f

=

1 n

i fi is µ1- strongly convex near the

optimal point in the sense of p-norm, that is,

w,

µ1 2

||w

-

w||p2



f (w)

-

f (w)

with p being any real number.

This assumption is essentially the same as the assumption for strong convexity that we stated before, since in practice we would choose p = 2 and then µ1 and µ would be the same. But here we are actually presenting our result in a stronger sense in that we can choose any real number p and the proof goes the same.
Now we are ready to prove the theorems. Note that the result of the following proof contains µ1 since we are proving a more general version of our theorems; substituting them with µ will lead to the same result that we stated before.
The sequence of the proofs are organized in the order of:

1. linear quantization (Theorem 1, Theorem 5 and Theorem 6) 2. logarithmic quantization (Theorem 2, Theorem 7 and Theorem 8) 3. normal floating point quantization (Theorem 3, Theorem 9 and Theorem 11) 4. denormal floating point quantization ( Theorem 4, Theorem 10, and Theorem 12)

First, we prove the theorems where linear quantization is applied.

Proof F.5 (Proof of Theorem 1) In low-precision SGD, we have:

ut+1 = wt - f~t(wt), wt+1 = Q(,b)(ut+1)

by lemma 4, we know that

E[

wt+1 - w

2 2

]

=

E[

Q(,b)(wt - f~t(wt)) - w

2
]
2



E[

wt - f~t(wt) - w

2
] + E[

f~t(wt)

]

21

= E[ wt - w 22] - 2E[(wt - w)T f~t(wt)]

+ 2E[

f~t(wt)

2
] + E[

f~t(wt)

]

21



E[

wt - w

22] - 2E[(f (wt) - f (w)) +

µ 2

wt - w 22]

+ 2E[

f~t(wt)

2
] + E[

f~t(wt)

]

21

=

(1 - µ)E[

wt - w

22] + 2E[

f~t(wt)

2
] + E[
2

f~t(wt)

]
1

- 2E[(f (wt) - f (w))]

where the second inequality holds due to the strongly convexity assumption. According to the assumptions we had, we have:

22

Under review as a conference paper at ICLR 2019

E[

fi(w)

2 2

]

=

E[

fi(w) - fi(w) + fi(w)

22]

=

E[

fi(w) - fi(w)

2 2

+

2(fi(w)

-

fi(w))T

fi(w)

+

fi(w)

2 2

]

=

E[

fi(w) - fi(w)

2 2

+

fi(w)

2 2

]

 L2 · E[ w - w 22] + 2 E[ fi(w) 1] = E[ fi(w) - fi(w) + fi(w) 1]
 E[ fi(w) - fi(w) 1 + fi(w) 1]  L1 · E[ w - w 2] + 1

where the last inequality holds due to assumption 2 where we let p = 2. Applying this result to the

previous formula and we will have:

E[

wt+1 - w

2 2

]



(1 - µ)E[

wt - w

22] + 2E[

f~t(wt)

2
] + E[
2

f~t(wt)

]
1

- 2E[(f (wt) - f (w))]



(1 - µ + 2L2)E[

wt - w

2 2

]

+



L1

E

[

wt - w

2]

- 2E[(f (wt) - f (w))] + 22 + 1

Here we introduce a positive constant C that we'll set later, and by basic inequality we get

L1E[

wt - w

2]  CE[

wt - w

2]2

+

22L12 4C

 CE[

wt - w

22]

+

22L12 4C

thus

E[ wt+1 - w 22]  (1 - µ + 2L2 + C)E[ wt - w 22] - 2E[(f (wt) - f (w))]

+

22

+

1

+

22L21 4C

one setting C to be µ - 2L2, we will have:

2E[(f (wt) - f (w))] 

E[

wt - w

22] - E[

wt+1 - w

22]

+

22

+

1

+

22L12 4(µ - 2L2)

since

we

can

set



to

be

small

enough

such

that

L2



µ 2

,

then

the

result

will

become:

2E[(f (wt) - f (w))] 

E[

wt - w

22] - E[

wt+1 - w

22] + 22 + 1 +

2L21 2µ

now we sum up this inequality from t = 0 to t = T - 1 and divide by 2T , then we get:

1 T

T -1
E[(f (wt)

-

f (w))]



w0 - w

2 2

-

E

[

wT

- w

22] + 2 + 1

+ 2L21

2T 2 4µ

t=o



w0 - w

2 2

+ 2 + 1

+ 212µ

2T 2 4

and since we sample w~ uniformly from (wo, w1, · · · , wT -1), we get

E[(f (w~) - f (w))]  1 2T

w0 - w

2 2

+

2

+ 2

1

+

221µ 4

Proof F.6 (Proof of Theorem 5) From the algorithm we know that

uk,t = wk,t-1 -  (fi(wk,t-1) - fi(w~k) + g~k) , wk,t = Q,b(uk,t)

let vk,t-1 = fi(wk,t-1) - fi(w~k) + g~k. Now we look at the expected distance-squared to the optimum, according to Lemma 4

E[

wk,t - w

22] =

E[

Q(,b)(uk,t) - w

2 2

]

=

E[

Q(,b)(wk,t-1 - vk,t-1) - w

2 2

]



E[

uk,t - w

2 2

]

+

E[

vk,t-1

1]

=

E[

uk,t - w

2 2

]

+

E

[

vk,t-1

1]

23

Under review as a conference paper at ICLR 2019

Applying the recursive definition of uk,t from the algorithm statement produces

E[

wk,t - w

2 2

]



E[

wk,t-1 - w -  (fi(wk,t-1) - fi(w~k) + g~k)

22] + E[

vk,t-1

1]

=

E[

wk,t-1 - w

2 2

-

2(wk,t-1

-

w)T

(fi(wk,t-1)

-

fi (w~k )

+

g~k )]

+ E[2 fi(wk,t-1) - fi(w~k) + g~k 22] + E[ vk,t-1 1]

=

E[

wk,t-1 - w

2 2

-

2(wk,t-1

-

w)T

f (wk,t-1)]

+ E[2 fi(wk,t-1) - fi(w~k) + g~k 22] + E[ vk,t-1 1]



E[

wk,t-1 - w

2 2

-

2(f

(wk,t-1)

-

f

(w))]

+ E[2 fi(wk,t-1) - fi(w~k) + g~k 22] + E[ vk,t-1 1],

where this last inequality follows from convexity of the function f . This second-order term can be

further bounded by

E[

fi(wk,t-1) - fi(w~k) + g~k

2 2

]

=

E[

fi(wk,t-1) - fi(w) - (fi(w~k) - fi(w) - g~k)

2 2

]



E[2

fi(wk,t-1) - fi(w)

2 2

+

2

fi(w~k) - fi(w) - g~k

2 2

]

= E[2 fi(wk,t-1) - fi(w) 22]

+ E[2 fi(w~k) - fi(w) - EjUnif(1,...,N) [fj (w~k) - fj (w)] 22]



E[2

fi(wk,t-1) - fi(w)

2 2

+

2

fi(w~k) - fi(w)

22]

where the first inequality holds because

x+y

2 2

2

x

2 2

+

2

y

2 2

and

the

second

holds

because

the variance is always upper bounded by the second moment. We can now apply Lemma 5 to this

last expression, which produces

E[

fi(wk,t-1) - fi(w~k) + g~k

2 2

]



E[4L(f

(wk,t-1)

-

f

(w))

+

4L(f

(w~k )

-

f

(w))]

and also,

E[ vk,t-1 1] = E[ fi(wk,t-1) - fi(w~k) + g~k 1]  E[ fi(wk,t-1) - fi(w) 1 + fi(w~k) - fi(w) 1 + f (w~k) - f (w) 1]
as a result of Lipschitz continuity property, we have:

fi(x) - fi(y) 1  L1 ||x - y||p

thus we have

vk,t-1 1  fi(wk,t-1) - fi(w) 1 + fi(w~k) - fi(w) 1 + f (w~k) - f (w) 1

 L1 ||wk,t-1 - w||p + 2 ||w~k - w||p

Substituting the results into previous expression and we will get

E[

wk,t - w

2 2

]



E[

wk,t-1 - w

2 2

]

-

2E[f

(wk,t-1

)

-

f

(w)]

+

42

LE[f

(wk,t-1)

-

f

(w

)]

+ 42LE[f (w~k) - f (w)] + L1 E[||wk,t-1 - w||p] + 2E[||w~k - w||p]

=

E[

wk,t-1 - w

2 2

]

-

2(1

-

2L)E

[f

(wk,t-1)

-

f

(w

)]

+ 4L2E[f (w~k) - f (w)] + L1E[||wk,t-1 - w||p + 2 ||w~k - w||p]

Summing this up across all T iterations of an epoch produces

T

E[

wk,t - w

2 2

]

t=1

TT



E[

wk,t-1 - w

2 2

]

-

2(1

-

2L)

E[f (wk,t-1) - f (w)]

t=1 t=1

T
+ 4L2T E[f (w~k) - f (w)] + L1 2T E[||w~k - w||p] + E[||wk,t-1 - w||p]
t=1

24

Under review as a conference paper at ICLR 2019

Now we cancel the terms from the first two sums and noticing that wk,0 = w~k, we have:

E[

wk,T - w

2 2

]

T
 E[ w~k - w 22] - 2(1 - 2L) E[f (wk,t-1) - f (w)]

t=1

T
+ 4L2T E[f (w~k) - f (w)] + L1 2T E[||w~k - w||p] + E[||wk,t-1 - w||p]
t=1

If we use option II to assign the next outer iterate, then

E[f (w~T )-f (w)]

=

1 T

T

E[f (wk,t-1)-f (w)],

E[||w~T

-

w||p]

=

1 T

T

E[||wk,t-1 - w||p]

t=1 t=1

then we'll get

E[

wk,T

- w

22] 

E[

w~k - w

2 2

]

-

2(1

-

2L)T

E[f

(w~T

)

-

f

(w)]

+ 4L2T E[f (w~k) - f (w)] + L1T E[||w~T - w||p + 2 ||w~k - w||p]

As a consequence of strong convexity property,

fi(w)

- fi(w)



µ 2

w - w

2 2

,

fi(w)

-

fi(w)



µ1 2

||w

-

w||2p

Here we denote [f (w~k) - f (w)] by hk, then we'll have

2(1 - 2L)T E[hk+1]



2(1 - 2L)T E[hk+1] + E[

wk,T

- w

2 2

]

 E[ w~k - w 22] + 4L2T E[hk] + L1T E[||w~T - w||p + 2 ||w~k - w||p]



2 µ

E[hk

]

+

4L2T

E[hk

]

+

L1

T

E[

2 µ1 hk+1 + 2

2 µ1 hk]



2 + 4L2T µ

E[hk] + L1

2 T
µ1

E[hk+1] + 2 E[hk]



where the last inequality holds due to the concavity of x. Here we denote

1  = µ(1 - 2L)T

2L

+

1

-

, 2L

 =  L1

,

2µ1(1 - 2L)

Hk =

then we'll have

Hk2+1 - Hk+1  Hk2 + 2Hk

E [hk ]

(10)

Now, suppose we want to have an expected contraction factor of  each epoch. That is, we want

1 2L µ(1 - 2L)T + 1 - 2L = .

This is equivalent to having which can be further reduced to

(1 - 2L) = 1 + 2L2, µT

0 = 1 -  + 2L(1 + )2. µT

This equation only has solutions when the discriminant is non-negative, that is, when

0  2 - 4 · 1 · 2L(1 + ). µT

25

Under review as a conference paper at ICLR 2019

The minimal value of T for which this will be able to hold will be when it holds with equality, or

when

8L(1 + ) 8(1 + ) T = µ2 = 2 .

If we choose this T , then the solution to the above quadratic equation is, by the quadratic formula,

to set  such that





=

2

·

2L(1

+

)

=

4L(1

+

. )

We can see that these are the settings of T and  prescribed in the theorem statement. With these settings of T and , we can use the fact that 0 <  < 1 < 4 and get:

Hk2+1

- Hk+1

+

2 4



Hk2

+ 2Hk

+

2 

by taking the square root, we have

Hk+1

-

 2



 Hk

+

 


subtracting the fixed point, we have:



Hk+1 -

2+ 2(1

-

 

)





 Hk +

 

+

 2

-

2+  2(1 - ) 



 2+ 

=



Hk

-

 2 (1

-

 )

denote the fixed point by

2 +   = 2(1 - ) 

then by induction we'll have:

Hn  n(H0 - ) + 

therefore

Hn2  

 

n(H0

-

)

+



(nH0 + )2

2

 2nH02 + 22

where the last inequality holds because (a + b)2  2a2 + 2b2. Now recall that

Hk = E[hk] = E[f (w~k) - f (w)]

and using the fact that 0    1, we have:

E[f (w~T ) - f (w)]

 2T E[f (w~0) - f (w)] + 22

= 2T E[f (w~0) - f (w)] +

 2+  2(1 - )

2 2L12 µ1(1 - 2L)2

=

2T E[f (w~0) - f (w)] +

(2 + )2(1 + )2 2L12

4(1 - )2

µ1

2(1 + ) 2+



2T E[f (w~0) - f (w)] +

362L12 µ1(1 - )2

=

2T E[f (w~0) - f (w)] +

36212µ1 (1 - )2

2

26

Under review as a conference paper at ICLR 2019

We need to mention that, here we use the fixed point (or convergence point):



2+ 



=

 2 (1

-

 )

while the optimal fixed point that we can get from equation 11 is:



=

1

3 -

 

but we can also see that

  =

22+(1-) 3

=

1-

 (1 + )(2 + )

(1

-

 )(2

-

 )

6 = 1 + 6

which is only a constant factor. Meanwhile, regarding the convergence rate, from equation 11we

have:

Hk+1 -  Hk - 



Hk

+

2+ 1-



Hk+1

+

2+ 1-



applying

the

optimal

convergence

limk Hk

=



=

3 1-

then

we

have:

lim
k

Hk+1 -  Hk - 



4 + 2 5+

using the fact that 0    1, we can see that this upper bound is actually larger than ,

which is the convergence in the case of . Therefore what we've achieved is reaching a suboptimal

convergence point that is only constant factor times of the optimum while guaranteeing a better

convergence rate.

Proof F.7 (Proof of Theorem 6) The analysis of the inner loop of HALP is identical to the analysis of LP-SVRG. By using the same argument as in the proof of Theorem 5, we can get that

(1 - 2L)E[hk+1] 

1 + 2L2 µT

E[hk] + L1

1 E[
2µ1

hk+1] + 2E[

hk ]

where hk = f (w~k) - f (w), similar as before. But unlike for LP-SVRG, for HALP, the value of  changes over time. While, the last equation takes expectation over the current epoch, we now takes

expectation over all epochs, that is, over all random variables and stochastic operations including

the quantization. Then we have:

(1 - 2L)E[hk+1] 

1 + 2L2 µT

E[hk] + L1

1 E[
2µ1

hk+1] + 2E[

hk ]

Specifically,  is assigned to



=

g~k µ(2b-1

2
-

. 1)

As a result, we have (1 - 2L)E[hk+1] 

1 + 2L2 µT

E[hk] + 2µ1µ(2Lb1-1 - 1) E[

hk+1] + 2E[

hk ]

And from Lemma 5, we know that

g~k

2 2

=

f (w~k) - f (w)

2 2

 2L (f (w~k) - f (w)) = 2Lhk.

thus (1 - 2L)E[hk+1] 


1 + 2L2 µT 1 + 2L2 µT

 E[hk] + µ1µL(21b-1L- 1) E[hk] + µ1µL(21b-1L- 1)

E[ hkhk+1] + 2E[hk]

E[

hk

+ hk+1 2

]

+

2E [hk ]

27

Under review as a conference paper at ICLR 2019

therefore



E[hk+1]



1 µT

+ 2L2 +

(1 - 2L) -

2µ51µL(21b-L1 -1) 2µ1 µL(12b-L1 -1)

E [hk ]

Now as before, suppose we want some  to be the contraction factor in expectation each step, and

in this case, we let



=

1 µT

+ 2L2 + µ31µ(L21b-1L-1)

(1 - 2L)

which, as we will prove later, is an upper bound of the coefficient in the induction formula before.

(Actually this is a looser form of the induction formula but it still works well and has a simpler form)

For simplicity as we induce, we denote



s

=

 µ1

L1 L µ(2b-1

-

1)

thus



=

1 µT

+ 2L2 + 3s

(1 - 2L)

then we have:

2L(1 + )2 - ( - 3s) + 1 = 0 µT

this equation only has solutions if the determinant is non-negative, that is:

( - 3s)2 - 4 · 2L(1 + ) · 1  0 µT

the minimal value of T that satisfies this condition is:

8L(1 + ) T = µ( - 3s)2

under this circumstance, the equation for  will have one and only one solution, that is:

 - 3s =
4L(1 + )

notice that, in order for these setting of  and T to be practical, we need  - 3s > 0, that is:



s

=

2µµL1(12b-L1

-

1)

<

 3

thus 

b > 1 + log2

1 + 3L1 L µ µ1

With these conditions, we'll have a practical assignment of T and  to achieve the contraction factor , and therefore, by using the fact that  < 1, we'll have:



E[hk+1] 

1 µT

+ 2L2

(1 - 2L)

+ -

2µ51µL(21b-L1 -1) 2µ1 µL(12b-L1 -1)

E [hk ]



1 µT

+

2L2 (1

+ -

µ31µ(L21b-1L-1)
2L)

E [hk ]

=

 E [hk ]

where

the

second

inequality

holds

due

to

the

fact

that

a-c b-c

<

a b

for

a b

<

1

and

a, b, c

>

0.

And

from

that on, the result of the theorem follows by induction.

Second, we look at theorems where non-linear quantization, or logarithmic quantization specifically, is applied.

28

Under review as a conference paper at ICLR 2019

Proof F.8 (Proof of Theorem 2 ) In low-precision SGD, we have:

ut+1 = wt - f~t(wt), wt+1 = Q(ut+1)

by lemma 6, we know that

E[

wt+1 - w

2 2

]

=

E[

Q(wt - f~t(wt)) - w

2
]
2



E[

wt - f~t(wt) - w

2
]

2

+ E[

f~t(wt)

] + E[
1

wt

2

f~t(wt)

] + E[

f~t(wt)

2
]

22



E[

wt - w

2 2

]

-

2E[(wt

-

w)T

f~t(wt

)]

+

2E[

f~t(wt)

2
]
2

+ E[

f~t(wt)

] + E[( wt - w
1

2+

w

2) · 

f~t(wt)

] + 2E[ f~t(wt)
2

2
]
2



E[

wt - w

2 2

]

-

2E[(f

(wt)

-

f (w))

+

µ 2

wt - w

2 2

]

+

2E[

f~t(wt)

2
]
2

+ E[

f~t(wt)

] + E[( wt - w
1

2+

w

2)

f~t(wt)

] + 2E[
2

f~t(wt)

2
]
2

=

(1 - µ)E[

wt - w

2 2

]

+

(1

+

)2

E

[

f~t(wt)

2
] - 2E[(f (wt) - f (w))]
2

+ E[( wt - w

2+

w

2)

f~t(wt)

] + E[ f~t(wt)
2

]
1

where the third inequality holds due to the strongly convexity assumption. According to the assumptions we had, we have:

E[

fi(w)

2 2

]

=

E[

fi(w) - fi(w) + fi(w)

22]

=

E[

fi(w) - fi(w)

2 2

+

2(fi(w)

-

fi(w))T

fi(w)

+

fi(w)

2 2

]

=

E[

fi(w) - fi(w)

2 2

+

fi(w)

2 2

]

 L2 · E[ w - w 22] + 2 fi(w) 2 = fi(w) - fi(w) + fi(w) 2
 fi(w) - fi(w) 2 + fi(w) 2  L · w - w 2 +  fi(w) 1 = fi(w) - fi(w) + fi(w) 1  fi(w) - fi(w) 1 + fi(w) 1  L1 · w - w 2 + 1

where the last inequality holds due to assumption 2 where we let p = 2. Apply this result to the previous formula, denote  = 1 + , and then we will have:

E[

wt+1 - w

2 2

]



(1 - µ)E[

wt - w

2 2

]

+



2E[

f~t(wt)

2
] - 2E[(f (wt) - f (w))]
2

+ E[( wt - w

2+

w

2)

f~t(wt)

] + E[
2

f~t(wt)

]
1

 (1 - µ +  2L2)E[ wt - w 22] + L1E[ wt - w 2] +  22 + 1 + E[( wt - w 2 + w 2)(L · w - w 2 + )] - 2E[(f (wt) - f (w))]

=

(1 - µ + L +  2L2)E[

wt - w

2 2

]

-

2E[(f

(wt)

-

f

(w

))]

+ (L1 + L w 2 + )E[ wt - w 2] +  22 + 1 +  w 2

29

Under review as a conference paper at ICLR 2019

Here we introduce a positive constant C that we'll set later, and by basic inequality we get

(L1 + L w 2 + )E[ wt - w 2]



CE[

wt - w

2]2

+

(L1

+

L w 4C

2 + )2



CE[

wt - w

22]

+

2(L1

+

L w 4C

2 + )2

thus

E[

wt+1 - w

2 2

]



(1 - µ + L +  2L2 + C)E[

wt - w

22] - 2E[(f (wt) - f (w))]

+  22 + 1 + 

w

2

+

2(L1

+

L w 4C

2 + )2

one setting C to be µ - L -  2L2, we will have:

2E[(f (wt) - f (w))] 

E[

wt - w

22] - E[

wt+1 - w

2 2

]

+  22 + 1 + 

w

2

+

2(L1 + L w 2 + )2 4(µ - L -  2L2)

since

we

can

set



to

be

small

enough

such

that

µ - L - 

2L2



1 2

µ,

then

the

result

will

become:

2E[(f (wt) - f (w))]  E[ wt - w 22] - E[ wt+1 - w 22]

+  22 + 1 + 

w

2

+

(L1

+

L w 2µ

2 + )2

now we sum up this inequality from t = 0 to t = T - 1 and divide by 2T , then we get:

1 T

T -1
E[(f (wt)

-

f (w))]

t=o



w0 - w

2 2

-

E[

wT

- w

22] +  2 + 1 + 

w

2 + (L1 + L

w

2 + )2

2T 2 4µ



w0 - w

2 2

+

 2 + 1 + 

w

2 + (L1 + L

w

2 + )2

2T 2

4µ

and since we sample w~ uniformly from (wo, w1, · · · , wT -1), we get

E[(f (w~) - f (w))]  1 2T

w0 - w

2 2

+



2

+

1 + 2



w

2 + (L1 + L w 4µ

2 + )2

Proof F.9 (Proof of Theorem 7 ) by lemma 6, we know that

E[

wk,t - w

22] =

E[

Q(wk,t-1 - vk,t-1) - w

2 2

]

 E[ wk,t-1 - vk,t-1 - w 22] + E[ vk,t-1 1] + E[ wk,t-1 2 vk,t-1 2] + E[ vk,t-1 22]

= E[ wk,t-1 - vk,t-1 - w 22] + E[ vk,t-1 1] + E[ wk,t-1 2 vk,t-1 2] + 2E[ vk,t-1 22]

where the first term

E[

wk,t-1 - vk,t-1 - w

2 2

]

=

E[

wk,t-1 - w -  (fi(wk,t-1) - fi(w~k) + g~k)

2 2

]

=

E[

wk,t-1 - w

2 2

-

2(wk,t-1

-

w)T

(fi(wk,t-1)

-

fi (w~k )

+

g~k )

+

2

=

E[

wk,t-1 - w

2 2

-

2(wk,t-1

-

w)T

f (wk,t-1)

+

2

vk,t-1 22]



E[

wk,t-1 - w

2 2

-

2(f

(wk,t-1)

-

f

(w

))

+

2

vk,t-1

2 2

]

vk,t-1 22]

30

Under review as a conference paper at ICLR 2019

where the last inequality holds due to the convexity of function f . And the vk,t-1 terms can be bounded by:

E[ vk,t-1 22] = E[ fi(wk,t-1) - fi(w~k) + g~k 22]

= E[ fi(wk,t-1) - fi(w) - (fi(w~k) - fi(w) - g~k) 22]



E[2

fi(wk,t-1) - fi(w)

2 2

+

2

fi(w~k) - fi(w) - g~k

22]

= E[2 fi(wk,t-1) - fi(w) 22]

+ E[2

fi(w~k) - fi(w) - EjUnif(1,...,N) [fj (w~k) - fj (w)]

2 2

]



E[2

fi(wk,t-1) - fi(w)

2 2

+

2

fi(w~k) - fi(w)

2 2

]

where the first inequality holds because

x+y

2 2

2

x

2 2

+

2

y

2 2

and

the

second

holds

because

the variance is always upper bounded by the second moment.We can now apply Lemma 5 to this last

expression, which produces

and also,

E[ vk,t-1 22]  E[4L(f (wk,t-1) - f (w)) + 4L(f (w~k) - f (w))].

E[ vk,t-1 1] = E[ fi(wk,t-1) - fi(w~k) + g~k 1]  E[ fi(wk,t-1) - fi(w) 1 + fi(w~k) - fi(w) 1 + f (w~k) - f (w) 1]
according to assumption, we'll know that:
vk,t-1 1  fi(wk,t-1) - fi(w) 1 + fi(w~k) - fi(w) 1 + f (w~k) - f (w) 1

 L1 ||wk,t-1 - w||p + 2 ||w~k - w||p

Meanwhile,

E[ wk,t-1 2 vk,t-1 2]  E ( wk,t-1 - w 2 + w 2) fi(wk,t-1) - fi(w) 2
+ fi(w~k) - fi(w) 2 + f (w~k) - f (w) 2)]
 E[( wk,t-1 - w 2 + w 2) L · wk,t-1 - w 2 + 2L · w~k - w 2 ]

= L·E

wk,t-1 - w

2 2

+

w 2 ·

wk,t-1 - w 2

+ 2 wk,t-1 - w 2 · w~k - w 2 + 2 w 2 · w~k - w 2

 L · E[2

wk,t-1 - w

2 2

+

w 2 ·

wk,t-1 - w 2 +

w~k - w

2 2

+

2

w 2 ·

w~k - w 2]

 L·E

4 µ

(f

(wk,t-1)

-

f

(w))

+

w 2 ·

wk,t-1 - w 2

+

2 µ

(f (w~k) - f (w)) + 2

w

2·

w~k - w

2

where the last inequality holds due to the strongly convexity assumption.

Denote  = 1 + , substitute the results into previous expression and we will get

E[ wk,t - w 22]



E[

wk,t-1 - w

2 2

]

-

2E[f (wk,t-1)

-

f (w)]

+

4

2LE[f (wk,t-1)

-

f (w)]

+ 4 2LE[f (w~k) - f (w)] + L1 ||wk,t-1 - w||p + 2 ||w~k - w||p

+ L · E

4 µ

(f

(wk,t-1)

-

f

(w))

+

w 2 ·

wk,t-1 - w 2

+

2 µ

(f (w~k) - f (w)) + 2

w

2·

w~k - w

2

=

E[

wk,t-1 - w

2 2

]

-

2(1

-

2

L

-

2)E[f (wk,t-1)

-

f (w)]

+ (4 L2 + 2)E[f (w~k) - f (w)] + L1E[||wk,t-1 - w||p + 2 ||w~k - w||p]

+ L w 2 E[ wk,t-1 - w 2 + 2 w~k - w 2]

31

Under review as a conference paper at ICLR 2019

Then we can sum this over all T epochs, note that we choose option II to assign the next outer iteration, so we have:

wk,0 = w~k,

E[f (w~T ) -

f (w)]

=

1 T

T

E[f (wk,t-1) - f (w)]

t=1

E[||w~T

-

w||p]

=

1 T

T
E[||wk,t-1 - w||p],

E[

w~T

- w

2] =

1 T

T
E[ wk,t-1 - w 2]

t=1 t=1

apply these results to the summation, then we'll have:

E[ wk,T - w 22]  E[ w~k - w 22] - 2(1 - 2 L - 2)T E[f (w~T ) - f (w)] + (4 L2 + 2)T E[f (w~k) - f (w)] + L1T E[||w~T - w||p + 2 ||w~k - w||p] + L w 2 T E[ w~T - w 2 + 2 w~k - w 2]

As a consequence of strong convexity property,

fi(w)

- fi(w)



µ 2

w - w

2 2

,

fi(w)

-

fi(w)



µ1 2

||w

-

w||p2

Here we denote [f (w~k) - f (w)] by hk, then we'll have

2(1 - 2 L - 2)T E[hk+1]



2(1 - 2 L - 2)T E[hk+1] + E[

wk,T

- w

2 2

]



E[

w~k - w

2 2

]

+

(4

L2

+

2 )T

E [hk ]

+ L1T E[||w~T - w||p] + 2 ||w~k - w||p

+ L w 2 T E[ w~T - w 2 + 2 w~k - w 2]



2 µ

E [hk ]

+

(4

L2

+

2 )T

E [hk ]

+

L1T

E[

2 µ1 hk+1 + 2

+ L w 2 T E[

2 µ hk+1 + 2

2 µ hk]

2 µ1 hk]



2 + (4 L2 + 2)T µ

E[hk] +

L1

2 + L
µ1

w

2

2 µ



where the last inequality holds due to the concavity of x. Here we denote

T

E[hk+1] + 2 E[hk]

=

1

+

2 L + 

L1 , =

1 2µ1

+ L

w

2

µ(1 - 2 L - 2)T 1 - 2 L - 2

1 - 2 L - 2

1 2µ
, Hk =

E [hk ]

then we'll have

Hk2+1 - Hk+1  Hk2 + 2Hk

(11)

Now, suppose we want to have an expected contraction factor of  each epoch. That is, we want

1 2 L +  µ(1 - 2 L - 2)T + 1 - 2 L - 2 = .

This is equivalent to having

(1 - 2 L - 2) = 1 + 2 L2 +  µT

which can be further reduced to

2 L(1 + )2 - ((1 - 2) - )  + 1 = 0 µT

32

Under review as a conference paper at ICLR 2019

This equation only has solutions when the discriminant is non-negative, that is, when 0  ((1 - 2) - )2 - 4 · 1 · 2 L(1 + ). µT

The minimal value of T for which this will be able to hold will be when it holds with equality, or when

8 L(1 + )

8(1 + )(1 + )

T = µ ((1 - 2) - )2 = ((1 - 2) - )2

If we choose this T , then the solution to the above quadratic equation is, by the quadratic formula, to set  such that

(1 - 2) -  (1 - 2) - 

=

2 · 2 L(1 + )

=. 4L(1 + )(1 + )

and by setting

 <
2( + 2)

we can guarantee that T and  are positive. We can see that these are the settings of , T and  prescribed in the theorem statement. With these settings of T and , we can use the fact that 0 <  < 1 < 4 and get:

Hk2+1

- Hk+1

+

2 4



Hk2

+ 2Hk

+

2 

by taking the square root, we have

Hk+1

-

 2



 Hk

+

 


subtracting the fixed point, we have:



2+ 

Hk+1

-

 2 (1

-

 )



 Hk +

 


+

 2

-

2+ 

 2 (1

-

 )





 =

2+  Hk - 2(1 - ) 

denote the fixed point by

2 +   = 2(1 - ) 

then by induction we'll have:

Hn  n(H0 - ) + 

therefore

Hn2  

 

n(H0

-

)

+



(nH0 + )2

2

 2nH02 + 22

where the last inequality holds because (a + b)2  2a2 + 2b2. Now recall that

Hk = E[hk] = E[f (w~k) - f (w)]

33

Under review as a conference paper at ICLR 2019

and

using

the

fact

that

0







1

and

that



=

(1-2)- 4L(1+)(1+)

,



<

 (2+4)

,

we

have:

E[f (w~T ) - f (w)]

 2T E[f (w~0) - f (w)] + 22



= 2T E[f (w~0) - f (w)] + 2

2+  2(1 - )

2
2

= 2T E[f (w~0) - f (w)]

(2 + )2(1 + )2 + 2 4(1 - )2

L1

1 + L
2µ1

w

2

36  2T E[f (w~0) - f (w)] +

+L1
µ1

 w 2L µ

(1 - )2

2

12

2(1 + )

2

2µ (1 - 2) - 3 + 2

72  2T E[f (w~0) - f (w)] +

221µ1 + 2

w

2 2

2

µ

(1 - )2

Proof F.10 (Proof of Theorem 8) The analysis of the inner loop of HALP is identical to the analysis of LP-SVRG. By using the same argument as in the proof of Theorem 5, we can get that

(1 - 2 L - 2)E[hk+1]



1 + 2 L2 +  µT

E[hk] +

L1

1 + L
2µ1

w

2

1 2µ

E[ hk+1] + 2E[ hk]

where hk = f (w~k) - f (w), similar as before. But unlike for LP-SVRG, for HALP, there are two modifications that we need to make. The first is that, the value of  changes over time. While the last
equation takes expectation over the current epoch, we now takes expectation over all epochs, that is,
over all random variables and stochastic operations including the quantization. The second is that, due to the change in the HALP algorithm, the w 2 term will be replaced with w~k - w 2, which also needs . Thus we have:

(1 - 2 L - 2)E[hk+1]



1 + 2 L2 +  µT

E[hk] + L1

1 E[
2µ1

hk+1 + 2

+ L

1 2µ

E[ w~k - w 2

hk+1 + 2 w~k - w 2

hk ]

hk ]

According to the assumption of strong convexity, we know that

w~k - w 2 

2 µ

[f

(w~k

)

-

f

(w)]

=

As for , we know from the setup of non-linear quantization that

qN

=

(1

+

 )N

 

-

 

so in order to cover the result in b bits, we need

q2b-1-1 

g~k 2 µ

Specifically,  is assigned to

= µ

 g~k 2 (1 + )(2b-1-1) - 1

.

and from Lemma 5 we know that

2 µ hk

g~k 2 =

f (w~k) - f (w)

2 2



2L (f (w~k) - f (w)) =

2Lhk

34

Under review as a conference paper at ICLR 2019

As a result, we have

(1 - 2 L - 2)E[hk+1]



1 + 2 L2 +  µT

 L E[hk] + µ

E[

hkhk+1] + 2E[hk]



+ µ1µ

L1 L (1 + )(2b-1-1) - 1

E[ hkhk+1] + 2E[hk]



1 + 2 L2 +  µT

E[hk] + 

E[

hk

+ hk+1 2

]

+

2E [hk ]



+ µ1µ

L1 L (1 + )(2b-1-1) - 1

E[

hk

+ hk+1 2

]

+

2E [hk ]

For simplicity as we induce, we denote s=  µ1µ

 L1 L (1 + )(2b-1-1) - 1

+ 

then

(1

-

2

L

-

2 

-

s 2 )E[hk+1]



1

+

2

L2

+



+

5 s

µT 2

E [hk ]

In this case, we let





=

1 µT

+ 2 L2 +  + 3s

=

1 µT

+ 2 L2 + 4 +

( )
µ1 µ

3L1 L (1+ )(2b-1 -1) -1

(1 - 2 L - 2)

(1 - 2 L - 2)

which, as we will prove later, is an upper bound of the coefficient in the induction formula before.

(Actually this is a looser form of the induction formula but it still works well and has a simpler form)

Now we have:

2 L(1 + )2 - ((1 - 2) - 3s - ) + 1 = 0 µT

this equation only has solutions if the determinant is non-negative, that is:

((1 - 2) - 3s - )2 - 4 · 2 L(1 + ) · 1  0 µT

the minimal value of T that satisfies this condition is:

8 L(1 + )

8(1 + )L(1 + )

T = µ((1 - 2) - 3s - )2 = µ((1 - 2) - 3s - )2

under this circumstance, the equation for  will have one and only one solution, that is:

(1 - 2) - 3s -  (1 - 2) - 3s - 

= =

4 L(1 + )

4(1 + )L(1 + )

notice that, in order for these setting of  and T to be practical, we need (1 - 2) - 3s -  > 0,

that is:



(1 - 2) -  µ1µ

3L1 L (1 + )(2b-1-1) - 1

- 3 -  > 0

thus 

b > 1 + log2 1 + log(1+)

1

+

 µ µ1

3 ((1

L1 L - 2)

-

4 )

notice

that

here

we

need

(1 - 2) - 4

>

0,

that

is,





 2(+2)

.

Then

by

setting



=

 4(+2)

(which is a small value) and substituting it to the previous result, we will have

b > 1 + log2

4( + 2) 1 + log


1 + 3L1 2( + 2) Lµ1

35

Under review as a conference paper at ICLR 2019


 =  - ,µ16µL(21b-L1-1) 8(1 + )L(1 + )

32(1 + )L(1 + )

T=

2

µ  - µ16µL(21b-L1-1)

With these conditions, we'll have a practical assignment of T and  to achieve the contraction factor , and therefore, by using the fact that  < 1, we'll have:

E[hk+1] 

1 µT

+ 2

L2

(1 - 2 L

+ -

 2 

+ -

5 2

s

1 2

s)

E [hk ]



1 µT

+ 2 L2 +  + 3s (1 - 2 L - 2)

E [hk ]

=

 E [hk ]

where

the

second

inequality

holds

due

to

the

fact

that

a-c b-c

<

a b

for

a b

<

1

and

a, b, c

>

0.

And

from

that on, the result of the theorem follows by induction.

Next, we prove the theorems for normal floating point quantization.

Proof F.11 (Proof of Theorem 3) In the normal FPQ case, the set of quantization points are:

D = {0} 

s·

x 1+
nm

· 2y

|

x = 0, 1, · · · , nm - 1,

y = - ne + 2, · · · , ne - 1 22

then the parameters for the nonlinear quantization bound can be computed as:



=

s

·

2-

ne 2

+2

=

4s 1

 2

ne ,

= , nm

2 1 = =
4(1 + ) 4nm(nm + 1)

For NLQ-SGD, the noise ball size according to theorem 2 is: 1 +  w 2 + (L1 + L w 2 + )2 2 4µ

Denote

this

as

1 2

A

+

1 4µ

B2

.

When

b

is

large,

, ,



are

small,

then

the

dominating

term

for

the

noise

ball is

A = 1 + 

w

2 = 4s1

1

 2

ne

+

w

1 2 nm = 4s1

1

 2

ne

+

w

ne 2C

let the derivative over ne to be 0 and we get:

A ne

=

-2(ln 2)s1

1

 2

ne

+

w

1 2 C = 0,

 2

ne

=

2(ln 2)s1C  w 2

ne = 2 log2

2(ln 2)s1C  w 2

, be = log2

2b + 2 log2

2(ln 2)s1  w 2

And when b is small, , ,  are large and the dominating term for the noise ball is

B = L1+L

w

2+ = 4sL1

1

 2

ne +(L

w

1 2+) nm = 4sL1

1

 2

ne +(L

w

2+)

ne C

let the derivative of ne to be 0 and we get:

B ne

=

-2(ln 2)sL1

1

 2

ne

+ (L

w

1

2

+ ) C

=

0,

 2

ne

=

2(ln 2)sL1C L w 2 + 

ne = 2 log2

2(ln 2)sL1C L w 2 + 

, be = log2

2b + 2 log2

2(ln 2)sL1 L w 2 + 

For b such that neither the terms dominates the result, we know the noise ball size is:

1 A+

1 B2 = 1 + 

w

2 + (L1 + L

w

2 + )2

2 4µ

2

4µ

36

Under review as a conference paper at ICLR 2019

then the derivative of ne is:



1 A+

1 B2

1 A B B =+

ne 2 4µ

2 ne 2µ ne

and

since

both

A ne

and

B ne

are

increasing

functions

and

we

know

that:

A

ne
ne=2 log2

2(ln 

2w)s21

C

= 0,

B

ne
ne=2 log2

2L(lnw2)s2L+1 C

=0

then

we

know

the

solution

of

 ne

and 2 log2

2(ln 2)sL1C L w 2+

.

1 2

A

+

1 4µ

B

2

= 0 is in the interval between 2 log2

2(ln 2)s1C  w 2

Proof F.12 (Proof of Theorem 9) For NLQ-SVRG, the size of the noise ball is:

36

+L1
µ1

 w 2L µ

(1 - )2

2

36 =

 1 µ1 + 

w

(1 - )2

 2 µ

2

we

need

to

mention

that

the

result

we

stated

is

(72 221µ1+2 w
(1-)2

2 2

2

µ)

,

which

is

a

looser

version,

but here we'll analyze the tighter result instead. To minimize the noise ball, we need to minimize

 1 µ1 + 

w

 2  µ = 1 µ1 ·

4s

 2

ne

+

w

2

 µ

ne C

let the derivative of ne to be 0 and we get:

ne = 2 log2

 -1 µ1

·

2(ln 2)s

 2

ne

+

w

1

2

µ =0 C

 2(lnw2)s2 1µµ1C , be = log2 ln 2b + 2 log2

 2(lwn 2)s21µµ1

Proof F.13 (Proof of Theorem 11) For FPQ-HALP, s is no longer a fixed parameter, and in each epoch we'll reassign s such that:

g~k 2  s · 2 - 1

·

2

ne 2

-1

µ nm

specifically, s is assigned to:

2-

ne 2

+1

s=

g~k

2

2

-

1 nm

µ

where

g~k 2  2Lhk

then in this case,



=

s

·

2-

ne 2

+2

=

2-ne+3

g~k

2

2

-

1 nm

µ

2ne

 8 2L

2

-

1 nm

µ

hk

and in HALP we have:

(1 - 2 L - 2)E[hk+1]



1 + 2 L2 +  µT

E[hk] + L1

+  E[ hkhk+1 + 2hk]

1 E[
2µ1

hk+1 + 2

hk ]

37

Under review as a conference paper at ICLR 2019

thus

(1 - 2 L - 2)E[hk+1]





1 + 2 L2 +  µT

E[hk] +  2ne





1 + 2 L2 +  µT

E[hk] +  2ne



8L1

2

-

1 nm

L 
µ µ1

+

 



8L1

2

-

1 nm

L 
µ µ1

+

 

E[ hkhk+1] + 2E[hk]

E[

hk

+ hk+1 2

]

+

2E [hk ]

For simplicity as we induce, we denote
x= 2ne



8L1 L

2

-

1 nm

 µ µ1

+



then

(1

-

2

L

-

2 

-

x 2 )E[hk+1]



1

+

2

L2

+

 

+

5 x

µT 2

E [hk ]

In this case, we let



=

1 µT

+ 2 L2 +  + 3x

=

1 µT

+

2

L2

+

4 

+

( )2ne

24L1

2-

1 nm

L µ µ1

(1 - 2 L - 2)

(1 - 2 L - 2)

follow the same proof and we'll have a similar conclusion: by setting



=

-

( )2ne

48L1

2-

1 nm

L µ µ1

(1 + )L

·

nm(nm + 1) 2(2nm + 1)2

,

T = 8(2nm + 1)2L(1 + )



µnm(nm + 1)



-

( )2ne

48L1

2-

1 nm

L µ µ1

2

satisfying
(1 - 2) - 2ne



24L1

2

-

1 nm

L 
µ µ1

-

3 

-



>

0,

1 = =
nm 4( + 2)

that is: then we'll have:

 

4( + 2)

nm =

, 

ne > log2  

48L1

2

-

1 nm

L 
µ µ1

E[hk+1]  E[hk]

At last, we look at the theorems where denormal floating point quantization is applied.

Proof F.14 (Proof of Theorem 4) In the denormal FPQ case, the set of quantization points are:

D=

s· x nm

·

2-

ne 2

+3

|

x = 0, 1, · · · , nm - 1



s·

x 1+
nm

· 2y

|

x = 0, 1, · · · , nm - 1,

y = - ne + 3, · · · , ne - 1 22

then the parameters for the nonlinear quantization bound is:

 = s·

1

·

2-

ne 2

+3

=

8

·

nm C

s2nene ,

1 = ,
nm

2 1 = =
4(1 + ) 4nm(nm + 1)

For NLQ-SGD, the noise ball size according to theorem 2 is: 1 +  w 2 + (L1 + L w 2 + )2 2 4µ

38

Under review as a conference paper at ICLR 2019

Denote

this

as

1 2

A

+

1 4µ

B2

.

When

b

is

large,

, , 

are

small

and

the

dominating

term

for

the

noise

ball is

A = 1 + 

w

2=

8s1 C

ne 2

ne

+

w

1 2 nm

=

8s1 C

ne 2

ne

+

w

ne 2C

let the derivative over ne to be 0 and we get:



A ne

=

8s1 C

1 - (ln 2

2)ne
ne

+

w

1 2C =0

denote

V

(x)

=

x

·

ex,

and

Lambert

W

function

W

(y)

=

V

-1(y),

y



-

1 e

.

then

we

need



A ne

=

8s1 C

1 - (ln 2

2)ne
ne

+

w

2

1 C

=

8s1 eC

V

(1

-

(ln

 2)ne

)

+



w

1 2C =0

thus we have:

ne

=

1

-

2 W
ln 2

e w 2 8s1

,

be = log2

1- 2 W ln 2

e w 2 8s1

And when b is small, , ,  are large and the dominating term for the noise ball is

B = L1 + L

w

2 +  =

8sL1 C

·

ne 2

ne

+ (L

w

2

+

)

ne C

let the derivative of ne to be 0 and we get:



B ne

=

8sL1 C

1 - (ln 2

2)ne
ne

+

(L

w

1 2 +) C

=

8sL1 V eC

 (1-(ln 2)ne)+(L

w

1 2 +) C

=0

thus we have:

ne

=

1

-

2 W
ln 2

e(L w 2 + ) 8sL1

be = 2 log2

1-

2 W

ln 2

e(L w 2 + ) 8sL1

For b such that neither the terms dominates the result, we know the noise ball size is:

1 A+

1 B2 = 1 + 

w

2 + (L1 + L

w

2 + )2

2 4µ

2

4µ

then the derivative of ne is:



1 A+

1 B2

1 A B B =+

ne 2 4µ

2 ne 2µ ne

and

since

both

A ne

and

B ne

are

increasing

functions

and

we

know

that:

A

ne

ne

=1-

2 ln 2

W

e w 2 8s1

= 0,

B

ne

ne

=1-

2 ln 2

W

e(L w 2+) 8sL1

=0

then

we

know

the

solution

of

 ne

1 2

A

+

1 4µ

B

2

2 ln 2

W

e w 2 8s1

and

1

-

2 ln 2

W

e(L w 2+) 8sL1

.

= 0 is in the interval between 1 -

Proof F.15 (Proof of Theorem 10) For NLQ-SVRG, the size of the noise ball is:

36

+L1
µ1

 w 2L µ

(1 - )2

2

36 =

 1 µ1 + 

w

(1 - )2

 2 µ

2

39

Under review as a conference paper at ICLR 2019

we

need

to

mention

that

the

result

we

stated

is

(72 221µ1+2 w
(1-)2

2 2

2

µ)

,

which

is

a

looser

version,

but here we'll analyze the tighter result instead. To minimize the noise ball, we need to minimize

 1 µ1 + 

w

2

 µ

=

 1 µ1

·

8 C

·

s2nene +

w

2

 µ

ne C

let the derivative of ne to be 0 and we get:



1

 µ1

·

8s C

·

1

-

(ln 
2

2)ne
ne

+

w

1

2

µ C

=

 1 µ1

·

8s C

·V

(1-(ln

 2)ne)+

w

1

2

µ C

=0

thus we have:

ne

=

1

-

2 W
ln 2

w

 2 µ

8s1 µ1

,

be = log2

1- 2 W ln 2

w

 2 µ

8s1 µ1

Proof F.16 (Proof of Theorem 12) For FPQ-HALP, s is no longer a fixed parameter, and in each epoch we'll reassign s such that:

g~k 2  s · 2 - 1

·

2

ne 2

-1

µ nm

specifically, s is assigned to:

2-

ne 2

+1

s=

g~k

2

2

-

1 nm

µ

where

g~k 2  2Lhk

then in this case,





=s·

1 nm

·

2-

ne 2

+3

=

2-ne+4 g~k 2 (2nm - 1)µ



16 2L 2ne (2nm - 1)µ

hk

follow the same proof and we'll have a similar conclusion: by setting



=

-

2ne (29n6mL1-1)Lµµ1
(1 + )L

·

nm(nm + 1) 2(2nm + 1)2

,

T=

8(2nm + 1)2L(1 + )


2

µnm(nm + 1)  - 2ne (29n6mL1-1)Lµµ1

satisfying



(1

-

2 )

-

48L1 2ne (2nm -

L 
1)µ µ1

-

3 

-



>

0,

1 = =
nm 4( + 2)

that is: then we'll have:



4( + 2) nm =  , ne > log2

96L1 (2nm -

L 
1)µ µ1

E[hk+1]  E[hk]

40

