Under review as a conference paper at ICLR 2019
QUASI-HYPERBOLIC MOMENTUM AND ADAM FOR
DEEP LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-theart result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. PyTorch code is immediately available. 1
1 INTRODUCTION
Stochastic gradient descent (SGD) serves as the optimizer of choice for many recent advances in deep learning across domains (Krizhevsky et al., 2012; He et al., 2016a; Gehring et al., 2017). SGD for deep learning is typically augmented with either the "heavy ball" momentum technique of Polyak (1964) or the accelerated gradient of Nesterov (1983). In the deterministic setting, these methods provably yield faster convergence in fairly general settings. In the stochastic setting, these methods lose many theoretical advantages. However, due to its implicit gradient averaging, momentum can confer the benefit of variance reduction, applying less noisy parameter updates than plain SGD. Recent work has explicitly shown the use of momentum as a variance reducer (Roux et al., 2018).
Algorithms Starting with gradient variance reduction as an informal and speculative motivation, we introduce the quasi-hyperbolic momentum (QHM) optimization algorithm in Section 3. Put as simply as possible, QHM's update rule is a weighted average of momentum's and plain SGD's update rule. We later propose a similar variant of Adam (QHAdam) in Section 5.
Connecting the dots QHM is simple yet expressive. In Section 4, we connect QHM with plain SGD, momentum, Nesterov's accelerated gradient, PID control algorithms (Recht, 2018; An et al., 2018), synthesized Nesterov variants (Lessard et al., 2016), noise-robust momentum (Cyrus et al., 2018), Triple Momentum (Scoy et al., 2018), and least-squares acceleration of SGD (Kidambi et al., 2018). Such connections yield reciprocal benefits ­ these algorithms aid in analyzing QHM, and conversely QHM recovers many of these algorithms in a more efficient and conceptually simpler manner. We then characterize the set of optimization algorithms that QHM recovers.
Practical validation and considerations In Section 6, we empirically demonstrate that QHM and QHAdam provide superior optimization in a variety of deep learning settings. We provide both comprehensive parameter sweep analyses on smaller models and case studies on large real-world models. We demonstrate improvements on strong (sometimes state-of-the-art) models simply by swapping out the vanilla algorithms with the QH counterpart. Notably, taking the WMT16 EN-DE translation model of Ott et al. (2018), we achieve a 40% improvement in stability, along with a new state-of-the-art result of 29.45 BLEU. We then offer some practical tips for QHM and QHAdam.
Miscellany We provide errata for Kingma & Ba (2015), Recht (2018), and Kidambi et al. (2018). We also offer evidence that momentum often yields negligible improvement over plain SGD.
1https://tinyurl.com/2018qhcode ­ will be made a public GitHub repository upon publication.
1

Under review as a conference paper at ICLR 2019

We emphasize QHM and QHAdam's efficiency and conceptual simplicity. QHM has no extra overhead vs. Nesterov's accelerated gradient, and QHAdam has very little overhead vs. Adam. Also, both algorithms are easily understood as an interpolation between two other well-known algorithms, so they are accessible to practitioners and can be tuned starting with existing practical intuitions. We believe that this contributes strongly to the algorithms' practical promise.

2 PRELIMINARIES

We begin with notation and a brief review of stochastic gradient descent (SGD) and momentum.
Primitives In this paper,   Rp denotes a vector of model parameters. L() : Rp  R denotes a loss function to be minimized via . L^() : Rp  R denotes an approximator of the loss function (e.g. over a minibatch). L denotes the gradient of function L. Unless otherwise specified, all vector operations are element-wise. We use g, a, s, v, w  Rp as auxiliary buffers, and g is typically the "momentum buffer". , L^(·), and all buffers are subscriptable by t, the optimization step.

Optimization algorithms We consider optimization algorithms that perform a sequence of steps (indexed by t), updating  at each step towards minimizing L(). For brevity, we write algorithms as "update rules", which describe the algorithm's behavior during a single step t, rather than as full pseudocode. Update rules take this basic form (optionally with one or more auxiliary steps):
t+1  t - [. . .]

Plain SGD The SGD algorithm, parameterized by learning rate   R, uses the update rule: t+1  t -  · L^t(t)

Momentum The momentum algorithm, parameterized by   R and   R, uses the update rule:

gt+1   · gt + (1 - ) · L^t(t)

(1)

t+1  t -  · gt+1

(2)

where g is commonly called the "momentum buffer". Note that  = 0 recovers plain SGD.

The exponential discount factor  controls how slowly the momentum buffer is updated. In the
stochastic setting,  also controls the variance of a normalized momentum buffer. A common rule of thumb for momentum SGD is  = 0.9 (Ruder, 2016). 2

In contrast to common formulations of momentum (Polyak, 1964; Sutskever et al., 2013), we normalize, or "dampen", the momentum buffer g by (1 - ) in (1). This serves both to remove dependence of the update step magnitude on , and to allow the interpretation of g as a weighted average
of past gradients (and thus a gradient estimator). Of course, this also shrinks the updates by a factor of 1 -  vs. common formulations; this is easily reversible with a corresponding increase to .

3 ALGORITHM: QUASI-HYPERBOLIC MOMENTUM (QHM)

In this section, we propose and discuss the quasi-hyperbolic momentum (QHM) algorithm. For brevity, we provide the original motivation for QHM in Appendix A. 3 The algorithm follows below:

QHM update rule

QHM, parameterized by   R,   R, and   R, uses the update rule: gt+1   · gt + (1 - ) · L^t(t) t+1  t -  (1 - ) · L^t(t) +  · gt+1

(3) (4)

2Additionally, Kingma & Ba (2015) recommends 1 = 0.9 for Adam, and 1 = 0.9 is the default for Adam in both the PyTorch and TensorFlow frameworks (Paszke et al., 2017; Abadi et al., 2015).
3The appendix also sheds some light on the nomenclature, adopted from the hyperbolic discounting work pioneered by Chung & Hernstein (1961), Phelps & Pollak (1968), and Laibson (1997) in consumer choice. We caution that "quasi-hyperbolic" does not directly relate to the geometry of hyperbolas.

2

Under review as a conference paper at ICLR 2019

Section 7.1 provides a recommended rule of thumb ( = 0.7 and  = 0.999).
Interpretation QHM introduces the immediate discount factor , encapsulating plain SGD ( = 0) and momentum ( = 1). A self-evident interpretation of QHM is as a -weighted average of the momentum update step and the plain SGD update step. As with the momentum buffer gt, the square bracket term in (4) can be viewed as a gradient estimator (modulo initialization bias).
Efficiency QHM, like momentum, requires 1 auxiliary buffer of memory. It also requires 1 in-place scalar-vector multiplication and 3 scaled vector additions per update step.

4 CONNECTIONS TO OTHER ALGORITHMS

We now present numerous connections between QHM and other optimization algorithms. The common theme is that QHM recovers almost all of these algorithms, and thus is a highly interpretable and more efficient implementation of these algorithms. The first few subsections present these connections, Table 1 summarizes these connections, and Section 4.5 provides discussion.

4.1 NESTEROV'S ACCELERATED GRADIENT
Nesterov (1983)'s accelerated gradient (NAG) can be viewed as a closely related cousin of momentum. In fact, replacing the gt+1 term in (2) with [(1 - ) · L^t(t) +  · gt+1] yields NAG.
Connection with QHM It follows from (4) that QHM recovers NAG with  = . This sheds light on the somewhat unintuitive NAG update rule by providing a natural interpretation as a -weighted average between momentum and plain SGD.
Efficiency NAG's compute/memory cost is equivalent to that of QHM.

4.2 PID CONTROL

Recht (2018) draws a strong connection between gradient-based optimization and PID control. We regurgitate the excellent exposition (with minor modifications) in Appendix B.
Update rule A PID control optimizer, parameterized by kP , kI , kD  R, uses the update rule:

et  -L^t(t)

vt   · vt-1 + (1 - )(et - et-1) t+1  0 + kP · et + kI · wt + kD · vt

wt  wt-1 + et

Connection with QHM We fully relate QHM and PID in Appendix C.2. To summarize, PID is a superfamily of QHM. Viewing  as a constant, QHM imposes a restriction on the ratio between kP and kD. Viewing  as a free variable, however, QHM can recover nearly all PID coefficients.
Efficiency Recht (2018) provides a transformation of variables that reduces the memory cost to 2 auxiliary buffers, and the compute cost to 1 in-place scalar-vector multiplication and 4 scaled vector additions per update step. This is still costlier than QHM.
Alternative PID setting In Appendix E, we briefly discuss another PID setting by An et al. (2018) and relate the resulting optimization algorithm to QHM. In short, the setting is degenerate as the P, I, and D terms are linearly dependent. Thus, QHM can recover the resulting PID control optimizer.

4.3 SYNTHESIZED NESTEROV VARIANTS (SNV)
Section 6 of Lessard et al. (2016) describes a "synthesized Nesterov variant" algorithm, which we call "SNV" for convenience. This algorithm is used to analyze and improve optimizer robustness under "relative deterministic noise" (i.e. multiplicative noise of the gradient). 4
4This is similar to the vanishing gradient assumption used in Loizou & Richta´rik (2017), which removes the need to consider variance reduction of the gradient.

3

Under review as a conference paper at ICLR 2019

Table 1: Summary of connections between QHM and other optimization algorithms

ALGORITHM

RELATION  EFF. 

BRIEF NOTES

Plain SGD Momentum (Polyak, 1964) NAG (Nesterov, 1983) PID (Recht, 2018) PID (An et al., 2018) SNV (Lessard et al., 2016) Robust M. (Cyrus et al., 2018) Triple M. (Scoy et al., 2018) AccSGD (Kidambi et al., 2018)

subfamily subfamily subfamily parent bijective bijective subfamily subfamily subfamily

better better same worse worse worse worse worse worse

recovered by QHM with  = 0 recovered by QHM with  = 1 recovered by QHM with  =  QHM's  restricts PID's kP /kD degenerate; either "PI" or "PD" used in handling multiplicative noise SNV w/ convergence guarantees "fastest" for str. convex, smooth L(·) acceleration for least-squares SGD

 "subfamily" means that QHM recovers the algorithm but not vice-versa. "parent" means that the algorithm recovers QHM but not vice-versa. "bijective" means that the algorithms recover each other.  Efficiency (compute and/or memory) vs. QHM.

Update rule SNV, parameterized by , 1, 2  R, uses the update rule: 5
t+1  t -  · L^t(t) + 1(t - t-1) t+1  t+1 + 2(t+1 - t)
Connection with QHM We fully relate QHM and SNV in Appendix C.3. To summarize, QHM and SNV recover each other. By extension, QHM recovers the Robust Momentum method, which is a specific parameterization of SNV (Cyrus et al., 2018). Moreover, since Robust Momentum recovers the Triple Momentum of Scoy et al. (2018), QHM also recovers Triple Momentum.
Efficiency SNV is costlier than QHM, requiring 2 auxiliary buffers and 5 scaled vector additions.

4.4 ACCSGD

Jain et al. (2017) and Kidambi et al. (2018) point out various failures of momentum and NAG in the
setting of stochastic least squares optimization. This motivates their proposal of the AccSGD algo-
rithm, which yields faster convergence over momentum and NAG in certain least-squares regression
settings. Here, we discuss the formulation of Kidambi et al. (2018). 
Update rule AccSGD, parameterized by  > 0,  > 1,   , and < 1, uses the update rule:

w¯t+1 

2 

·

w¯t

+

1 - 2 

wt -  · L^t(t)

t+1

= wt+1



 +



wt -  · L^t(t)

 +
+

 · w¯t+1

Connection with QHM We fully relate QHM and AccSGD in Appendix C.4. To summarize, QHM recovers AccSGD. In the reverse direction, AccSGD does not recover QHM; specifically, we disprove the claim in Kidambi et al. (2018) that AccSGD recovers NAG.
Efficiency AccSGD, like QHM, requires 1 auxiliary buffer. Computationally, AccSGD is costlier, requiring 2 in-place scalar-vector multiplications and 4 scaled vector additions per update step.

4.5 DISCUSSION
Unifying two-state optimization algorithms These connections demonstrate that many two-state optimization algorithms are functionally similar or equivalent to each other. However, they are often implemented inefficiently and their parameterizations can be inaccessible to practitioners. QHM yields a highly accessible and efficient version of these algorithms.
5The learning rate is  in the original paper; we use  to avoid confusion with QHM's .

4

Under review as a conference paper at ICLR 2019

In Appendix D, we characterize the set of two-state optimization algorithms recoverable by QHM. Our hope here is to provide future work with a routine conversion to QHM so that they may leverage the accessibility and efficiency benefits, as well as the many connections to other algorithms.
Theoretical convergence results We note that various convergence results follow simply via these connections. In the deterministic (full-batch) case, since QHM recovers Triple Momentum, QHM also recovers the global linear convergence rate of 1 - 1/  for strongly convex, smooth loss functions. 6 For first-order methods, this is the fastest known global convergence rate for such functions. In the stochastic (minibatch) case, QHM's recovery of AccSGD gives QHM the same convergence results as in Kidambi et al. (2018)'s least-squares regression setting, of O(  · log  · log 1 ) iterations for -approximation of the minimal loss.

5 ALGORITHM: QHADAM

The Adam optimizer (Kingma & Ba, 2015) has enabled many compelling results in deep learning (Xu et al., 2015; Vaswani et al., 2017; Yu et al., 2018). We propose to replace both of Adam's moment estimators with quasi-hyperbolic terms, and we name the resulting algorithm QHAdam.

QHAdam update rule

QHAdam, parameterized by ,  0, 1, 2  [0, 1), and 1, 2  R, uses the update rule:

gt+1  1 · gt + (1 - 1) · L^t(t)

st+1  2 · vt + (1 - 2)(L^t(t))2

 t+1  t -  

 (1 - 1) · L^t(t) + 1 · gt+1

(1 - 2)(L^t(t))2 + 2 · st+1 +

gt+1  1 - 1t+1 -1 · gt+1 st+1  1 - 2t+1 -1 · vt+1

Note that only the last expression differs from vanilla Adam. In fact, QHAdam recovers Adam when 1 = 2 = 1. Moreover, modulo bias correction, QHAdam recovers RMSProp (Hinton et al., 2012) when 1 = 0 and 2 = 1, and NAdam (Dozat, 2016) when 1 = 1 and 2 = 1. We note that Adam has inspired many variants such as AMSGrad (Reddi et al., 2018) and AdamW (Loshchilov & Hutter, 2017), which can be analogously modified.
Efficiency QHAdam incurs four extra scaled vector additions over Adam.
Practical notes We leave formal convergence analysis of QHAdam to future work. However, a couple of informal points are worth mentioning. Firstly, 1 and 1 can be reasoned about in a similar manner to QHM's  and . Secondly, when replacing Adam with QHAdam, setting 2 = 1 and 2 unchanged is usually reasonable if the original Adam training is stable. Thirdly, when Adam training is not stable, it is possible that setting 2 < 1 can improve stability ­ Appendix F discusses in depth, disproving in the process the Adam step size bound claimed in Kingma & Ba (2015).

6 EXPERIMENTS
We perform two categories of experiments: parameter sweeps and case studies. For brevity, all experimental settings are summarized in Table 2 and comprehensively detailed in Appendix H.
6.1 PARAMETER SWEEPS
With parameter sweeps, we aim to comprehensively study the various parameterizations of the QH algorithms using relatively small models. We train for 90 epochs with size-64 minibatches. For QHM, we initialize  = 1 and decay it 10-fold every 30 epochs. The sweep grid for QHM (encap-
6Here,  is the ratio between the Lipschitz constant of L(·) and the strong convexity parameter of L(·).

5

Under review as a conference paper at ICLR 2019

Table 2: Summary of experimental settings

SHORT NAME

MODEL

DATASET/TASK

Logistic-EMNIST-QHM PS Logistic-EMNIST-QHAdam PS MLP-EMNIST-QHM PS MLP-EMNIST-QHAdam PS RN18-CIFAR10-QHM PS RN50-ImageNet-QHM PS RN152-ImageNet-QHM CS FConvLM-WikiText103-QHM CS TD3-MuJoCo-QHAdam CS TF-WMT16ENDE-QHAdam CS

logistic regression logistic regression 3-layer tanh MLP 3-layer tanh MLP PreActResNet18 ResNet50 ResNet152 FConvLM TD3 Transformer

EMNIST digits EMNIST digits EMNIST digits EMNIST digits CIFAR10 ILSVRC2012 ILSVRC2012 WikiText-103 MuJoCo WMT16 EN-DE

PS Parameter sweep experiment (Section 6.1). CS Case study (Section 6.2).

OPTIMIZER
QHM QHAdam QHM QHAdam QHM QHM QHM QHM QHAdam QHAdam

MLP-EMNIST-QHM

MLP-EMNIST-QHAdam

RN50-ImageNet-QHM

Figure 1: Selected parameter sweep results (full results in Appendix I). Top row shows train loss. Bottom row shows validation error. "Best" refers to the optimal parameterization within the sweep, with respect to the metric. Shaded bands indicate ±1 standard deviation.
sulating various parameterizations of plain SGD, momentum, and NAG) is:
  {0, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99, 0.995, 0.998, 0.999, 0.9995, 1}   {0, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99, 0.995, 0.998, 0.999, 0.9995}
For QHAdam, we fix  = 10-3, = 10-8, 2 = 1, and 2 = 0.999, and sweep over 1 and 1.
"Default"  and  values Motivated by the popular momentum/NAG "default" of  = 0.9, we select a QH "default" of  = 0.7 and  = 0.999 based on preliminary experimentation on the MNIST dataset (LeCun, 1998) along with the intuitions from Appendix A. In the following figures, we show these defaults along with the globally optimal parameterizations.
Results Fig. 1 presents selected results of these sweep experiments (full results in Appendix I). Perhaps the most immediate observation is that the QH algorithms improve both training and validation metrics. Even the hardcoded default  = 0.7 and  = 0.999 handily outperforms the optimal parameterization of NAG or Adam in all settings. In some settings, there remains a large gap between the QH and vanilla algorithms at the end of training. In other settings, the gap shrinks to smaller levels. However, even for these latter settings, the QH algorithm converges much faster, suggesting that a more aggressive learning rate schedule can significantly reduce training time.
What about plain SGD? We note that in most of these experiments, there is little difference between the performance of plain SGD and NAG (particularly when compared to QHM). Although not shown in the figures, there is also little difference between plain SGD and momentum. This indicates that the benefit of momentum and NAG (in the common, unnormalized formulations) comes
6

Under review as a conference paper at ICLR 2019

Figure 2: Case study results. Top row: RN152-Imagenet-QHM (left), FConvLM-WikiText103-
QHM (center), and TF-WMT16ENDE-QHAdam (right). Bottom row: TD3-MuJoCo-QHAdam Ant (left), HalfCheetah (center), Hopper (right). Shaded bands indicate ±1 standard deviation.

Table 3: Case study results

SHORT NAME
RN152-ImageNet-QHM ERR FConvLM-WikiText103-QHM PPL TD3-MuJoCo-QHAdam HalfCheetah AR TD3-MuJoCo-QHAdam Hopper AR TD3-MuJoCo-QHAdam Walker2d AR TD3-MuJoCo-QHAdam Ant AR TD3-MuJoCo-QHAdam InvPendulum AR TD3-MuJoCo-QHAdam InvDoublePendulum AR TD3-MuJoCo-QHAdam Reacher AR TF-WMT16ENDE-QHAdam BLEU
ERR Validation top-1 error rate. PPL Validation perplexity. AR Average reward.  State-of-the-art result.

QH BASELINE

0.2128±0.0005 34.45±0.17 10001.66±1106.50 2948.14±1078.70 4282.15±707.78 4657.12±881.03 986.30±59.70 8890.59±1941.56 -3.99±0.55 29.45±0.06 

0.2137±0.0011 34.92±0.33 9829.95±872.29 2401.39±1566.00 4356.95±860.73 4303.32±1279.86 950.27±186.63 9333.49±20.02 -3.98±0.56 29.17±0.07

BLEU Validation BLEU score.

in large part from the increase in effective step size. We thus suspect that much of the folk wisdom about momentum's benefits for SGD should instead be folk wisdom about using sensible learning rates. In contrast, QHM provides significant benefits without changing the effective step size.
6.2 CASE STUDIES
With case studies, we apply the QH algorithms to diverse settings, with (currently or recently) stateof-the-art models. Our case studies cover image recognition, language modeling, reinforcement learning, and neural machine translation. Each case study features a baseline setting and a QH setting, which are identical modulo the optimizer used. Results are presented in Fig. 2 and Table 3.
Image recognition (RN152-ImageNet-QHM) We train a ResNet152 model (He et al., 2016a) on the ILSVRC2012 dataset (Russakovsky et al., 2015). The baseline setting is nearly identical to the size-256 minibatch baseline in Goyal et al. (2017), using NAG with  = 0.9 and a decaying learning rate schedule. The QH setting swaps out NAG for QHM, with  = 0.7 and  = 0.999. 7
Running 3 seeds, QHM plainly trains much faster than NAG, and QHM converges to a marginally superior validation error as well. 8
7Here, we did not sweep over alternate parameterizations. 8We also train the model using plain SGD, again finding that plain SGD performs nearly as well as NAG throughout training. Although not shown, plain SGD in fact performs better than momentum. The validation
7

Under review as a conference paper at ICLR 2019
Language modeling (FConvLM-WikiText103-QHM) Deep learning for NLP often features "spiky" gradient distributions (e.g. encountering rare words). We train a FConv language model (Dauphin et al., 2016) on the WikiText-103 dataset (Merity et al., 2016). The baseline setting precisely follows the original paper, using NAG with  = 0.99. The QH setting swaps out NAG for QHM, with  = 0.98 and  = 0.998. 7 We suspect that high  improves stability in the presense of spiky gradients, and QHM's  allows the use of high .
Running 10 seeds, QHM outperforms the NAG baseline on validation perplexity by half a point.
Reinforcement learning (TD3-MuJoCo-QHAdam) Reinforcement learning presents a challenging task for gradient-based optimization, since the objective L is not stationary. QH algorithms provide a natural way of upweighting the most recent gradient. Here, we apply the TD3 algorithm (Fujimoto et al., 2018) to various MuJoCo environments (Todorov et al., 2012). The baseline precisely follows Fujimoto et al. (2018)'s setup, which uses Adam with 1 = 0.9 and 2 = 0.999. The QH setting swaps out Adam for QHAdam, with 1 = 0.9 and other parameters identical. 9
Running 10 seeds, QHAdam yields improvements in average reward on four environments out of seven tested, and virtually ties on another.
Neural machine translation (TF-WMT16ENDE-QHAdam) Many state-of-the-art neural machine translation (NMT) models are fragile to train. As in language modeling, the gradient distribution is often "spiky"; thus, Adam training often fails to converge due to a very small number of large parameter updates. 10 Here, we empirically demonstrate that QHAdam improves both performance and robustness by using 2 to control the maximum per-step update. We train a large transformer model (Vaswani et al., 2017) on the WMT16 English-German dataset. The baseline setting precisely follows the state-of-the-art setup of Ott et al. (2018), using 1 = 0.9 and 2 = 0.98 for Adam. The QH setting swaps out Adam for QHAdam, with 1 = 0.8, 1 = 0.95, 2 = 0.7, and 2 = 0.98. 11
Running 10 seeds, the Adam baseline explodes on 4 seeds. QHAdam is more robust, converging for all seeds. Ultimately, QHAdam yields a new state-of-the-art-result of 29.45 BLEU. Thus, we improve both the stability and performance of the state-of-the-art with a simple optimizer swap.
7 DISCUSSION
7.1 PRACTICAL SUGGESTIONS
We offer some practical suggestions for deep learning practitioners, particularly those who default to momentum, NAG, or Adam with  = 0.9 as a rule of thumb:
· Consider using QHM or QHAdam, instead of momentum, NAG, or Adam. · While QHM parameters should be tuned when feasible, a decent rule of thumb is to set
 = 0.7 and  = 0.999. QHAdam parameter selection is somewhat more situational, although as discussed in Section 5, 2 = 1 and 2 unchanged is usually reasonable when replacing a stable Adam optimizer with QHAdam. · Be mindful of learning rate differences between (unnormalized) momentum/NAG and QHM. Convert learning rates from the former to the latter via multiplication by (1 - )-1. For example, momentum/NAG with  = 0.1 and  = 0.9 should be replaced by QHM with  = 1. This conversion is unnecessary for Adam, as it already normalizes all buffers.
7.2 FUTURE WORK
This paper has only scratched the surface when it comes to empirical evaluation of QHM and QHAdam. Future work could apply the algorithms to other well-studied tasks and architectures, both to assess the extent of their performance gains in diverse domains, and to further develop insights into hyperparameter choice.
loss curves for plain SGD, momentum, and NAG are indistinguishable throughout training, suggesting that momentum/NAG is not needed in Goyal et al. (2017).
9Here, we tried higher values of 1. Significantly increasing 1 was not fruitful for either algorithm. 10Refer to Appendix F for a more detailed theoretical treatment. 11Here, we tried two other parameterizations (higher 1) with marginal success.
8

Under review as a conference paper at ICLR 2019
Effective hyperparameter autotuning methods can improve the practicality of any optimization algorithm. Thus, a useful direction for future work is to create an effective ,  adapter, possibly based on techniques such as YellowFin (Zhang et al., 2017) or via continuous-time optimal control analysis, as in Li et al. (2017). Moreover, learning rate adaptation techniques such as Hypergradient Descent (Baydin et al., 2018) can be applied to both QHM and QHAdam.
Future work could develop convergence results for QHAdam. Convergence results for QHM in a reasonably general stochastic setting would also be appealing, although we are not aware of compelling analogous results for momentum or NAG.
Finally, momentum has been studied in the distributed, asynchronous setting, with some noting that the delays in asynchronous SGD are, in some sense, akin to adding momentum (Mitliagkas et al., 2016). As a result, the optimal momentum constant  shrinks as more asynchronous workers are added to optimization. It would be interesting to extend these results to QHM, especially to disentagle the implicit effects of asynchrony on  and .
7.3 CONCLUSION
QHM and QHAdam are computationally cheap, intuitive to interpret, and simple to implement. They can serve as excellent replacements for momentum/NAG and Adam in a variety of settings. In particular, they enable the use of high exponential discount factors (i.e. ) through the use of immediate discounting (i.e. ). QHM recovers numerous other algorithms in an efficient and accessible manner. Parameter sweep experiments and case studies demonstrate that the QH algorithms can handily outpace their vanilla counterparts. We hope that practitioners and researchers will find these algorithms both practically useful and interesting as a subject of further study.
REFERENCES
K. J. Aastro¨m and T. Ha¨gglund. PID Controllers: Theory, Design, and Tuning. Instrument Society of America, Research Triangle Park, NC, 2 edition, 1995.
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane´, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vie´gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal of Machine Learning Research, 18:221:1­221:51, 2017. URL http://jmlr.org/papers/ v18/papers/v18/16-410.html.
Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. A pid controller approach for stochastic optimization of deep networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=BkrsAzWAb.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/ abs/1606.01540.
Shin-ho Chung and Richard J Hernstein. Relative and absolute strength of response as a function of frequency of reinforcement 1, 2. Journal of the experimental analysis of behavior, 4(3):267­272, 1961.
9

Under review as a conference paper at ICLR 2019
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre´ van Schaik. EMNIST: an extension of MNIST to handwritten letters. CoRR, abs/1702.05373, 2017. URL http://arxiv.org/ abs/1702.05373.
Saman Cyrus, Bin Hu, Bryan Van Scoy, and Laurent Lessard. A robust accelerated optimization algorithm for strongly convex functions. In 2018 Annual American Control Conference, ACC 2018, Milwaukee, WI, USA, June 27-29, 2018, pp. 1376­1381, 2018. doi: 10.23919/ACC.2018. 8430824. URL https://doi.org/10.23919/ACC.2018.8430824.
Yann N Dauphin, Angela Fan, Michael Auli Auli, and David Grangier. Language modeling with gated convolutional networks. arXiv preprint arXiv:1612.08083, 2016.
Aaron Defazio, Francis R. Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646­1654, 2014.
Timothy Dozat. Incorporating nesterov momentum into adam. ICLR Workshop, 2016.
Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.
Priya Goyal, Piotr Dolla´r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706. 02677.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, pp. 630­645, 2016b. doi: 10.1007/ 978-3-319-46493-0\ 38. URL https://doi.org/10.1007/978-3-319-46493-0_ 38.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning: Lecture 6a, 2012.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448­456, 2015. URL http://jmlr. org/proceedings/papers/v37/ioffe15.html.
Prateek Jain, Sham Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent. CoRR, abs/1704.08227, 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pp. 315­323, 2013.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of existing momentum schemes for stochastic optimization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJTutzbA-.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of 3rd International Conference on Learning Representations, 2015.
10

Under review as a conference paper at ICLR 2019
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097­1105, 2012.
David Laibson. Golden eggs and hyperbolic discounting. The Quarterly Journal of Economics, 112 (2):443­478, 1997.
Yann LeCun. The mnist database of handwritten digits. 1998.
Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57­95, 2016. doi: 10.1137/15M1009597. URL https://doi.org/10.1137/15M1009597.
Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradient algorithms. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 2101­2110, 2017. URL http: //proceedings.mlr.press/v70/li17f.html.
Kuang Liu. Pytorch cifar. https://github.com/kuangliu/pytorch-cifar, 2017.
Nicolas Loizou and Peter Richta´rik. Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods. CoRR, abs/1712.09677, 2017. URL http://arxiv.org/abs/1712.09677.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR, abs/1711.05101, 2017. URL http://arxiv.org/abs/1711.05101.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.
Ioannis Mitliagkas, Ce Zhang, Stefan Hadjis, and Christopher Re´. Asynchrony begets momentum, with an application to deep learning. In 54th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2016, Monticello, IL, USA, September 27-30, 2016, pp. 997­ 1004, 2016. doi: 10.1109/ALLERTON.2016.7852343. URL https://doi.org/10.1109/ ALLERTON.2016.7852343.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o (1/k^ 2). In Dokl. Akad. Nauk SSSR, volume 269, pp. 543­547, 1983.
Myle Ott, Sergey Edunov, David Grangier Grangier, and Michael Auli. Scaling neural machine translation. arXiv preprint arXiv:1806.00187, 2018.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Pytorch examples. https: //github.com/pytorch/examples, 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Edmund S Phelps and Robert A Pollak. On second-best national saving and game-equilibrium growth. The Review of Economic Studies, 35(2):185­199, 1968.
BT Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964.
Ben Recht. The best things in life are model free. argmin (personal blog), 2018. URL http: //www.argmin.net/2018/04/19/pid/.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=ryQu7f-RZ.
11

Under review as a conference paper at ICLR 2019
Nicolas Le Roux, Reza Babanezhad, and Pierre-Antoine Manzagol. Online variance-reducing optimization, 2018. URL https://openreview.net/forum?id=r1qKBtJvG.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and FeiFei Li. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015. doi: 10.1007/s11263-015-0816-y. URL https://doi.org/ 10.1007/s11263-015-0816-y.
Mark W. Schmidt, Nicolas Le Roux, and Francis R. Bach. Minimizing finite sums with the stochastic average gradient. CoRR, abs/1309.2388, 2013. URL http://arxiv.org/abs/1309. 2388.
Bryan Van Scoy, Randy A. Freeman, and Kevin M. Lynch. The fastest known globally convergent first-order method for minimizing strongly convex functions. IEEE Control Systems Letters, 2 (1):49­54, 2018. doi: 10.1109/LCSYS.2017.2722406. URL https://doi.org/10.1109/ LCSYS.2017.2722406.
Fanhua Shang, Yuanyuan Liu, James Cheng, and Jiacheng Zhuo. Fast stochastic variance reduced gradient method with momentum acceleration for machine learning. CoRR, abs/1703.07948, 2017. URL http://arxiv.org/abs/1703.07948.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139­1147, 2013.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026­5033, 2012. doi: 10.1109/IROS.2012. 6386109. URL https://doi.org/10.1109/IROS.2012.6386109.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 6000­6010, 2017. URL http: //papers.nips.cc/paper/7181-attention-is-all-you-need.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 2048­2057, 2015. URL http://jmlr.org/ proceedings/papers/v37/xuc15.html.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541, 2018.
Jian Zhang, Ioannis Mitliagkas, and Christopher Re´. Yellowfin and the art of momentum tuning. CoRR, abs/1706.03471, 2017. URL http://arxiv.org/abs/1706.03471.
12

Under review as a conference paper at ICLR 2019
APPENDICES
Organization This paper's appendices are ordered as follows: · Appendix A presents a view of momentum and QHM as discounted sums, and provides the original motivation for the development of QHM. · Appendix B regurgitates Recht (2018)'s excellent exposition of gradient-based optimization as PID control, with minor modifications. · Appendix C presents analyses of various other algorithms, towards connecting them to QHM. · Appendix D describes the set of all two-state optimization algorithms recovered by QHM. · Appendix E briefly discusses a PID control optimization setting by An et al. (2018). · Appendix F derives a tight upper bound on the updates of Adam and QHAdam (consequently disproving the bound in Kingma & Ba (2015)), then discusses the implications on training stability. · Appendix G provides miscellaneous derivations that do not cleanly fit in other sections. · Appendix H comprehensively describes the setup of this paper's parameter sweep and case study experiments. · Appendix I comprehensively presents the results of this paper's parameter sweep experiments.
13

Under review as a conference paper at ICLR 2019

A DISCOUNTED SUM ESTIMATORS (ORIGINAL VARIANCE REDUCTION MOTIVATION FOR QHM)

We now provide an interpretation of the momentum buffer as a discounted sum estimator, seeking to motivate the QHM algorithm from a variance reduction perspective.

A.1 DISCOUNTED SUMS

For a discount function  : N0  R and a sequence of vectors x0...t  Rp, we define a discounted sum DS(x0...t) as:
t
DS(x0...t) = (i) · xt-i
i=0

When

t i=0

(i)

=

1

for

all

t



0,

we

call

this

a

discounted

sum

average.

When

 i=0

(i)

=

1,

we call this a discounted sum average (modulo initialization bias).

A.2 EXPONENTIAL DISCOUNTING AND EWMA
For   (-1, 1), we define the exponential discount function EXP, as:
EXP,(i) = (1 - )i
and the exponentially weighted moving average EWMA(x0...t) as:
EWMA (x0...t) = DSEXP, (x0...t)
t
= (1 - ) · i · xt-i
i=0
The EWMA is a discounted sum average (modulo initialization bias), so it can be viewed as an estimator of the expectation of a random variable x if x0...t  x. Note that the momentum buffer gt from (1) is precisely an EWMA ­ specifically, gt = EWMA(L^0...t(0...t)).
It is well known that the exponential discount function is the only time-consistent (commonly "memoryless"), discount function ­ i.e. for any i,   0, the ratio d(i +  )/d(i) depends only on  . This is precisely why the EWMA can be tracked with no auxiliary memory ­ for example, as in momentum's update rule.

A.3 EWMA AS VARIANCE REDUCTION

We now provide the following fact about the covariance of the EWMA when x0...t are random variables.

Fact A.1 (Limit covariance of EWMA). Assume that x0...t are independent random vectors, each with the covariance matrix . Then:

lim
t

Cov[EWMA

(x0...t)]

=

(1 1

- )2 - 2

·



Proof. Corollary of Fact A.2.

This means that arbitrary variance reduction of the EWMA is possible by increasing . For example,



=

0.9

implies

that

the

covariance

is

reduced

to

1 19

·

,

and



=

0.99

implies

that

the

covariance

is

reduced

to

1 199

·

.

This provides an intuitive explanation of momentum as a variance reduction technique. Assuming that the momentum buffer is normalized (and thus interpretable as an estimator of the gradient), applying momentum will reduce the variance of the update steps, with higher  leading to more variance reduction.

14

Under review as a conference paper at ICLR 2019

However, the flip side is that higher  induces more bias (informally, "staleness") in the momentum buffer with respect to the true gradient, as the momentum buffer becomes extremely slow to update. Thus, the question arises: can we achieve variance reduction while guaranteeing that recent gradients contribute significantly to the update step? For this, we must introduce time-inconsistency.

A.4 (PURE) HYPERBOLIC DISCOUNTING AND HWMA

Hyperbolic discounting, first proposed by Chung & Hernstein (1961), is the classical timeinconsistent discount function in consumer choice. It is commonly used to model individual behaviors such as impatience. We consider its use in the setting of stochastic optimization (in place of the EWMA buffer of momentum).

For constants c, k > 0, we define the hyperbolic discount function as: 12 c
H,c,k(i) = 1 + ki

and the hyperbolic weighted moving average HWMAc,k(x0...t) as:

HWMAc,k(x0...t) = DSH,c,k (x0...t)

=c·

t

1

1 +

ki

·

xt-i

i=0

Note that the hyperbolic discount function is time-inconsistent, since:

H,c,k(i +  ) = 1 + ki

H,c,k (i)

1 + k(i +  )

depends on both i and  .

Unlike the EWMA, the HWMA is not a discounted sum average ­ in fact,

 i=0

H,c,k (i)

=



holds

regardless of choice of c or k. Thus, to use an HWMA of gradients in an optimization algorithm,

c (or the learning rate ) must be decayed at a logarithmic rate. More concerning, however, is the

computational inefficiency of the HWMA; specifically, the sum must be recomputed from scratch at

each iteration from all past gradients. This is unacceptable for use in most practical applications.

However, in preliminary stochastic optimization experiments, we did observe a marked benefit of HWMA over EWMA (i.e. momentum), limiting the number of past gradients used for tractability. This indicates that time-inconsistency might be a useful property to have in a stochastic optimizer.

A.5 QUASI-HYPERBOLIC DISCOUNTING AND QHWMA

Quasi-hyperbolic discounting, proposed by Phelps & Pollak (1968) and popularized in consumer choice by Laibson (1997), seeks to qualitatively approximate the time-inconsistency of hyperbolic discounting by applying a discontinuous "upweighting" of the current step. Its tractability has resulted in much wider adoption in consumer choice vs. pure hyperbolic discounting, and we find that it is also more suited for use in practical optimization.

For constants   R and   (-1, 1), we define the quasi-hyperbolic discount function as: 13

1 - 

i=0

QH,,(i) = (1 - )i i > 0

and the quasi-hyperbolic weighted moving average QHWMA,(x0...t) as:
QHWMA, (x0...t) = DSQH,, (x0...t)
t
= (1 - ) · x0 + (1 - ) · i · xt-i
i=1 t
= (1 - ) · x0 + (1 - ) · i · xt-i
i=0

(5)

12Slightly adapted from the original formulation. 13Significantly adapted from (but still equivalent to) the original formulation.

15

Under review as a conference paper at ICLR 2019

The QHWMA, like the EWMA, is a discounted sum average (modulo initialization bias), so it can also be viewed as an estimator under the same assumptions.

When  = 1, the QHWMA is precisely the EWMA (with identical ), and the quasi-hyperbolic discount function is precisely the exponential discount function (and thus time-consistent). When  = 1, the quasi-hyperbolic discount function, like the hyperbolic discount function, is timeinconsistent since:

QH,,(i +  ) = QH,, (i)

(1-) 1-





i=0 i>0

depends on both i and  ; specifically, i = 0 yields a different ratio than i > 0.
Note from (5) that the QHWMA is a -weighted average of the EWMA (with identical ) and x0. This means that the QHWMA can be easily computed online by simply keeping track of the EWMA, thus requiring no additional memory.

A.6 VARIANCE OF QHWMA

We now characterize the variance of a QHWMA using this fact:
Fact A.2 (Limit covariance of QHWMA). Assume that x0...t are independent random vectors, each with the covariance matrix . Then:

lim
t

Cov[QHWMA, (x0...t )]

=



·



where  is defined as:



=

(1

-

)2

+

[(1 - )]2 1 - 2

Proof. Provided in Appendix G.

 is essentially a scaling factor for the covariance of the QHWMA. It can be verified that  decreases (thus inducing variance reduction) with both increasing  and increasing .

A.7 MOTIVATING QHM
This leads to our motivation for QHM, which simply replaces the EWMA momentum buffer with a QHWMA. Starting with any momentum parameterization ( = 1 and   (0, 1)),  can be increased towards variance reduction (i.e. lowering ). Then,  can be decreased to make the QHWMA less biased as a gradient estimator, thus mitigating the aforementioned "staleness" problem. Note, however, that since decreasing  will also increase , we cannot simply decrease  to zero. Specifically, any  < 1 imposes a tight lower bound of (1 - )2 on , regardless of choice of .
The efficient implementation of QHM is described in Section 3.

A.8 RELATED WORK IN VARIANCE REDUCTION
Section 4 presents numerous connections to other optimization algorithms that shed light on both deterministic and stochastic convergence properties of QHM. However, we do not formally analyze the convergence properties of QHM from a variance reduction standpoint; this remains future work. Here, we briefly discuss other work in variance reduction.
Finite sums Recently, much effort has been devoted towards reducing the variance of the stochastic gradients used in optimization algorithms. Perhaps the most widely-studied setting is the "finite sum", or offline, stochastic optimization setting. Methods analyzed in the finite-sum setting include SAG (Schmidt et al., 2013), SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), FSVRG (Shang et al., 2017), Katyusha (Allen-Zhu, 2017), and others. We do not comment in detail on the finite sum setting due to its limited practical applicability to large-scale deep learning; for a fuller discussion of such methods, see Kidambi et al. (2018).

16

Under review as a conference paper at ICLR 2019 Momentum as variance reduction Some work in variance reduction has drawn an explicit connection to momentum. For example, Roux et al. (2018) propose a method involving Bayesian updates of gradient estimates, which induces adaptive gradient averaging. The authors note that this method boils down to momentum with an adaptive .
17

Under review as a conference paper at ICLR 2019

B PID CONTROLLERS AND OPTIMIZATION

We follow Recht (2018) in describing the connection between PID control and gradient-based optimization.

Continuous PID We slightly adapt the setting from Aastro¨m & Ha¨gglund (1995). t denotes time. There is a setpoint (i.e. target state), r(t), and a process variable (i.e. current state), y(t). The error of the system is defined as e(t) d=ef r(t) - y(t). A "controller" outputs a control signal u(t), usually towards the goal of making the error zero. The controller's choice of u(t) affects y(t) in some unspecified manner.

A PID controller, parameterized by kP , kI , and kD, uses the control function:

t de(t)

u(t) = kP [e(t)] + kI

e(t )dt + kD
0

dt

(6)

Here, the terms in the square brackets are typically referred to as the P, I, and D terms, respectively.

Discrete approximation In discrete time, the setpoint, process variable, and error are trivially discretized as rt, yt, and et d=ef rt - yt, respectively. The I term, which we label wt, is discretized as:

wt  wt-1 + et

The D term, which we label vt, could be discretized as vt = et - et-1 (first differences). However, a low-pass filter is often applied to mitigate noise, thus resulting in:
vt  vt-1 + (1 - )(et - et-1)

We simplify exposition by considering e-1, w-1, and v-1 to be 0. Finally, the PID control function (6) is trivially discretized as:
ut = kP · et + kI · wt + kD · vt

(7)

Optimization Recht (2018) relates optimization to PID control as follows:

yt = L^t(t) ; rt = 0 ; et d=ef rt - yt = -L^t(t) ; ut = t+1 - 0

That is, the process variable is the stochastic gradient, the controller's goal is to make this gradient
zero, and the controller achieves this by choosing the next step's model parameters according to the update rule t+1  ut + 0. The update rule for a PID control optimizer is thus:

et  -L^t(t)

vt  vt-1 + (1 - )(et - et-1) t+1  0 + kP · et + kI · wt + kD · vt

wt  wt-1 + et

Recht demonstrates that PID in this setting encapsulates gradient descent, momentum, and NAG; for example, gradient descent is recovered when kP = kD = 0 and kI = .

Intuition Finally, to provide some additional intuition, we can state the following fact about the D term (vt):
Fact B.1 (D term is gradient and momentum). vt can be written as:

1- vt = 

-L^t(t) + (1 - ) ·

t

i · L^t-i(t-i)

i=0

Proof. Provided in Appendix G.

Thus, the D term is simply a weighted sum of an EWMA of gradients (i.e. momentum buffer) and the current gradient, and a PID control optimizer's output is simply a weighted sum of the momentum buffer, the current gradient, and the sum of all past gradients.

18

Under review as a conference paper at ICLR 2019

C LINEAR OPERATOR ANALYSIS

Along the lines of Lessard et al. (2016), we consider optimizers as linear operators, interrupted by
a nonlinear step (the gradient evaluation). In this setting, optimizers have b internal state buffers, which we write as a stacked vector St  Rb·p. Optimizers accept the current optimizer state (St) and gradient (L^t(t)), and they produce the new optimizer state (St+1) and parameters (t) using a square matrix T  R(b+2)p×(b+2)p. 14

Update rule For convenience, we impose the restriction that the output t can only depend on the state St. Then, for analytical purposes, the optimizer can be written as the following update rule:

[St+1  ]  T St L^t(t) 0p [  t+1]  T [St+1 0p 0p]

where 0p denotes the size-p zero vector,  denotes throwaway values, and [v1 v2 . . .] denotes vector stacking.

Coordinate-wise decomposition Since we only consider optimizers that act coordinate-wise (except for the gradient evaluation), we can write T as the Kronecker product of a coordinate-wise transition matrix A  R(b+2)×(b+2) and the identity matrix Ip. That is, T = A  Ip.
Then, for t > 0, we can write t in terms of the initial state S0,{1...b} and all past gradients, using the last row of various matrix powers of A:

b
t =
i=1

t
At+1 b+2,i S0,i +
i=1

Ai+1 b+2,b+1 · L^t-i(t-i)

(8)

C.1 QHM

The internal state of QHM includes two buffers: gt (momentum buffer) and t (model parameters).

The transition matrix TQHM, mapping from gt t L^t(t) 0p to [gt+1 t+1 0p t] , is:

  0 1 -  0

AQHM

=

-  0

1 0

-(1 - ) 0

0  0

01 0 0

TQHM = AQHM  Ip

For n > 0, routine computation yields the last row of the (n + 1)-th matrix power:

AQn+HM1

=
4

-

(1-n 1-

)

1

-(1 - n)

0

Applying (8), the optimizer state t can be written as:

t
t = (AtQ+H1M)4,1 · g0 + (AtQ+H1M)4,2 · 0 + (AiQ+H1M)4,3 · L^t-i(t-i)
i=1

=

-

(1 - 1-

t)

g0

+

0

-



·

t
(1 - i) · L^t-i(t-i)

i=1

In the typical case of g0 = 0, we have:

t
t = 0 -  · (1 - i) · L^t-i(t-i)

(9)

i=1

14Typically, the output would be t+1; however, we lag one step so that the parameters can be specified in terms of the current-step state. This is done purely for notational convenience.

19

Under review as a conference paper at ICLR 2019

C.2 PID

The internal state of a PID control optimizer includes four buffers: et-1 (P term), wt-1 (I term), vt-1 (D term), and 0 (initial parameters). 15

The transition matrix TPID, mapping from et-1 wt-1 vt-1 0 L^t(t) 0p [et wt vt 0 0p t] , is:

to

0

0 0 0 -1 0

0

1 0 0 -1 0

APID

=

-(1 -  0

)

0 0

 0

0 -(1 - ) 0 
1 0 0



0

0 0 0 0 0

kP kI kD 0 0 0

TPID = APID  Ip

For n > 0, routine computation yields the last row of the (n + 1)-th matrix power:

APnI+D1 6 =

- kD n-1(1-)


kI

kD  n

1

APnI+D1 6,5 0

where:

APnI+D1 6,5 =

- [kP + kI + (1 - )kD] - kI - (1 - )2n-2kD

n=1 n>1

Applying (8), the optimizer state t can be written as:

t = AtP+ID1 6,1 · e-1 + APt+ID1 6,2 · w-1 AtP+ID1 6,3 · v-1 + APt+ID1 6,4 · 0

t

+ APi+ID1 6,5 · L^t-i(t-i)
i=1

=

-

kD  t-1 (1 

-

)

·

e-1

+

kI

·

w-1

+

kD  n

·

v-1

t

+ 0 +

AiP+ID1 6,5 · L^t-i(t-i)

i=1

Relationship with QHM In the typical case of e-1 = w-1 = v-1 = 0p, we have:

t

t = 0 +

AiP+ID1 6,5 · L^t-i(t-i)

i=1

Then, equating with (9), we have that QHM is PID with: 16

kP

=

-

 1-

kI = 

2 kD = (1 - )2

or that PID is QHM with:

 = kI



=

kP2 kD · kI



=

kD kD - kP

Viewing  as a constant, the following restriction holds on the PID coefficients that QHM can recover:
kD = -  kP 1 - 
15The offset of -1 in the P, I, and D term subscripts is purely for convenience. 16This is inconsistent with Recht (2018)'s derivation by a factor of (1 - ) in kP , kD and (1 - )/ in kI . While (1 - ) is explainable as the difference between a normalized and unnormalized momentum buffer, we suspect that the extra factor of  in kI is a mistake in the original derivation.

20

Under review as a conference paper at ICLR 2019

This restriction is looser than those for plain SGD (which has the additional restriction kP = kD = 0), momentum (which has the additional restriction kP /kI = kD/kP ), and NAG (which has the additional restriction kP /kI = kD/kP ).
Viewing  as a hyperparameter, QHM can recover all PID coefficients except when kI = 0 (i.e. P, D, or PD controller), or kP = 0 = kD (i.e. PI controller).

C.3 SNV

The internal state of a SNV optimizer includes two buffers: t and t-1. The transition matrix TSNV, mapping from t t-1 L^t(t) 0p to [t+1 t 0p t] , is:

1 + 1 -1 - 0

ASNV

=

 

1 0

0 00 
0 0 0

1 + 2 -2 0 0

TSNV = ASNV  Ip

For n > 0, routine computation gives us the last row of the (n + 1)-th matrix power:

ASnN+V1 4 =

1 1-1

(1

+

n)

-

1 1-1

(1

+

n)

-

1

 (1-1

)

(1

+

n)

0

where:

n = 1n (2(1 - 1) - 1)

Applying (8), the optimizer state t can be written as:

t

t = TStN+V1 4,1 · w¯0 + TStN+V1 4,2 · w0 +

TSiN+V1 4,3 · L^t-i(t-i)

i=1

= 1

1 - 1

(1

+

t)

·

0

-

1

1 - 1

(1

+

t)

·

-1

-

t i=1

 1(1 -

1)

(1

+

i)

·

L^ t-i (t-i )

Relationship with QHM Initialize 0 = -1 = 0. The optimizer state t is:

t

=

0

-

t i=1

 1(1 -

1)

(1

+

i)

·

L^ t-i (t-i )

Then, equating with (9), we have that QHM is SNV with:

 = (1 - )

1 = 

2

=

(1

-

) 1

 -



or that SNV is QHM with: 
= 1 - 1

 = 1



=

1

-

1

- 1 1

2

C.4 ACCSGD
The internal state of an AccSGD optimizer includes two buffers: w¯t (a buffer) and wt (the iterate, identical to t).

21

Under review as a conference paper at ICLR 2019

The transition matrix TAccSGD, [w¯t+1 wt+1 0p t] , is:

mapping from

w¯t wt L^t(t) 0p



1-

2 



AAccSGD

=

 + 



1-

2 

0

0

TAccSGD = AAccSGD  Ip

2  + 32/ + 
0 1

- 

-

+ 2 +

2 

0

0

 0
0  0 0

For n > 0, routine computation gives us the last row of the (n + 1)-th matrix power:

AAn+cc1SGD 4 =

- 2-(- 2)n
(1+ )

(+ )+(- 2)n
(1+ )

-

(1+)-( -1)n 1+

0

where:

 - 2 =
+ 

Applying (8), the optimizer state t can be written as:

t

t = TAt+cc1SGD 4,1 · w¯0 + TAt+cc1SGD 4,2 · w0 +

TAi+cc1SGD 4,3 · L^t-i(t-i)

i=1

 - 2 -  - 2 t

( + ) +  - 2 t

= (1 + )

· w¯0 +

(1 + )

· w0

t
-

(1 + ) - (  - 1)i 
1+

· L^t-i(t-i)

i=1

to

Relationship with QHM Fix  (0, 1), and initialize w¯0 = w0 = 0. The optimizer state t is:

t
t = 0 -

(1 + ) - (  - 1)i 
1+

· L^t-i(t-i)

i=1

Then, equating with (9), we have that QHM is AccSGD with:

 = (1 - )

( + )(  + 1)  = (1 - )(1 - )

+1  = (1 - )

or that AccSGD is QHM with:

 (1 + ) =
1+

 - 2 =
+ 

-1 =
(1 + )

 The original paper mentions that AccSGD recovers NAG when  = ; interestingly, we do not reproduce this finding. Based on the above analysis, NAG (i.e.  = ) is recovered when  =

1 2

1- +

4 + (1 - )2 . Empirical simulations confirm this finding. Note that for  (0, 1),

this is an inadmissible parameterization for AccSGD.

22

Under review as a conference paper at ICLR 2019

D GENERAL TWO-STATE OPTIMIZER

This appendix describes a generic two-state optimizer ("TSO") where one of the states is the iterate (t) and the other is an auxiliary buffer (at). The optimizer is parameterized by h, k, l, m, q, z  R, and the update rule is:
at+1  h · at + k · t + l · L^t(t) t+1  m · at + q · t + z · L^t(t)

We can write this as a transition matrix TTSO  R3×3:
hk TTSO = m q
00

l z 0

To simplify further derivations we diagonalize TTSO as:

 = (h - q)2 + 4km

 = km - hq

 lq-kz



QTSO

=

hz-lm


1

h-q- 2m
1
0

h-q+ 
2m
1
0

0 0

TSO = 0

1 2

(h

+

q

-

)

00

TTSO = QTSOTSOQ-TS1O

0

0

1 2

(h

+

q

+

)

Relationship with QHM

If



=

0,



=

0,





1,

1 2

(h

+

q

+

)

=

1,

h-q

+

=

0,

and

1

-

l

=

1 2

(h

+

q

-

),

then

QHM

implements

the

TSO

optimizer

with:

g0

=

-

(2 - (h + q - (h + q - )(lq

)) - kz)

·

a0

 = 1 [(h - q - )(lm - hz) + 2m(lq - kz)] 2

2m(lq - kz)  = (h - q - )(lm - hz) + 2m(lq - kz)



=

1 (h

+

q

-

)

2

Proof. We can write down the unrolled TSO update rule for t, as follows:
t-1
t  (TTtSO)2,1 · a0 + (TTtSO)2,2 · 0 + (TTtS-Oi)2,3 · L^i(i)
i=0
Similarly, for QHM we can define a transition matrix TQHM  R3×3 that advances state gt t L^t(t) as:

 0 (1 - )

TQHM = - 1 -(1 - )

00

0

Thus, the unrolled update rule for QHM takes the following form:

t-1
t  (TQtHM)2,1 · g0 + (TQtHM)2,2 · 0 + (TQt-HMi )2,3 · L^i(i)
i=0

23

Under review as a conference paper at ICLR 2019

Now we match the corresponding coefficients in both of the update rules to establish dependencies:

(TQt-HMi )2,3 · L^i(i) = (TTtS-Oi)2,3 · L^i(i) i  [0, t - 1] (TQtHM)2,1 · g0 + (TQtHM)2,2 · 0 = (TTtSO)2,1 · a0 + (TTtSO)2,2 · 0
By solving the first equation we can establish values for , , and :

(TQt-HMi )2,3 = (TTtS-Oi)2,3 i  [0, t - 1]

(TQt-HMi )2,3 = (QTSOTt-SOi Q-TS1O)2,3 i  [0, t - 1]

-(1 - t-i)

=

1 2

(tT-SOi )2,2

[(h

-

q

+ )(lm -

hz) +

2m(lq

- kz)]

-

1 2

(Tt-SOi )3,3

[(h

-

q

-

)(lm

-

hz)

+

2m(lq

-

kz)]

i  [0, t - 1]

Per

our

assumption

(TSO)3,3

=

1 2

(h

+

q

+

)

=

1

and

h-q+

=

0,

we

can

recover

the

following

relationships:



=

(TSO)2,2

=

1 2

(h

+

q

-

)

 = 1 [(h - q - )(lm - hz) + 2m(lq - kz)] 2

2m(lq - kz)  = (h - q - )(lm - hz) + 2m(lq - kz)

We can solve the second equation to find g0:

(TQtHM)2,1 · g0 + (TQtHM)2,2 · 0 = (TTtSO)2,1 · a0 + (TTtSO)2,2 · 0

- 1 - t 1-

· g0

+ 0

=

m 

(tTSO)3,3 - (tTSO)2,2

· a0

1 +
2

(Tt SO)2,2(h - q + ) - (Tt SO)3,3(h - q - )

· 0

Given that (TSO)3,3 = 1 and h - q +  = 0 = h - q -  = -2, we can simplify:

- 1 - t 1-

· g0

+ 0

=

m 

(1

-



t

)

·

a0

+ 0

g0

=

- (1 - )m 

·

a0

(2 - (h + q - ))

g0

=

- (h

+

q

-

)(lq

-

kz)

·

a0

as desired.

24

Under review as a conference paper at ICLR 2019

E AN ALTERNATIVE PID SETTING

We very briefly comment on An et al. (2018)'s PID control setting.

Update rule An et al. (2018)'s PID control optimizer, parameterized by r, kD,  > 0, uses the following update rule: 17

et  -L^t(t)

vt   · vt-1 - (1 - )(et - et-1) ut  wt + kD · vt t+1  t + ut

wt   · wt-1 + r · et

Discussion This setting departs somewhat from typical PID control, in that the signal ut controls the derivative of the controller's output (i.e. t+1 - t) rather than the output itself (i.e. t+1 - 0).
To avoid parameter blowup, this formulation necessitates the addition of exponential decay to the I term, with discount factor . 18

The I term thus becomes the momentum buffer. However, recall from Fact B.1 that the D term is a weighted sum of the momentum buffer and the P term. It follows that the D term is a weighted sum of the P and I terms, and that this setting is degenerate (either "PI" or "PD").

As a consequence, the proposed PID algorithm of An et al. (2018) is less expressive than that of Recht (2018). Specifically, applying Fact B.1 demonstrates a mapping into QHM:

r = 1-

 = 1 - kD(1 - )2 r

 = unchanged

Efficiency This PID control optimizer is costlier than QHM. It requires 2 auxiliary buffers of memory. Computationally, it requires 2 in-place scalar-vector multiplications and 5 scaled vector additions per update step.

17The exponential discount is  in the original paper; we use  to avoid confusion. 18One final oddity is that the D term (vt) calculates the negation of the derivative.
25

Under review as a conference paper at ICLR 2019

F (QH)ADAM'S UPDATE BOUND

This appendix elaborates on Adam and QHAdam's stability properties through the lens of a step size upper bound.
It is well known that the training process for deep learning models can often "explode" due to a very small number of large parameter updates. With Adam, these large updates can occur if there exist parameters whose stochastic gradients are almost always near zero but incur rare "spikes". 19. This is because the square root of the second moment estimate, used in normalizing the gradient for the update step, will be far below the magnitude of these spikes.
There are three main ways to address this instability:
· Firstly, one can simply decrease the learning rate . However, this may be undesirable due to slower training.
· Secondly, one can increase the hyperparameter. However, the appropriate setting of depends on the exact magnitudes of these gradient spikes, which is often unknown. Setting too high effectively turns Adam into SGD. Thus, setting often reduces to guesswork.
· Thirdly, one can clip gradients. However, the appropriate magnitude of the gradient clipping also depends on the stochastic gradient distribution. Thus, this solution also involves a fair amount of guesswork.
However, Adam does provide a useful guarantee ­ unlike SGD, Adam has an upper bound on the per-step update (Kingma & Ba, 2015). This upper bound is independent of the gradient distribution (or even temporal correlation), depending only on the hyperparameters , 1, and 2. Thus, no matter the gradient distribution, Adam will restrict the magnitude of the per-step updates to some known constant.
We show that the step size upper bound claimed in Section 2.1 of Kingma & Ba (2015) is incorrect, by providing the correct tight bound for both Adam and QHAdam. We then demonstrate that with QHAdam, one can lower the maximum per-step update (and thus improve stability) simply by lowering 2 to be below 1.

F.1 SETTING

We make two simplifications. Firstly, we fix = 0. 20 Secondly, we remove the bias correction of the moment estimators (i.e. we use gt+1  gt+1 and st+1  st+1).
In this setting, QHAdam applies the following update rule:

t+1



t

-



·

g~t+1 s~t+1

where:

t
g~t+1 = (1 - 1) · L^t(t) + 1(1 - 1) · 1i · L^t-i(t-i)
i=0
t
s~t+1 = (1 - 2)(L^t(t))2 + 2(1 - 2) · 2i (L^t-i(t-i))2
i=0

(10) (11)

F.2 IMPLICIT UPDATE BOUND
We now bound QHAdam's update (before scaling by ) by a constant dependent only on 1, 2, 1, and 2:
19Somewhat more concretely, a stochastic gradient is "spiky" if its distribution has extremely large higherorder cumulants relative to its typical magnitudes.
20The analysis with free follows the same approach but is somewhat messier.

26

Under review as a conference paper at ICLR 2019

Fact F.1 (QHAdamtight upper bound on update). Assume that s~t+1 is nonzero at each coordinate and that 0 < 1 < 2 < 1. Then, the following per-coordinate tight upper bound holds:

g~t+1



s~t+1 

(1 - 11)2 1 - 22

+

[11(1 - 1)]2 1 -

12 2

2(1 - 2)(2 - 12)

t

Proof. Provided in Appendix G.

Consider the limit case of t  . Then, the bound in Fact F.1 simplifies to:

g~t+1



s~t+1 

(1 - 11)2 1 - 22

+

[11(1 - 1)]2 2(1 - 2)(2 - 12)

(12)

For vanilla Adam (i.e. 1 = 2 = 1), (12) simplifies further to:

g~t+1 s~t+1

 (1 - 1)


2 (1 - 2)(2 - 12)

(13)

Note that since the bound in (13) is tight, this result contradicts the claim in Section 2.1 of Kingma & Ba (2015) that Adam's per-coordinate step size is bounded above by  · max{1, (1 - 1)/ 1 - 2}. 21 In the following discussion, we use the correct bounds from (12) and (13).

F.3 DISCUSSION
The recommended vanilla Adam setting of 2 = 0.999 in Kingma & Ba (2015) makes the right-hand side of (13) to be large, and various work has employed Adam with a significantly lower 2; e.g. 0.98 (Vaswani et al., 2017; Ott et al., 2018). 22 Decreasing 2 is undesirable, often slowing down training. Moving from Adam to QHAdam, an alternative solution is to decrease 2 to be below 1. This decreases the right-hand side of (12), up to a point, and thus imposes a tighter constraint on the magnitudes of updates than the vanilla Adam setting of 2 = 1. Fig. 3 shows an example of this phenomenon using a fixed 1, 1, and 2.

Figure 3: Bound from (12), fixing 1 = 0.8, 1 = 0.95, and 2 = 0.98, and varying 2.
21The difference can be rather large ­ for example, for the recommended Adam parameters of 1 = 0.9 and 2 = 0.999, Kingma & Ba (2015) claim an upper bound of 3.16 · , while (13) implies a tight bound of
7.27 · . 22We performed experiments on these models indicating that increasing 2 far beyond 0.98 led to training explosion. We suspect that these instability issues are especially prevalent in settings with rare inputs or labels, such as machine translation.
27

Under review as a conference paper at ICLR 2019

G MISCELLANEOUS DERIVATIONS
This appendix provides miscellaneous derivations that do not cleanly fit elsewhere.

Fact A.2 (Limit covariance of QHWMA). Assume that x0...t are independent random vectors, each with the covariance matrix . Then:

where  is defined as:

lim
t

Cov[QHWMA, (x0...t )]

=



·





=

(1

-

)2

+

[(1 - )]2 1 - 2

Proof. Due to the independence assumption, the covariance matrix of the QHWMA for t > 0 is simply:

Cov[QHWMA,(x0...t)] = =

t
Q2H,, (i) 

i=0

t

(1 - )2 +

(1 - )i 2



i=1
t
= (1 - )2 + ((1 - ))2 2i 

i=1

=

(1

-

)2

+

2(1

-

)22(1 1 - 2

-

2t)



The desired result follows immediately.

Fact B.1 (D term is gradient and momentum). vt can be written as:

1- vt = 

-L^t(t) + (1 - ) ·

t

i · L^t-i(t-i)

i=0

Proof. We expand vt as follows, recalling that v-1 = 0: vt d=ef  · vt-1 + (1 - )(et - et-1)
t
= (1 - ) · i(et-i - et-i-1)
i=0

We then proceed by separating out the sum in (14), recalling that e-1 = 0:

tt

vt = (1 - )

i · et-i - i-1 · et-i

i=0 i=1

t
= (1 - ) et - i-1(1 - ) · et-i

i=1

= (1 - )

et

-

1

- 



·

et

-

t

i-1(1 - ) · et-i

i=0

1- =


et - (1 - )

t

i · et-i

i=0

The desired result follows by substituting et = -L^t(t) into (15).

28

(14) (15)

Under review as a conference paper at ICLR 2019

Fact F.1 (QHAdamtight upper bound on update). Assume that s~t+1 is nonzero at each coordinate and that 0 < 1 < 2 < 1. Then, the following per-coordinate tight upper bound holds:

g~t+1



s~t+1 

(1 - 11)2 1 - 22

+

[11(1 - 1)]2 1 -

12 2

2(1 - 2)(2 - 12)

t

Proof. Firstly and without loss of generality, we can treat the gradients as single coordinates xi  R. That is, xi = L^i(i)  R for i  [0, t].
We perform the following simplification of (10) and (11):

t
g~t+1 = (1 - 11) · xt + 1(1 - 1) · 1i · xt-i
i=1
t
s~t+1 = (1 - 22) · x2t + 2(1 - 2) · 2i · x2t-i
i=1

(16) (17)

We

now

wish

to

find

the

values

of

xi

that

maximize

.g~t2+1
s~t+1

Applying

(16)

and

(17),

these

values

are

characterized by the following first-order conditions:

 s~t+1

xi

=

 g~t+1
s~t+1

 g~t+1

1 (1-1 )1i 2 (1-2 )2i
1-1 1 1-2 2

i<t i=t

(18)

Since

the

quantity

g~t2+1 s~t+1

is

invariant

to

scalar

multiplication

of

all

xi,

we

can

simplify

(18)

to:

xi =

1 (1-1 )1i

2 (1-2 1-1 1

)2i

1-2 2

i<t i=t

(19)

Plugging the values of xi from (19) into (16) and (17) yields:

max g~t2+1 s~x0...t t+1

=

(1 - 11)2 1 - 22

+

[11(1 - 1)]2 1 -

12 2

2(1 - 2)(2 - 12)

t

The desired result follows immediately.

29

Under review as a conference paper at ICLR 2019

H FULL DETAILS OF EXPERIMENTAL SETUP

Environment All experiments use Python 3.7 and PyTorch 0.4.1 (Paszke et al., 2017). Experiments are run on a mix of NVIDIA P100 and V100 GPUs, along with a mix of CUDA 9.0 and 9.2.

H.1 PARAMETER SWEEP EXPERIMENTS
Common settings (all experiments) Training occurs over 90 epochs (minibatch size 64). The first epoch uses linear warmup of the learning rate  (i.e.  starts from zero and grows to its "regular" value by the end of the epoch). Each training run uses a single GPU.
Each parameterization is run 3 times with different seeds, and we report training loss, training top-1 error, and validation top-1 error.
Common settings (QHM experiments) We use a step decay schedule for the learning rate:   {1, 0.1, 0.01}. That is, the first 30 epochs use  = 1.0, the next 30 epochs use  = 0.1, and the final 30 epochs use  = 0.01. 23
We sweep over  and  using the following two-dimensional grid:
  {0, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99, 0.995, 0.998, 0.999, 0.9995, 1}   {0, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99, 0.995, 0.998, 0.999, 0.9995}
Note that this grid encapsulates numerous parameterizations of plain SGD, momentum, and NAG (specifically, all parameterizations with the  values enumerated above). Common settings (QHAdam experiments) We fix  = 0.001, 2 = 0.999, and = 10-8, as suggested in Kingma & Ba (2015). We also fix 2 = 1.
We sweep over 1 and 1 using the same grid as for QHM's  and .

H.1.1 EXPERIMENT: LOGISTIC-EMNIST-QHM

Model The model is multinomial logistic regression with pixel vector input.

Task The task is digit recognition over the EMNIST dataset ­ specifically, the digits subset (Cohen et al., 2017).

Optimizer The model is optimized with QHM. The optimization objective is cross-entropy loss,

plus

L2

regularization

with

coefficient

1 2

·

10-4.

H.1.2 EXPERIMENT: LOGISTIC-EMNIST-QHADAM
Model Same as in Logistic-EMNIST-QHM. Task Same as in Logistic-EMNIST-QHM. Optimizer The model is optimized with QHAdam. The optimization objective is the same as in Logistic-EMNIST-QHM.

H.1.3 EXPERIMENT: MLP-EMNIST-QHM
Model The model is a multilayer perceptron (specifically, 3 layer feed forward network) with pixel vector input. The hidden layer sizes are 200, 100, and 50 units, and all hidden units are tanh nonlinearities. The final layer is followed by softmax.
Task Same as in Logistic-EMNIST-QHM.
Optimizer Same as in Logistic-EMNIST-QHM.
23These learning rates may seem high, but recall that the effective step size is identical to that of "typical", unnormalized momentum/NAG with   {0.1, 0.01, 0.001} and  = 0.9.

30

Under review as a conference paper at ICLR 2019

H.1.4 EXPERIMENT: MLP-EMNIST-QHADAM Model Same as in MLP-EMNIST-QHM. Task Same as in MLP-EMNIST-QHM. Optimizer Same as in Logistic-EMNIST-QHAdam.

H.1.5 EXPERIMENT: RN18-CIFAR10-QHM

Model The model is a 18-layer convolutional residual network with preactivations (He et al., 2016b).
Task The task is image recognition on the CIFAR-10 dataset (Krizhevsky, 2009).

Optimizer The model is optimized with QHM. The optimization objective is cross-entropy loss,

plus

L2

regularization

with

coefficient

1 2

·

(5

·

10-4).

Other details The implementation generally follows Liu (2017). Data augmentation includes horizontal flipping at random, as well as random 32-pixel crops with 4-pixel padding. For batch normalization (Ioffe & Szegedy, 2015), we use online calculation of moments with 0.99 exponential decay.

H.1.6 EXPERIMENT: RN50-IMAGENET-QHM

Model The model is a 50-layer convolutional residual network (He et al., 2016a).

Task The task is image recognition on the ILSVRC2012 ("ImageNet") 1000-class dataset (Russakovsky et al., 2015).

Optimizer The model is optimized with QHM. The optimization objective is cross-entropy loss,

plus

L2

regularization

with

coefficient

1 2

·

10-4.

Other details The implementation generally follows Paszke et al. (2016). Data augmentation includes horizontal flipping at random, as well as random 224-pixel crops. Validation is performed on 224-pixel center crops. For batch normalization, we use online calculation of moments with 0.99 exponential decay. The model is trained with half-precision floating point.

H.2 CASE STUDIES

H.2.1 EXPERIMENT: RN152-IMAGENET-QHM

Model The model is a 152-layer convolutional residual network (He et al., 2016a).

Task Same as in RN50-ImageNet-QHM.

Optimizer (baseline) We use the baseline configuration (NAG with size-256 minibatches) de-

scribed in Goyal et al. (2017). Specifically, the learning rate schedule is  = 0.1 for the first 30

epochs,  = 0.01 for the next 30 epochs,  = 0.001 for the next 20 epochs, and  = 0.0001 for

the final 10 epochs. The optimization objective is cross-entropy loss, plus L2 regularization with

coefficient

1 2

· 10-4.

The

only

departure

from

Goyal

et

al.

(2017)

is

that

we

employ

a

first-epoch

linear warmup of  (as in the parameter sweep experiments).

Optimizer (QHM) The non-baseline optimizer is QHM with  = 0.7 and  = 0.999. Following Section 7.1, we increase the learning rate () 10-fold. All other details are identical to the baseline.

Evaluation For each optimizer, we run 3 seeds and report validation top-1 error.

Other details See RN50-ImageNet-QHM for implementation details.

H.2.2 EXPERIMENT: FCONVLM-WIKITEXT103-QHM
Model The model is the GCNN-14 variant of the gated convolutional language model described in Dauphin et al. (2016). Dataset The task is language modeling on the WikiText-103 language dataset (Merity et al., 2016).

31

Under review as a conference paper at ICLR 2019
Optimizer (baseline) For the baseline, we use the configuration described in Dauphin et al. (2016). Specifically, the model is optimized with 60 epochs of NAG ( = 0.99). We initialize the learning rate to  = 1.0, and we halve it when validation loss begins to increase. The optimization objective is adaptive cross-entropy, plus direct weight decay with coefficient 5 · 10-6. We also clip gradient norm with maximum norm of 0.1.
Optimizer (QHM) The non-baseline optimizer is QHM with  = 0.98 and  = 0.998. Following Section 7.1, we increase the initial learning rate () 100-fold. All other details are identical to the baseline.
Evaluation For each optimizer, we run 10 seeds and report validation perplexity.
Other details The implementation is exactly that of fairseq-py (Gehring et al., 2017). We train each model on 8 GPUs with half-precision floating point.
H.2.3 EXPERIMENT: TD3-MUJOCO-QHADAM
Model We use the Twin Delayed Deep Deterministic Policy Gradients (TD3) algorithm (Fujimoto et al., 2018) for actor/critic learning. Both the actor and the critic are represented as multilayer perceptrons.
Task We use a suite of MuJoCo continuous control tasks (Todorov et al., 2012). In particular, we perform evaluation on the following environments: HalfCheetah, Hopper, Walker2d, Ant, Reacher, InvertedPendulum, and InvertedDoublePendulum.
Optimizer (baseline) For the baseline, we use Adam with the default parameters ( = 0.001, 1 = 0.9, 2 = 0.999, and = 10-8) as in Fujimoto et al. (2018). We train for 106 iterations. Optimizer (QHAdam) The non-baseline optimizer is QHAdam. We set 1 = 0.9, 2 = 1, and otherwise leave the baseline setting unchanged.
Evaluation For each optimizer, we run 10 seeds. For each seed, we report average reward (on 10 episodes) every 5000 iterations of training, following Fujimoto et al. (2018). Other details We use Fujimoto et al. (2018)'s open-sourced implementation 24, along with version 2 of OpenAI Gym (Brockman et al., 2016).
H.2.4 EXPERIMENT: TF-WMT16ENDE-QHADAM
Model We use a Transformer-based model (Vaswani et al., 2017) described in Ott et al. (2018).
Task We train our models on a filtered version of the WMT16 English-German machine translation dataset as in Vaswani et al. (2017), and evaluate on newstest14 for English-German. Our evaluation setup is identical to the one in Ott et al. (2018). Optimizer (baseline) For the baseline, we use Adam (1 = 0.9, 2 = 0.98, and = 10-8) as in Ott et al. (2018), training over 70 epochs. We use the same learning rate schedule as in Vaswani et al. (2017) and Ott et al. (2018). Specifically, we increase the learning rate from 10-7 to 10-3 linearly for 4000 steps, then decay it proportionally to the inverse square root of the number of steps. We optimize the label smoothed cross-entropy loss as in Vaswani et al. (2017), with label smoothing of 0.1.
Optimizer (QHAdam) We use QHAdam (1 = 0.8, 1 = 0.95, 2 = 0.7, 2 = 0.98, and = 10-8) as a non-baseline optimizer. The other settings are identical to the baseline.
Evaluation For each optimizer, we run 10 seeds and report validation perplexity and validation BLEU scores. We observe that 4 seeds for the baseline Adam optimizer "explode" (fail to converge). Thus, we only consider the 6 best seeds for each optimizer.
Other details We use Ott et al. (2018)'s open-sourced implementation in fairseq-py (Gehring et al., 2017). We train each model on 8 GPUs with half-precision floating point. Note that Ott et al. (2018) uses 128 GPUs; to eliminate this discrepancy and precisely reproduce the training environment, we accumulate gradients over 16 minibatches before each optimization step.
24https://github.com/sfujim/TD3
32

Under review as a conference paper at ICLR 2019
I FULL PARAMETER SWEEP RESULTS
Fig. 4 shows summary graphs for all parameter sweep experiments. These summary graphs display selected parameterizations and optimal parameterizations of both the vanilla and QH algorithms. Full details of experimental settings can be found in Appendix H. Since each experimental setting contains nearly 200 parameterizations with 3 seeds each, we cannot fully present the data with graphs or tables. Thus, we provide data files describing all runs in CSV format at https://tinyurl.com/2018qhdata.

(a) Logistic-EMNIST-QHM train- (b) Logistic-EMNIST-QHM train- (c) Logistic-EMNIST-QHM vali-

ing loss

ing error

dation error

(d) Logistic-EMNIST-QHAdam (e) Logistic-EMNIST-QHAdam (f) Logistic-EMNIST-QHAdam

training loss

training error

validation error

(g) MLP-EMNIST-QHM training (h) MLP-EMNIST-QHM training (i) MLP-EMNIST-QHM validation loss error error

(j) MLP-EMNIST-QHAdam train- (k) MLP-EMNIST-QHAdam train- (l) MLP-EMNIST-QHAdam vali-

ing loss

ing error

dation error

Figure 4: Full parameter sweep results (part 1 of 2). Shaded bands indicate ±1 standard deviation.

33

Under review as a conference paper at ICLR 2019

(m) RN18-CIFAR10-QHM train- (n) RN18-CIFAR10-QHM training (o) RN18-CIFAR10-QHM valida-

ing loss

error

tion error

(p) RN50-ImageNet-QHM training (q) RN50-ImageNet-QHM training (r) RN50-ImageNet-QHM validaloss error tion error
Figure 4: Full parameter sweep results (part 2 of 2). Shaded bands indicate ±1 standard deviation.

34

