Under review as a conference paper at ICLR 2019
BETTER ACCURACY WITH QUANTIFIED PRIVACY: REPRESENTATIONS LEARNED VIA RECONSTRUCTIVE ADVERSARIAL NETWORK
Anonymous authors Paper under double-blind review
ABSTRACT
The remarkable success of machine learning, especially deep learning, has produced a variety of cloud-based services for mobile users. Such services require an end user to send data to the service provider, which presents a serious challenge to end-user privacy. To address this concern, prior works either add noise to the data or send features extracted from the raw data. They struggle to balance between the utility and privacy because added noise reduces utility and raw data can be reconstructed from extracted features. This work represents a methodical departure from prior works: we balance between a measure of privacy and another of utility by leveraging adversarial learning to find a sweeter tradeoff. We design an encoder that optimizes against the reconstruction error (a measure of privacy), adversarially by a Decoder, and the inference accuracy (a measure of utility) by a Classifier. The result is RAN, a novel deep model with a new training algorithm that automatically extracts features for classification that are both private and useful. It turns out that adversarially forcing the extracted features to only conveys the intended information required by classification leads to an implicit regularization leading to better classification accuracy than the original model which completely ignores privacy. Thus, we achieve better privacy with better utility, a surprising possibility in machine learning! We conducted extensive experiments on five popular datasets over four training schemes, and demonstrate the superiority of RAN compared with existing alternatives.
1 INTRODUCTION
Today's most robust and accurate models are boosted by deep learning techniques, which benefit a lot of mobile intelligent services, such as speech-based assistant (e.g. Siri), face recognition enabled phone-unlock (e.g. FaceID). However, the uncontrolled submission of raw sound, image, and human activity data from mobile users to service provider has well-known privacy risks Abadi et al. (2016).
Existing solutions addressing the privacy concern struggle to balance between two seemingly conflicting objectives: privacy vs. utility. That is, the data sent to the service provider must be both private and useful. Privacy can be quantified by the difficulty of reconstructing raw data via a generative model, or the reconstruction error. For classification services, utility can be quantified by the inference accuracy, achieved by the service provider using a discriminative model, which is the inverse of discriminative error. An obvious and widely practiced solution to the above problem is to encode the raw data into features and upload the features only, like Google Now GoogleNow (2018); Google Cloud Machine Learning Engine also provides API to preprocess the raw data into engineering features before uploading GoogleCloud (2018). This solution not only alleviates the privacy concern but also reduces the mobile network data usage. However, it does not provide any quantifiable privacy guarantee. It is well known that we can reconstruct the raw data from the features Mahendran & Vedaldi (2015). As a result, Ossia et al. (2017) further apply dimensionality reduction and add noise to the features before sending them to the service provider, which unfortunately result in inference accuracy degradation.
1

Under review as a conference paper at ICLR 2019
Unlike previous work, we aim to systematically derive deep features for a sweeter tradeoff between privacy and utility, by leveraging adversarial training. Our key idea is to judiciously combine generative learning, for maximizing reconstruction error, and discriminative learning, for minimizing discriminative error. Specifically, we present Reconstructive Adversarial Network (RAN), an endto-end deep model with a new training algorithm. RAN controls two types of descent gradients, i.e., reconstruction error and discriminative error, in back-propagation process to guide the training of a feature extractor or Encoder.
As shown in Figure 2, a RAN consists of three parts: a feature extractor (Encoder), a utility discriminator (Classifier), and an adversary reconstructor (Decoder). The output of the Encoder feeds to the input of the Classifier and the Decoder. We envision the Encoder runs in mobile devices and processes raw data into features. The Classifier runs in an untrusted platform, e.g. the cloud. A malicious party can seek to reconstruct the raw data from the features using the Decoder. There is no theoretic guarantee on collaborated discriminative model and generative model. Therefore, we present a novel algorithm to train a RAN via an adversarial process, i.e., training the Encoder with a Classifier first to improve intermediate features' utility for discriminative tasks and confronting the Encoder with an adversary Decoder to enhance the features' privacy. All three parts, Encoder, Classifier and Decoder, are iteratively trained using gradient descent. From the manifold perspective, the two separate flows across RAN's Encoder, Decoder and Classifier, i.e., decent gradients of discrimination error and reconstruction error from the end of Classifier and Decoder in back-propagation, guide the exact model parameter updating, which can iteratively derive the privacy-specific and utility-imposed feature manifold.
Using MNIST LeCun (1998), CIFAR-10Krizhevsky et al. (2014), ImageNet Deng et al. (2009), Ubisound Sicong et al. (2017) and Har UCI (2017) benchmark datasets, we show that RAN is effective in training an Encoder for mobile users to generate deep features that are both private and useful.
Surprisingly, we observe that adversarial learned features to remove redundant information, for privacy, even surpass the accuracy of the original model. Removing redundant information enhances the generalization. See section 3 for more details. This better generalization is as auspicious illustration that in practice, with machine learning, we can gain both utility as well as privacy at the same time.
In the rest of this paper, we elaborate RAN's design in § 2 and evaluate the performance of RAN in § 3. We next review the related work in § 4 and conclude this work in § 5. We finally present the theoretic interpretation of RAN in Appendix § A.
2 DESIGN OF RAN
This section first formulates the privacy preserving problem, and then elaborates on RAN's design.
2.1 PROBLEM DEFINITION OF MOBILE PRIVACY PRESERVING
Many services exist today to analyze data from end users. In this work, we do not trust service providers for the privacy of data: they could be malicious or subject to malicious exploits. For example, as shown in Fig 1, an end user takes a picture of a product and send it to a cloud-based service to find a place to purchase it, which is indeed a service Amazon provides. A lot of sensitive information could accidentally come with the picture, such as personal information and user location in the background.
Our key insight is that most services actually do not really need the raw data. Therefore, the mobile user can encode raw data into features through a multi-layer Encoder (E) on the client side and only deliver features to the service provider. Such features ideally should have following two properties: Utility: the features contain enough essential information of raw data so that they can be useful for the intended service, e.g., high accuracy for object recognition; Privacy: it is hard to recover the sensitive information of raw data based on the features through a reverse deep model.
In this work, we focus on classification services. Therefore, utility is quantified as the inference accuracy of a discriminative model, employed by the service provider. And privacy is quantified by the reconstruction error in a reverse deep model, X, employed by a malicious party. Since the En-
2

Under review as a conference paper at ICLR 2019

Mobile User

Service Provider

Deep feature Results

Tea bag

Encoder

Classifier

Figure 1: Framework of mobile data privacy preserving. Mobile users leverage the learned Encoder to generate deep features from the raw data (i.e., "tea bag" picture) before submit it. And the service provider use the learned Classifier based on the received deep features, to recognize the object in the picture and recommend a seller.

coder is distributed to mobile users, we assume it is available to both service providers and potential attackers. That is, both the service provider and the malicious party can train their models using raw data and their corresponding Encoder output. As such we can restate the desirable properties for the Encoder output as:

Utility:

M ax
E

prob(Yi

= Yi), i  T

Privacy:

M ax
E

M in
X

|Ii

-

Ii |2 ,

i



T

(1)

where, prob(Yi = Yi) denotes the correct inference probability, i.e., accuracy, in the classification service with the testing data T. Yi and Yi is the inference class and the true label, respectively. |Ii - Ii|2 is the Euclidean distance, i.e., reconstruction error, between the raw data Ii and the mimic data Ii reconstructed by a malicious party with the Encoder output.

The first objective (Utility) is well-understood for discriminative learning. It can be achieved through

a standard optimization process, i.e., minimizing the cross entropy between the predicted label Yi

and ground truth Yi in a supervised manner Kruse et al. (2013). The inner part of the second objec-

tive,

M in
X

|Ii

-

Ii |2 ,

is

also

well-understood

for

generative

learning.

On

the

other

hand,

the

outer

part

M ax
E

|Ii

- Ii|2

is

the

opposite,

i.e.,

maximizing

the

reconstruction

error.

Therefore,

the

Encoder

and the reverse deep model employed by the malicious party (X) are adversarial to each other in

their optimization objectives.

Achieving above two objectives at the same time is difficult. Utility, i.e., maximized accuracy, and privacy, i.e., maximized reconstruction error, are conflicting objectives to the feature extractor, i.e., Encoder. When improving Utility, the Encoder must extract features to represent the relevant essence of data; when improving Privacy, the Encoder can discard the utility-relevant essence of the data. When not done properly, the Encoder output optimized for Utility leads to effective data reconstruction by a reverse model and therefore poor Privacy Rifai et al. (2011).

2.2 ARCHITECTURE OF RAN
To tackle above challenges, we present RAN to train a feature extractor, i.e., Encoder, with good trade-offs between privacy and utility. As shown in Fig 2, RAN employ two additional neural network modules, Decoder (D) and Classifier (C), to train the Encoder (E). The Classifier simulates the intended classification service; when RAN is trained by the service provider, the Classifier can be the same discriminative model eventually used. The Decoder simulates a malicious attacker that attempts to reconstruct the raw data from the Encoder output. All the three modules are end-to-end trained to establish the Encoder (E) for end-users to extract deep features E(I) from raw data I. The training is an iterative process that will be elaborated in §2.3. Below we first introduce RAN's neural network architecture, along with some empirically gained design insights.
· The Encoder (E) consists of an input layer, multiple convolutional layers, pooling layers, and batch-normalization layers. We note that the clever usage of pooling layers and batchnormalization layers contribute to deep feature's utility and privacy. The batch-normalization

3

Under review as a conference paper at ICLR 2019

Raw data 

Encoder (E) ...

. E()
/ E()
-

Decoder (D)
...
Classifier (C)
. . . .

Generated data I& = D(E  )
Inference result Y = C(E  )

Figure 2: Architecture of reconstructive adversarial network (RAN).

Algorithm 1: Mini-batch stochastic training of reconstructive adversarial network (RAN)
Input: Dataset T Output: RAN's Weights {e, d, c} 1 Initialize e, d, c ; 2 for n epochs do 3 Sample mini-batch I of m samples from T; 4 for k steps do 5 Update e and c by gradient ascent with learning rate l1: minimize Od ; 6 Update d by gradient ascent with learning rate l2: minimize Og ; 7 end 8 Update e and c by gradient ascent with learning rate l3: minimize Oa 9 end

layer helps the features' utility because it normalize the activation to avoid being too high or too low thus has an regularization affect Ioffe & Szegedy (2015). It contributes to features' privacy as well since it is hard for Decoder to recover detail information from normalized features. And then, the max-pooling layer is helpful to enhance feature's privacy, because none of un-pooling techniques can recover fine details from size-reduced features through shifting small parts to precisely arrange them into a larger meaningful structure Milletari et al. (2016).
· The Decoder (D) is a usual Encoder turned upside down, composed of multiple un-pooling layers Mahendran & Vedaldi (2015) and deconvolutional layers Zeiler et al. (2010). We note that the use of Decoder in training Encoder is to simulate a malicious party. After obtaining a (binary) version of the Encoder, a malicious party is free to explore any neural architectures to reconstruct the raw data. The reason we choose this specific architecture is that an exactly reversed mode is intuitively the most powerful adversarial against the Encoder. We also note that the architecture and training algorithm of RAN can easily incorporate other architectures as the Decoder.
· the Classifier (C) builds a multi-layer perceptron (MLP) to process deep features and output inference results with several full-connected layers Kruse et al. (2013). As we noted for the Decoder above, a service provider can explore any neural architectures for its discriminative model, given the Encoder. The reason we choose this specific architecture to train the Encoder is because some of the most successful CNN architectures, e.g. VGG and AlexNet, which can be viewed as as the Encoder plus the Classifier of our choice.
2.3 TRAINING ALGORITHM OF RAN
Our goal with RAN is to train an Encoder that can produce output that is both useful, i.e., leading to high inference accuracy when used for classification tasks, and private, i.e., leading to high reconstructive error when reverse engineered by an attacker. As we noted in §2.1, these two objectives can be competing when taken naively. The key idea of RAN's training algorithm is to train the Encoder along with the Classifier and the Decoder, which simulate the service provider and a malicious attacker, respectively. Given a training dataset T of m pairs of I, the raw data, and Y, the true label, we train a RAN through an iterative process with three stages:
4

Under review as a conference paper at ICLR 2019

Table 1: Summary of the mobile applications and corresponding datasets for evaluating RAN.

No. Task Dataset

Description

T1 Digit MNIST LeCun (1998)

70, 000 images, 10 classes

T2 Image CIFAR-10 Krizhevsky et al. (2014) 60, 000 images, 10 classes

T3 Image ImageNet Deng et al. (2009)

65, 000 images, 5 classes

T4 Audio UbiSound Sicong et al. (2017)

7, 500 audio clips, 9 classes

T5 Human Har UCI (2017)

10, 000 records of accelerometer

activity

and gyroscope data, 7 classes

1. Discriminative training to maximize the Classifier accuracy; mathematically, it minimizes

the cross entropy H between predicted class C(E(Ii)) and true label Yi:

m

Od = H(Yi - C(E(Ii))).

(2)

i=1

2. Generative training to minimize the reconstructive error by the Decoder:

m
Og = |Ii - D(E(Ii))|2
i=1

(3)

3. Adversarial training to find a tradeoff point between utility and privacy:

m
Oa =  H |Yi - C(E(Ii))| - (1 - )|Ii - D(E(Ii))|2

(4)

i=1

It is essentially a Lagrangian function of the objectives of the first two stages.  is the

Lagrange multiplier that can be used to balance between utility and privacy.

Algorithm 1 summarizes the three-stage training algorithm. And we leverage mini-batch techniques to balance the training robustness and efficiency (line 3) Li et al. (2014). Within each epoch, we first perform the discrminative and generative stages (line 5, 6) to initialize model weights. And then, we perform the adversarial stage (line 8) to seek a balance between utility and privacy. We note that k in line 4 is a hyper-parameter of first two stages. These k steps followed by a single iteration of the third stage is trying to synchronize the convergence speed of these three training stages well, borrowing existing techniques in generative adversarial network Goodfellow et al. (2014). Our implementation uses an empirically optimized value, k = 3. And we leverage the AdamOptimizer Kingma & Ba (2014) with an adaptive learning rate for all three stages (line 5, 6 and 8).

3 EVALUATION
In this section, we first compare RAN's performance in terms of privacy and utility with three baselines and then visualize the utility and privacy of resulting Encoder output.
Evaluation datasets. We evaluate RAN, especially the resulting Encoder, with five popular classification services, for which the corresponding benchmark datasets are summarized in Table 1. Specifically, RAN is evaluated for hand-written digit recognition dataset (D1: MNIST LeCun (1998)), image classification dataset (D2: CIFAR-10 Krizhevsky et al. (2014), high resolution image classification datasets (D3: ImageNet Deng et al. (2009)), non-speech acoustic event dataset (D4: UbiSound Sicong et al. (2017)), and human activity recognition dataset (D5: Har UCI (2017)).
3.1 UTILITY VS. PRIVACY TRADEOFFS
This experiment illustrates the superiority of RAN compared with three state-of-the-art mobile privacy-preserving methods. It does so with five tasks (Table 1). However, due to space limit we do not show the results for CIFAR-10 because they are similar to those for ImageNet.
· RAN generates special deep features using the trained Encoder, which is end-to-end trained with Classifier and Decoder. The utility of the generated features is the test accuracy in RAN's Classifier, and its privacy is tested by a separately trained decoder.

5

Under review as a conference paper at ICLR 2019

RAN Noisy Resized DNN

RAN Noisy Resized DNN

RAN Noisy Resized DNN

RAN Noisy Resized DNN

(a) Digit(MNIST) (b) Image(ImageNet) (c) Sound(UbiSound) (d) Activity (Har)
Figure 3: Performance comparison of RAN with three baselines on four datasets (MNIST, ImageNet, UbiSound and Har). The green dashed line shows the privacy quantified by RAN's Decoder. Y-axis is the reconstruction error on test data, normalized by log operation. And X-axis represents the accuracy on test data.
· Noisy Data method adds random Laplace noise to the raw data I and submit noisy data I to the service provider, which is a typical differential privacy method He & Cai (2017). The utility of noisy data is the test accuracy by a standard DNN, and its privacy is evaluated by data loss |I -I|2. We add Laplace noise with diverse noise factors to train DNN and test the utility and privacy.
· DNN encodes raw data into deep feature using start-of-the-art model (i.e., LeNet, VGG and AlexNet), and only deliver features to the service provider GoogleCloud (2018); GoogleNow (2018). The privacy and utility is tested by separately trained Decoder and Classifier.
· DNN(resized) method resizes deep features using Principal Components Analysis (PCA) and adds Laplace noise to preserve feature's privacy, according to Ossia et al. (2017). Here, we also use various noise factors to simulate the randomness of noisy features for performance testing.
Figure 3 summarizes the privacy-utility tradeoff of three baselines and RAN on test data. First, we see RAN's Encoder output achieves the best and narrow (robust) tradeoff, compared to those encoded by other three baselines. Specially, RAN achieves the best accuracy on test data ( 93% for MNIST, Ubisound and Har,  82% for ImageNet), while noisy data (Noisy) and DNN (resized) methods' accuracy on test data is extremely unstable, e.g. ranging 12% to 93% on UbiSound, and from 21% to 95% on Har, due to the diverse Laplace noise factors added to the test data. Second, the RAN's Encoder output achieves a higher privacy than DNN and competitive with Noisy and DNN (resized) Baselines. As mentioed in § 2.2, obtaining the Encoder, a malicious party can separately train his own Decoder to reconstruct the raw data. We notice that RAN's feature privacy tested by the separately trained Decoder is experimentally larger than that quantified by RAN's Decoder, which certify RAN's features' privacy.
An important step for RAN is to determine the Lagrangian multiplier  in adversarial training stage, i.e., Eq.(4), which impact RAN's performance tradeoff. As shown in Figure 4, the best performance tradeoff comes from different Lagrangian multiplier for diverse tasks. We compare seven discrete settings of : (0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9) to evaluate the impact of Lagrangian multiplier on RAN's performance. Particularlly, the optimal Lagrange multiplier for digit task (MNIST) is  = 0.4, for image (CIFAR-10) is  = 0.5 , for larger image (ImageNet) is  = 0.6, for human activity (Har) is  = 0.7 , and for non-speech sound (Ubisound) is  = 0.8.
Summary. Overall, RAN outperforms other three baselines to attain a better tradeoff between test accuracy (utility) and reconstruction error (privacy) in five recognition tasks. This is because the reconstruction error and discriminative error are dynamically feedback and included in RAN's end-to-end training process through back-propagation. Second, the proposed collaborative deep learning and adversarial learning algorithm achieves increased generalization with robust privacy through removing sensitive component (refer to § A), which explores a surprising possibility in machine learning. Third, the setup of Lagrange multiplier  in Eq.(4) brings the flexibility to RAN's performance tradeoff based on different requirements.
6

Under review as a conference paper at ICLR 2019

Testing privacy Testing privacy Testing privacy Testing privacy

3.3

=0.3

3.5

=0.4 =0.5

=0.6

3.7

=0.7 =0.8

=0.9

3.09.90 0T.e95sting1.0u0tility

(a) MNIST

0.3

=0.3

0.1

=0.4 =0.5

=0.6

0.1

=0.7 =0.8

=0.9

0.30.7 0.8Tes0t.i9ng u1.t0ility

(b) ImageNet

0.0

=0.3

=0.4

=0.5

0.6

=0.6 =0.7

=0.8

=0.9

1.02.85 0.9T0e0s.t9i5ng1.u0t0ility

(c) Har

2.5 =0.3

=0.4

=0.5

3.5

=0.6 =0.7

=0.8

4.05.85

=0.9
0.9T0es0t.9in5g1u.0t0ility

(d) Ubisound

Figure 4: The impact of Lagrange factor  on RAN's performance.

DNN

RAN

MNIST

ImageNet

Ubisound

Har

Figure 5: 3D visualization of the highly separable features learned by standard DNN and RAN's Encoder output in the feature space. Different color in each figure standards for one class.

3.2 UTILITY VISUALIZATION OF RAN'S ENCODER OUTPUT
To illustrate RAN's Encoder output is highly separable, and how their distribution is different from DNN's features, we leverage low-dimensional visualization method to show them in Figure 5. Each color in Figure 5 stands for a class and each point is a piece of testing data. We see that RAN's Encoder output are separable with the slight shift in the feature space compared to DNN's features, which reflects the high utility of the features learned by RAN for subsequent classification tasks.
Summary. The private features generated by RAN's Encoder is highly separable in feature space as standard DNN do, which indicates the high utility for the subsequent classification tasks.
3.3 VISUALIZING OF RAN'S PRIVACY
In this experiment, we visualize the privacy of RAN's Encoder output, i.e. private features, in comparison to other approaches, using two example images from ImageNet. Figure 6 illustrates the pixel image of the raw data, the noisy data, the mimic data reconstructed from DNN's deep feature, and mimic data reconstructed from RAN's private features from two "bus" images from ImageNet datasets. We can find that the image reconstructed by RAN's Decoder are dramatically corrupted and hard to be distinguish the exact information of raw images. As mentioned in § 3.1, the RAN's Decoder is more powerful than a separately trained Decoder.
Summary. First, the corrupted reconstructed images by RAN certify the improved privacy of RAN's Encoder output. Second, the reconstructed images from DNN's features recover both object (bus) and background (road) information, while RAN's Encoder try to contain object information and remove background information. This brings better privacy and utility (generalization) to RAN, refer to appendix § A.
4 RELATED WORK
Our work is closely related to the following categories of research.
Privacy Preserving for Mobile Data: Unlike the typical privacy preserving techniques which are adopted by data collectors (service providers) to release data for public data mining, RAN keeps

7

Under review as a conference paper at ICLR 2019

Image 1

Image 2

Raw

Noisy

DNN DNN(resized) RAN

Figure 6: From left to right: Original image from ImageNet (Raw), image with Laplace noise (Noisy), and images reconstructed from DNN's features, resized DNN's features, and RAN's private features, respectively. The images reconstructed from RAN's private features are least recognizable by human eyes yet the features lead to even better classification accuracy.

the raw data under end-user's control, i.e., the the user submits private features only, rather than raw data, to service providers. For example, randomized noise addition He & Cai (2017) and Differential privacy Dwork et al. (2014); Abadi et al. (2016) techniques have been widely used by service providers to anonymize/remove personally identifiable information or only releases statistical information to publicly release datasets. RAN outperforms Noisy data (a differential privacy method) with better classification utility and competitive privacy (§ 3.1), because RAN's Encoder is end-to-end trained with collaborative utility-specified deep learning and privacy-imposed adversarial learning for a good trade-off between features utility and privacy.
Privacy Preserving with Deep Learning: Generally, prior works adopt two classes of approaches to protect end-user's raw data: the end user modifies raw data before delivering them to service providers Ossia et al. (2017) or multiple end users cooperate to learn a global data mining results, without revealing their individual raw data Li et al. (2017). However, these segmented systematic methods inevitably incur utility drops in subsequent recognition tasks. We has compared RAN with resized noisy deep features according to Ossia et al. (2017) (§3.1), and concluded RAN achieves a better utility against altering raw data into resized deep features. This is because RAN's Encoder is also trained along with a accuracy discriminator (Classifier) to guarantee utility.
Deep Feature Learning Techniques: In order to generate special features to facilitate the subsequent classification utility and protect raw data's sensitive information from recovering by generative models, RAN is the first to present an end-to-end deep architecture to sidestep the black-box of collaborative discriminative and generative learning via an end-to-end adversarial process. Today's extensions of discriminative models, generative models, or both, have been studied to seek latent feature variables, which contributes to inference accuracy but incurs easy data reconstruction by reverse techniques Radford et al. (2015); Zhong et al. (2016). And some components used in existing generative models, such as sensitivity penalty in contractive autoencoder Rifai et al. (2011), data probability distribution in generative adversarial network Goodfellow et al. (2014) and KL divergence in variational autoencoder Doersch (2016), can be further integrated into RAN's framework to define and enhance application-based privacy.
5 CONCLUSION
This paper presents to establish a deep model for data contributors, i.e., mobile users, to encode the raw data into special features before delivering it to the data collector or miner, i.e., service provider. To realize it, we present RAN, a novel deep model for private and useful feature generating. RAN is the first to not only maximize feature's classification accuracy but also maximize its reconstruction error via an end-to-end adversarial training process. In particular, RAN consists an Encoder for feature extracting, a Decoder for data reconstruction error (privacy) quantification from Encoder output and a Classifier for accuracy (utility) discrimination. The proposed training algorithm upon RAN's contains three stages: discriminative learning function on Encoder and Classifier to boost
8

Under review as a conference paper at ICLR 2019
their discriminative abilities, a generative stage on Decoder to improve its data generative ability which stand in the position of Encoder's adversary, and an adversarial stage on Encoder, Classifier and Decoder to achieve our design objectives. Evaluations on five widely used datasets show that RAN's Encoder output attains a great tradeoff between privacy and utility. In the future, we plan to investigate finer-grained manifold learning techniques on RAN for feature generalization and privacy improvements. A few aspects of RAN do invite further research. First, the training of the two adversaries in RAN's, i.e., Encoder and Decoder, must be synchronized to avoid model degradation. It is because of the convergence diversity of Encoder and Decoder. Therefore, some more efforts are needed except RAN's practices on setting proper iteration steps k for fist two training stages (line 4 in Algorithm 1) and selecting adaptive learning rates for three stages (line 5, 6, 8 in Algorithm 1). Second, this paper is based on a generic measure, i.e., reconstruction error, to quantify privacy. RAN's architecture and training algorithm, however, can easily incorporate other measures, especially task-specific ones.
REFERENCES
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of SIGSAC, pp. 308­318. 2016.
Jen-Tzung Chien and Ching-Huai Chen. Deep discriminative manifold learning. In Proceeding of ICASSP, pp. 2672­2676. 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of CVPR. 2009.
Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Journal of Foundations and Trends in Theoretical Computer Science, pp. 211­407, 2014.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680. 2014.
GoogleCloud. Data preparation. https://cloud.google.com/ml-engine/docs/ tensorflow/data-prep, 2018.
GoogleNow. Google now launcher. https://en.wikipedia.org/wiki/Google_Now, 2018.
Jianping He and Lin Cai. Differential private noise adding mechanism: Basic conditions and its application. In American Control Conference (ACC), 2017, pp. 1673­1678. IEEE, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Alex Krizhevsky, Nair Vinod, and Hinton Geoffrey. The cifar-10 dataset. https://goo.gl/ hXmru5, 2014.
Rudolf Kruse, Christian Borgelt, Frank Klawonn, Christian Moewes, Matthias Steinbrecher, and Pascal Held. Multi-layer perceptrons. pp. 47­81. Springer, 2013.
Yann LeCun. The mnist database of handwritten digits. https://goo.gl/t6gTEy, 1998.
Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efficient mini-batch training for stochastic optimization. In Proceedings of SIGKDD, pp. 661­670. ACM, 2014.
Ping Li, Jin Li, Zhengan Huang, Tong Li, Chong-Zhi Gao, Siu-Ming Yiu, and Kai Chen. Multi-key privacy-preserving deep learning in cloud computing. Future Generation Computer Systems, pp. 76­85, 2017.
9

Under review as a conference paper at ICLR 2019

Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Proceedings of CVPR, pp. 5188­5196. 2015.
Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In Proceedings of 3DV, pp. 565­571. 2016.
Seyed Ali Ossia, Ali Shahin Shamsabadi, Ali Taheri, Hamid R Rabiee, Nic Lane, and Hamed Haddadi. A hybrid deep learning architecture for privacy-preserving mobile analytics. arXiv preprint arXiv:1703.02952, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive autoencoders: Explicit invariance during feature extraction. In Proceedings ICML, pp. 833­840. 2011.
Liu Sicong, Zhou Zimu, Du Junzhao, Shangguan Longfei, Jun Han, and Xin Wang. Ubiear: Bringing location-independent sound awareness to the hard-of-hearing people with smartphones. Journal of IMWUT, 2017.
UCI. Har: Dataset for human activity recognition. https://goo.gl/m5bRo1, 2017.
Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional networks. 2010.
Guoqiang Zhong, Li-Na Wang, Xiao Ling, and Junyu Dong. An overview on data representation learning: From traditional feature learning to recent deep learning. Journal of Finance and Data Science, pp. 265­278, 2016.

A MANIFOLD BASED INTERPRETATION

We resort to the manifold perspective of the deep model. It is common in literature to assume that the high-dimensional raw data lies on a lower dimensional manifold, refers to latent variables Chien & Chen (2016). A DNN can also be viewed as a parametric manifold learner utilizing the nonlinear mapping with multi-layer architecture and connection weights.

We decompose the input data I into two orthogonal lower dimensional manifolds:

I = IOD + IOD

(5)

Here, the component IOD is the ideal manifold component that is both necessary and sufficient
for object detection. Thus, ideally, we want our training algorithm to rely on this information for
object detection solely. Formally, for the discriminative classifier, this implies that prob(Y |I) = prob(Y |IOD). And the other manifold component IOD, orthogonal to IOD, may or may not contain information for object class, but it is dispensable for object detection. In practice, the real data does have redundant correlations, thus IOD may be learned for object detection, but unnecessary. However, revealing IOD is likely to contain sensitive information thus hurt the privacy. If we assume that there does exist a sweet-spot trade-off between utility and privacy, that we hope to find, then it must be the case that IOD is not sensitive (as it is necessary and sufficient).

The features F learned by standard deep learning algorithms to minimize the training error based
on information from I, will mostly likely overlap (non-zero projection) with both IOD and IOD. And the overlap with IOD compromises the privacy (as evident from our experiments). Apart from privacy, the redundant correlation in IOD is also likely only be spurious in training data. Thus, merely minimizing training loss can lead to over-fitting.

This is where we can shoot two stones via an adversarial process. In RAN, the Encoder is trained by
utility-specified discriminative learning objective (Eq.(2)) and privacy-imposed adversarial learning objective (Eq.(4)), to find features F as shown in Figure 7. The manifold I formulated by paramet-
ric Encoder is forced by discriminative learning objective (Eq.(2)), just like the traditional approach, to contain information from both IOD as well as IOD. However, the adversarial training objective (Eq(4)) will push features F away (or orthogonal) from IOD. In this way, we get privacy as well,

10

Under review as a conference paper at ICLR 2019

Adversarial learning

arg m | - |4

* *

** +

+ +

+*++ ++ *

+ ++ + ** * * **

Discriminative learning arg min (, )

+++

Component ./ * * * Sensitive component 5./

Figure 7: A new manifold pushed by RAN to form the feature extractor, i.e., Encoder, for utility and privacy. The utility-specified discriminative learning objective (Eq.(2)) push it to contain IOD and IOD, and the privacy-imposed adversarial training objective (Eq(4)) pushes it away from sensitive component IOD.

since F as a function of I which has two manifolds, being orthogonal to IOD forces it to only depend on IOD.
Meanwhile, from a generalization perspective, in the training data, the spurious information from IOD that might over-fit the training data is iteratively removed by the adversarial training objective (Eq.(4)) automatically leading to enhanced generalization. For example, as shown in Figure 6, if we want to discriminate between "bus" and "sailboat", the "road" in the picture can help, but it is obviously a bad way of classifying and may not generalize if the test image contains "bus" without the "road". However, "road" may be most of the background and retain some information to ease image reconstruction, which is unintended. Adding noise will obfuscate both "road" and "bus", compromising object detection at the cost of privacy. The RAN, instead, will only obfuscate "road", making reconstruction impossible without compromising the utility. In fact, RAN will get increased utility due to better generalization.
This is an auspicious illustration that in machine learning we can gain both utility and privacy in practice. A rigorous formalism and study of this phenomena could be an independent field in itself.

11

