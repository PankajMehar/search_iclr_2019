Under review as a conference paper at ICLR 2019
ITERATIVELY LEARNING FROM THE BEST
Anonymous authors Paper under double-blind review
ABSTRACT
We study a simple generic framework to address the issue of bad training data; both bad labels in supervised problems, and bad samples in unsupervised ones. Our approach starts by fitting a model to the whole training dataset, but then iteratively improves it by alternating between (a) revisiting the training data to select samples with lowest current loss, and (b) re-training the model on only these selected samples. It can be applied to any existing model training setting which provides a loss measure for samples, and a way to refit on new ones. We show the merit of this approach in both theory and practice. We first prove statistical consistency, and linear convergence to the ground truth and global optimum, for two simpler model settings: mixed linear regression, and gaussian mixture models. We then demonstrate its success empirically in (a) saving the accuracy of existing deep image classifiers when there are errors in the labels of training images, and (b) improving the quality of samples generated by existing DC-GAN models, when it is given training data that contains a fraction of the images from a different and unintended dataset. The experimental results show significant improvement over the baseline methods that ignore the existence of bad labels/samples.
1 INTRODUCTION
This paper is motivated by the problem that training large machine learning models requires wellcurated training sets with lots of samples. This is an issue in both supervised and unsupervised settings. For supervised problems (e.g. multi-label classification) we need very many properly labeled examples; generating these requires a lot of human effort. Errors in labels can significantly affect the accuracy of naive training. For example, a faulty CIFAR-10 dataset with 20% of automobile images mis-labeled as "airplane" (and so on for the other classes) leads to the accuracy of a neural architecture like WRN of Zagoruyko & Komodakis (2016) to go from over 90% to about 70%. Even in unsupervised problems, where there are no labels, spurious samples are a nuisance. Consider for example the task of training a generative adversarial network (GAN) Goodfellow et al. (2014) to produce realistic images of faces. While this has been shown to work well using a dataset of face images 1, like Celeb-A Liu et al. (2015), it degrades quickly if the training dataset is corrupted with non-face images. Our experiments illustrate these issues.
Main idea We study a simple framework that addresses the problem of spurious data in both supervised and unsupervised settings. Note that if we knew which samples were clean, we could learn a model; conversely, if we had a good model, we could find the clean samples. But we do not have either; instead we have a chicken-and-egg problem: finding the best subset of samples needs a model, but finding a model needs a subset of samples. In this paper, we take the natural approach to this problem by (0) fitting an initial model on all samples, and then alternating between (1) finding the best subset of samples given a model, and (2) fitting a model given a subset. 2 Our approach is most related to the works of Bhatia et al. (2015) and Vainsencher et al. (2017), and we contrast to these in the related work section.
1https://github.com/carpedm20/DCGAN-tensorflow 2Our framework sounds initially similar to EM-style algorithms like k-means. Note however that EM needs to postulate a model for all the data points, while we search over a subset and do not worry about the loss on corrupted points. We are alternating between a simple search over subsets and a fitting problem on only the selected subset; this is not an instance of EM.
1

Under review as a conference paper at ICLR 2019

Our approach can be applied to any existing model that provides (a) confidence scores for samples, and (b) a way to train the model for a set of samples. Indeed in our experiments we use existing models and training code, but with iterative (re)selections of samples as above. In supervised settings, any model that maps features to responses typically associates a confidence to every sample, e.g., in linear regression, given a model , the confidence in a sample (xi, yi) increases as its squared error (yi - xi )2 decreases. For a neural network classifier, on the other hand, the confidence can be measured by the cross entropy loss between the output probability distribution and the true distribution. In unsupervised settings, the confidence of a sample is given by its "likelihood" under the model. If we are lucky, e.g., in Gaussian mixture models, this likelihood is explicit and can be evaluated. However, if there is no explicit likelihood, we need to rely on a surrogate. E.g., in our GAN setting below, we rely on the output of the discriminator as a confidence measure. We also note an important caveat: this paper is not focused on (and may not be effective for) the settings of adversarial examples, or of gross corruptions of features. Rather, our focus is to address the issue of bad labeling or curation of data, as motivated above.
OUR RESULTS AND PAPER OUTLINE
We show that our framework (outlined in Section 3) can address the spurious sample problem, both theoretically and empirically and for both supervised and unsupervised problems:

(1) Mixed linear regression (Section 4) and Gaussian mixture model (Section 5) For each of these mixture problem settings, under the standard statistical assumptions, we establish both statistical consistency and linear convergence of our iteratively updated model parameters.

(2) Neural network classifiers (Section 6) : We investigate image classification in CIFAR-100, CIFAR-10 and MNIST datasets (each with a different and appropriate existing neural architecture) in the setting where there are systematic errors in the labels ­ i.e. every bad example in one label class comes from the same other label class. We show that our framework can significantly improve the classification accuracy over the baseline method that ignores the existence of bad samples.

(3) Deep generative models (Section 7): We show both quantitatively and qualitatively the improvement of using our algorithm for image generation when the dataset consists of two types of images. Specifically, investigate training a GAN to (a) generate face images as in the "intended" Celeb-A dataset, but when the given training data has a fraction of the samples being the "spurious" CIFAR-10 images, and (b) generate intended MNIST digits when training data contains a fraction of samples from the spurious Fashion-MNIST dataset. A crucial innovation here is that the output of the discriminator network in the GAN can be used as a confidence metric for the training samples.
In Section 8, we discuss the positives and negatives of our results, underlying intuition and also some future directions. All of our experiments were done on clearly identified publicly available neural network architectures, datasets, and training software. We also clearly describe our error and noise effects; we expect them to be easily reproducible.

2 RELATED WORK

Alternating minimization for robustness Two works are most related to ours. Bhatia et al. (2015)

proposes an iterative hard thresholding algorithm for adversarial noise in linear regression responses;

they establish statistical guarantees and linear convergence under this harder noise model. We focus

on multiple mixed linear regression, a different setting that results in an easier noise model, and

we

can

handle

a

larger

fraction

(

1 65

vs

1 2

-

) of bad samples. In addition we also prove these for

Gaussian mixture models, which is not studied in their work. Vainsencher et al. (2017) proposes

what is essentially a soft version of our method, and prove local convergence and generalization.

However they do not have any initialization, and hence no global or consistency guarantees. Neither

of these empirically explore the overall approach for more complex / neural network models.

Robust regression Recent work on robust regression consider strong robustness in terms of both the inputs and the outputs and provide guarantees for constant corruption ratio Diakonikolas et al.

2

Under review as a conference paper at ICLR 2019

(2018); Prasad et al. (2018); Klivans et al. (2018). Chen et al. (2013); Liu et al. (2018) study the high dimensional setting and provide algorithms for recovering the true parameters. These algorithms require much more computation, compared with methods for dealing with noisy outputs, e.g., Bhatia et al. (2015). Another type of robustness considers heteroscedastic noise for every sample. From the learning perspective, Anava & Mannor (2016); Chaudhuri et al. (2017) both require strong constraints on the noise model which largely reduces the degree of freedom. Anava & Mannor (2016) considers the online learning setting, while Chaudhuri et al. (2017) considers the active learning setting.
Mixed linear regression Alternating minimization type algorithms are used for mixed linear regression with convergence guarantee Yi et al. (2014). Chen et al. (2014) provides a convex formulation for the problem. (Balakrishnan et al., 2017) shows expectation-maximization (EM) algorithm is provably effective for a set of tasks including the basic setting of mixed linear regression and Gaussian mixture model. (Sedghi et al., 2016) extends to the multiple component setting, where provable tensor decomposition methods are used. More recently, Li & Liang (2018) gives a more general result for learning mixtures of linear regression. On the other hand, (Ray et al., 2018) studies the problem of finding a single component in mixed linear regression problem using side information.
Noisy labels Classification tasks with noisy labels are also of wide interest. Fre´nay et al. (2014) gives an overview of the related methods. Theoretical guarantee for noisy binary classification has been studied under different settings Scott et al. (2013); Natarajan et al. (2013); Menon et al. (2016). More recently, noisy label problems have been studied for deep neural network based models. Reed et al. (2014) and Malach & Shalev-Shwartz (2017) develop the idea of bootstrapping and query-bycommittee into neural network based models. On the other hand, Khetan et al. (2017) and Zhang & Sabuncu (2018) provide new losses for training under the noise. Sukhbaatar & Fergus (2014) adds a noise layer into the training process, while Ren et al. (2018) provides a meta-algorithm for learning the weights of all samples by referencing to a clean subset of validation data during training.
Others (Bora et al., 2018) studies the problem of noisy images for GANs. In their setting, all images are of low quality under some measurements, while in our problem, we consider the image dataset consists of (possibly more than) two types of images, which is different. There are also studies that consider noisy samples more generally. The classical RANSAC method Fischler & Bolles (1981) provides a paradigm for fitting a model to experimental data, and can be potentially used for the noisy data setting. However, the algorithm requires multiple times of random sampling to find a valid consensus score, which is conceptually different from our idea of doing iterative filtering. EM algorithm Moon (1996) can also be used for detecting noisy samples, e.g., in the setting of mixture models Balakrishnan et al. (2017). However, our algorithm does not need to know the likelihood for the bad samples for the iterative updates.

3 ITERATIVELY LEARNING FROM THE BEST ( ILFB )

We now describe the setup and algorithm, in a unified way that covers both supervised and unsu-
pervised settings. We will then specialize to the many specific problems we study. We are provided with samples s1, · · · , sn, using which we want to train a model parameterized by   Rd. We are also provided with a loss function; let f(s) denote the loss of a sample s when the parameters are . Let   [0, 1] be the fraction of data we want to fit. With this setup, the idea of learning from the
best can be viewed as trying to solve

min min

f (si ).

 S:|S|= n

iS

We do this by alternating between finding S and finding , as described below in Alg. 1.

3

Under review as a conference paper at ICLR 2019

Algorithm 1 ILFB Update
1: input: samples {si}ni=1, fraction  , max. iteration number T 2: initialize: set t  0 and initially fit all samples 0  arg min 3: while t < T do 4: calculate current losses ft (si) for all samples si, i  [n] 5: find new best set of samples

i[n] f(si),

St  arg min
S:|S|= n

ft (si)

iS

by sorting them according to losses ft (si) and choosing the ones with smallest losses. 6: fit a model for this new set

t+1  arg min f(si)
 iSt

7: t  t + 1 8: return: T

Comments Steps 4, 5 and 6 above can be done approximately, when exact solutions are unavailable. For example, in our experiments involving deep neural networks (both the classifiers and the generative models), for step 6 we do not fit a model exactly, but rather do training using stochastic gradient descent. Also, for our experiments on training GANs on the best subset, we do not have an explicit loss; we instead use the loss at the output of the discriminator as a surrogate (more details on that below). Similarly, the initialization step above can possibly be replaced with some problem-setting dependent alternative, if one is available.
In the following, we describe how this directly specializes to each of our four settings.

4 ILFB FOR MIXED LINEAR REGRESSION

We first describe ILFB as specialized to the case when the  represents the parameters of a linear regression, and then describe the statistical setting of mixed linear regression where we provide rigorous statistical guarantees on its performance.

Algorithm The samples are si = (xi, yi) with features xi  Rd and responses yi  R, and   Rd are the parameters of the linear model. The loss is f(x, y) = (y - x )2. The algorithm thus initializes 0 by doing an ordinary least squares (OLS) on all the samples, and then alternates between finding the set of  n samples with the lowest squared error for the current , and then
finding the new  by doing an OLS on this set.

Statistical setting In the standard and widely studied mixed linear regression problem there are n
samples (xi, yi), with features xi  N (0, Id) being standard gaussians in d dimensions. The full set of samples S = [n] can be splitted into a set of "good" samples Sgood, and m sets of "bad" samples j[m]Sbjad. The response variables yi are given by:

yi =

xi  + i, xi (j) + i,

if i  Sgood if i  Sbjad

(1)

where i  N (0, 2) is the additive noise, and  as well as all the (j) are assumed to be unit norm vectors. We analyze our algorithm in the fresh sample setting, where iteration t finds t+1 from t using a new set of samples. This too is standard in the statistical analysis of mixed linear regression, and corresponds to the case of using mini-batches of samples to learn a model.
We now prove two results; the first shows that the iterates converge linearly to the true  once t is closer to  than any of the bad (j). The second shows that, with some additional conditions on the  and (j)'s, the initialization condition ensures a 0 that satisfies this condition.

4

Under review as a conference paper at ICLR 2019

Theorem 1 (local linear convergence). For the mixed linear regression setting as above, suppose that some iterate t satisfies t -  < minj[m] t - (j) , and the  in Algorithm 1 is less than
the true ratio of good samples. Then, for any C < 1, the next iterate t+1 of the algorithm satisfies

t+1 -   C ( t -  + )

with

high

probability

(i.e.,

with

probability

1

-

n-c0

),

provided

the

number

of

samples

n



c1

d C2

4

,

where c1 is a constant.

Theorem 1 establishes a linear dependence of sample complexity on the dimension d, and a fourthorder dependence on the fraction  of good points. It would be interesting to see if this fourth-order dependence can be improved.

Theorem 2 (initialization). Given the model described in (1), assume j[m]  - (j) |Sbjad| 

n 

minj[m]



- (j)

for some  > 2, and define dm := minj[m]



- (j) . We have:



- 0



1 2 dm

with

high

probability,

where

0

=

MLE,

for

n



max{

g ()2 d 2 d2m

,

cg()m log dm

m

},

g()

=

(+2) -2

.

The assumption in Theorem 2 requires the total sum of distance of the parameters for bad samples should not be too large. For a fixed dm, if the distance for some (j) becomes larger, it may become harder for the OLS estimator to be in the local region close to  .

The proofs for the theorems are in Appendix C.1 and C.2, and synthetic experiments verifying the performance of ILFB is in Appendix C.3.

5 ILFB FOR GAUSSIAN MIXTURE MODEL

In this section, we describe ILFB applied to the setting of Gaussian mixture model, and provide statistical guarantees on its performance.

Algorithm The samples si = xi are points in Rd,   Rd is the mean we want to estimate, and f(x) is proportional to the negative log-likelihood, i.e., f(x) = x -  2. The algorithm thus initializes the mean by finding the average of all n points, and then iteratively finds the set of  n
closest points closest to the current mean, and updating the mean to be the average of this set.

Statistical setting Given a dataset D = {xi | i  S = {1, · · · , n}}, where S can be splitted into disjoint sets S = Sgood  Sb1ad  · · ·  Sbmad, the samples follows:

xi  N  , 2I , if i  Sgood xi  N (j), 2I , if i  Sbjad

(2)

We now prove two results: the first shows that iterates converge linearly under the fresh sample setting in a local region, while the second shows with some additional conditions, that the initialization of ILFB falls into this local region.

Theorem 3 (local linear convergence). We run Alg. 1 on the given dataset D generated following

(2), if 0 - 



3 4

minj[m]

0 - (j)

, then, with high probability (i.e., with probability 1 -

n-c0 ),



t+1 -   c1 t -  + c2 d,

where c1  (0, 1), c2 are constant.

Here, the 3/4 factor in the assumption is chosen for convenience of the analysis. Essentially, what we need is this value being less than one. Intuitively, given this initialization condition, the samples selected by the initialization parameter contains more than half good samples, and we can show that the algorithm is guaranteed to converge. Notice that there is no dependency between n and d. In the extreme case where  = 0, we only need n to be a constant value: from any good initialization point, the algorithm will converge to  in one step.

5

Under review as a conference paper at ICLR 2019

Table 1: Classification with systematic label error: Performance for MNIST, CIFAR-10, CIFAR100 datasets as the ratio of good samples varies from 60% to 90%. Here Baseline : Naive training using all the samples; ILFB : Our iterative update algorithm; Oracle : Training with all good samples. We see significant improvement of ILFB over Baseline for all the settings.

dataset
# good # total
60% 70% 80% 90%

MNIST

Baseline
70.26 85.95 92.62 93.88

ILFB
90.00 92.09 94.30 94.41

Oracle
94.30 94.50 94.61 94.92

CIFAR-10

Baseline
64.63 71.05 78.84 84.12

ILFB
75.16 86.09 89.14 91.03

Oracle
91.26 92.25 92.85 93.47

CIFAR-100

Baseline
58.28 66.75 72.68 78.12

ILFB
65.25 73.83 78.70 81.10

Oracle
80.39 81.65 82.50 83.41

Theorem 4 (initialization). Given a dataset D generated following (2), define dm :=

minj[m]  - (j) , and assume

j[m]( - (j))|Sbjad|  ndm. Consider taking

the overall mean as the initialization 0, then, if n 

,cd2

(

3 7

-)2

dm2

we

have

0 - 

3 4

minj[m]

0 - (j)

with high probability.



The proofs for the theorems are in Appendix D.1 and D.2, and synthetic experiments verifying the performance of ILFB is in Appendix D.3.

6 ILFB FOR DEEP IMAGE CLASSIFICATION WITH SYSTEMATIC TRAINING LABEL ERROR
Experimental setting We consider systematic errors in labels, i.e. the setting where all the samples that actually come from a class "a" are given the same bad label "b", a setting that is less benign than one with more randomness in the errors. We investigated the ability of our framework to account for these errors for the following dataset ­ neural architecture pairs: (a) MNIST (LeCun et al., 1998) dataset with a standard 2-layer CNN 3 (b) CIFAR-10 (Krizhevsky & Hinton, 2009) with a 16-layer WideResNet 4 (Zagoruyko & Komodakis, 2016) (c) CIFAR-100 5 with a 28-layer WideResNet 6 In each of these cases, the neural network has excellent performance when there are no label errors in training; this degrades significantly as the fraction of spurious samples increases.
Algorithm In each case, the algorithm involves first training the NN on all the samples, and then iteratively alternating between (a) picking the fraction  of samples in each label class with the lowest cross-entropy loss under the current , and (b) retraining the model from scratch with these picked samples, to get a new . Training was done on MXNet. Mapping back to the development in Section 3, here  represents the weights of the neural network, and f(s) is the standard crossentropy loss used in training.
Further details When training on all the samples, we run simple stochastic gradient descent algorithm with initial learning rate 0.5/0.1/0.1 and batch size 1000/256/64 for MNIST/CIFAR10/CIFAR-100, respectively. The learning rate is divided by 5 at the 50-th epoch and each experiment runs for 80 epochs. For each iteration of update using our ILFB algorithm, since less samples are used at each epoch, we adjust the total epoch number accordingly, so that the training for every iteration uses the same number of SGD updates. We set  to be 5% less than the true ratio of "good" samples.
3follow the implementation in https://mxnet.incubator.apache.org/tutorials/python/mnist.html 4cifar wideresnet16 10 from the online model zoo: https://gluon-cv.mxnet.io/api/model zoo.html 5We use the coarse labels as the classification target 6WideResNet: depth 28, width factor 10, dropout rate 0.3
6

Under review as a conference paper at ICLR 2019

Table 2: Generative models from mixed training data: A quantitative measure of the efficacy of our approach is to find how many of the good training samples the final discriminator can identify; this is shown here for the three different "good"/"bad" dataset pairs. For each pair, the fraction of "good" samples is 90%, 80% or 70%. The table depicts the ratio of the good samples in the training data that are recovered by the discriminator when it is run on the training samples. The higher this fraction, the more effective the generator. For MNIST-Fashion and CelebA-CIFAR10, our approach shows significant improvements with iteration count. For CIFAR10-CelebA dataset, the error is extremely simple to be corrected, likely because faces are easier to discriminate against when compared to natural images.

orig
ILFB iter-1 ILFB iter-2 ILFB iter-3 ILFB iter-4 ILFB iter-5

MNIST(good)-Fashion(bad)

90% 80% 70%

91.90% 96.05% 99.15% 100.0% 100.0%

76.84% 91.95% 96.14% 99.67% 100.0%

77.77% 79.12% 85.66% 91.51% 97.00%

CIFAR10(good)-CelebA(bad)

90% 80% 70%

100.0% 100.0% 100.0% 100.0% 100.0%

99.99% 99.85% 99.91% 99.96% 99.99%

98.67% 99.10% 99.69% 99.75% 99.95%

CelebA(good)-CIFAR10(bad)

90% 80% 70%

97.12% 97.33% 97.43% 97.53% 98.14%

81.34% 88.11% 89.48% 92.89% 92.94%

75.57% 76.45% 86.63% 82.15% 94.02%

(a) baseline

(b) 1st iteration

(c) 3rd iteration

(d) 5th iteration

Figure 1: Qualitative performance of ILFB for GANs: We apply ILFB to a dataset of 90% MNIST "good" images + 10% Fashion-MNIST "bad". The panels show the fake images from 32 randomly chosen (and then fixed) latent vectors, as ILFB iterations update the GAN weights. Baseline is the standard training of fitting to all samples. We can see that the baseline generates both digit and fashion images, but by the 5th iteration it hones in on digit images.

Results Table 1 shows the results for the baseline, oracle and our methods. Please see the caption thereof. We observe significant improvement over the baseline under all experiments. In Appendix B, we provide a comparison of using different initialization methods, where ILFB performs better than its counterpart with random initialization.
7 ILFB FOR DEEP GENERATIVE MODELS WITH MIXED TRAINING DATA
Experimental setting We consider training a GAN ­ specifically, the DC-GAN architecture Radford et al. (2015) ­ to generate images similar to those from a good dataset, but when the training data given to it contains some fraction of the samples from a different bad dataset. All images are unlabeled, and we do not know which training sample comes from which dataset. We investigated the efficacy of our approach in three such settings: (a) When the good dataset is the Celeb-A face images, and the bad dataset is CIFAR-10.
7

Under review as a conference paper at ICLR 2019

(a) baseline

(b) 1st iteration

(c) 3rd iteration

(d) 5th iteration

Figure 2: Qualitative performance of ILFB for GANs: Given training data consisting of 70% CelebA "good" images + 30% CIFAR-10 "bad" images, the four panels above each show the performance after iterations of our ILFB algorithm. First we choose 18 random vectors in latent pace, and fix them for all iterations. In each iteration, we retrain the GAN on corresponding selected samples to get new weighs, which are then used to generate the 18 fake images. Baseline refers to the standard training where we fit to all the samples (this is also our initialization). Visually, we can see that the generator is able to improve its generation quality and only generate face-like images after the 5th iteration.

(b) When the good dataset is the CIFAR-10 face images, and the bad dataset is Celeb-A. (c) When the good dataset is MNIST digits, and the bad is Fashion-MNIST Xiao et al. (2017). For each of these, we consider different fractions of bad samples in the training data, evaluate their effect on standard GAN training, and then the efficacy of our approach as we execute it for upto 5 iterations.
Algorithm Recall that training a GAN consists of updating the weights of both a generator network and a discriminator network; our model  is the parameters of both of these networks. Unlike the classification seeing, we now need to develop a loss f(s) ­ and unlike in the simple Gaussian mixture model setting we do not have explicit access to a likelihood function for the generative model. Our crucial innovation is to use the loss7 at the output of the discriminator ­ the same one used to train the discriminator ­ as the f(s). The algorithm starts by training on all samples, and then alternates between picking training samples with the smallest discriminator loss, and retraining the model on these picked samples. Here training means updating both the generator and discriminator network weights, which is done via SGD. Again, we set  to be 5% less than the true ratio of "good" samples.
Results We evaluate the effect of bad training samples, and the improvement by our approach, in two ways: quantitatively in Table 2 and qualitatively in Figures 1 and 2. Figure 3 illustrates a failure case, when the bad set is too big, which we feel gives insight into what is going on. Please see the respective captions. The failure example happens for MNIST-Fashion dataset when ratio of MNIST images is 60%. In fact, for every iteration, ILFB selects all images from FashionMNIST (which counts for 0.4n) and 0.15n MNIST images (the least number of MNIST images the algorithm can select). See Fig. 3 for qualitative evaluation of the results.
7Notice that for different GAN architectures, the loss function for training the discriminator varies, however, we can always find a surrogate loss by modifying the original loss function, and use the loss of the discriminator for real images as the surrogate loss for ILFB .
8

Under review as a conference paper at ICLR 2019

(a) baseline

(b) 1st iteration

(c) 3rd iteration

(d) 5th iteration

Figure 3: Illustrative failure case: This figure shows that when the fraction of "bad" samples is too
large, ILFB cannot clean them out. The setting is exactly the same as in Figure 1, but now with 60% MNIST "good" images + 40% Fashion-MNIST "bad" images. We can see that now the 5th iteration
still retains the fake fashion images. Please see the discussion section for some intuition on this.

8 DISCUSSION
We demonstrated, both theoretically for simpler settings, and empirically for more challenging neural network ones, the merit of iteratively fitting to the best samples ­ in both supervised and unsupervised problems. The underlying intuition is that ILFB can focus in on the "good" samples, provided it is properly initailized; and when the number of bad samples is significant but not overwhelming, initial fitting on all the data is good enough to get started. We now add several discussions on the merits and otherwise of our results: (1) One way to view the effect of ILFB in the GAN setting: a discriminator is likely least certain on the smallest modes of a distribution, and our process of dropping training samples based on this results in the elimination of smaller modes, which correspond to modes from the bad samples. This is illustrated by the failure case in Fig. 3, where the presence of a high fraction of "bad" fashionMNIST samples makes their modes comparable to those of the "good" MNIST. (2) Retaining samples best fit by the current model is inherently local, in the sense that samples with large errors are ignored. An initial fit on all the samples is thus important; without this we have bad performance. (3) Our theory results show convergence to the noise floor when n is O(d), but it is not clear that with much larger number of samples the algorithm cannot go below.
REFERENCES
Oren Anava and Shie Mannor. Heteroscedastic sequences: beyond gaussianity. In International Conference on Machine Learning, pp. 755­763, 2016.
Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the em algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77­120, 2017.
Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding. In Advances in Neural Information Processing Systems, pp. 721­729, 2015.
Ashish Bora, Eric Price, and Alexandros G. Dimakis. AmbientGAN: Generative models from lossy measurements. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=Hy7fDog0b.
9

Under review as a conference paper at ICLR 2019
Ste´phane Boucheron, Maud Thomas, et al. Concentration inequalities for order statistics. Electronic Communications in Probability, 17, 2012.
Kamalika Chaudhuri, Prateek Jain, and Nagarajan Natarajan. Active heteroscedastic regression. In International Conference on Machine Learning, pp. 694­702, 2017.
Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial corruption. In International Conference on Machine Learning, pp. 774­782, 2013.
Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In Conference on Learning Theory, pp. 560­604, 2014.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018.
Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24 (6):381­395, 1981.
Beno^it Fre´nay, Ata Kaba´n, et al. A comprehensive introduction to label noise. In ESANN, 2014.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled data. arXiv preprint arXiv:1712.04577, 2017.
Adam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regression. arXiv preprint arXiv:1803.03241, 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Yuanzhi Li and Yingyu Liang. Learning mixtures of linear regressions with nearly optimal complexity. arXiv preprint arXiv:1802.07895, 2018.
Liu Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. High dimensional robust sparse regression. arXiv preprint arXiv:1805.11643, 2018.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". In Advances in Neural Information Processing Systems, pp. 961­971, 2017.
Aditya Krishna Menon, Brendan Van Rooyen, and Nagarajan Natarajan. Learning from binary labels with instance-dependent corruption. arXiv preprint arXiv:1605.00751, 2016.
Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6): 47­60, 1996.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In Advances in neural information processing systems, pp. 1196­1204, 2013.
Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estimation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
10

Under review as a conference paper at ICLR 2019
Avik Ray, Joe Neeman, Sujay Sanghavi, and Sanjay Shakkottai. The search problem in mixture models. Journal of Machine Learning Research, 18(206):1­61, 2018.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. arXiv preprint arXiv:1803.09050, 2018.
Clayton Scott, Gilles Blanchard, and Gregory Handy. Classification with asymmetric label noise: Consistency and maximal denoising. In Conference On Learning Theory, pp. 489­511, 2013.
Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized linear models. In Artificial Intelligence and Statistics, pp. 1223­1231, 2016.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv preprint arXiv:1406.2080, 2(3):4, 2014.
Daniel Vainsencher, Shie Mannor, and Huan Xu. Ignoring is a bliss: Learning with large noise through reweighting-minimization. In Conference on Learning Theory, pp. 1849­1881, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear regression. In International Conference on Machine Learning, pp. 613­621, 2014.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. arXiv preprint arXiv:1805.07836, 2018.
11

Under review as a conference paper at ICLR 2019

A EFFECT OF MIS-SPECIFICATION

We test the performance of ILFB under mis-specification we choose random   Rd,  = 1,  = 1, ^ = 0.95, d = 100, n  {500, 1000, 1500}, T = 30. As shown in Fig. 4, as  gets larger,
the gap between Top-Sel and OLS estimator (in terms of 2 distance) increases linearly.

0.30

d=100, n:500-1500
oracle, n=500

oracle, n=1000

0.25

oracle, n=1500 ILFB, n=500

ILFB, n=1000

0.20 ILFB, n=1500

loss of recovered parameter

0.15

0.10

0.05

0.00 0.10 0.15 0.20 0.25nois0e.30level0.35 0.40 0.45 0.50

Figure 4: Effect of mis-specification for linear regression model

B RANDOM INITIALIZATION FOR CLASSIFICATION

We study the effect of initialization for the classification problem using MNIST (5%) dataset. In Table 3, we present the accuracy after 5-th iteration of training for both random initialization and the original ILFB methods. Even with random initialization, the performance is consistently better than the baseline, however, the improvement of accuracy is smaller when compared to the original ILFB , which uses all samples for training as initialization.

# good # total
Baseline Random Initialization + ILFB ILFB

60% 70% 80% 90%
70.26 85.95 92.62 93.88 82.05 91.48 93.80 93.79 90.00 92.09 94.30 94.41

Table 3: MNIST-5% dataset with systematic label error using ILFB with different initialization methods. The accuracy at the 5-th iteration is shown for both initialization methods. Results are averaged over 5 runs.

C LINEAR REGRESSION MODEL

C.1 PROOF FOR UPDATE STEPS IN LINEAR REGRESSION MODEL

Theorem 5 (Restate of Theorem 1 ). For the mixed linear regression setting as above, suppose that

some iterate t satisfies t -  < minj[m] t - (j) , and the  in Algorithm 1 is less than the true ratio of good samples. Then, for any C < 1, the next iterate t+1 of the algorithm satisfies

t+1 -   C ( t -  + )

(3)

12

Under review as a conference paper at ICLR 2019

with

high

probability

(i.e.,

with

probability

1

-

n-c0

),

provided

the

number

of

samples

n



c1

d C2

4

,

where c1 is a constant.

For simplicity of the notation, we present the proof by assuming yi for bad samples have the form yi = ni + ei, where ni corresponds to xi (j), such that i  Sbjad.

Let t be the current learned parameter. t+1 is learned by using one iteration in Algorithm 1. Namely, we first select a subset St of size ^n with the smallest loss (yi - xi t)2, then re-calculate the MLE estimator only using those samples. Denote Wt as the diagonal matrix whose diagonal
entry Wii equals 1 when the i-th sample is in set St, otherwise equals 0. Then,

t+1 = X WtX -1 X Wty,

(4)

where we used the fact that Wt2 = Wt. Notice that W is the ground truth for Wt (accordingly, define S for the ground truth of St). For clearness of the presentation, we ignore the subscript t when there is no ambiguation. Then, according to the definition of y and reorganizing the terms, (4)
can be expanded as

t+1 -  = X WX -1 X W (W X + (I - W ) n + e) -  = X WX -1 X WW X + X Wn - X WW n - X WX + X We

= X WX -1 X (WW - W) (X - n - e) + X WX -1 X WW e

where as we addressed at the beginning of this section, for vector n, given i  Sbad, ni = xi (j) for some j  [m]. Therefore,

t+1 -   X WX -1 · X (WW - W) (X - n - e) + X WX -1 X WW e

(5) (6)

T1 T2
We bound the three terms on the right hand side of (6) separately.

T3

C.1.1 HELPFUL LEMMAS

Lemma 6 (Sub-Gamma property for the q-quantile). Let n  3, let X(1)  · · ·  X(n) be the order

statistics of the absolute value of standard Gaussian samples. qn is an integer for some constant

q



(0, 1).

Let

vn

=

qn

8 log

2

.

For

all

0





<

qn 2 vn

,

log E e(X(qn)-E[X(qn)]) 

vn2

2 1 - 2

vn qn

(7)

This

shows

the

sub-gamma

property

of

X(qn),

and

as

a

result,

for

all

t

>

0,

and

constant

1 2

<

q

<

1,

E X(qn) = -1

1-q 2

(8)

P

X(qn) - E X(qn) 

2c1vn log n + 2c1

vn log n  n-c1 . qn

(9)

Proof of Lemma 6. This result is generalized from the result in Boucheron et al. (2012).

Lemma 7. Suppose we have two distributions D1 = N (0, 2), D2 = N (0, 1) following normal

Gaussian distribution. We have f n i.i.d. samples from D1 and (1 - f )n i.i.d. samples from D2. Denote the set of the top f¯n samples with smallest abstract values as Sf¯n, where f¯  f . Then, with

high

probability,

for





1,

at

least

1 2

f¯2n

samples

in

Sf¯n

are

from

distribution

D1.

Proof of Lemma 7. We only need to consider the boundary case where  = 1, which makes D1 = D2. Now, the problem becomes counting the number of red balls in the first f¯n balls where each ball is colored with red with probability f¯. The expected number of red balls is f¯2n. By Chernoff

lower

tail

bound,

with

probability

at

least

1

-

e-

f¯2 n 12

,

the

number

of

red

balls

is

at

least

1 2

f¯2

n.

13

Under review as a conference paper at ICLR 2019

C.1.2 T1 IN (6):

Lemma 8. Assume t satisfies t -  < minj[m] t - (j) . For the term X WX -1 in (6), with high probability, we have:

min

X

WX



1 2

|St



S

| C(c1,  ),

(10)

where |St  S

|



1 2



2

n,

and

C(c1,

)

is

a

function

with

positve

value

that

is

monotonically

in-

creasing with both c1 and  .

Proof of Lemma 8. We focus on finding concentration results for the smallest eigenvalue of X WX. Here, we can not directly apply classic concentration results since W is not fixed. Instead, each entry in the diagonal of W is determined by some rule based on X,  , and t. Rewrite X WX as
X WX = xixi 1 [li,t  arg  n- min (l1,t, · · · , ln,t)]
i

xixi 1 [li,t  arg  n- min (l1,t, · · · , ln,t)] 1 [i  S ]

(11)

i

where 1[·] is the indicator function. Note that the values {li,t}ni=1 are defined as: li,t = yi - xi t 2 .

More specifically, consider the model setting, we have:

li,t = xi ( - t) + i 2 , i  S

li,t = ni + i - xi t 2 , i  [n]\S

For all i  S , li,ts have the same distribution as µ, where µ is a 2(1) random variable, and 

is the magnitude  =



- t

2 2

+

2.

The bad samples i  [n]\S

, for i  Sbjad have the same

distribution as (2 + (j) - t 22)µ. Also, the assumption in t assumes that the magnitude of the

good distribution is smaller. Applying the results in Lemma 6 and Lemma 7, for given t,  , there

exists a constant c1 such that the separation value for deciding top- n or not is lower bounded by

c1Q(

1 2

+

 4

)

with

high

probability,

where

Q(q)

is

the

q-quantile

of

a

normal

Gaussian

distribution.

For a fixed c0, we consider the following quantity:

(c0) = E xixi | |xi ( - t) + i|  c0



- t

2 2

+

2

.

By the geometry of the constraint, we have that

min ( (c0)) 

c0 s2 1

e-

s2 2

ds

=

1

-

2(c0)

-

c0

e-

c20 2

-c0 2

In other words, the lower bound we have for the smallest eigenvalue of the expectation of (11) is

monotonically increasing with c0. We denote the value of 1 - 2

 4

)

as

C(c1,  ),

which

is

monotonically

increasing

with

both

c1

(c0) and

-

c0

e-

c20 2

when c0

= c1Q(

 . Therefore, considering

1 2

+

the

sub-exponential property, we get the concentration result based on the above expected value:

min X WX

min

xixi 1 [li,t  arg  n- min (l1,t, · · · , ln,t)] 1 [i  S ]

i



1 2

|St



S

| C(c1,  ),

where for the term |St  S

|, according to Lemma 7, with high probability, |St  S

|

1 2

^2n.

According to Lemma 8,

T1



|St



S

2 |C(c1,  )



4  2nC(c1,  ) .

14

(12)

Under review as a conference paper at ICLR 2019

C.1.3 T2 IN (6) For i  St\S , we know that:

yi = ni + ei, yi - xi t 2  yj - xj t 2 , i  St\S , j / St.

That means, for j  S \St, we know

ni + ei - xi t 2  xj ( - t) + ei 2 .

(13)

Now, define

j,t xj ,  - t + ei , t = min j,t.



- t

2 2

+

2

jS \St

(14)

We let  be the true ratio of good samples, 0 <  <   1. For simplicity, we may assume  / is a constant.
Lemma 9 (Bounding t). Consider j,t as defined in (14). Define interval

I (q) := Q (q) - c1

log q

n n

-

log c2 q

n , Q (q) n

+

c1

log n log n q n + c2 q n .

Then, with high probability,

1

t = min j,t  I
jS \St

+ 2 2

.

Proof. We know that the relationship between the size of S \St and St\S as follows:

|S \St| = |St\S | + ( -  ) n.

This shows that the set S \St has at least ( -  ) n samples. According to the definition of j,t, for

a normal vector xj  N (0, Id), j,t follows the distribution of the abstract value of a normal random

variable. We can not bound t by directly bounding the minimum value among all j,ts in the set

since the set S \St is dependent with all xis, and j,ts are not independent anymore. However,

we can bound t by only looking at the good samples in S . Intuitively, t is approximately upper

bounded by the

1 2

+

 2

-quantile of a normal Gaussian distribution. We use concentration of

order statistics, which is a generalized result based on Boucheron et al. (2012).

Let as consider the q-quantile of a normal Gaussian random variable with  n samples. Based on Lemma 6, we know that with high probability, the q-quantile value belongs to the range

log n log n

log n log n

I (q) := Q (q) - c1 q n - c2 q n , Q (q) + c1 q n + c2 q n .

As a result, t = minjS \St j,t  I

1 2

+

^ 2

with high probability.

15

Under review as a conference paper at ICLR 2019

T2 can be bounded as follows: X (WW - W) (X - n - e) 2

= xi  - ni - ei xi

iSt \S

2

= xi  - xi t + xi t - ni - ei xi

iSt \S

2

 xixi ·  - t +

xi t - ni - ei xi

iSt \S

iSt \S

2

 xixi
i[n]
  n 1+c

·  - t +

xi t - ni - ei xi

iSt \S

2

log n n

·  - t +
iSt \S

xi t - ni - ei xi
2

(15)

The second term in (15) is bounded as follows:

2

xi t - ni - ei xi

iSt \S

2

= (Xt - n - e) (WW - W) XX (WW - W) (Xt - n - e)

 XX  d+c

(Xt - n - e) (WW - W) (Xt - n - e)

d log n

xi t - ni - ei 2

iSt \S

 d + c d log n |St\S | t2  - t 2 + 2

with high probability. As a consequence,

 T2  2 n + t d + c d log n |St\S |  - t + t d + c d log n |St\S |

 c2 n 

1  - t 2 + c2Q( 2 +  )  n

 d + 4 d log n

(

- t 2 + )

(16)

C.1.4 T3 IN (6)

For this part, we can reuse the result we proved for T1. Also, notice that with high probability:

X

WW e

2 2

=

e

W WXX

WW e  (XX

)e

e2

d+c

d log n n2

Therefore,



c3 nd + n d log n

T3 

|St  S |C(c1,  ) 

(17)

16

Under review as a conference paper at ICLR 2019

C.1.5 PUTTING THINGS TOGETHER

Combining (12), (16), and (17), we have:

t+1 -  2



c2

 n+ c2Q(



c2Q(

1 2

+

 

)

n

 d+ 4d

1 2

+

 

1 4

2nc1Q(21

+

 4

)



)  n d+ 4d

log

n

log +

n c3

+

1 4



2nc1Q(

1 2

+

 4

)

 - t 2 
nd + n d log n



with high probability, since C(c1,  )

=

c1

Q(

1 2

+

 4

).

When n



( )128c22Q2

1 2

+

 

 d+32c22+128dc3

( )C2c21 4Q2

1 2

+

 4

=

O(

d C2

4

),

we

have:

t+1 -  2  C ( t -  2 + ) .

C.2 PROOF OF THE INITIALIZATION FOR SE MODEL
Proof of Theorem 2. We write out the MLE estimator as follows:
MLE = X X -1 X y = X X -1 X (W X + (I - W ) n + e) = X X -1 X (X - (I - W )X + (I - W )n + e) = - X X -1 X (I - W ) (X - n) + X X -1 X e

Therefore,

MLE - 

M


X X -1 xixi  - (j) + X X -1 X e

j=1 iSbjad



j[m]



- (j) · |Sbjad| n - c log n

+

mc

log

n

+



(d + c d log n)(n + c log n)

n - c log n





j[m]  - (j) · |Sbjad| + mc log n

2c log n 1+

+

 2(d +c d log n)

n nn



j[m]  - (j) · |Sbjad|

2c log n 1+

nn



mc log n ++

2(d +c

d log n) 

2c log n 1+

nn

n



1 mc log n   dm + n +

2(d +c

d log n) 

n

2c log n 1+
n

2c log n 1+ 
n (18)

1  2 dm

for

n



c

max{

2 (+2)2 d 2 (-2)2 d2m

,

m(+1) log (-2)dm

m

},

where

in

(18),

we

use

the

assumption

(j)

|Sbjad|



n 

minj[m]



- (j)

.

j[m]  -

17

Under review as a conference paper at ICLR 2019

loss of recovered parameter loss of recovered parameter

d=100, n=1000

MLE

0.8

Oracle Super-Oracle

R-Init+ILFB

0.6 M-Init+ILFB

0.4

d=100, n=1000

R-Init, Noise: 0.01 M-Init, Noise: 0.01 R-Init, Noise: 0.05 M-Init, Noise: 0.05

10 1 0.2

0.50 0.55 pe0r.6c0ent of g0.o65od sam0p.7l0es 0.75 0.80
Figure 5: 2 loss of recovered parameter to the true parameter for (a) MLE: naive MLE/OLS estimator; (b) Oracle: MLE estimator for the subset of good samples; (c) Super-Oracle: MLE estimator for the whole set of samples given the correct output for bad samples; (d) RInit+ILFB: our algorithm with random initialization; (e)M-Init+ILFB: our algorithm with MLE as initialization.  = 0.2, systematic error setting.

10 2
0 2 4 #6 round8s 10 12 14
Figure 6: Speed of convergence under systematic error setting, number of good samples is set as 600. M-Init: ILFB using MLE initialization; RInit: ILFB using random initialization.

C.3 SIMULATION RESULTS
We verify the performance of ILFB on linear regression model via synthetic experiments. When constructing the dataset, we consider a special case where all bad samples are generated from a single j. We test the performance when  varies in the interval [0.5, 0.8] with  = 0.2 in Fig. 5 We compare: (1) MLE: All samples are treated as good samples to find the best parameter; (2) Oracle: learn from the  n good samples given oracle access; (3) Super-Oracle: learn from n samples given oracle access; (4) R-Init+ILFB : ILFB with random initialization; (5) M-Init+ILFB : ILFB using MLE as initialization. The parameter recovery (y-axis) is measured by alg -  22. We can see that ILFB with MLE as initialization performs better than the random initialization counterpart, which shows the effectiveness of taking good initailizations. The speed of convergence is shown in Fig. 6 with multiple levels of measurement noise. The plots show linear convergence of each update step, which matches the result in Theorem 1.

D GAUSSIAN MIXTURE MODEL

D.1 PROOF FOR UPDATE STEPS IN GAUSSIAN MIXTURE MODEL

Proof. Similar to the proof for linear regression setting, we use St to denote the set of selected samples according to parameter t. Then,

1

t+1 =  n

xi.

iSt

(19)

For i  Sbad  St, the norm of xi - t is small. More specifically,

xi - t  xl - t , i  Sbad  St, l  S \St

(20)

For any good sample with index l, the value xl - t  xl -  +  - t , where the square of

1 2

xl - 

follows 2(d) distribution. Denote B(q) be the q-quantile value of 2(d) distribution,

with high probability, the q-quantile of

1 2

xl - 

for all good samples is bounded by 2B(q).

18

Under review as a conference paper at ICLR 2019

Then,

t+1 - 

 1 = n
iSt \S



xi -

xi

iS \St

(21)

t+1 - 

1

= n

(xi - t + t) -

(xi -  +  )

iSt \S

iS \St



1

 n



(xi - t) + |S \St|  - t +

xi - 

j[m] iSbjadSt

iS \St

 1 |S n

\St|

 2  - t + 4 dB

|S \St| n



 2|S \St| n



- t

+ c0|S \St| n

d 

(22)   (23) (24)
(25)

where B

|S \St| n

is uppder bounded by some constant c0, since |S

\St|

<

n 2

with high proba-

bility.

Given t - 



3 4

minj[m]

t - j

, there exists c1

 (0, 1), such that |S

\St|



c1 2



n

with

high probability. As a consequence,



t+1 -   c1 t -  + c0c1 d.

(26)

D.2 PROOF OF INITIALIZATION FOR GAUSSIAN MIXTURE MODEL

Assume

j[m]( - (j))|Sbjad|  (1- )n minj[m]  - (j) . Consider the initialization

1n

0

= n

xi

i=1

0 - 

1 =
n

n
xi - 

i=1



1

= n



xi - 

iSgood


+ xi -  
iSbad

(27) (28) (29)

1 n

( - (j))|Sbjad| + c

d n

j[m]

 1 (1 -  )n min  - (j) + c n j[m]
 3 min  - (j) 7 j[m]

d n

if

n



.cd2

(

3 7

-(1-

))2

d2k

This

will

imply

0 - 



3 4

minj[m]

0 - (j) .

(30)
(31) (32)

D.3 SIMULATION RESULTS

We also verify the performance of ILFB for Gaussian mixture model through synthetic experiments. In Gaussian mean estimation, we are interested in recovering the mean of the dominant mixture

19

loss of recovered parameter loss of recovered parameter

Under review as a conference paper at ICLR 2019

component. The bad samples are generated from another Gaussian distribution, and the distance of the two centers is set as 1 for convenience. According to Fig. 7, ILFB performs close to Oracle, and is much better than the naive method and ILFB with random initialization. In 8, we see linear convergence at the first few rounds until reaching a static point.

d=100, n=1000

MLE

d=100, n=1000

0.8 Oracle

Super-Oracle

100

R-Init+ILFB

0.6 M-Init+ILFB

0.4 0.2
0.50 0.55 perc0e.6n0 t of go0o.6d5 samp0le.7s0 0.75

10 1

R-Init, No: 0.10 M-Init, No: 0.10

R-Init, No: 0.15

M-Init, No: 0.15

R-Init, No: 0.20

M-Init, No: 0.20

0 2 4 6# roun8 ds 10 12 14 16

Figure 7: Performance of different algorithms for Figure 8: Convergence speed of ILFB under difGaussian mixture model, measured by the 2 loss ferent noise levels, using both MLE initialization of the recovered parameter to the true parameter, and random initialization.  = 0.15

20

