Under review as a conference paper at ICLR 2019
REDUCED-GATE CONVOLUTIONAL LSTM DESIGN USING PREDICTIVE CODING FOR NEXT-FRAME VIDEO PREDICTION
Anonymous authors Paper under double-blind review
ABSTRACT
Spatiotemporal sequence prediction is an important problem in deep learning. We study next-frame video prediction using a deep-learning-based predictive coding framework that uses convolutional, long short-term memory (convLSTM) modules. We introduce a novel reduced-gate convolutional LSTM architecture. Our reduced-gate model achieves better next-frame prediction accuracy than the original convolutional LSTM while using a smaller parameter budget, thereby reducing training time. We tested our reduced gate modules within a predictive coding architecture on the moving MNIST and KITTI datasets. We found that our reducedgate model has a significant reduction of approximately 40 percent of the total number of training parameters and training time in comparison with the standard LSTM model which makes it attractive for hardware implementation especially on small devices.
1 INTRODUCTION
The brain in part acquires representations by using learning mechanisms that are triggered by prediction errors found in sensory input (Friston, 2005; Bastos et al., 2012). An early implementation using this approach is used by Rao & Ballard (1999) to model non-classical receptive field properties in the neocortex. The hypothesized brain mechanisms underlying this "predictive coding" are based on the concept of bi-directional interactions between the higher and lower-level areas of the visual cortex. The higher-level areas send predictions about the incoming sensory input to the lower-level areas. The lower-level areas compare the predictions with ground truth sensory input and calculate the prediction errors. These are in turn forwarded to the higher-level areas to update their predictive representations in light of the new input information.
Lotter et al. (2017) introduced the predictive coding architecture, PredNet, for next-frame video prediction. The architecture was based on a deep neural-network framework that used a hierarchy of LSTMs. Since it was a deep network, it could readily be implemented and studied using off-theshelf deep learning frameworks (e.g. Keras, PyTorch, TensorFlow). This was in contrast to earlier models with a purely mathematical formulation (Friston, 2005). The LSTM modules in PredNet were based on the convolutional LSTMs (convLSTM) of Shi et al. (2015a). However, PredNet used an LSTM architecture (Greff et al., 2017) without peephole connections, whereas the convLSTMs of Shi et al. (2015a) included peephole connections. ConvLSTMs differ from regular LSTMs because they process images instead of feature vectors. Since convLSTMs process images, Shi et al. (2015a) replaced the matrix multiplication operation (affine transformation) with the convolution operation.
In the present paper, our work uses the PredNet architecture (Lotter et al., 2017) but is novel in the LSTM-like modules used to form the architecture. Unlike the modules in PredNet, we use peephole connections. More important, we make two novel changes to the Shi et al. (2015a) convLSTM design. First, Shi et al. (2015a) implemented peephole connections by using the elementwise (Hadamard) multiply (Horn, 1990). Although this technique is used in conventional LSTMs, convolutional LSTMs process images. In a convLSTM, the Hadamard operation will multiply corresponding pixels between two images to create a new image. This is suitable for feature vectors, but for images it creates an excessive number of trainable parameters (N 2 for an N × N image), and we have found that performance can be improved by replacing the Hadamard operation with con-
1

Under review as a conference paper at ICLR 2019
volutional operations. Our second modification is to use a novel, reduced-gate convolutional LSTM (rgcLSTM). Specifically, we use one trainable gate (the forget gate) and couple it by equality to the traditional input and output gates. This results in fewer trainable parameters but with equal or better performance accuracy, according to our experiments, than the original PredNet. Our rgcLSTM design is shown in Figure 1. The present paper motivates the design of our rgcLSTM, specifically, the convolution-based peephole connections and the single forget-gate coupled to the input and output gates. We present performance results on the moving MNIST and KITTI datasets with and without these modifications. We also study the effects of using the input and output gates coupled to the forget gate in contrast to leaving out the input and output gates entirely. We find that our architecture gives better next-frame prediction accuracy on these data sets while using a smaller parameter budget in comparison to the original Shi et al. (2015a) convLSTM which uses elementwise multiply for peephole connections and vanilla convLSTM which lacks peephole connections.
2 RELATED WORK
Recurrent neural networks (RNNs) are used to process sequential data. Spatiotemporal datasets such as video are sequential datasets where both temporal and spatial information is related. Spatiotemporal prediction (e.g. video prediction) is a challenge that has received intense interest in deep learning over the last few years. Spatiotemporal prediction typically uses unsupervised learning. There are several mechanisms to predict future video frames using unsupervised learning such as (Srivastava et al., 2015; Shi et al., 2015b;a; Lotter et al., 2017; Finn et al., 2016). However, the models all use complex architectures and a large parameter budget.
The long short-term memory (LSTM) network, introduced by Hochreiter & Schmidhuber (1997), was the first gated RNN approach to mitigate the vanishing and/or exploding gradient problem that prevented RNNs from learning long-term dependencies. Hochreiter & Schmidhuber (1997) introduced a recurrent block that contained a memory cell which was updated by a constant-errorcarousel (CEC) mechanism. The CEC was the key modification to mitigate the vanishing/exploding gradient problem. The main drawback of this first-generation LSTM was that if a continuous input stream was presented to the model long enough it could cause saturation of the output activation function due to the unlimited growth of the memory cell state values (Gers et al., 2000).
To address this problem, Gers et al. (2000) added a forget gate to the LSTM block. This design -- which used a CEC, input, output, and forget gates -- has since become known as the vanilla LSTM (Greff et al., 2017). The forget gate allowed the LSTM to update and/or reset its memory cell. This improved performance for learning long-term dependencies. However, this model also had weaknesses. First, the model had no direct connection from the memory state to the three gates. Thus, there was no control from the memory to the gates to assist in preventing the gradient from vanishing or exploding. Second, the CEC had no influence over the forget and input gates when the output gate was closed. This could harm the model due to the lack of primary information flow within the model (Gers et al., 2002). These problems were handled by adding peephole connections from the memory cell to each of the LSTM gates (Gers et al., 2002). Although the peephole generalization of the vanilla LSTM became a powerful model, there was a significant increase in the number of trainable parameters, the training time, and memory requirements.
There were attempts to design gated models to reduce the gate count while preserving learning power. Cho et al. (2014) proposed a gated recurrent unit (GRU) model. Instead of three gates, it used two: an update gate and a reset gate. The update gate combined the input gate, forget gate, and memory unit. The reset gate functioned as the output gate of the LSTM block. This GRU model eliminated the output activation function, memory unit, and CEC. The GRU yielded a reduction in trainable parameters compared with the vanilla LSTM. Zhou et al. (2016) used a single gate recurrent model called a minimal gated unit (MGU). Both models reduced the number of trainable parameters and gave results comparable to the LSTM (Chung et al., 2014). However, neither model preserved the CEC. This may lead to exploding and/or vanishing gradients. There was a study based on the LSTM model in (Gers et al., 2000) to examine the role and significance of each gate in the LSTM (Heck & Salem, 2017). This study showed that the most significant gate in the LSTM was the forget gate. However, it also showed that the forget gate needs other support to enhance its performance.
While the above gated RNNs are designed to process sequential data, they must be augmented somehow to process spatial temporal data which contains images. The convLSTM operated on
2

Under review as a conference paper at ICLR 2019
Figure 1: An unrolled block of the rgcLSTM. The single network gate (output indicated by ) sends information to three locations which correspond to the outputs of the forget, input, and output gates of the vanilla LSTM.
sequences of images. Shi et al. (2015b) proposed a convolutional LSTM to enhance performance for spatiotemporal prediction. This model replaced the matrix multiplications (affine transformations) by convolutional operations for the input and recurrent input of each gate. The model achieved higher accuracy in comparison to the classical LSTM model. However, the number of parameters remained high because the peephole connections still used elementwise multiplication. In recent research, Lotter et al. (2017) showed how to build a predictive coding model using a convolutional LSTM architecture. The LSTM had three gates but no peephole connections. Greff et al. (2017) has termed this design the vanilla LSTM. The model achieved significant improvement in a predictive model as it did not need an encoder and decoder. However, the number of parameters was large and grew linearly with the number of layers in the model. There were other attempts to design smaller recurrent gated models based on the standard LSTM. They were based on either removing one of the gates or the activation functions from the standard LSTM unit. Nonetheless, empirical analysis (Greff et al., 2017) compared these models and the standard LSTM. The conclusion found in Greff et al. (2017) was that these models had no significant improvement in either performance or training time. Moreover, these models had no significant reduction in trainable parameters. This empirical analysis (Greff et al., 2017) also stated that the critical components of the LSTM model were the forget gate and the output activation function. Our new model is named the reduced-gate, convolutional LSTM (rgcLSTM). Based on the empirical results of Greff et al. (2017), our model preserves the critical components of the LSTM while removing parameter redundancy within the LSTM block. We use our model within a predictive coding framework introduced by Lotter et al. (2017) as a state-of-the-art approach for spatiotemporal prediction. The Lotter et al. (2017) model uses vanilla convLSTM modules (no peephole connections) within their predictive coding architecture. We replace those modules with rgcLSTM. Our model showed an efficiency comparable to Lotter et al. (2017) design. However, our rgcLSTM reduces the number of trainable parameters and memory requirements by about 40% and the training time by 50% which makes it more desirable to be implemented on hardware and trainable on low power devices.
3 REDUCED-GATE CONVLSTM ARCHITECTURE
The proposed rgcLSTM model block appears in Figure 1. The architecture has one trainable gated unit which we will call the network gate. The model also preserves the cell memory state and keeps the CEC to avoid vanishing and/or exploding gradients. There is a peephole connection from the cell state to the network gate but we have converted its operator from elementwise multiplication to convolution. This reduces the learning capacity of the rgcLSTM in comparison to a full LSTM with elementwise peephole connections to all three gates. However, it preserves information needed to
3

Under review as a conference paper at ICLR 2019

allow the memory state to exert control over the network gate. Our model is a convolutional model which is preferable for image and video related tasks. Also, our model retains the output activation function. Thus, our model preserves the critical components of the LSTM as stated by Greff et al. (2017) while removing much of the parameter redundancy in the LSTM unit. This results in a significant reduction in the number of required trainable parameters, training time, memory, and hardware requirements compared to the vanilla convolutional LSTM and recurrent gated architectures. Furthermore, our rgcLSTM model preserves these model's prediction accuracy results.
During the forward pass within a block at time step t, the net input image, a(t)  R××(r+2n), to the single network gate f (t) is calculated by

a(t) = [Wfx, Ufh, Wfc]  x(t), h(t-1), c(t-1) + bf

(1)

where x(t) is the input video frame at time t, x(t)  R×× where  is the width,  is the height, and  is number of channels (depth). h(t-1) is the output image stack of the block at time t-1, and c(t-1) is the image stack representing the block's internal state at time t-1. Both h(t-1), c(t-1)  R×× where , , and  are the width, height, and number of channels, respectively. Wfx, Ufh, and Wfc are convolution weight sets operating on their respective images or stacks. Wfx is  Rm×m××r and both Ufh, and Wfc are  Rm×m××n. m is the kernel width and height, n and r are the number of kernel channels. In some cases, n and r may be equal. For a particular block, the kernels all have the same size. All three weight sets Wfx, Ufh, and Wfc and biases bf are trainable. The square brackets indicate stacking. We will let Wf = [Wfx, Ufh, Wfc]  Rm×m×(+2)×(r+2n). Also, we let If = x(t), h(t-1), c(t-1)  R××(+2). The convolution Wf  If  R××(r+2n). Finally, bf  R(r+2n)×1 but is added to each channel in Wf  If by broadcasting to the appropriate image channel. Note that the convolution operation between Wfc and c(t-1) represents a departure from Shi et al. (2015b) where an elementwise multiply was used.
The total number of trainable parameters for the network gate is

fg#ate = m2( + 2) + 1 · (r + 2n) .

(2)

The network gate image value, fg(at)te  R××(r+2n), is obtained by applying a pixelwise activation function G to the net input image

fg(at)te = G(a(t)).

(3)

Depending on the application, G can be either the logistic () or hard sigmoid (hardSig) (Gulcehre et al., 2016). The pixel values of f t will fall in the range (0, 1) or [0, 1], depending on which function is used. Using , the gate value f t is calculated by

f (t) = (Wf  If + bf ).

(4)

Stacking makes the learning process more powerful than the non-stacked weights due to the influence of the xt, h(t-1) and c(t) on the whole convolutional weight set. Lotter et al. (2017) and Heck
& Salem (2017) show empirically that stacking the input for recurrent units achieves better results
than the non-stacked input.

The input update (memory activation) uses a similar equation as for the network gate.

g(t) = tanh (Wg  Ig + bg)

(5)

In the above, Wg = [Wgx, Ugh, Wgc] and Ig = If . Wgx, Ugh, and Wgc are convolution weight sets with dimensions matching Wfx, Ufh, and Wfc, respectively. However, the weight set Wfc is
clamped to zero and cannot be trained. Because of being clamped to zero, it ignores input from c(t-1) and matches the information flow in Fig. 1. This approach is taken so that the dimension
of gg(ta)te  R××(r+2n), which matches the dimension of fg(at)te. Similarly, the dimension of bg matches that of bf . Finally, the number of trainable parameters for the input update is

4

Under review as a conference paper at ICLR 2019

Figure 2: One layer of the PredNet architecture with our rgcLSTM module substituted for the LSTM in the original model.

gu#pdate = m2( + ) + 1 · (r + 2n) .

(6)

Eqns. 2 and 6 count the total number of trainable parameters for the rgcLSTM module/block, so the final count is given by

rgcLSTMb#lock = fg#ate + gu#pdate The final equations to complete the specification of the rgcLSTM are given below.

(7)

c(t) = f (t) h(t) = f (t)

c(t-1) + f (t) tanh(c(t))

g(t)

(8) (9)

The symbol denotes elementwise multiplication.  is constrained to equal r + 2n so that the dimensions match for the elementwise multiplication operations.

4 OVERALL ARCHITECTURE

Since we use our rgcLSTM module within the PredNet predictive coding framework of Lotter et al. (2017), we named our variant as Pred-rgcLSTM. One layer of the architecture appears in Fig. 2. This predictive coding design is based on Lotter et al. (2017) and our contribution lies in the design of the convLSTM block, where it uses our rgcLSTM. In this design, whether PreNet or Pred-rgcLSTM, learning only occurs within the LSTM-like blocks. The error evaluation module in the figure calculates the error between the input frame and the predicted frame from the current rgcLSTM layer block. This module stacks the difference between the predicted image frame p and the target (desired) frame d by subtracting the two frames and applying the ReL as follows:

err1 = ReLU (p - d)

(10)

err2 = ReLU (d - p) Elayer = [err1, err2]

(11) (12)

Elayer is a stack of the two error images from Eqns. 10 and 11. The Elayer feedback is sent to the rgcLSTM input arranger unit and to the next higher layer. The rgcLSTM block learns the spatiotemporal changes using the training input data to predict future frames. Inputs to the rgcLSTM module are its internal recurrent input and its error feedback. In the multi-layer case, the input contains an

5

Under review as a conference paper at ICLR 2019

Figure 3: Three unrollment steps for one layer of the Pred-rgcLSTM model using three input frames: F1, F2, and F3. F4 is the next-frame prediction.

Table 1: Experimental conditions for recurrent gate-based blocks. N/A means that there cannot be peephole connections because there is no memory cell state.
MODEL MEMORY CELL GATES PEEPHOLE

rgcLSTM LSTM PLSTM GRU MGU

Yes Yes Yes No No

1 Yes 3 No 3 Yes 2 N/A 1 N/A

additional parameter which is the recurrent output of the directly higher rgcLSTM block. The input arranger unit stacks these inputs to fit into the rgcLSTM block. The convolution module resizes the rgcLSTM output to match the dimensions of the input target (desired) video frame d. The update process as a function of the sequential input within one layer of the proposed model as shown in Fig. 3. For illustration, we show three unrollment (unfolding) steps and we start tracking the model from the leftmost part of Fig. 3. In the beginning, the error module evaluates the prediction error by the difference between the assumed default prediction and the first input which is the first video frame. This prediction error is then sent to the rgcLSTM module as an input. The rgcLSTM module processes this input, its initial state and initial output through its internal gates to produce the next video frame as its output. Next, the current output of the rgcLSTM module passes through the error module to evaluate the next prediction error which is forwarded to the next unfolded rgcLSTM module. At this time the prediction error acts as a correction factor for the rgcLSTM module which guides the rgcLSTM module to adjust its weights and state according to the current frame and the previous prediction of this frame. The process repeats until the final predicted frame.

As our model targets the spatiotemporal prediction, we used hardSig as the recurrent activation function and tanh as the activation function. The hardSig function is calculated as follows:

hardSig(x) = max(min(0.25x + 0.5, 1), 0)

(13)

We selected the hardSig because it has empirically shown better results in our earlier experiments than the sigmoid in LSTM spatiotemporal prediction models (Elsayed et al., 2018). Also, selecting the hardSig activation instead of the logistic sigmoid helps to escape from local minima due to its hard saturation nature (Gulcehre et al., 2016).

5 METHODS
Greff et al. (2017) showed that the vanilla LSTM (three gates, no peephole connections) exceeds the accuracy performance of other LSTM based approaches. Our experiments compare our rgcLSTM

6

Under review as a conference paper at ICLR 2019

Table 2: Moving MNIST training time and standard error.

MODEL

TRAINING TIME STANDARD ERROR (SE)

vanilla convLSTM 101.66875

rgcLSTM

74.138764

0.000307 0.000104

with the vanilla convLSTM. Moreover, we compare our model which uses the rgcLSTM with the most significant spatiotemporal prediction approaches that use recurrent gated units.
Table 1 shows the properties of models we consider. These properties are the existence of a memory cell, the number of gates, and the existence of peephole connections. The models we consider are our novel rgcLSTM, the vanilla LSTM (Gers et al., 2000), the peephole LSTM (PLSTM) (Gers et al., 2002), gated recurrent unit (GRU) (Cho et al., 2014), and the minimal gated unit (MGU) (Zhou et al., 2016). These models have been applied on several approaches for video prediction. The next section compares our approach with the most significant of these previous approaches.
For training time comparisons, the models were trained on Intel(R) Core i7-7820HK CPU with 32GB memory and an NIVIDIA GeForce GXT 1080 graphics card for the first experiment. For the second experiment, both the models were trained on Intel(R) Core i7-6700@4.00GHx8 processor with 64 GB memory and NIVIDIA GeForce GXT 980 Ti/PCle/SSE2 graphics card.
6 RESULTS
We conducted two experiments on next-frame prediction of a spatiotemporal (video) sequence. The first experiment used gray-scale videos from the moving MNIST dataset which consists of MNIST digits moving in a 2D plane (Srivastava et al., 2015). The second experiment was applied to a RGB Kitti traffic video dataset. The moving MNIST dataset consists of 10, 000 different video sequences of two randomly selected moving digits from the MNIST data set. Each sequence is 20 frames long, with a frame size of 64x64 pixels. We divided the dataset into 6000 video sequences for training, 3000 for validation and 2000 for testing. The training process was completed in one epoch.
We trained our model using three rgcLSTM layers. The number of kernels in each layer were 48, 96, and 192 respectively. We used the Adam optimizer (Kingma & Ba, 2014) with initial learning rate  = 0.001, 1 = 0.9 and 1 = 0.999. We down-sampled the frame by factor of 2 moving upwards through the layers. We up-sampled the frame by factor of 2 moving down through the layers.
We compared the rgcLSTM and vanilla-convLSTM. The training time (in minutes) comparison is shown in Table 2. Using the rgcLSTM block instead of the vanilla-convLSTM reduced training time by approximately 45% for moving MNIST training using one epoch. Table 2 also shows the SE of model testing loss for both the vanilla-convLSTM and our rgcLSTM models. The sample size of SE was n = 3 for each model. The SEs are very small compared to the mean differences.
Since we implemented our rgcLSTM model within the predictive-coding framework of Lotter et al. (2017), we named it Pred-rgcLSTM. We compared Pred-rgcLSTM with several other unsupervised recurrent gate-based prediction models (i.e. either LSTM or GRU based models) for the moving MNIST dataset. Performance measures included mean squared error (MSE) (Goodfellow et al., 2016), mean absolute error (MAE) (Goodfellow et al., 2016) and the structural similarity index (SSIM) (Wang et al., 2004) which is a measure for image quality similarity structure between predicted and target images. We also compared our model to PredNet which our model is based on but using the vanilla-convLSTM. For the remaining models that we were unable to test due to hardware limits we obtained the results from their published work. All the models are convolutional variants.
Performance comparisons among the models for moving MNIST are in Table 3. Our Pred-rgcLSTM shows a reduction of both the MSE and MAE compared to all of the other models. This includes FCLSTM (Srivastava et al., 2015), CDNA (Finn et al., 2016), DFN (Jia et al., 2016), VPN (Kalchbrenner et al., 2016), ConvLSTM (Shi et al., 2015b), ConvGRU, TrajGRU, (Shi et al., 2017), PredRNN (Wang et al., 2017), PredRNN++ (Wang et al., 2018) and Prednet (Lotter et al., 2017). The main comparison is between our rgcLSTM block and the LSTM block in the same model architec-
7

Under review as a conference paper at ICLR 2019

Table 3: Moving MNIST performance comparison.

MODEL

MSE MAE SSIM

FCLSTM (Shi et al., 2015b; Srivastava et al., 2015) CDNA (Wang et al., 2018; Finn et al., 2016) DFN (Wang et al., 2018; Jia et al., 2016) VPN (Wang et al., 2018; Kalchbrenner et al., 2016) ConvLSTM (Wang et al., 2018; Shi et al., 2015b) ConvGRU (Wang et al., 2018; Shi et al., 2017) TrajGRU (Wang et al., 2018; Shi et al., 2017) PredRNN++ (Wang et al., 2018) PredRNN (Wang et al., 2017; 2018) PredNet (Lotter et al., 2017) Pred-rgcLSTM

1.865 0.974 0.890 0.641 1.420 1.254 1.138 0.465 0.568 0.011 0.009

2.094 1.753 1.728 1.310 1.829 2.254 1.901 1.068 1.261 0.049 0.017

0.690 0.721 0.726 0.870 0.707 0.601 0.713 0.898 0.867 0.915 0.924

ture. Our model also has the highest (best) structural similarity index measurement (SSIM) among the considered models.
Based equation 7, the number of trainable parameters for one rgcLSTM block is calculated as follows:

numParamrgcLST M = fg#ate + gu#pdate = 2 × ( m2( + 2) + 1 · (r + 2n))

(14)

The multiplication by 2 because of the forget gate and the input update activation units.

For the vanilla-convLSTM. , the number of parameters is calculated by:

numParamLST M = fg#ate + gu#pdate + ig#ate + ou#pdate = 4 × ( m2( + 2) + 1 · (r + 2n))

(15)

Where i#gate and o#gate are the number of trainable parameters for the input and ourput gates of the vanilla-convLSTM block. The multiplication by 4 due to the input update activation, forget gate,
input gate, and output gate that each has the same number of trainable parameters.

Hence, the number of trainable parameters is approximately halved in the rgcLSTM compared to the vanilla-convLSTM. Some additional parameters increasing the halving are due to the peephole connection which increases the size of the input of the rgcLSTM block.

Table 4: Moving MNIST number of trainable parameters in each model. MODEL TRAINING PARAMETERS

Model PredNet ConvGRU TrajGRU ConvLSTM FCLSTM Pred-rgcLSTM

Training Parameters 6,909,834 8,010,000 4,770,000 7,585,296 142,667,776 4,751,700

Using Keras API, we compared the number of training parameters used in each of the models in Table 4. Our model uses the smallest number of trainable parameters. Our model has fewer trainable parameters approximately by 40%-50% than the ConvGRU and ConvLSTM respectively. Our PredrgcLSTM model that is based on rgcLSTM block has approximately 40% fewer trainable parameters

8

Under review as a conference paper at ICLR 2019
Figure 4: Visual results of Moving MNIST predictions after training based on our rgcLSTM and other models.
Figure 5: Next-frame prediction on the KITTI dataset. The predicted image is used as input for predicting the next frame. that the original PredNet model that uses vanilla LSTM modules. The visual predictions for the moving MNIST data set appear in Fig. 4. In contrast to most of the models, both PredNet and our Pred-rgcLSTM do not require an initial input trajectory for predicting the next ten upcoming frames. It requires only the current frame and the current state of the trained LSTM to predict the next frame. Our rgcLSTM has comparable results with the other models and has better visual contours than the convolutional LSTM used in PredNet. Our second experiment was applied to the KITTI dataset (Geiger et al., 2013). This dataset is a collection of real life traffic videos that were captured by a camera mounted on a car roof driving in an urban environment. The data is divided into three categories: city, residential, and road. Each category contained 57 videos. For training both the rgcLSTM and vanilla-convLSTM we used the same initialization, training, validation and data down-sampling which was recommended by Lotter et al. (2017). After preprocessing, each frame size was 128x160 pixels. The length of the training dataset was approximately 41K frames. The visual testing results of our rgcLSTM and the vanillaconvLSTM are shown in Figure 5. This figure shows that the rgcLSTM has a visual improvement in precise prediction of object contours as compared with the the vanilla-convLSTM with smaller training time and number of parameters. The results of the KITTI dataset benchmark training time (in minutes), number of trainable parameters (# Params), mean squared error (MSE) (Goodfellow et al., 2016), mean absolute error (MAE) (Goodfellow et al., 2016), standard error (SE) (Goodfellow et al., 2016), and structural similarity
9

Under review as a conference paper at ICLR 2019

Table 5: Performance comparison on the KITTI traffic video dataset.

MODEL

TRAIN TIME # Params MSE MAE SE

SSIM

Conv-vanilla LSTM 230.982405

rgcLSTM

140.530364

6,915,948 0.0035 0.030 0.000006 0.931 4,755,810 0.0035 0.030 0.000005 0.931

Figure 6: KITTI dataset training and validation losses for both Pred-rgcLSTM and PredNet. Left: Pred-rgcLSTM. Middle: Prednet. Right: both graphs superimposed.
index measurement (SSIM) (Wang et al., 2004) of our rgcLSTM and vanilla-convLSTM are shown in Table 5. The MSE, MAE, SE, and SSIM have a sample size of n = 3. Both models have approximately the same MSE, MAE, and SSIM values. However, the rgcLSTM uses fewer trainable parameters and the training time is reduced by about 40% - 50% as in the moving MNIST (gray-scale) prediction task and a smaller standard error value (SE).
Fig. 6 shows the training and validation loss through 150 epochs of KITTI dataset training using both the proposed Pred-rgcLSTM and PredNet. The graphs show that the training and validation losses of both models is about the same. Therefore, the rgcLSTM is comparable with the vanillaconvLSTM in regard to accuracy while using fewer trainable parameters, and less training time and memory.
7 CONCLUSION
The novelty of our rgcLSTM architecture lies in the following features. First, there is one network gate which serves the function of the forget, input, and output gates. This reduces the number of gate parameters to one third that of a vanilla LSTM. Second, in this reduced model there is still a peephole connection from the cell memory state to the network gate. Finally, the rgcLSTM uses a convolutional architecture and we have replaced the elementwise mulitiply operation originally used in Shi et al. (2015a) with the convolution operation. This reduces the number of trainable parameters as well as being a more appropriate operator for spatio-temporal image processing.
The proposed rgcLSTM model outperforms the vanilla-convLSTM and other gate-based recurrent architectures. Also, the model required fewer trainable parameters, memory, and training time. This was achieved by maintaining the critical components of the standard LSTM model, enhanced by the peephole connections and removing redundant training parameters. These results make our rgcLSTM model attractive for future hardware implementation on small and mobile devices.
REFERENCES
Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J Friston. Canonical microcircuits for predictive coding. Neuron, 76(4):695­711, 2012.
Kyunghyun Cho, Bart Van Merrie¨nboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
10

Under review as a conference paper at ICLR 2019
Nelly Elsayed, Anthony S. Maida, and Magdy Bayoumi. Empirical activation function effects on unsupervised convolutional LSTM learning. In 30th International Conference on Tools with Artificial Intelligence. IEEE, 2018. in press.
Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In Advances in neural information processing systems, pp. 64­72, 2016.
Karl Friston. A theory of cortical responses. Phil. Trans. R. Soc. B, 360:815­836, 2005.
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research, 32(11):1231­1237, 2013.
Felix A Gers, Ju¨rgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with LSTM. Neural Computation, pp. 2451­2471, 2000.
Felix A Gers, Nicol N Schraudolph, and Ju¨rgen Schmidhuber. Learning precise timing with LSTM recurrent networks. Journal of machine learning research, 3(Aug):115­143, 2002.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org.
Klaus Greff, Rupesh K Srivastava, Jan Koutn´ik, Bas R Steunebrink, and Ju¨rgen Schmidhuber. LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10):2222­2232, 2017.
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions. In International Conference on Machine Learning, pp. 3059­3068, 2016.
Joel Heck and Fathi M Salem. Simplified minimal gated unit variations for recurrent neural networks. arXiv preprint arXiv:1701.03452, 2017.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735­1780, 1997.
Roger A Horn. The hadamard product. In Proc. Symp. Appl. Math, volume 40, pp. 87­169, 1990.
Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. In Advances in Neural Information Processing Systems, pp. 667­675, 2016.
Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video prediction and unsupervised learning. In International Conference on Learning Representations. Computational and Biological Learning Society, 2017.
Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79, 1999.
X. Shi, Z. Chen, H. Wang, D. Yueng, W. Wong, and W. Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasing. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 802­810. Curran Associates, Inc., 2015a.
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in neural information processing systems, pp. 802­810, 2015b.
Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wangchun Woo. Deep learning for precipitation nowcasting: A benchmark and a new model. In Advances in Neural Information Processing Systems, pp. 5617­5627, 2017.
11

Under review as a conference paper at ICLR 2019
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using LSTMs. In International conference on machine learning, pp. 843­852, 2015.
Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and S Yu Philip. PredRNN: Recurrent neural networks for predictive learning using spatiotemporal LSTMs. In Advances in Neural Information Processing Systems, pp. 879­888, 2017.
Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S Yu. PredRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. arXiv preprint arXiv:1804.06300, 2018.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600­ 612, 2004.
Guo-Bing Zhou, Jianxin Wu, Chen-Lin Zhang, and Zhi-Hua Zhou. Minimal gated unit for recurrent neural networks. International Journal of Automation and Computing, 13(3):226­234, 2016.
12

