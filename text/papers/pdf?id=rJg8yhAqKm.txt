Under review as a conference paper at ICLR 2019
TRANSFER AND EXPLORATION VIA THE INFORMATION BOTTLENECK
Anonymous authors Paper under double-blind review
ABSTRACT
A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out decision states. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space. We demonstrate that this approach to exploration outperforms several state-of-the-art exploration methods in several challenging domains.
1 INTRODUCTION
Deep reinforcement learning has enjoyed many recent successes in domains where large amounts of training time and a dense reward function are provided. However, learning to quickly perform well in environments with sparse rewards remains a major challenge. Providing agents with useful signals to pursue in lieu of environmental reward becomes crucial in these scenarios. Here, we propose to incentive agents to learn about and exploit multi-goal task structure in order to efficiently explore in new environments. We do so by first training agents to develop useful habits as well as the knowledge of when to break them, and then using that knowledge to efficiently probe new environments for reward.
We focus on multi-goal environments and goal-conditioned policies (Foster and Dayan, 2002; Schaul et al., 2015; Plappert et al., 2018). In these scenarios, a goal G is sampled from p(G) and the beginning of each episode and provided to the agent. The goal G provides the agent with information about the environment's reward structure for that episode. For example, in spatial navigation tasks, G might be the location or direction of a rewarding state. We denote the agent's policy (A | S, G), where S is the agent's state, A the agent's action, and  the policy parameters.
We incentive agents to learn task structure by training policies that perform well under a variety of goals, while not overfitting to any individual goal. We achieve this by training agents that, in addition to maximizing reward, minimize the policy dependence on the individual goal, quantified by the conditional mutual information I(A; G | S). This approach is inspired by the information bottleneck approach (Tishby et al., 1999) to training deep neural networks for supervised learning (Alemi et al., 2017; Chalk et al., 2016; Achille and Soatto, 2016; Kolchinsky et al., 2017), where classifiers are trained to achieve high accuracy while simultaneously encoding as little information about the input as possible. This form of "information dropout" has been shown to promote generalization performance (Achille and Soatto, 2016; Alemi et al., 2017). Here, we show that minimizing goal information promotes generalization in an RL setting as well. We refer to this approach as InfoBot (from information bottleneck).
This approach to learning task structure can also be interpreted as encouraging agents to develop useful habits. To see this, note that our regularizer can also be written as I(A; G | S) =
1

Under review as a conference paper at ICLR 2019

E [DKL[(A | S, G) | 0(A | S)]], where (A | S, G) is the agent's multi-goal policy, E denotes an expectation over trajectories generated by , DKL is the Kuhlback-Leibler divergence, and 0(A | S) = g p(g) (A | S, g) is a "habit" policy with the goal marginalized out. While the agent never actually follows the habit policy 0 directly, it can be viewed as what the agent might do in the absence of any knowledge about the goal. Thus, our approach encourages the agent to learn useful habits and to follow those habits closely, except where diverting from doing so leads to significantly higher reward. Humans too demonstrate an affinity for relying on habits when they can (Kool and Botvinick, 2018), which we take as encouraging support for this line of work (Hassabis et al., 2017).
We refer to states where diversions from habits occur as decision states, based on the intuition that they require the agent not to rely on their habit but instead to make a goal-dependent "decision." Our approach to exploration then involves encouraging the agent to seek out these decision states in new environments. The intuition behind this approach is that such decision states are natural subgoals for efficient exploration because they are boundaries between achieving different goals (van Dijk and Polani, 2011). By encouraging an agent to visit them, the agent is encouraged to 1) follow habitual trajectories that work across many goals and 2) uniformly explore across the many "branches" of decision-making. We encourage the visitation of decision states by first training an agent to recognize them by training with the information regularizer introduced above. Then, we freeze the agent's policy, and use DKL[(A | S, G) | 0(A | S)] as an exploration bonus for training a new policy. Crucially, this approach to exploration is tuned to the family of tasks the agent is trained on, and we show that it promotes more efficient exploration than other task-agnostic approaches to exploration (Houthooft et al., 2016; Pathak et al., 2017).
To summarize our contributions:
· We regularize RL agents in multi-goal settings with I(A; G | S), an approach inspired by the information bottleneck and the cognitive science of habits, and show that it promotes generalization across tasks.
· We use policies as trained above to then provide an exploration bonus for training new policies in the form of DKL[(A | S, G) | 0(A | S)], which encourages the agent to seek out decision states. We demonstrate that this approach to exploration performs more effectively than other state-of-the-art methods, including a count-based bonus, VIME (Houthooft et al., 2016), and curiosity (Pathak et al., 2017).

2 OUR APPROACH

Our objective is to train an agent on one set of tasks (environments) T  ptrain(T ), but to have the agent perform well on another different, but related, set of tasks T  ptest(T ). To do so, we propose to maximize the following objective in the training environments:
J()  E [r] - I(A; G | S)
= E [r - DKL [(A | S, G) | 0(A | S)]] , (1)
where E denotes an expectation over trajectories generated by the agent's policy,  > 0 is a tradeoff parameter, DKL is the Kuhlback-Leibler divergence, and 0(A | S)  g p(g) (A | S, g) is a "habit" policy with the agent's goal marginalized out.

policy

 (A
<latexit sha1_base64="vvmsRn44bLbpO2HtyQPL9QhgcNY=">AAACDnicbVDJSgNBEO2JW4xb1KOX1hCIIGFGBHOMeNBjRLNAJoSeTk3SpGehu0YIQ77Ai7/ixYMiXj1782/sLAeNPih4vFdFVT0vlkKjbX9ZmaXlldW17HpuY3Nreye/u9fQUaI41HkkI9XymAYpQqijQAmtWAELPAlNb3g58Zv3oLSIwjscxdAJWD8UvuAMjdTNF91YdFMXB4Bs7B66EnwsXVA3ED16e3LlKtEf4HE3X7DL9hT0L3HmpEDmqHXzn24v4kkAIXLJtG47doydlCkUXMI45yYaYsaHrA9tQ0MWgO6k03fGtGiUHvUjZSpEOlV/TqQs0HoUeKYzYDjQi95E/M9rJ+hXOqkI4wQh5LNFfiIpRnSSDe0JBRzlyBDGlTC3Uj5ginE0CeZMCM7iy39J47Ts2GXn5qxQrczjyJIDckRKxCHnpEquSY3UCScP5Im8kFfr0Xq23qz3WWvGms/sk1+wPr4ByeGbPQ==</latexit>

|

S, G)

decoder

pdec(A
<latexit sha1_base64="Iq/stS8ZR9f6WF3RymXwlPi7Qrs=">AAACDXicdVBNSyNBEO3R9WOzfkT3uJdeswsKMvQY8+FN8eJR0ahsJoSenpqkseeD7hoxDPkDXvavePGgiFfv3vw3dmIW3MV9UPB4r4qqekGmpEHGXpyp6U8zs3Pzn0tfFhaXlssrq6cmzbWAlkhVqs8DbkDJBFooUcF5poHHgYKz4GJ/5J9dgjYyTU5wkEEn5r1ERlJwtFK3/CPr+ghXWIQghv53X0GE63t+LEN6vPnL17LXx41uucLcHVb3ajXK3Gq1XquPyFajyhpN6rlsjAqZ4LBbfvbDVOQxJCgUN6btsQw7BdcohYJhyc8NZFxc8B60LU14DKZTjL8Z0p9WCWmUalsJ0rH6fqLgsTGDOLCdMce++dcbiR957RyjZqeQSZYjJOJtUZQriikdRUNDqUGgGljChZb2Vir6XHOBNsCSDeHPp/T/5HTL9ZjrHW1XdpuTOObJN7JG1olHGmSXHJBD0iKCXJMbckfund/OrfPgPL61TjmTma/kLzhPryDVm40=</latexit>

|

S, Z)

A

encoder

penc(Z
<latexit sha1_base64="bZhBgZvFGMw2E2nN/u8tNTfEbf0=">AAACDXicdVBNS1tBFJ3nV2PqR9Slm9EoKMjjjU1MshNctEtFE4N5Icyb3JcMzvtg5j4xPPIH3PSvdNNFpXTbfXf+Gycxgi32wMDhnHu5c06QKmnQ856cufmFxaUPheXix5XVtfXSxmbLJJkW0BSJSnQ74AaUjKGJEhW0Uw08ChRcB7dnE//6DrSRSXyFoxS6ER/EMpSCo5V6pb205yPcYw6xGPs7voIQD278SPbp5dFnX8vBEA97pbLnHrMKqzWo51ar7FO9YknjpOLVGWWuN0WZzHDeK/3x+4nIIohRKG5Mh3kpdnOuUQoF46KfGUi5uOUD6Fga8whMN5+mGdN9q/RpmGj7YqRT9e1GziNjRlFgJyOOQ/OvNxHf8zoZhvVuLuM0w0na6aEwUxQTOqmG9qUGgWpkCRda2r9SMeSaC7QFFm0Jr0np/0nr2GWeyy4q5dP6rI4C2Sa75IAwUiOn5As5J00iyAP5Rn6QR+er89356fx6GZ1zZjtb5C84v58BNqKbmg==</latexit>

| S, G)

S

Z G

Figure 1: Policy architecture.

2.1 TRACTABLE BOUNDS ON INFORMATION

We parameterize the policy (A | S, G) using an encoder penc(Z | S, G) and a decoder pdec(A | S, Z) such that (A | S, G) = z penc(z | S, G) pdec(A | S, z) (see figure 1).1 The encoder output Z is meant
1In practice, we estimate the marginalization over Z using a single sample throughout our experiments.

2

Under review as a conference paper at ICLR 2019

to represent the information about the present goal G that the agent believes is important to access in the present state S in order to perform well. The decoder takes this encoded goal information and the current state and produces a distribution over actions A.

We suppress the dependence of penc and pdec on , but  in the union of their parameters. Due to the data processing inequality (DPI) (Cover and Thomas, 2006), I(Z; G | S)  I(A; G | S), so
minimizing the goal information encoded by penc also minimizes I(A; G | S).

Thus, we instead maximize this lower bound on J():

where p(Z | S) =

J ()  E [r] - I(Z; G | S) = E [r - DKL [penc(Z | S, G) | p(Z | S)]] ,
g p(g) penc(Z | S, g) is the marginalized encoding.

(2)

In practice, performing this marginalization over the goal may often be prohibitive, since the agent might not have access to the goal distribution p(G), or even if the agent does, there might be many or a continuous distribution of goals that makes the marginalization intractable. To avoid this marginalization, we replace p(Z | S) with a variational approximation q(Z | S) (Kingma and Welling, 2014; Alemi et al., 2017; Houthooft et al., 2016; Strouse et al., 2018). This again provides a lower bound on J() since:

I (Z ;

G

|

S)

=

z,s,g

p(z,

s,

g)

log

p(z | p(z

g, s) | s)

= p(z, s, g) log p(z | g, s) - p(s) p(z | s) log p(z | s)

z,s,g

z,s

(3)

 p(z, s, g) log p(z | g, s) - p(s) p(z | s) log q(z | s) ,

z,s,g

z,s

where the inequality in the last line, in which we replace p(z | s) with q(z | s), follows from that DKL [p(Z | s) | q(Z | s)]  0  z p(z | s) log p(z | s)  z p(z | s) log q(z | s).
Thus, we arrive at the lower bound J~() that we maximize in practice:

J ()  J~()  E [r - DKL [penc(Z | S, G) | q(Z | S)]] .

(4)

In the experiments below, we fix q(Z | S) to be unit Gaussian, however it could also be learned, in which case its parameters should be included in . Although our approach is compatible with any RL method, we maximize J~() on-policy from sampled trajectories using a score function estimator
(Williams, 1992; Sutton et al., 1999a). As derived by Strouse et al. (2018), the resulting update at time step t, which we denote J~(t), is:

2.2 OPTIMIZATION FROM SAMPLED TRAJECTORIES

J~(t) = R~t log((at | st, gt)) - DKL [penc(Z | st, gt) | q(Z | st)] ,

(5)

where R~t 

T u=t



u-tr~u

is

a

modified

return,

r~t



rt

+

DKL [penc(Z

|

st, g)

|

q(Z

|

st)]

is

a

modified reward, T is the length of the current episode, and at, st, and gt are the action, state, and

goal at time t, respectively. The first term in the gradient comes from applying the REINFORCE

update to the modified reward, and can be thought of as encouraging the agent to change the policy in

the present state to revisit future states to the extent that they provide high external reward as well

as low need for encoding goal information. The second term comes from directly optimizing the

policy to not rely on goal information, and can be thought of as encouraging the agent to directly

alter the policy to avoid encoding goal information in the present state. We stress that while we take a

Monte Carlo policy gradient, or REINFORCE, approach here, our regularizer and thus our method

are compatible with any RL algorithm.

2.3 POLICY AND EXPLORATION TRANSFER
The end result of training with Eqn 5 is that the agent learns to rely on its (goal-independent) habits as much as possible, deviating only in decision states (as introduced in Section 1) where it makes

3

Under review as a conference paper at ICLR 2019

goal-dependent modifications. We demonstrate in Section 4 that this regularization alone already
leads to generalization benefits (that is, increased performance on T  ptest(T ) after training on T  ptrain(T )).

However, our main purpose in training with Eqn 5 is to train the agent to identify decision states
for the purpose of providing an exploration bonus in new environments. That is, after training on T  ptrain(T ), we freeze the agent's encoder penc(Z | S, G) and marginal encoding q(Z | S), discard the decoder pdec(A | S, Z), and use the encoders to provide DKL [penc(Z | S, G) | q(Z | S)] as a state and goal dependent exploration bonus for training a new policy (A | S, G) on T  ptest(T ). To ensure that the new agent does not pursue the exploration bonus solely (in lieu of reward), we also
decay the bonus with continued visits by weighting with a count-based exploration bonus as well.
That is, we divide the KL divergence by c(S), where c(S) is the number of times that state has been visited during training, which is initialized to 1. Letting re(t) be the environmental reward at time t, we thus train the agent to maximize the combined reward rt:

rt = re(t) +

 c(st

)

DKL

[penc(Z

|

st,

gt)

|

q(Z

|

st)]

.

(6)

Our approach is summarized in algorithm 1.

Algorithm 1 Transfer and Exploration via the Information Bottleneck
Require: A policy (A | S, G) = z penc(z | S, G) pdec(A | S, z), parameterized by  Require: A variational approximation q(Z | S) to the goal-marginalized encoder Require: A regularization weight  Require: Another policy (A | S, G), along with a RL algorithm A to train it Require: A set of training tasks (environments) ptrain(T ) and test tasks ptest(T ) Require: A goal sampling strategy p(G | T ) given a task T
for episodes = 1 to Ntrain do Sample a task T  ptrain(T ) and goal G  p(G | T ) Produce trajectory  on task T with goal G using policy (A | S, G) Update policy parameters  over  using Eqn 5
end for
Optional: use  directly on tasks sampled from ptest(T ) for episodes = 1 to Ntest do
Sample a task T  ptest(T ) and goal G  p(G | T ) Produce trajectory  on task T with goal G using policy (A | S, G) Update policy parameters  using algorithm A to maximize the reward given by Eqn 6
end for

3 RELATED WORK
van Dijk and Polani (2011) were the first to point out the connection between action-goal information and the structure of decision-making. They used information to identify decision states and use them as subgoals in an options framework (Sutton et al., 1999b). We build upon their approach by combining it with deep reinforcement learning to make it more scaleable, and also modify it by using it to provide an agent with an exploration bonus, rather than subgoals for options.
Our decision states are similar in spirit to the notion of "bottleneck states" used to define subgoals in hierarchical reinforcement learning. A bottleneck state is defined as one that lies on a wide variety of rewarding trajectories (McGovern and Barto, 2001; Stolle and Precup, 2002) or one that otherwise serves to separate regions of state space in a graph-theoretic sense (Menache et al., 2002; S¸ ims¸ek et al., 2005; Kazemitabar and Beigy, 2009; Machado et al., 2017). The latter definition is purely based on environmental dynamics and does not incorporate reward structure, while both definitions can lead to an unnecessary profileration of subgoals. To see this, consider a T-maze in which the agent starts at the bottom and two possible goals exist at either end of the top of the T. All states in this setup are bottleneck states, and hence the notion is trivial. However, only the junction where the lower and upper line segments of the T meet are a decision state. Thus, we believe the notion of a decision state is a more parsimonious and accurate indication of good subgoals than is the above notions of a
4

Under review as a conference paper at ICLR 2019
bottleneck state. The success of our approach against state-of-the-art exploration methods (Section 4) supports this claim.
We use the terminology of information bottleneck (IB) in this paper because we limit (or bottleneck) the amount of goal information used by our agent's policy during training. However, the correspondence is not exact: while both our method and IB limit information into the model, we maximize rewards while IB maximizes information about a target to be predicted. The latter is thus a supervised learning algorithm. If we instead focused on imitation learning and replaced E[r] with I(A; A | S) in Eqn 1, then our problem would correspond exactly to a variational information bottleneck (Alemi et al., 2017) between the goal G and correct action choice A (conditioned on S).
Whye Teh et al. (2017) trained a policy with the same KL divergence term as in Eqn 1 for the purposes of encouraging transfer across tasks. They did not, however, note the connection to variational information minimization and the information bottleneck, nor did they leverage the learned task structure for exploration. Parallel to our work, Strouse et al. (2018) also used Eqn 1 as a training objective, however their purpose was not to show better generalization and transfer, but instead to promote the sharing and hiding of information in a multi-agent setting.
Popular approaches to exploration in RL are typically based on: 1) injecting noise into action selection (e.g. epsilon-greedy, (Osband et al., 2016)), 2) encouraging "curiosity" by encouraging prediction errors of or decreased uncertainty about environmental dynamics (Schmidhuber, 1991; Houthooft et al., 2016; Pathak et al., 2017), or 3) count-based methods which incentivize seeking out rarely visited states (Strehl and Littman, 2008; Bellemare et al., 2016; Tang et al., 2016; Ostrovski et al., 2017). One limitation shared by all of these methods is that they have no way to leverage experience on previous tasks to improve exploration on new ones; that is, their methods of exploration are not tuned to the family of tasks the agent faces. Our transferrable exploration strategies approach in algorithm 1 however does exactly this. Another notable recent exception is Gupta et al. (2018), which took a meta-learning approach to transferrable exploration strategies.
4 EXPERIMENTAL RESULTS
In this section, we demonstrate the following experimentally:
· The policy-goal information bottleneck leads to much better policy transfer than standard RL training procedures (direct policy transfer).
· Using decision states as an exploration bonus leads to better performance than a variety of standard task-agnostic exploration methods (transferable exploration strategies).

(a) MultiRoomN4S4 (b) MultiRoomN12S10 (c) FindObjS5

(d) FindObjS6

Figure 2: MultiRoomNXSY and FindObjSY MiniGrid environments. See text for details.

4.1 MINIGRID ENVIRONMENTS
The first set of environments we consider are partially observable grid worlds generated with MiniGrid (Chevalier-Boisvert and Willems, 2018), an OpenAI Gym package (Brockman et al., 2016). We consider the MultiRoomNXSY and a FindObjSY task domains, as depicted in Figure 2. Both environments consist of a series of connected rooms, sometimes separated by doors that need opened. In both tasks, black squares are traversible, grey squares are walls, black squares with colored
5

Under review as a conference paper at ICLR 2019

borders are doors, the red triangle is the agent, and the shaded area is the agent's visible region. The MultiRoomNXSY the environment consists of X rooms, with size at most Y , connected in random orientations. The agent is placed in a distal room and must navigate to a green goal square in the most distant room from the agent. The agent receives an egocentric view of its surrounding, consisting of 3×3 pixels. The task increases in difficulty with X and Y . The FindObjSY environment consists of 9 connected rooms of size Y - 2 × Y - 2 arranged in a grid. The agent is placed in the center room and must navigate to an object in a randomly chosen outer room (e.g. yellow circle in bottom room in Figure 2c and blue square in top left room in Figure 2d). The agent again receives an egocentric observation, this time consisting of 7×7 pixels, and again the difficulty of the task increases with Y . For more details of the environment, see Appendix C.
Solving these partially observable, sparsely rewarded tasks by random exploration is difficult because there is a vanishing probability of reaching the goal randomly as the environments become larger. Transferring knowledge from simpler to more complex versions of these tasks thus becomes essential. In the next two sections, we demonstrate that our approach yields 1) policies that directly transfer well from smaller to larger environments, and 2) exploration strategies that outperform other task-agnostic exploration approaches.

4.2 DIRECT POLICY GENERALIZATION ON MINIGRID TASKS
We first demonstrate that training an agent with a goal bottleneck alone already leads to more effective policy transfer. We train policies on smaller versions of the MiniGrid environments (MultiRoomN2S6 and FindObjS5 and S7), but evaluate them on larger versions (MultiRoomN10S4, N10S10, and N12S10, and FindObjS7 and S10) throughout training.
Figure 3 compares an agent trained with a goal bottleneck (first half of Algorithm 1) to a vanilla goal-conditioned A2C agent (Mnih et al., 2016) on MultiRoomNXSY generalization. As is clear, the goal-bottlenecked agent generalizes much better. The success rate is the number of times the agent solves a larger task with 10-12 rooms while it is being trained on a task with only 2 rooms. When generalizing to 10 small rooms, the agent learns to solve the task to near perfection, whereas the goal-conditioned A2C baseline only solves <50% of mazes (Figure 3a).
Table 1 compares the same two agents on FindObjSY generalization. In addition, this comparison includes an ablated version of our agent with  = 0, that is an agent with the same architecture as in Figure 1 but with the no information term in its training objective. This is to ensure that our method's success is not due to the architecture alone. As is evident, the goal-bottlenecked agent again generalizes much better.
We analyzed the agent's behaviour to understand the intuition of why it generalizes well. In the MultiRoomNXSY environments, we find that the agent quickly discovers a wall following strategy. Since these environments are partially observable, this is indeed a good strategy that also generalizes well to larger mazes. In the FindObjSY environments, on the other hand, the agent sticks toward the center of the rooms, making a beeline from doorway to doorway. This is again a good strategy, because the agent's field of view in these experiments is large enough to see the entire room in which its in to determine if the goal object is present or not.

Method
Goal-conditioned A2C InfoBot with  = 0 InfoBot

FindObjS7
56% 44% 81%

FindObjS10
36% 24% 61%

Table 1: Policy generalization on FindObjSY . Agents trained on FindObjS5, and evaluated on FindObjS7 and S10.

4.3 TRANSFERABLE EXPLORATION STRATEGIES ON MINIGRID TASKS
We now evaluate our approach to exploration (the second half of Algorithm 1). We train agents with a goal bottleneck on one set of environments (MultiRoomN2S6) where they learn the sensory cues that correspond to decision states. Then, we that knowledge to guide exploration one another set of environments (MultiRoomN3S4, MultiRoomN4S4, and MultiRoomN5S4).We compare to several

6

Under review as a conference paper at ICLR 2019

Success Rate (%) Success Rate (%) Success Rate (%)

Trained on N2S6, Evaluation on N10S4 100

80

60

40

20

0 InfoBot Baseline

0.2 0.4 0.6 0.8 1.0

Timesteps

×106

(a)

Trained on N2S6, Evaluation on N10S10 30

25

20

15

10

5 InfoBot
0 Baseline

0.2 0.4 0.6 0.8 1.0

Timesteps

×106

(b)

Trained on N2S6, Evaluation on N12S10 10

8

6

4

2

0 InfoBot

Baseline

-2 0.2 0.4 0.6 0.8 1.0

Timesteps

×106

(c)

Figure 3: Policy generalization on MultiRoomNXSY . Success is measured by the percent of time the agent can find the goal in an unseen maze. Error bars are standard deviations across runs. Baseline is a vanilla goal-conditioned A2C agent.

Method
Goal-conditioned A2C TRPO + VIME Count based exploration Curiosity-based exploration InfoBot (decision state exploration bonus)

MultiRoomN3S4
0% 54% 95% 95% 90%

MultiRoomN5S4
0% 0% 0% 54% 85%

Table 2: Transferable exploration strategies on MultiRoomNXSY . InfoBot encoder trained on MultiRoomN2S6. All agents evaluated on MultiRoomN3S4 and N5S4. While several methods perform well with 3 rooms, InfoBot performs far better as the number of rooms increases to 5.

standard task-agnostic exploration methods, including count-based exploration (Eqn 6 without the DKL, that a bonus of / c(s)), VIME (Houthooft et al., 2016), and curiosity-driven exploration (Pathak et al., 2017), as well as a goal-conditioned A2C baseline with no exploration bonuses. Results are shown in Table 2 and Figure 4.
On a maze with three rooms, the count-based method and curiosity-driven exploration slightly outperform the proposed learned exploration strategy. However, as the number of rooms increases, the count-based method and VIME fail completely and the curiosity-based method degrades to only 54% success rate. This is in contrast to the proposed exploration strategy, which by learning the structure of the task, maintains a high success rate of 85%.

Average Task Return Average Task Return Average Task Return

Exploration Bonus on N3S4 1.0

0.8

0.6

0.4

0.2 InfoBot
0.0 Count-Based Baseline

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Timesteps

×104

(a)

Exploration Bonus on N4S4 1.0

0.8

0.6

0.4

0.2

0.0 InfoBot

-0.2

Count-Based Baseline

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Timesteps

×104

(b)

Exploration Bonus on N5S4 1.0

0.8

0.6

0.4

0.2
0.0 0.0

InfoBot Count-Based Baseline
0.5 1.0 1.5 2.0 Timesteps
(c)

2.5 3.0
×104

Figure 4: Transferable exploration strategies on MultiRoomNXSY . As the number of rooms increases (from left to right), a count-based exploration bonus alone cannot solve the task, whereas the proposed exploration bonus, by being tuned to task structure, enables success on these more difficult tasks.

7

Under review as a conference paper at ICLR 2019

Average Reward Average Reward Average Reward

4.4 TRANSFERABLE EXPLORATION STRATEGIES FOR CONTINUOUS CONTROL

1200 1000 800 600 400 200
00

10000

Humanoid-v1

Baseline InfoBot Infobot-low-value Infobot-zero-kl

20000

30000

40000

Iterations

50000

60000

4000 3500 3000 2500 2000 1500 1000 500
0 -5000

Walker2d-v1
Baseline InfoBot Infobot-low-value Infobot-zero-kl 2000 4000 6000 8000 10000 12000 14000 16000
Iterations

3000 2500 2000 1500 1000 500
00

Hopper-v1
Baseline InfoBot Infobot-low-value Infobot-zero-kl 2000 4000 6000 8000 10000 12000 14000 16000
Iterations

(a) (b) (c)

Figure 5: Transferable exploration strategies on Humanoid, Walker2D, and Hopper. The "baseline" is PPO (Schulman et al., 2017). Experiments are run with 5 random seeds and averaged over the runs.

To show that the InfoBot architecture can also be applied to continuous control, we evaluated the performance of InfoBot on three continuous control tasks from OpenAI Gym (Brockman et al., 2016). Because InfoBot depends on the goal, in the control domains, we use high value states as an approximation to the goal state following Goyal et al. (2018). We maintain a buffer of high value states, and at each update, we sample a high value state from the buffer which acts as a proxy for the goal. We compared to proximal policy optimization (PPO) (Schulman et al., 2017), as well as two ablated versions of our model: 1) instead of taking high value states, we take low value states from the buffer as proxy to the goal ("InfoBot-low-value") and 2) we use the same InfoBot policy architecture but do not use the information regularizer (i.e.  = 0) ("InfoBot-zero-kl"). The results in Figure 5 show that InfoBot improves over all three alternatives.

4.5 TRANSFERABLE EXPLORATION STRATEGIES FOR ATARI
To show that the InfoBot formulation generalizes to more complex domains, we evaluated the performance of our model on Atari games (Bellemare et al., 2013) and compare to a standard A2C baseline (Mnih et al., 2016). In this domain, we again use high value states as an approximation to the goal state following Goyal et al. (2018). Figure 6 shows the performance of our InfoBot compared to the A2C baseline on four of the Atari games. Although the gains are more modest than in the other domains we tested, InfoBot still performs significantly better.

Average Task Return Average Task Return Average Task Return Average Task Return

Pong 20

10

0

-10

-20 InfoBot Baseline

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5

Timesteps

×106

Qbert 8

6

4

2

InfoBot 0 Baseline

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5

Timesteps

×106

Seaquest

20

15

10

5

0 InfoBot Baseline

012345

Timesteps

×106

BreakOut 15.0

12.5

10.0

7.5

5.0

2.5

0.0 -2.5

InfoBot Baseline

012345

Timesteps

×106

Figure 6: Transferable exploration strategies on Pong, Qbert, Seaquest, and Breakout. The baseline is a vanilla A2C agent. Results averaged over three random seeds.

5 CONCLUSION
In this paper, we proposed to train agents to develop "useful habits" as well as the knowledge of when to break those habits, using an information bottleneck between the agent's goal and policy. We demonstrated empirically that this training procedure leads to better direct policy transfer across tasks. We also demonstrated that the states in which the agent learns to deviate from its habits, which we call "decision states", can be used as the basis for a learned exploration bonus that leads to more effective training than other task-agnostic exploration methods.

8

Under review as a conference paper at ICLR 2019
REFERENCES
A. Achille and S. Soatto. Information Dropout: Learning Optimal Representations Through Noisy Computation. ArXiv e-prints, Nov. 2016.
A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. International Conference on Learning Representations (ICLR), abs/1612.00410, 2017. URL http://arxiv.org/abs/1612.00410.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253­279, 2013.
M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying Count-Based Exploration and Intrinsic Motivation. ArXiv e-prints, June 2016.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI Gym. ArXiv e-prints, June 2016.
M. Chalk, O. Marre, and G. Tkacik. Relevant sparse codes with variational information bottleneck. ArXiv e-prints, May 2016.
M. Chevalier-Boisvert and L. Willems. Minimalistic gridworld environment for openai gym. https: //github.com/maximecb/gym-minigrid, 2018.
T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 2006.
D. Foster and P. Dayan. Structure in the space of value functions. Machine Learning, 49(2):325­346, Nov 2002. ISSN 1573-0565. doi: 10.1023/A:1017944732463. URL https://doi.org/10. 1023/A:1017944732463.
A. Goyal, P. Brakel, W. Fedus, T. Lillicrap, S. Levine, H. Larochelle, and Y. Bengio. Recall traces: Backtracking models for efficient reinforcement learning. arXiv preprint arXiv:1804.00379, 2018.
A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine. Meta-Reinforcement Learning of Structured Exploration Strategies. ArXiv e-prints, Feb. 2018.
D. Hassabis, D. Kumaran, C. Summerfield, and M. Botvinick. Neuroscience-inspired artificial intelligence. Neuron, 95(2):245­248, 2017. URL https://doi.org/10.1016/j.neuron. 2017.06.011.
R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109­1117, 2016.
S. J. Kazemitabar and H. Beigy. Using strongly connected components as a basis for autonomous skill acquisition in reinforcement learning. In International Symposium on Neural Networks, pages 794­803. Springer, 2009.
D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. International Conference on Learning Representations (ICLR), Dec. 2014.
A. Kolchinsky, B. D. Tracey, and D. H. Wolpert. Nonlinear Information Bottleneck. ArXiv e-prints, May 2017.
W. Kool and M. Botvinick. Mental labour. Nature Human Behaviour, 2018.
I. Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://github. com/ikostrikov/pytorch-a2c-ppo-acktr, 2018.
M. C. Machado, M. G. Bellemare, and M. Bowling. A laplacian framework for option discovery in reinforcement learning. arXiv preprint arXiv:1703.00956, 2017.
A. McGovern and A. G. Barto. Automatic discovery of subgoals in reinforcement learning using diverse density. 2001.
9

Under review as a conference paper at ICLR 2019
I. Menache, S. Mannor, and N. Shimkin. Q-cut--dynamic discovery of sub-goals in reinforcement learning. In European Conference on Machine Learning, pages 295­306. Springer, 2002.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928­1937, 2016.
V. Mnih, A. Puigdomènech Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. ArXiv e-prints, 2016.
I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped dqn. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4026­ 4034. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/ 6501-deep-exploration-via-bootstrapped-dqn.pdf.
G. Ostrovski, M. G. Bellemare, A. van den Oord, and R. Munos. Count-Based Exploration with Neural Density Models. ArXiv e-prints, Mar. 2017.
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised prediction. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2778­ 2787, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR. URL http: //proceedings.mlr.press/v70/pathak17a.html.
M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, V. Kumar, and W. Zaremba. Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research. ArXiv e-prints, Feb. 2018.
T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In F. Bach and D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1312­1320, Lille, France, 07­09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/schaul15.html.
J. Schmidhuber. Curious model-building control systems. In In Proc. International Joint Conference on Neural Networks, Singapore, pages 1458­1463. IEEE, 1991.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal Policy Optimization Algorithms. ArXiv e-prints, July 2017.
Ö. S¸ ims¸ek, A. P. Wolfe, and A. G. Barto. Identifying useful subgoals in reinforcement learning by local graph partitioning. In Proceedings of the 22nd international conference on Machine learning, pages 816­823. ACM, 2005.
M. Stolle and D. Precup. Learning options in reinforcement learning. In International Symposium on abstraction, reformulation, and approximation, pages 212­223. Springer, 2002.
A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision processes. J. Comput. Syst. Sci., 74(8):1309­1331, Dec. 2008. ISSN 0022-0000. doi: 10.1016/j. jcss.2007.08.009. URL http://dx.doi.org/10.1016/j.jcss.2007.08.009.
D. Strouse, M. Kleiman-Weiner, J. Tenenbaum, M. Botvinick, and D. Schwab. Learning to share and hide intentions using information regularization. In Advances in Neural Information Processing Systems (NIPS) 31. 2018.
R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99, pages 1057­1063, Cambridge, MA, USA, 1999a. MIT Press. URL http://dl.acm.org/citation.cfm?id=3009657.3009806.
10

Under review as a conference paper at ICLR 2019
R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2):181­211, Aug. 1999b. ISSN 0004-3702. doi: 10.1016/S0004-3702(99)00052-1. URL http://dx.doi.org/10.1016/ S0004-3702(99)00052-1.
H. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. ArXiv e-prints, Nov. 2016.
N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. Proceedings of The 37th Allerton Conference on Communication, Control, and Computing, pages 368­377, 1999.
S. G. van Dijk and D. Polani. Grounding subgoals in information transitions. IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2011. URL https: //ieeexplore.ieee.org/document/5967384/.
Y. Whye Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell, N. Heess, and R. Pascanu. Distral: Robust Multitask Reinforcement Learning. ArXiv e-prints, 2017.
R. J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Machine Learning, 8(3-4):229­256, 1992. ISSN 0885-6125. doi: 10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.
11

Under review as a conference paper at ICLR 2019

Average Task Return Average Task Return

Transfer to Pong 30

20

10

0

-10

-20 InfoBot Baseline

0123456

Timesteps

×106

(a) Transfer task to Pong

Transfer to Qbert 7

6

5

4

3

2

1 InfoBot 0 Baseline

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5

Timesteps

×106

(b) Transfer task to Qbert

Figure 7: Transfer across ALE Games (Pong, Qbert and Freeway) using egocentric encoder to provide exploration bonus, trained from Seaquest. Comparison of InfoBot (A2C + KL Regularizer) with a Baseline A2C. Experiment results averaged over four random seeds. See Section A.

A TRANSFERRED EXPLORATION STRATEGY IN ATARI
We demonstrate that the encoder can be used to transfera exploration strategy across Atari games to help agents learn a new game quickly. To initialize the encoder, we train an agent on the game Seaquest, where the agent is trained to identify the decision states. We then re-use this encoder on another Atari environment to provide an exploration bonus. Our experiments on the Atari games are evaluated on the games of Pong and Qbert. On Pong, an agent with learns to get a task return of 20 in 3M time steps. We show that the identification of bottleneck states can be used to transfer exploration strategies across Atari games (Fig. 7).
B ALGORITHM IMPLEMENTATION DETAILS
We evaluate the InfoBot framework using Adavantage Actor-Critic (A2C) to learn a policy (a|s, g) conditioned on the goal. To evaluate the performance of InfoBot, we use a range of maze multiroom tasks from the gym-minigrid framework (Chevalier-Boisvert and Willems, 2018) and the A2C implementation from (Chevalier-Boisvert and Willems, 2018). For the maze tasks, we used agent's relative distance to the absolute goal position as "goal".
For the maze environments, we use A2C with 48 parallel workers. Our actor network and critic networks consist of two and three fully connected layers respectively, each of which have 128 hidden units. The encoder network is also parameterized as a neural network, which consists of 1 fully connected layer. We use RMSProp with an initial learning rate of 0.0007 to train the models, for both InfoBot and the baseline for a fair comparison. Due to the partially observable nature of the environment, we further use a LSTM to encode the state and summarize the past observations.
For the Atari experiments, we use the open-source A2C implementation from Kostrikov (2018) and condition the goal state into the policy network. For the actor network, we use 3 convolution layers with ReLU activations. For training the networks, we use RMSProp with pre-defined learning rates for our algorithm and the baseline. The goal state in our experiments is used as the high value state following (Goyal et al., 2018).
For the Mujoco Experiments, we use Proximal Policy Optimization (PPO) (Schulman et al., 2017) with the open-source implementation available in Kostrikov (2018), using the defined architectures and hyperparameters as given in the repository for both our algorithm and the baseline. Again, for the goal-conditioned policies, the goal state for Mujoco experiments is defined as the high value state following (Goyal et al., 2018).
For reproducibility purposes of our experiments, we will further release the code on github that will be available on 2
Hyperparameter Search for Grid World Experiments: For the proposed method, we only varied the weight of KL loss. We tried 3 different values 0.1, 0.9, 0.01, 0.09, 0.005 for each of the env. and
2https://github.com/anonymous
12

Under review as a conference paper at ICLR 2019
plotted the results which gave the most improvement. We used CNN policy for the case of FindObjSY env, and MLP policy for the case of MultiRoomNXSY. We used the agent's relative distance to goal as a goal for our goal conditioned policy.
Hyperparameter Search for Control experiments We have not done any hyper-parameter search for the baseline. For the proposed method, we only varied the weight of KL loss. We tried 3 different values 0.5, 0.01, 0.09 for each of the env. and plotted the results which gave the most improvement over the PPO baseline.
C MINIGRID ENVIRONMENTS FOR OPENAI GYM
The FindObj and MultiRoom environments used for this research are part of MiniGrid, which is an open source gridworld package3. This package includes a family reinforcement learning environments compatible with the OpenAI Gym framework. Many of these environments are parameterizable so that the difficulty of tasks can be adjusted (eg: the size of rooms is often adjustable).
C.1 THE WORLD
In MiniGrid, the world is a grid of size NxN. Each tile in the grid contains exactly zero or one object. The possible object types are wall, door, key, ball, box and goal. Each object has an associated discrete color, which can be one of red, green, blue, purple, yellow and grey. By default, walls are always grey and goal squares are always green.
C.2 REWARD FUNCTION
Rewards are sparse for all MiniGrid environments. In the MultiRoom environment, episodes are terminated with a positive reward when the agent reaches the green goal square. Otherwise, episodes are terminated with zero reward when a time step limit is reached. In the FindObj environment, the agent receives a positive reward if it reaches the object to be found, otherwise zero reward if the time step limit is reached. The formula for calculating positive sparse rewards is 1 - 0.9  (step_count/max_steps). That is, rewards are always between zero and one, and the quicker the agent can successfully complete an episode, the closer to 1 the reward will be. The max_steps parameter is different for each environment, and varies depending on the size of each environment, with larger environments having a higher time step limit.
C.3 ACTION SPACE
There are seven actions in MiniGrid: turn left, turn right, move forward, pick up an object, drop an object, toggle and done. For the purpose of this paper, the pick up, drop and done actions are irrelevant. The agent can use the turn left and turn right action to rotate and face one of 4 possible directions (north, south, east, west). The move forward action makes the agent move from its current tile onto the tile in the direction it is currently facing, provided there is nothing on that tile, or that the tile contains an open door. The agent can open doors if they are right in front of it by using the toggle action.
C.4 OBSERVATION SPACE
Observations in MiniGrid are partial and egocentric. By default, the agent sees a square of 7x7 tiles in the direction it is facing. These include the tile the agent is standing on. The agent cannot see through walls or closed doors. The observations are provided as a tensor of shape 7x7x3. However, note that these are not RGB images. Each tile is encoded using 3 integer values: one describing the type of object contained in the cell, one describing its color, and a flag indicating whether doors are open or closed. This compact encoding was chosen for space efficiency and to enable faster training. The fully observable RGB image view of the environments shown in this paper is provided for human viewing.
3https://github.com/maximecb/gym-minigrid
13

Under review as a conference paper at ICLR 2019 C.5 LEVEL GENERATION The level generation in this task works as follows: (1) Generate the layout of the map (X number of rooms with different sizes (at most size Y) and green goal) (2) Add the agent to the map at a random location in the first room. (3) Add the goal at a random location in the last room. MultiRoomNXSY - In this task, the agent gets an egocentric view of its surroundings, consisting of 3×3 pixels. A neural network parameterized as MLP is used to process the visual observation. FindObjSY - In this task, the agent's egocentric observation consists of 7 × 7 pixels. We use Convolutional Neural Networks to encode the visual observations.
14

