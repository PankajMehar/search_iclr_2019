Under review as a conference paper at ICLR 2019
A MAX-AFFINE SPLINE PERSPECTIVE OF RECURRENT NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASO). We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator. The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper. First, we show that an RNN internally partitions the input space during training using vector quantization and that it builds up the partition through time. Second, we show that the affine parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input. Third, by closely examining the MASO RNN formula, we prove that injecting Gaussian noise in the initial hidden state in RNNs corresponds to an explicit 2 regularization on the affine parameters, which links to exploding gradient issues and improves generalization. Extensive experiments on several datasets of various modalities demonstrates and validates each of the above analyses. In particular, using initial hidden states elevates simple RNNs to state-of-the-art performance on these datasets.
1 INTRODUCTION
Recurrent neural networks (RNNs) are a powerful class of models for processing sequential inputs and have been the basic building block for more advanced models that have found success in challenging problems, including classification (e.g., sentiment analysis (Socher et al., 2013; Li et al., 2016; Teng et al., 2016)), sequence generation (e.g., machine translation (Bahdanau et al., 2014)), speech recognition and image captioning. Despite the successes, our understanding of how RNNs work still remains limited. An attractive theoretical result on RNNs are their universal approximation property which states that RNNs can approximate an arbitrary function (Scha¨fer & Zimmermann, 2006; Siegelmann & Sontag, 1995; Hammer, 2000). These classical theoretical results are mostly obtained from a dynamic system (Siegelmann & Sontag, 1995; Scha¨fer & Zimmermann, 2006) and measure theory (Hammer, 2000) perspective. These theories provide bounds on approximation error but unfortunately provide limited guidance on using RNNs and understanding their performance in practice.
In this paper, we study a specific class of RNNs with piecewise affine and convex nonlinearities. We provide a new angle of understanding RNNs with max-affine spline operators (MASOs) (Magnani & Boyd, 2009; Hannah & Dunson, 2013) from approximation theory. MASOs are piecewise affine approximation to arbitrary functions that provide a useful framework to examine neural networks. For example, Balestriero & Baraniuk (2018); Balestriero & Baraniuk (2018) have provided detailed analysis in the context of feedforward networks that highlights the practical value that spline operators bring to interpreting deep networks. Here, we go one step further and demonstrate the new insights and interpretations from the MASO perspective for RNNs. Below is a summary of our key contributions:
Contribution 1. We prove that RNNs with piecewise affine and convex nonlinearities and can be rewritten as composition of MASOs, making an RNN a piecewise affine spline operator that has an elegant analytical form (Section 2).
Contribution 2. We leverage the vector quantization (VQ) of piecewise affine spline operators to analyze the input space partitioning that RNN implicitly performs. We show that RNN calculates
1

Under review as a conference paper at ICLR 2019

Wr( )

h( ,t-1)

Wr( )

W( ) Wr( -1) h( -1,t-1)

Wr( -1)

W ( -1)

h( ,t)

Wr( )

W( ) h( -1,t)

Wr( -1)

W ( -1)

h( ,t+1) W( )
h( -1,t+1) W ( -1)

Figure 1: Visualization of an RNN. A cell and a layer are highlighted.

a new, high dimensional embedding of the input sequence that captures informative underlying characteristics of the input. We also provide a new perspective for RNN dynamics by visualizing the evolution of RNN VQ partitioning through time. (Section 3).
Contribution 3. We show the piecewise affine mapping in an RNN associated with a given input sequence is an input-dependent template, from which we can interpret the RNN as performing greedy matched filtering at every RNN cell. (Section 4).
Contribution 4. We rigorously prove that adding noise to the initial hidden state of an RNN corresponds to an explicit regularizer that links to exploding gradient, and empirically show that such regularization improves RNN performance on four datasets of different modalities (Section 5).

1.1 BACKGROUND: RECURRENT NEURAL NETWORKS (RNNS)

For concreteness, we study a specific class of simple RNNs (Elman, 1990). The RNN unit per time step t and layer , referred to as a "cell", performs the following recursive computation:

h( ,t) =  W ( )h( -1,t) + Wr( )h( ,t-1) + b( ) ,

(1)

where h( ,t)  RD( ) is the hidden state at timestep t and layer , h(0,t) := x(t),  is an activation function and W ( ), Wr( ) and b( ) are time-invariant model parameters. Unrolling the RNN through time gives an intuitive view of the RNN dynamics, which we visualize in Figure 1. The output of the overall RNN is usually an affine transformation of hidden state at every time step t of the last layer L:

z(t) = W h(L,t) + b.

(2)

In the special case when the RNN has one output at the end of processing the entire input sequence x(1:T ) of length T, , the RNN output is an affine transformation of the hidden state at the last time step T , i.e., z = z(T ) = W h(L,T ) + b.

1.2 BACKGROUND: MAX-AFFINE SPLINE OPERATORS (MASOS)

A max-affine spline operator (MASO) is piecewise affine and convex with respect to each output dimension k = 1, . . . , K. It is defined as S[A, B] : RD  RK with parameters A  RK×R×D and B  RK×R. This operator leverages K independent max-affine splines Magnani & Boyd (2009), each with R regions. Its output is produced via

[z( )]k = S[A( ), B( )](z( -1)) = max ( [A( )]k,r,., z( -1) + [B( )]k,r),
k r=1,...,R( )

(3)

where [z( )]k denotes the kth dimension of the vector z( ).
We highlight two key MASO properties relevant to the discussions in Section 3 and 4. First, MASOs perform vector quantization (VQ) to build their input partition, which is made explicit by rewriting Eq. 3 as

R( )
[z( )]k = [Q( )]k,r( [A( )]k,r,., z( -1) + [B( )]k,r),
r=1

(4)

2

Under review as a conference paper at ICLR 2019

where Q( )  RD( )×R( ) is the VQ matrix1. The VQ matrix contains D( ) stacked one-hot row vectors, each with the one-hot position at index [q( )]k  {1, . . . , R} corresponding to the arg max over r = 1, . . . , R( ) of Eq. 3. Second, given the partition that an input belongs to, the output of
MASO of each dimension k reduces to a simple linear transformation of the input with parameters
corresponding to the input's partition region. This is a direct consequence of Eq. 3.

2 RNNS AS PIECEWISE AFFINE SPLINE OPERATORS

We now leverage the MASO framework to rewrite, interpret and analyse RNNs. We focus on RNNs with piecewise affine and convex nonlinearities to derive analytical results. Our goal of this section is to show that an RNN becomes an simple, elegant affine mapping using MASOs. The analysis of RNNs with convex, non-piecewise affine and non-convex nonlinearities are left for future work.

We start by deriving the MASO formula for an RNN cell (Eq. 1), then extend to one layer of timeunrolled RNNs and finally to multi-layer, time-unrolled RNNs. Let z( ,t) = h( -1,t) , h( ,t-1) denote the input to an RNN cell which is the concatenation of the current input h( -1,t) and the previous hidden state h( ,t-1). Then we have the following result, which is a straightforward extension of Prop. 4 in Balestriero & Baraniuk (2018).
Proposition 1. An RNN cell of the form in Eq. 1 can be written as a MASO as follows:

h( ,t) =A( ,t) z( ,t) z( ,t) + B( ,t) z( ,t) ,

(5)

where A( ,t) z( ,t) = A( ,t) z( ,t) W ( ), Wr( ) , B(t) z( ,t) A( ,t) z( ,t) are the affine parameters .

= A( ,t) z( ,t) b( ) and

To simplify notation, we drop the dependencies of the affine parameters except for the final affine parameter. We then proceed to derive the explicit affine mapping of a time-unrolled RNN at layer . Let h( -1,1:T ) = h( -1,1) , · · · , x( -1,T ) be the entire input sequence to the RNN at layer
and h( ,1:T ) = h( ,1) , · · · , x( ,T ) be the hidden states of all time steps which are the outputs of the RNN at layer . With some algebra and simplification, we arrive at the following result. Theorem 1. The th layer of an RNN is a piecewise affine mapping defined as follows:

AT(

) :T

. . . A(1:T) A( ,T )W (

)

...

0

 h( -1,T )

1

 A(t:T) B( +,t) A0( :T) h( ,0)

h( ,1:T=) 

...

...

...

 

0 . . . A1( :1)

... 0

... ...

A(

... ,1)W (

  ) h(

...

t=T + 

-1,1)  A(1:1) B(

... +,t) A0( :1) h(

,0)

   

= A(RN) N h( -1,1:T ), h( ,0) h( -1,1:T ) + BR( N) N h( -1,1:T ), h( ,0) ,

(6)

where At(:T) =

t+1 s=T

A( ,s)Wr( )

for t < T and identity otherwise 2, h( ,0) is the initial hidden

state of the RNN at layer and A(RN) N h( -1,1:T ), h( ,0) and BR( N) N h( -1,1:T ), h( ,0) are affine parameters that depend on h( -1,1:T ) and h( ,0).

We present the proof for Thm. 1 in Appendix F. The key point here is that, by leveraging MASOs, we can represent the time-unrolled RNN as a simple affine mapping from the entire input sequence into the th layer hidden states of all time steps (Eq. 6). Note also that the initial hidden state affects the layer output by influencing the affine parameters and contributing a bias term A0(:t)h( ,0) to the bias term BR( N) N h( -1,1:T ), h( ,0) We study the impact of h(0) in more detail in Section 5.
We are now ready to generalize the above result to multi-layer RNNs. Let h(0,1:T ) = x(1:T ) =
x(1) , · · · , x(T ) denote the input sequence to the multi-layer RNN and and z(1:T ) =

1Prior work denotes the VQ matrix as T (x). However, in the RNNs context, T usually denotes the length

of the input sequence. We therefore denote VQ matrix as Q(x) in this work to avoid notation conflicts.

2In our context,

n i=m

ai

:=

am

·

am-1

· · · an+1

·

an

for

m

>

n

as

opposed

to

the

empty

product.

3

Under review as a conference paper at ICLR 2019

z(1) , · · · , z(T ) the output sequence. We state the following result for the overall mapping of a multi-layer RNN.

Theorem 2. An RNN of L layers is a piecewise affine spline operator defined as follows:

z(1:T ) = f

x(1:T ),

h( ,0)

L =1

=W

ARNN

x(1:T ),

h( ,0)

L =1

x(1:T ) + BRNN

x(1:T ),

h( ,0)

L =1

+b,

(7)

RNN formula

where

ARNN

x(1:T ),

h( ,0)

L =1

=

1 =L

A(RN) N

x(1:T ), h( ,0)

and

BRNN

x(1:T ),

h( ,0)

L =1

=

L =1

L-1 =

A(RN) N

x(1:T ), h( ,0)

BR( N) N x(1:T ), h( ,0)

are the affine parameters of the entire RNN. W and b are parameters of the output fully con-

nected layer where W = [W , W , . . . , W ] when the RNN outputs at every time step and

W = [W , 0, . . . , 0] when the RNN outputs only at the last time step.

Thm. 2 shows that, using MASOs, we have a simple, elegant affine mapping between the input and output sequence of a multi-layer RNN, and that the output is computed via locally very simple functions. We now leverage the affine formula of RNN and apply it to visualize and understand RNNs. First, we analyze and visualize the VQ partitioning that the RNN forms over time. Second, we analyze the form of the affine parameters and link them to matched filtering. Third, we study the impact of initial hidden state and justify the use of noise in initial hidden state.

3 APPLICATION: INTERNAL INPUT SPACE PARTIONING IN RNNS

In this section, we provide a new perpective of the dynamics of an RNN using its affine formula. For simplicity, we assume here that initial hidden states are set to 0. In the previous section, we denote the input region dependency in the affine parameters as A( ) x(1:T ) RNN and B( )[x(1:T )]RNN. We now make region dependency explicit by introducing the VQ tensor (from Eq. 4) to the RNN case:

Q(L,T ) x(1:T )

Q x(1:T ) =  

...

Q(1,T ) x(1:T )

· · · Q(L,1) x(1:1) 

...

...

, 

· · · Q(1,1) x(1:1)

(8)

where each Q( ,t) x(1:t) is the VQ matrix as presented in Eq. 4 and applied to the MASO formula of the RNN cell in Prop. 1. Thus, each Q( ,t) x(1:t) is the partition of the truncated input timeseries x(1:t) from the beginning to the current time step t. We can then view the RNN as developing a partition for an input sequence through time, receiving new input at each time step and refining its partitioning. We can see this from the form of the VQ tensor in Eq. 8, which accumulates local VQs from right to left through time.

We demonstrate the development of RNN input space partitioning using a one layer ReLU RNN trained on MNIST dataset as an example. We flatten each image into a 1-dimensional sequence of length T = 784. Details of model and experiments are in Appendix B. For ReLU, the VQ for each input time-serie is simply the concatenation of binarized hidden states since ReLU induces 2 partition regions. Figure 2 visualizes the input space partitioning of the MNIST test set using tSNE3 for various time steps. The figure clearly shows the evolution of the RNN input space partitioning, from hardly any separation to forming clear clusters through time. Many other visualizations on the RNN input space partitioning are available in Section C.

4 APPLICATION: RNNS AS A MATCHED FILTER BANK
In this section, we provide another insight into the RNN computation from the matched filtering perspective. We again assume zero initial hidden state for simplicity. Combining Eqs. 3 and 5, we
3Here, distance between two sequences is the 2 norm of the difference between their VQ tensors, which is a special case for ReLU. Generally, normalized Hamming distance is used to compute the VQ distance. We refer the reader to Balestriero & Baraniuk (2018) for more details.

4

500 time steps 100 time steps All time steps 300 time steps

Under review as a conference paper at ICLR 2019
Figure 2: tSNE visualization of the evolution of RNN VQ partitioning on MNIST test set. This visualization is enabled by the new parametrization that the RNN computes using VQ. Each color represents one class. We can see that RNN gradually refines the VQ partitioning through time. As the VQ is built through the time steps, as the classes become
see that the computation of an RNN cell is equivalently the maximization of the affine transformation of the input at the current time step. Thus, we can obtain the locally optimal template, i.e., the linear map A( ,t) z(1:T ) RNN in the affine parameters, by finding the one that produces the largest inner product with the input. For RNNs that produce a single output at the last time step, the overall matched filter bank ARNN x(1:T ) then corresponds to the composition of optimal matched obtained at each RNN cell and can be computed simply via dz/dx(1:T ). Thus, we can view the RNN as a matched filter bank that computes the output by finding the maximum inner product between the overall template and the input. The overall template is also known in the machine learning community as salience map, the visual examination of which is helpful for qualitatively diagnosing a model. For RNNs, salience map visualization has been explored previously by Li et al. (2016). Our insight and emphasis here is that a good template produces a larger inner product with the input regardless of the visual quality of the template. A template matching view of RNNs thus provide a quantitative model diagnosis by examining the inner products between inputs and templates. As an example, we train a one layer ReLU RNN on the polarized Stanford Sentiment Treebank (SST2) (Socher et al., 2013), which is a binary classification problem, and show in Figure 3 both correct and incorrect class templates of an input of negative sentiment. We see that the input has a much bigger inner product with the correct class template (left plot) than the incorrect class template (right plot), which demonstrates the template matching that the RNN performs. Additional experimental results are in Appendix D.
5 APPLICATION: IMPROVING RNN PERFORMANCE VIA NOISY INITIAL HIDDEN STATE
In this section, we provide theoretical insights on the use of noise in initial hidden state. Little is understood about setting the value for the initial hidden state except for Zimmermann et al. (2012)'s argument of using noisy initial hidden state from a dynamical system's perspective. Therefore the initial hidden state is typically set to zero without much justification. Leveraging the MASO formula of RNNs, we prove that noisy initial hidden state is equivalent to explicit regularization of the affine parameter associated with the initial hidden state.
5.1 NOISY INITIAL HIDDEN STATE AS EXPLICIT REGULARIZER Let L denote the loss with noise injected to the initial hidden state. We first state the theoretical result for one layer RNN with one output z at the last hidden state. and thus remove the layer index
to simplify notation. We then link our results to the exploding gradient problem.
5

Under review as a conference paper at ICLR 2019

Figure 3: Templates of an example from the SST-2 dataset. Quantitatively, we can see that the inner product between input and the correct class template (left) produces a bigger value than that between input and the incorrect class template (right), which demonstrate the template matching that an RNN performs. The visual quality of the correct class template is also telling since semantically meaningful words have noticeable values whereas the incorrect class template does not show any noticeable value.

Theorem 3. Let  N(0, 2I) be a Gaussian noise where 2 is small. We have that

E L = L + R,

(9)

where R

=

2 2N

N n=1

diag

2

dyni znj i=j

Ah xn(1:T )

for cross entropy loss with softmax output

and

R

=

2 2N

N n=1

Ah

x(n1:T )

2
for mean squared error loss. Ah

x(n1:T )

= A1:T

xn(1:T )

=

1 s=T

A(

,s)

x(n1:T )

Wr( ) and i, j  {1, · · · , C} are the class index where C is the total number

of classes.

We prove for the cross entropy case in Appendix F.2. We can see that the noise standard deviation  controls the importance of the regularization term and recovers the standard case for  = 0. Additionally, the regularizer does not depend on the accuracy of the classifier, which is a regularization scheme on the model itself (Wager et al., 2013).

5.2 INTERPRETATION AND CONNECTIONS TO PRIOR WORK

We now propose to understand the implication of this finding by demonstrating below how this relates to the problem of exploding gradient in RNN. Closely inspecting the form of Ah, we see that this term is the gradient of the RNN output with respect to the initial hidden state. In fact, the
gradient of the RNN output with respect to hidden state at any time step takes this form:

L L z h(t) = z h(T )

T h(T ) h(s)
s=t

= LW z

T
D(s)Wr
s=t

,

(10)

which is the basis of studying gradient exploding problems in RNNs, since it is clear that constraining this term will help limit the gradient in the backward pass. A more detailed review of exploding gradient problem is provided in Appendix G; We note here that studying this term has inspired a number of works on the unitary or orthogonal parametrization of the recurrent weight in order to maintain unitarity of the matrix multiplication in Eq. 10 (Arjovsky et al., 2016; Wisdom et al., 2016; Helfrich et al., 2018; Jing et al., 2017; Hyland & Ra¨tsch, 2017). By introducing noise to the initial hidden state, we regularize the term with the most number of matrix multiplications in the gradient calculation of the recurrent weight. While whether regularizing this term amounts to regularizing every other term in the recurrent weight gradient calculation remains to be investigated, we provide empirical evidence that the regularization introduced by noisy initial hidden state already reduces the magnitude of the recurrent weight gradient and improves model performance.

6

Under review as a conference paper at ICLR 2019

Figure 4: Visualization of the regularization effect on the adding task (T = 100). Top: norm of Ah at every 100 iterations; Middle: norm of gradient of recurrent weight at every 100 iterations; Bottom: validation loss at every epoch. Each epoch contains 1000 iterations.

5.3 EXPERIMENTS

We provide extensive empirical evidence to advocate for the use of noisy initial hidden state and demonstrate that a properly chosen noise standard deviation  improves performance for all the datasets that we experiment on. Unless otherwise mentioned, we use identity-initialized ReLU RNNs of 128-dimensional hidden state (denoted as RNN) in all our experiments. We summarize in this section the gist of our experimental results; Details on models, datasets, experiment setup and additional results are available in Appendix B and E.

Visualizing the Regularization Effect of the Noisy Initial Hidden State. We first visualize the regularization effect when injecting noise into the initial hidden state on a simulation task of adding 2 sequences of length 100. This is a ternary classificaion problem with input X  R2×T and target
y  {0, 1, 2}, y = i 1X2i=1 · X1i. The first row of X contains randomly chosen 0's and 1's, and
the second row of X contains 1's at 2 randomly chosen indices and 0's everywhere else. Note that prior work treat this task as a regression task instead (Arjovsky et al., 2016). Results for treating this task as a regression problem is provided in Appendix E.1.

In Figure 4, we visualize the norm of Ah, the norm of recurrent weight gradient and validation loss

against training iterations/epochs for various noise standard deviations. The top two plots clearly

demonstrate the regularization effect of noisy initial hidden state in regularize both Ah and norm

of the recurrent weight gradient, since larger 

reduces

the

magnitudes

of

both

Ah

and

dL dWr

.

Notably, this regularization effect happens even when the regularizer does not directly regularize

these two terms, which validates our intuition. The plot in the last row shows that setting a big 

can negatively impact learning. This can be explained by having too much regularization effect with

a large  . This brings the question of choosing  in practice, which we investigate below.

Choosing the Noise Parameter. We examine the performance of different noise standard deviations using RMSprop and SGD with varying learning rates to provide insights on choosing the noise parameter  . We perform experiments on MNIST dataset, each image flattened to a length 784 sequence (recall Section 3). Experimental results are included in Appendix E.2. Here, we report two interesting findings . First, for both optimizers, using noisy initial hidden state permits the use of higher learning rates that lead to exploding gradient when training model without noisy initial hidden state. Second, RMSprop is more tolerable to the choice of  than SGD and achieves favorable accuracies even when  is very large (e.g.,  = 5). This might be due to the gradient smoothing that RMSprop performs during optimization. We therefore recommend the use of RMSprop with noisy initial hidden state to improve model performance.

We then use RMSprop to train ReLU RNNs of one or two layers and with or without noisy initial hidden state on MNIST, permuted MNIST4 and SST-2 datasets. Table 1 shows the classification
accuracies of these models as well as a few state-of-the-art results. These results further suggest

4We apply a fixed permutation to all MNIST images in this case.

7

Under review as a conference paper at ICLR 2019

Table 1: Test set classification accuracies on MNIST and permuted MNIST datasets for various models. We see that using noise in the simplest RNNs elevates these RNNs to strong competitors of many complex, stateof-the-art models.

Model
RNN, 1 layer RNN, 1 layer, noise
RNN, 2 layer RNN, 2 layer, noise

MNIST
0.970 0.981 ( = 0.1)
0.969 0.987 ( = 0.5)

Dataset
permuted MNIST
0.891 0.922 ( = 0.01)
0.873 0.927 ( = 0.005)

SST-2
0.871 0.873 ( =0.1)
0.884 0.888 ( =0.005)

LSTM (Wisdom et al., 2016) uRNN (Arjovsky et al., 2016) scoRNN (Helfrich et al., 2018)
C-LSTM (Zhou et al., 2015) Tree-LSTM (Tai et al., 2015) Bi-LSTM+SWN-Lex (Teng et al., 2016)

0.978 0.951 0.985
-

0.913 0.914 0.966
-

0.849 -
0.878 0.88 0.892

the use of noisy initial hidden state in improving performance of ReLU RNN, elevating such simple model to a strong competitor of a number of more complicated, state-of-the-art models.
Noisy Initial Hidden State in Complex Models. We extend to RNNs with non-piecewise affine nonlinearities and provide promising preliminary results on the use of initial hidden state in complex models. We perform an experiment on the bird audio dataset; details of the dataset is available in Appendix B. This dataset involves a binary classification problem to detect whether or not an audio recording contain bird songs. We use the area under the curve (AUC) as the evaluation metric, since the dataset is highly imbalanced.
We implement and train a convolutional-recurrent model composed of 4 convolution layers followed by 2 gated recurrent unites (GRU)5, with and without noise added to the initial hidden state in both layers of the GRU. Experimental results show that, without added noise, we achieve 91% test AUC; With added noise, we achieve 93% test AUC, which is the new state-of-the-art on this task. This results implies that noisy hidden state has the potential to improve performance even for complex models with non-piecewise affine nonlinearities such as GRUs.
6 CONCLUSIONS AND FUTURE WORK
We have provided a novel perspective of understanding RNNs from the use of max-affine spline operators (MASOs). With MASOs, RNNs with piecewise affine and convex nonlinearities becomes a simple, elegant affine spline operator. Leveraging this formulation, we provide new insights and visualizations from vector quantization and matched filtering perspectives. Furthermore, we show that injecting noise in the initial hidden state of an RNN corresponds to an explicit regularization, which improves generalization by alleviating the exploding gradient problem. Extensive empirical studies of the use of noisy initial hidden state in vanilla ReLU RNNs and more complex models on various datasets have demonstrated the promise of using noisy initial state in improving RNN performance.
The MASO framework opens a new door for understanding RNNs. In this paper, we merely scratched the surface of RNNs from a MASO perspective. For example, the analyses presented in this paper are only applicable to piecewise affine and convex nonlinearities. We will extend our analyses to RNNs with noncontex and non-piecewise nonlinearities such LSTMs and GRUs that are more widely applicable in practice. We will also study RNNs under different problem settings such as sequence generation which outputs at every time step. These future analyses that leverage MASOs have the potential to provide new and practical insights into the inner workings of other RNN categories.
5See (Cakir et al., 2017) for details of the model and preprocessing steps.
8

Under review as a conference paper at ICLR 2019
REFERENCES
M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In Proc. Int. Conf. Mach. Learn., volume 48, pp. 1120­1128, Jun. 2016.
D. Bahdanau, K. Cho, and Y. Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. ArXiv e-prints, September 2014.
R. Balestriero and R. G. Baraniuk. Mad max: Affine spline insights into deep learning. ArXiv e-prints, May 2018.
R. Balestriero and R. G. Baraniuk. A spline theory of deep networks. In Proc. Int. Conf. Mach. Learn., volume 80, pp. 374­383, Jul 2018.
E. Cakir, S. Adavanne, G. Parascandolo, K. Drossos, and T. Virtanen. Convolutional recurrent neural networks for bird audio detection. In European Signal Processing Conference (EUSIPCO), pp. 1744­1748, Aug 2017.
T. Cooijmans, N. Ballas, C. Laurent, C. Gulcehre, and A. C. Courville. Recurrent batch normalization. In Proc. International Conference on Learning Representations, Apr. 2017.
A. Dieng, R. Ranganath, J. Altosaar, and D. Blei. Noisin: Unbiased regularization for recurrent neural networks. In Proc. International Conference on Machine Learning, volume 80, pp. 1252­ 1261, Jul. 2018.
J. L. Elman. Finding structure in time. Cogn. Sci., 14:179­211, 1990.
B. Hammer. On the approximation capability of recurrent neural networks. Neurocomputing, 31(1): 107­123, Mar. 2000.
L. A. Hannah and D. B. Dunson. Multivariate convex regression with adaptive partitioning. J. Mach. Learn. Res., 14:3261­3294, 2013.
K. Helfrich, D. Willmott, and Q. Ye. Orthogonal recurrent neural networks with scaled Cayley transform. In Proc. Int. Conf. Mach. Learn., volume 80, pp. 1969­1978, Jul. 2018.
M. Henaff, A. Szlam, and Y. LeCun. Recurrent orthogonal networks and long-memory tasks. In Proc. International Conference on Machine Learning, volume 48, pp. 2034­2042, Jun. 2016.
S. L. Hyland and G. Ra¨tsch. Learning unitary operators with help from u (n). In Proc. AAAI conference on Artificial Intelligence, pp. 2050­2058, Feb. 2017.
L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y. LeCun, M. Tegmark, and M. Soljacic´. Tunable efficient unitary neural networks (EUNN) and their application to RNNs. In Proc. International Conference on Machine Learning, volume 70, pp. 1733­1741, Aug. 2017.
C. Jose, M. Cisse, and F. Fleuret. Kronecker recurrent units. In Proc. International Conference on Learning Representations, Apr. 2018.
D Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio, H. Larochelle, A. C. Courville, and C. Pal. Zoneout: Regularizing rnns by randomly preserving hidden activations. In Proc. International Conference on Learning Representations, Apr. 2017.
Q. V. Le, N. Jaitly, and G. E. Hinton. A Simple Way to Initialize Recurrent Networks of Rectified Linear Units. ArXiv e-prints, Apr. 2015.
J. Li, X. Chen, E. Hovy, and D. Jurafsky. Visualizing and understanding neural models in NLP. In Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol., pp. 681­691, Jun. 2016.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579­2605, 2008.
A. Magnani and S. P. Boyd. Convex piecewise-linear fitting. Optimization Eng., 10(1):1­17, Mar. 2009.
9

Under review as a conference paper at ICLR 2019
Z. Mhammedi, A. Hellicar, A. Rahman, and J. Bailey. Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. In Proc. International Conference on Machine Learning, volume 70, pp. 2401­2409, Aug. 2017.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In Proc. Int. Conf. Mach. Learn., pp. 1310­1318, Jun. 2013.
J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proc. Conf. Empirical Methods Natural Language Proc. (EMNLP), pp. 1532­1543, Oct. 2014.
V. Pham, T. Bluche, C. Kermorvant, and J. Louradour. Dropout improves recurrent neural networks for handwriting recognition. In Proc. International Conference on Frontiers in Handwriting Recognition, pp. 285­290, Sept. 2014.
A. M. Scha¨fer and H. G. Zimmermann. Recurrent neural networks are universal approximators. In Proc. Int. Conf. Artificial Neural Netw., pp. 632­640, Sept. 2006.
H. T. Siegelmann and E. D. Sontag. On the computational power of neural nets. J. Comput. Syst. Sci., 50(1):132­150, Feb. 1995.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. Conf. Empirical Methods Natural Language Proc. (EMNLP), pp. 1631­1642, Oct. 2013.
D. Stowell and M. D. Plumbley. An open dataset for research on audio field recording archives: Freefield1010. In Proc. Audio Eng. Soc. 53rd Conf. Semantic Audio (AES53), pp. 1­7, 2014.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1556­1566, Jul. 2015.
S. S. Talathi and A. Vartak. Improving performance of recurrent neural network with relu nonlinearity. In Proc. International Conference on Learning Representations, May 2016.
Z. Teng, D. T. Vo, and Y. Zhang. Context-sensitive lexicon features for neural sentiment analysis. In Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1629­1638, Nov. 2016.
S. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Proc. Advances Neural Inform. Process. Syst., volume 1, pp. 351­359, Dec. 2013.
S. Wisdom, T. Powers, J. R. Hershey, J. Le Roux, and L. Atlas. Full-capacity unitary recurrent neural networks. In Proc. Advances Neural Inform. Process. Syst., Dec. 2016.
W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent Neural Network Regularization. ArXiv e-prints, 2014.
C. Zhou, C. Sun, Z. Liu, and F. C. M. Lau. A C-LSTM Neural Network for Text Classification. ArXiv e-prints, November 2015.
H. Zimmermann, C. Tietz, and R. Grothmann. Forecasting with recurrent neural networks: 12 tricks. Neural Networks: Tricks of the Trade: Second Edition, pp. 687­707, 2012.
10

Under review as a conference paper at ICLR 2019

NOTATION

L, T, t C, c N, n
R( ) D( ) K, k Q x(t)

Total number of layers: L  0; Index of the layer in an RNN:  {0, · · · , L} Total number of time steps: T  0; Index of time steps of an RNN: t  {0, · · · , T } Total number of output classes: C > 0; Index of the output class: c  {1, · · · , C} total number of examples: N  1; Index of example in a dataset: n  {1, · · · , N }
Index of the partition region induced by the piecewise nonlinearity at layer l
Dimension of input to the RNN at layer Total number of output dimensions of a MASO, K  0; Index of MASO output dimension, k  {1, · · · , K} The vector quantization (VQ) matrix tth time step of a discrete time-serie, x(t)  RD(0)

x X y(x) yn
h( ,t)

Concatenation of the whole length T time-serie: x = x(1) , · · · , x(T ) , x  RD(0)T
A dataset of N time-series: X = {xn}N1 Output/prediction associated with input x
True label (target variable) associated with the nth time-serie example xn. For classification yn  {1, . . . , C}, C > 1; For regression yn  RC , C  1
Output of an RNN cell at layer and time step t; Alternatively, input to an RNN cell at layer + 1 and time step t - 1

h( ) Concatenation of hidden state h( ,t) of all time steps at layer : h( ) = h( ,1) , · · · , x( ,T ) , h( )  RD( )T

z( ,t) Wr( ) W( )
W b( ) b (·)  A( ,t) A( ,t), B( ,t)

Concatenated input to an RNN cell at layer and time step t: z( ,t) = h( -1,t) , h( ,t-1) , z( ,t)  R2D( )

th layer RNN weight associated with the input h( ,t-1) from the previous time step: Wr( )  RD( )×D( )

th layer RNN weight associated with the input h( -1,t) from the previous layer: W ( )  RD( )×D( -1)

Weight of the last fully connected layer: W  RC×D(L)

th layer RNN bias: b( )  RD( )

Bias of the last fully connected layer: b  RC

Pointwise nonlinearity in an RNN

Standard deviation of noise injected into the initial hidden state h( ,0) 

MASO formula of the RNN activation (·) at layer and time step t: A  RD ×D( )

MASO parameters of an RNN at layer and time step t:

A(

,t)



RD(

) ×R(

) ×D(

-1)
,

B(

,t)



RD(

) ×R(

)

A DATASETS AND PREPROCESSING STEPS
Below we describe the dataset and explain the preprocessing steps for each dataset. MNIST. The dataset6 consists of 60k images in the training set and 10k images in the test set. We randomly select 10k images from the training set as validation set. We flatten each image to a 1-
6http://yann.lecun.com/exdb/mnist/
11

Under review as a conference paper at ICLR 2019
dimensional vector of size 784. Each image is also centered and normalized with mean of 0.1307 and standard deviation of 0.3081 (PyTorch default values). For permuted MNIST, we apply a fixed permutation to all images in the dataset.
SST-2. The dataset7 consists of 6920, 872, 1821 sentences in the training, validation and test set, respectively. Total number of vocabulary is 17539, and average sentence length is 19.67. Each sentence is minimally processed into sequences of words and use a fixed-dimensional and trainable vector to represent each word. We initialize these vectors either randomly or using GloVe (Pennington et al., 2014). Due to the small size of the dataset, the phrases in each sentence that have semantic labels are also used as part of the training set in addition to the whole sentence during training. Dropout of 0.5 is applied to all experiments. Phrases are not used during validation and testing, i.e., we always use entire sentences during validation and testing.
Bird Audio Dataset. The dataset8 consists of 7, 000 field recording signals of 10 sec. sampled at 44 kHz from the Freesound Stowell & Plumbley (2014) audio archive representing slightly less than 20 hours of audio signals. The audio waveforms are extracted from diverse scenes such as city, nature, train, voice, water, etc., some of which include bird songs. The labels regarding the bird detection task can be found in freefield10109. Performance is measured by Area Under Curve (AUC) due to the unbalanced distribution of the classes (3 for 1). We preprocess every audio clip by first using short-time Fourier transform (STFT) with 40ms and 50% overlapping Hamming window to obtain audio spectrum and then by extracting 40 log mel-band energy features. After preprocessing, each input is of dimension D = 96 and T = 999.
B EXPERIMENT SETUP
Experiment setup for various datasets is summarized in Table 2. Some of the experiments do not appear in the main text but in the appendix; we include setup for those experiments as well. A setting common to all experiments is that we use learning rate scheduler so that when validation loss plateaus for 5 consecutive epochs, we reduce the current learning rate by a factor of 0.7.
Setup of the experiments on influence of various noise standard deviation under different settings. We use  chosen in {0.001, 0.01, 0.1, 1, 5} and learning rates in {1 × 10-5, 1 × 10-4, 1.5 × 10-4, 2 × 10-4} for RMSprop and {1 × 10-7, 1 × 10-6, 1.25 × 10-6, 1.5 × 10-6} plain SGD.
Setup of input space partitioning. For the results in the main text, we use tSNE visualization (Maaten & Hinton, 2008) with 2 dimensions and the default settings from the python sklearn package. Visualization is performed on the whole 10k test set images. For finding the nearest neighbors of examples in the SST-2 dataset, since the examples are of varying lengths, we constrain the distance comparison to within +/-10 words of the target sentence. When the sentence lengths are not the same, we simply pad the shorter ones to the longest one, then process it with RNN and finally calculate the distance as the L-2 distance of the VQ (i.e., concatenation of all hidden states) that RNN computes. We justify the comparison between examples of different lengths using padding by noting that batching examples, a common practice in modern NLP tasks, pad the examples to the longest example in the batch.
C ADDITIONAL INPUT SPACE PARTITION VISUALIZATIONS
We provide ample additional visualizations to demonstrate the internal input space partitioning that an RNN performs. Here, the results are focused more on the properties of the final input space partitioning, i.e., partition of the entire input sequence, instead of part of the input sequence. Several additional sets of experimental results are shown; the first three on MNIST and the last one on SST-2.
First, we visualize the nearest and farthest neighbors of several MNIST digits in Figure 5. The left column is the original image; The next five columns are the five nearest neighbors to the original image; The last five columns are five farthest neighbors. This figure shows that the images are well clustered.
7https://nlp.stanford.edu/sentiment/index.html 8http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/ 9http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/
12

Under review as a conference paper at ICLR 2019

Table 2: Various experiment setup. Curly brackets indicate more than one value is attempted for this experiment.

Settings
Type of RNN #Layers Input size Hidden size Output size Initial Learning Rate Optimizer Batch size Epochs

Add task ReLU RNN
1, 2 2 128 3
1e-4 RMSprop
50 100

MNIST ReLU RNN
1, 2 1 128 3
1e-4 RMSprop, SGD
64 200

Dataset Permuted MNIST
ReLU RNN 1, 2 1 128 10
1e-4 RMSprop
64 200

SST-2 ReLU RNN
1, 2 128 300
2
1e-4 Adam
64 100

Bird detection GRU 2 96 256 2
1e-4 Adam
64 50

Figure 5: Internal input space partitioning visualization for pixel-by-pixel (i.e., flattened to a 1dimensional, length 784 vector) MNIST dataset using a trained ReLU RNN (one layer, 128dimensional hidden state). Here, we visualize the nearest 5 and farthest 5 images of one selected image from each class. Leftmost column is the original image; the middle 5 images are the 5 nearest neighbors; the rightmost 5 images are the farthest neighbors.
Second, we visualize the nearest neighbors of a few wrongly classified images in Figure 6. This figure further shows the impressive input clustering that a simple RNN is able to achieve, in that even wrongly classified images have neighbors that are clearly of the same class as the wrongly classified ones. Finally, we visualize 5 nearest and 5 farthest neighbors of a selected sentence from the SST-2 dataset to demonstrate that the partitioning effect on dataset of another modality. The results are shown in Figure 7. We can see that all sentences that are nearest neighbors are of similar sentiment to the target sentence, whereas all sentences that are farthest neighbors are of the opposite sentiment.
D ADDITIONAL TEMPLATE VISUALIZATIONS
We provide here more templates on images and texts in Figures 8 and 9. Notice here that although visually the templates may look similar or meaningless, they nevertheless have meaningful inner product with the input. The class index of the template that produces the largest inner product with the input is typically the correct class, as can be seen in the two figures.
E ADDITIONAL EXPERIMENTAL RESULTS FOR NOISY INITIAL HIDDEN STATES
Here, we include additional experimental results for Section 5.
13

Under review as a conference paper at ICLR 2019

Figure 6: Nearest neighbors of some wrongly classified MNIST images. We can see the superior input partitioning of the RNN since even images that are wrongly classified have neighbors that are the same class as those that are wrongly classified.

Original text

It is a film that will have people walking out halfway through , will encourage others to stand up and applaud , and will , undoubtedly , leave both camps engaged in a ferocious debate for years to come . (+)

Nearest 2 neighbors

Farthest 2 neighbors

Well-written , nicely acted and beautifully shot and scored , the film works on several levels , openly questioning social mores while ensnaring the audience with its emotional pull . (+, 22.00)

Marries the amateurishness of The Blair Witch Project with the illogic of Series 7 : The Contenders to create a completely crass and forgettable movie . (-, 37.60)

A stunning piece of visual poetry that will , hopefully , be remembered as one of the most important stories to be told in Australia 's film history . (+, 22.23) Cute , funny , heartwarming digitally animated feature film with plenty of slapstick humor for the kids , lots of in-jokes for the adults and heart enough for everyone . (+, 22.61)

K-19 may not hold a lot of water as a submarine epic , but it holds even less when it turns into an elegiacally soggy Saving Private Ryanovich . (-, 37.42)
This is a great subject for a movie , but Hollywood has squandered the opportunity , using it as a prop for warmed-over melodrama and the kind of choreographed mayhem that director John Woo has built his career on . (-, 37.24)

Though it is by no means his best work , LaissezPasser is a distinguished and distinctive effort by a bona-fide master , a fascinating film replete with rewards to be had by all willing to make the effort to reap them . (+, 22.78) An absorbing trip into the minds and motivations of people under stress as well as a keen , unsentimental look at variations on the theme of motherhood . (+, 22.89)

Flotsam in the sea of moviemaking , not big enough for us to worry about it causing significant harm and not smelly enough to bother despising . (-, 37.15)
If you 're not a prepubescent girl , you 'll be laughing at Britney Spears ' movie-starring debut whenever it does n't have you impatiently squinting at your watch . (-, 36.98)

Figure 7: Nearest and furthest neighbors of a postive movie review. The sentiment (+ or -) and the euclidean distance between the input and the neighbor vector quantizations are shown in parenthesis after each neighbor.

E.1 REGULARIZATION EFFECT FOR REGRESSION PROBLEM
We present the regularization effect on adding task formulated as a regression problem, following setup in Arjovsky et al. (2016). Result is shown in Figure 10. We see regularization effect similar to that presented in Figure 4, which demonstrates that the regularization effect does indeed happens for both classification and regression problems, as Thm. 3 suggests.
E.2 CHOOSING NOISE PARAMETER
Table 3 shows the classification accuracies under various settings. The discussion of the results is in Section 5.
14

Under review as a conference paper at ICLR 2019

Input image

0, -31.9 1, 2.9 2, -21.9 3, -26.1 4, -24.9 5,, -23.1 6, -24.3 7, -18.5 8, -24.7 9, -26.1

Input image

0, -6.2 1, -3.9 2, 5.0 3, -5.8 4, -13.5 5,, -10.6 6, -9.0 7, -9.3 8, -14.0 9, -14.9

Input image

0, -5.5 1, -9.0 2, -2.4 3, -3.7 4, -8.8 5,, -6.6 6, -5.7 7, -8.0 8, 1.8 9, -5.9

Figure 8: Templates of three selected MNIST images. The leftmost column are the original input image. The next ten images of each row are the ten templates of a particular input image corresponding to each class. For each template image, we show the class and the inner product of this template with the input. Text under the template of the true class of each input image is bolded.

Figure 9: Additional template visualizations of an example from the SST-2 dataset. Each word in the sentence is marked as a tick label in the x axis. The values of inner products are marked below each template. The template that has the bigger inner product is the true class of the sentence. We see that the template corresponding to the correct class produces a significantly bigger inner product with the input than other templates.
F PROOFS
We present detailed proofs for several results in the main text.
15

Under review as a conference paper at ICLR 2019

Figure 10: Various plots during training of add problem (T=100, regression). Top: norm of Ah at every 100 iterations; Middle: norm of gradient of recurrent weight at every 100 iterations; Bottom:
validation loss at every epoch. Each epoch contains 1000 iterations.

Table 3: Classification accuracy for MNIST dataset under 2 different optimizers, various learning rates and different standard deviation  in the initial hidden state noise. Results suggest a RMSprop tolerates various choices of  while SGD works for smaller  .


0 0.001 0.01 0.1 1 5

1e-5
0.960 0.963 0.962 0.955 0.956 0.952

RMSprop

1e-4 1.5e-4

0.973 0.974 0.980 0.976 0.977 0.981

0.114 0.978 0.978 0.981 0.980 0.973

2e-4
0.114 0.970 0.976 0.976 0.976 0.981

1e-7
0.837 0.835 0.834 0.803 0.520 0.471

SGD

1e-6 1.25e-6

0.879 0.895 0.898 0.833 0.640 0.098

0.870 0.913 0.922 0.913 0.901 0.098

1.5e-6
0.098 0.875 0.918 0.908 0.098 0.098

F.1 PROOF OF THM. 1
To simplify notation, in the proof we drop the affine parameters' dependencies on the input, but keep in mind the input-dependent property of these parameters. We first derive the expression for the hidden state h( ,t) at a given time step t and layer . Using Prop. 1, we start with unrolling the RNN cell of layer at time step t for two time steps to t - 2 by recursively applying Eq. 1 as follows:
16

Under review as a conference paper at ICLR 2019

h( ,t) =  W ( )h( -1,t) + b( ) + Wr( )h( ,t-1)

(11)

= A( ,t)W ( )h( -1,t) + A(t)b( ) + A(t)Wr( )h( ,t-1)
= A( ,t)W ( )h( -1,t) + A(t)b( ) + A( ,t)Wr( ) A( ,t-1)W ( )h( -1,t-1) + A( ,t-1)b( ) + A( ,t-1)Wr( )h( ,t-2)

(12) (13)

= A( ,t)W ( )h( -1,t) + A( ,t)Wr( )A( ,t-1)W ( )h( -1,t-1)

+ A( ,t) + A( ,t)Wr( )A( ,t-1) b( ) + A( ,t)Wr( )A( ,t-1)Wr( )h( ,t-2).

(14)

From Eq. 11 to Eq. 12, we use the result of Prop. 1. From Eq. 12 to Eq. 13, we expand the term that involves the hidden state h( ,t-1) at the previous time step recursively using Eq. 12. From Eq. 13 to
Eq. 14, we group terms and write them compactly.

Now define As(:t,s) :=

s+1 k=t

A( ,k)Wr(

)

for

s

<

t

and

At(:t,t)

:=

I

where

I

is

the

identity

matrix.

Using this definition, we rewrite Eq. 14 and proceed with the unrolling to the initial time step as

follows:

h( ,t) = A(t:t) At(-)1:t

A( ,t)W ( ) A( ,t-1)W ( )

h( -1,t) h( -1,t-1)

t-1
+ As(:t)B( ,s)
s=t

(15)

···

A( ,t)W ( )  h( -1,t)  t-1

= At(:t)

···

A1( :t)

 

...

 

...

+ 

A(s:t)B( ,s) + A0( :t)h( ,0), (16)

A( ,1)W ( )

h( -1,1)

s=t

where B( ,s) = A( ,s)b( ) as defined in Prop. 1.

Repeat the above derivation for t  {1, · · · , T } and stack h( ,t) in decreasing time steps from top to bottom, we have:

AT(

) :T

. . . A(1:T) A( ,T )W (

)

...

0

 h( -1,T )

1

 At(:T) B( +,t) A0( :T) h( ,0)

h( ,1:T=) 

...

...

...

 

0 . . . A1( :1)

... 0

... ...

A(

... ,1)W (

  ) h(

...

t=T + 

-1,1)  A(1:1) B(

... +,t) A0( :1) h(

,0)

   

= AR( N) N h( -1,1:T ), h( ,0) h( -1,1:T ) + BR( N) N h( -1,1:T ), h( ,0) ,

(17)

where







AR( N) N h( -1,1:T ), h( ,0)



























BR( N) N h( -1,1:T ), h( ,0)

























 h(

-1,1:T )









A(T

) :T

. . . A1( :T) 

=

 

...

...

...

 

,



0 . . . A(1:1)

1

 A(t:T) B( +,t) A0( :T) h( ,0)

t=T



=  

...

,  

A(1:1) B( +,t) A(0:1) h( ,0) h( -1,T )

= 

...

, 

h( -1,1)

17

Under review as a conference paper at ICLR 2019

which concludes the proof. Thm. 2 follows from the recursive application of the above arguments for each layer  {1, · · · , L}.

F.2 PROOF OF THM. 3

We prove for the case of multi-class classification problem with softmax output. The proof for the case of regression problems easily follows.

Let ai x(n1:T ) be the ith row of the input-dependent affine parameter Ah xn(1:T ) and c be the

index of the correct class. We first rewrite the cross entropy loss with noise injected hidden state

LCE = LCE softmax f xn(1:T ), h(0) +

as follows:

LCE

=

1 N

N
-log

softmax

f

xn(1:T ), h(0) +

n=1



1 =

N
-log 

N

n=1

exp(znc + ac xn(1:T ) )

C j=1

exp(znj

+

aj

xn(1:T )


 )



1 =
N

N

 -znc - ac

x(n1:T )

n=1 

C

+ log

exp(znj + aj xn(1:T )

j=1


 ).


(18)

Taking expectation of the L with respect to the error distribution, we have

E[LCE] = LCE + R,

where



R

=

1 N

N
 E
n=1 

log

C
exp(znj + aj x(n1:T )
j=1

)



C



- log

exp(znj) .

j=1 

(19) (20)

We note that similar forms of Eq. 20 have been previously derived by Wager et al. (2013).
We now simplify Eq. 20 using second order Taylor expansion on h(0) of the summation inside the log function. Let the function u(xn(1:T ), h(0)) = log( j exp(znj)) = log( j exp(f (x(n1:T ), h(0)))). Then, we can approximate Eq. 20 as follows:

R 1 N N

E

u xn(1:T ), h(0)

+

du

x(n1:T ), h(0) dh(0)

n=1

1 +
2

u xn(1:T ), h(0) 2 d2h(0)

- u xn(1:T ), h(0)

1 N1

= N

2 E Tr

n=1

du xn(1:T ), h(0) 2 d2h(0)

1 N1 = Tr
N2
n=1

du x(n1:T ), h(0) 2

d2h(0)

E

1 N 2 = Tr
N2
n=1

du xn(1:N), h(0) 2 d2h(0)

,

(21)

18

Under review as a conference paper at ICLR 2019

where

( )u xn(1:T ),h(0) 2
d2 h(0)

is the Hessian matrix:

du x(n1:N), h(0))2 d2h(0)

= u xN(1:N), h(0))2 il hi(0)hl(0)

 = h(l0)

exp (zni)

C j=1

exp

(znj

)

ai

x(n1:T )

yni

=

yni znl

znl h(l 0)

ai

xn(1:T )

= yni(1i=l - ynl) ai x(n1:T ) , al xn(1:T )

.

Then, we can write the trace term in Eq. 21 as follows:

Tr du x(n1:T ), h(0) 2 d2h(0)

= diag

yni znl i=l

Ah x(n1:T )

2
.

As a result, using the above approximations, we can rewrite the loss with noisy initial state in Eq. 18 as:

LCE

=

LCE

+

2 2N

N

diag

n=1

yni znl i=l

Ah xn(1:T )

2
.

(22)

We see that this regularizer term does not dependent on the correct class

G LITERATURE REVIEW ON EXPLODING GRADIENT IN RNNS
The problem of exploding gradients has been widely studied from different perspectives. First approaches have attempted to directly control the amplitude of the gradient through gradient clipping (Pascanu et al., 2013). A more model driven approach has leveraged the analytical formula of the gradient when using specific nonlinearities and topologies in order to develop parametrization of the recurrent weights. This has led to various unitary reparametrizations of the recurrent weight (Arjovsky et al., 2016; Wisdom et al., 2016; Helfrich et al., 2018; Henaff et al., 2016; Jing et al., 2017; Mhammedi et al., 2017; Hyland & Ra¨tsch, 2017; Jose et al., 2018). A soft version of such parametrization lies in regularization of the DNNs. This includes dropout applied to either the output layer (Pham et al., 2014) or hidden state (Zaremba et al., 2014), noisin (Dieng et al., 2018), zoneout (Krueger et al., 2017) and recurrent batch normalization (Cooijmans et al., 2017). Lastly, identity initialization of ReLU RNNs has been studied in Le et al. (2015) and Talathi & Vartak (2016). Our results complements prior works in that simply injecting noise to the initial hidden state and without changing the RNN structure also relieves the exploding gradient problem by regularization the potentially largest term in the recurrent weight gradient.

19

