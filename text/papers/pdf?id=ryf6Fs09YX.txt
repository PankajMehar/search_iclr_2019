Under review as a conference paper at ICLR 2019
GO GRADIENT FOR EXPECTATION-BASED OBJECTIVES
Anonymous authors Paper under double-blind review
ABSTRACT
Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters  for expectation-based objectives Eq(y)[f (y)]. Most existing methods either (i) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or (ii) they only apply to distributions of continuous random variables and employ a reparameterization trick. To address these limitations, we propose a General and One-sample (GO) gradient that (i) applies to distributions associated with continuous or discrete random variables, and (ii) has the same low-variance as the reparameterization trick. We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired). Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to random variables.
1 INTRODUCTION
Neural networks, typically trained using back-propagation for parameter optimization, have recently demonstrated significant success across a wide range of applications. There has been interest in coupling neural networks with random variables, so as to embrace greater descriptive capacity. Recent examples of this include black-box variational inference (BBVI) (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2014; Hernández-Lobato et al., 2016; Ranganath et al., 2016b; Li & Turner, 2016; Ranganath et al., 2016a; Zhang et al., 2018) and generative adversarial networks (GANs) (Goodfellow et al., 2014; Radford et al., 2015; Zhao et al., 2016; Arjovsky et al., 2017; Li et al., 2017; Gan et al., 2017; Li et al., 2018). Unfortunately, efficiently backpropagating gradients through general distributions (random variables) remains a bottleneck. Most current methodology focuses on distributions with continuous random variables, for which the reparameterization trick may be readily applied (Kingma & Welling, 2014; Grathwohl et al., 2017).
As an example, the aforementioned bottleneck greatly constrains the applicability of BBVI, by limiting variational approximations to reparameterizable distributions. This limitation excludes discrete random variables and many types of continuous ones. From the perspective of GAN, the need to employ reparameterization has constrained most applications to continuous observations. There are many forms of data that are more-naturally discrete.
The fundamental problem associated with the aforementioned challenges is the need to efficiently calculate an unbiased low-variance gradient wrt parameters  for an expectation objective of the form Eq(y)[f (y)]. We are interested in general distributions q (y), for which the components of y may be either continuous or discrete. Typically the components of y have a hierarchical structure, and a subset of the components of y play a role in evaluating f (y).
Unfortunately, classical methods for estimating gradients of Eq(y)[f (y)] wrt  have limitations. The REINFORCE gradient (Williams, 1992), although generally applicable (e.g., for continuous and discrete random variables), exhibits high variance with Monte Carlo (MC) estimation of the expectation, forcing one to apply additional variance-reduction techniques. The reparameterization trick (Rep) (Salimans et al., 2013; Kingma & Welling, 2014; Rezende et al., 2014) works well, with as few as only one MC sample, but it is limited to continuous y. Many efforts have been devoted to improving these two formulations, as detailed in Section 6. However, none of these methods is
1

Under review as a conference paper at ICLR 2019

characterized by generalization (applicable to general distributions) and efficiency (working well with as few as one MC sample).
The key contributions of this work are based on the recognition that REINFORCE and Rep are seeking to solve the same objective, but in practice Rep yields lower-variance estimations, albeit for a narrower class of distributions. Recent work (Ranganath et al., 2016b) has made a connection between REINFORCE and Rep, recognizing that the former estimates a term the latter evaluates analytically. The high variance by which REINFORCE approximates this term manifests high variance in the gradient estimation. We extend these ideas, and use them to propose a new General and One-sample (GO) gradient, that inherits REINFORCE's generalization and Rep's efficiency; the "One sample" motivating the name GO is meant to highlight the low variance of the proposed method, although of course one may use more than one sample if desired. We find that the core of the GO gradient is something we term a variable-nabla, which can be interpreted as the gradient of a random variable wrt a parameter. Utilizing variable-nablas to propagate the chain rule through distributions, we broaden the applicability of the GO gradient and present statistical back-propagation, a statistical generalization of classic back-propagation (Rumelhart & Hinton, 1986). Through this generalization, we may couple neural networks to general random variables, and compute needed gradients with low variance.

2 BACKGROUND

To motivate this paper, we begin by briefly elucidating common machine learning problems for which

there is a need to efficiently estimate gradients of  for functions of the form Eq(y)[f (y)]. Assume access to data samples {xi}i=1,N , drawn i.i.d. from the true (and unknown) underlying distribution

q(x). We seek to learn a model p(x) to approximate q(x). A classic approach to such learning is to maximize the expected log likelihood ^ = argmax Eq(x)[log p(x)], perhaps with an added regularization term on . Expectation Eq(x)(·) is approximated via the available data samples, as

^

=

argmax

1 N

N i=1

log

p

(xi

).

It is often convenient to employ a model with latent variables z, i.e., p(x) = p(x, z)dz =
p(x|z)p(z)dz, with prior p(z) on z. The integral wrt z is typically intractable, motivating introduction of the approximate posterior q(z|x), with parameters . The well-known evidence lower bound (ELBO) (Ranganath et al., 2014; Hernández-Lobato et al., 2016; Ranganath et al., 2016b;
Li & Turner, 2016; Ranganath et al., 2016a) is defined as

ELBO(, ; x) = Eq(z|x)[log p(x, z) - log q(z|x)]

(1)

= log p(x) - KL[q(z|x) p(z|x)]  log p(x)

(2)

where p(z|x) is the true posterior, and KL(· ·) represents the Kullback-Leibler divergence. Varia-

tional learning seeks (^, ^ ) = argmax,

N i=1

ELBO(,

;

xi).

While computation of the ELBO has been considered for many years, a problem introduced recently
concerns adversarial learning of p(x), or, more precisely, learning a model that allows one to efficiently and accurately draw samples x  p(x) that are similar to x  q(x). With generative adversarial networks (GANs) (Goodfellow et al., 2014), one seeks to solve

min max


Eq(x)[log D(x)] + Ep(x)[log(1 - D(x))]

,

(3)

where D(x) is a discriminator with parameters , quantifying the probability x was drawn from q(x), with 1 - D(x) representing the probability that it was drawn from p(x). There have been many recent extensions of GAN (Radford et al., 2015; Zhao et al., 2016; Arjovsky et al., 2017; Li
et al., 2017; Gan et al., 2017; Li et al., 2018), but the basic setup in (3) holds for most.

To optimize (1) and (3), the most challenging gradients that must be computed are of the form
 Eq(y)[f (y)]; for (1) y = z and  = , while for (3) y = x and  = . The need to evaluate expressions like  Eq(y)[f (y)] arises in many other machine learning problems, and consequently it has generated much prior attention.

Evaluation of the gradient of the expectation is simplified if the gradient can be moved inside the expectation. REINFORCE (Williams, 1992) is based on the identity

 Eq (y)[f (y)] = Eq (y) f (y) log q (y) .

(4)

2

Under review as a conference paper at ICLR 2019

While simple in concept, this estimate is known to have high variance when the expectation Eq(y)(·) is approximated (as needed in practice) by a finite number of samples.

An approach (Salimans et al., 2013; Kingma & Welling, 2014; Rezende et al., 2014) that has attracted
recent attention is termed the reparameterization trick, applicable when q (y) can be reparametrized as y =   ( ), with  q( ), where q( ) is a simple distribution that may be readily sampled. In this case we have

 Eq (y)[f (y)] = Eq( ) [   ( )][yf (y)]|y=  ( ) .

(5)

This gradient, termed Rep, is typically characterized by relatively low variance, when approximating

Eq( )(·) with a small number of samples  q( ). This approach has been widely employed for computation of the ELBO and within GAN, but it limits one to models that satisfy the assumptions of

Rep.

3 GO GRADIENT

The reparameterization trick (Rep) is limited to reparameterizable random variables y with continuous components. There are situations for which Rep is not readily applicable, e.g., where the components of y may be discrete or nonnegative. We seek to gain insights from the relationship between REINFORCE and Rep, and generalize the types of latent variables y for which the latter approach may be effected. We term our proposed approach a General and One-sample (GO) gradient. In practice, we find that this approach works well with as few as one sample for evaluating the expectation, and it is applicable to more general settings than Rep.

Recall that Rep was first applied within the context of variational learning (Kingma & Welling,
2014), as in (1). Specifically, it was assumed q (y) = v q (yv), omitting explicit dependence on data x, for notational convenience; yv is component v of y. In Kingma & Welling (2014) q (yv) corresponded to a Gaussian distribution q (yv) = N (yv; µv(), v2()), with mean µv() and variance v2(). In the following we generalize q (yv) such that it need not be Gaussian. Applying integration by parts (Ranganath et al., 2016b)

 Eq (y)[f (y)] = Ev q (y-v) f (y) q (yv)dyv

(6)

=

Ev q (y-v )

f (y) Q (yv)

 -

-

[ Q (yv)][yv f (y)]dyv

,

(7)

"0" "Key"

where y-v denotes y with yv excluded, and Q (yv) is the cumulative distribution function (CDF) of q (yv). The "0" term is readily proven to be zero for any Q (yv), with the assumption that f (y)
doesn't tend to infinity faster than  Q (yv) tending to zero when yv  ±.

The "Key" term exactly recovers the one-dimensional Rep when reparameterization yv =  ( v), v  q( v) exists (Ranganath et al., 2016b). Further, applying  q (yv) = q (yv) log q (yv) in (6) yields REINFORCE. Consequently, it appears that Rep yields low variance by analytically
setting to zero the unnecessary but high-variance-injecting "0" term, while in contrast REINFORCE
implicitly seeks to numerically implement both terms in (7).

We generalize q (y) for discrete yv, here assuming yv  {0, 1, . . . , }. It is shown in Appendix A.2 that this framework is also applicable to discrete yv with a finite alphabet. It may be shown (see Appendix A.2) that

 Eq (y)[f (y)] =

Ev q (y-v )

f (y) q (yv)
yv

= Ev q (y-v) [f (y) Q (yv)]|yv= - yv [ Q (yv)][f (y-v, yv + 1) - f (y)] (8)

"0" "Key"

where Q (yv) =

yv n=0

q

(n),

and

Q

()

=

1

for

all

.

Theorem 1 (GO Gradient). For expectation objectives Eq(y)[f (y)], where q (y) satisfies (i) q (y) = v q (yv); (ii) the corresponding CDF Q (yv) is differentiable wrt parameters ; and (iii) one can calculate  Q (yv), the General and One-sample (GO) gradient is defined as

 Eq (y)[f (y)] = Eq (y) Gq (y)Dy[f (y)] ,

(9)

3

Under review as a conference paper at ICLR 2019

where

Gq (y)

specifies

q(y)
G

· · · , gq(yv), · · · with variable-nabla gq(yv)

Dy[f (y)] = · · · , Dyv [f (y)], · · · T , and

Dyv [f (y)]

yv f (y),

Continous yv

f (y-v, yv + 1) - f (y), Discrete yv

-1 q(yv

)



Q(yv

),

All proofs are provided in Appendix A, where we also list gq(yv) for a wide selection of possible q (y), for both continuous and discrete y. Note for high-dimensional discrete y, calculating Dy[f (y)] is computationally expensive. Fortunately, for f (y) often used in practice special properties hold that can be exploited for efficient parallel computing. Also for discrete yv with finite support, it is possible that one could analytically evaluate a part of expectations in (9) for lower variance, mimicking
the local idea in Titsias & Lázaro-Gredilla (2015); Titsias (2015). Appendix I shows an example
illustrating how to handle these two issues in practice.

4 DEEP GO GRADIENT

The GO gradient in Theorem 1 can only handle mean-field q (y), characterized by an independence assumption on the components of y. One may enlarge the descriptive capability of q (y) by modeling it as a marginal distribution of a deep model (Ranganath et al., 2016b; Bishop, 2006). Hereafter,
we focus on this situation, and begin with a 2-layer model for simple demonstration. Specifically,
consider

q (y) = q (y, )d = qy (y|)q ()d =

v qy (yv|) · k q (k)d,

where  = {y, }, y is the leaf variable, and the internal variable  is assumed to be continuous. Components of y are assumed to be conditionally independent given , but upon marginalizing out
 this independence is removed. The objective becomes

and via Theorem 1

Eq (y)[f (y)] = Eqy (y|)q ()[f (y)],

y Eq (y)[f (y)] = Eq (y,)

q y
Gy

(y|)

Dy

[f

(y)]

.

(10)

Lemma 1. Equation (10) exactly recovers the Rep gradient in (5), if  = y and q (y) has reparameterization y =   (),   q() for easily sampled q().

For the gradient wrt , we first apply Theorem 1, yielding

 Eq (y)[f (y)] = Eq ()

Gq


()

D

Eqy (y|)[f (y)]

.

For continuous internal variable  one can apply Theorem 1 again, from which

 Eq (y)[f (y)] = Eq (y,) Gq ()Gqy (y|)Dy[f (y)] .

(11)

Now extending the same procedure to deeper models with L layers, we generalize the GO gradient in Theorem 2. Random variable y(L) is assumed to be the leaf variable of interest, and may be continuous or discrete; latent/internal random variables {y(1), . . . , y(L-1)} are assumed continuous (these generalize  from above).
Theorem 2 (Deep GO Gradient). For expectation objectives Eq(y(L))[f (y(L))] with q (y(L)) being the marginal distribution of

q y(1), · · · , y(L) = q(1) y(1)

L
ql=2 (l)

y(l)|y(l-1)

,

where  = {(1), · · · , (L)}, all internal variables y(1), · · · , y(L-1) are continuous, the leaf variable y(L) is either continuous or discrete, q(l) (y(l)|y(l-1)) = v q(l) (yv(l)|y(l-1)), and one has

4

Under review as a conference paper at ICLR 2019

access

to

variable-nablas

gq 

(l)

(yv(l)

|y(l-1)

)

 (l)

and

gq(l) (yv(l)|y(l-1))
y(l-1)

,

as

defined

in

Theorem

1,

the

General and One-sample (GO) gradient is defined as

(l) Eq (y(L))[f (y(L))] = Eq (y(1),··· ,y(L))

G BPq 

(l)

(y

(l)

|y(l-1)

)

 (l)

y(l)

,

(12)

where

BP[y(L)]

=

Dy(L) [f (y(L))]

and

BP[y(l)]

=

G BP[y ]q(l+1) (y(l+1)|y(l))
y(l)

(l+1)

for

l

<

L.

Corollary 1. The deep GO gradient in Theorem 2 exactly recovers back-propagation (Rumelhart
& Hinton, 1986) when each element distribution q(l) (yv(l)|y(l-1)) is specified as the Dirac delta function located at the activated value after activation function.

Figure 1 is presented for better understanding. With variable-nablas, one can readily verify that the deep GO gradient in Theorem 2 in expectation obeys the chain rule.

Assume all continuous variables y, and

abuse notations for better understanding.

Dy[f (y)]  yf (y),

q(y)
G



y

Figure 1: Relating back-propagation (Rumelhart & Hinton, 1986) with the deep GO gradient in Theorem 2. (i) In deterministic deep neural networks, one forward-propagates information using activation functions, like ReLU, to sequentially activate {y(l)}l=1,··· ,L (black solid arrows), and then back-propagates gradients from Loss f (·) to each parameter (l) via gradient-flow through {y(k)}k=L,··· ,l (red dashed arrows). (ii) Similarly for the deep GO gradient with 1 MC sample, one forward-propagates information to calculate the expected loss function f (y(L)) using distributions as statistical activation functions, and then uses variable-nablas to sequentially back-propagate gradients through random variables {y(k)}k=L,··· ,l to each (l), as in (12).

5 STATISTICAL BACK-PROPAGATION AND HIERARCHICAL VARIATIONAL INFERENCE

Recall the motivating discussion in Section 2, in which we considered generative model p(x, z) and inference model q(z|x), the former used to model synthesis of the observed data x and the latter used

for inference of z given observed x. In recent deep architectures, a hierarchical representation for cu-

mulative latent variables z = (z(1), . . . , z(L)) has been considered (Rezende et al., 2014; Ranganath

et al., 2015; Zhou et al., 2015b;a; Ranganath et al., 2016b; Cong et al., 2017; Zhang et al., 2018). As

an example, there are models with p(x, z) = p(x|z(1))

L-1 l=1

p

(z

(l)|z(l+1)

)

p (z (L) ).

When

performing inference for such models, it is intuitive to consider first-order Markov chain structure

for q(z|x) = q(z(1)|x)

L l=2

q(z(l)|z(l-1)).

The

discussion

in

this

section

is

most

relevant

for

variational inference, for computation of Eq(z(1),...,z(L)|x)[f (z(1), . . . , z(L))], and consequently

we specialize to that notation in the subsequent discussion (we consider representations in terms of z,

rather than the more general y notation employed in Section 4).

Before proceeding, we seek to make clear the distinction between this section and Section 4. In the latter, only the leaf variable z(L) appears in  Eq(z(L))[f (z(L))]; see (12), with y(L)  z(L). That is because in Section 4 the underlying model is a marginal distribution of z(L), i.e., q (z(L)), which is relevant to the generators of GANs; see (3), with x  z(L) and p(x)  q (z(L)). Random variables z(1), . . . , z(L-1) were marginalized out of q (z(1), . . . , z(L-1), z(L)) to represent q (z(L)). As discussed in Section 4, z(1), . . . , z(L-1) were added there to enhance the modeling flexibility of q (z(L)). In this section, the deep set of random variables z = (z(1), . . . , z(L)) are inherent components of the underlying generative model for x, i.e., p(x, z) = p(x, z(1), . . . , z(L)). Hence, all components of z manifested via inference model q(z|x) = q(z(1), . . . , z(L)|x) play a role in f (z). Besides, no specific structure is imposed on p(x, z) and q(z|x) in this section, moving beyond the aforementioned first-order Markov structure. For a practical application, one may employ domain knowledge to design suitable graphical models for p(x, z) and q(z|x), and then use the following Theorem 3 for training.

5

Under review as a conference paper at ICLR 2019

Theorem 3 (Statistical Back-Propagation). For expectation objectives

Eq({z(i)}iL=1) f ({z(i)}Li=1) ,
where {z(i)}iI=1 denotes I continuous internal variables with at least one child variable, {z(j)}Lj=I+1 represents L - I continuous or discrete leaf variables with no children except f (·), and q(·) is constructed as a hierarchical probabilistic graphical model

q({z(i)}Li=1) = q({z(i)}iI=1)

L q(z(j)|{z(i)}iI=1)
j =I +1

with each element distribution q(zv|pa(zv)) having accessible variable-nablas as defined in Theo-
rem 1, pa(zv) denotes the parent variables of zv, the General and One-sample (GO) gradient for k   is defined as

 Ek q({z(i)}iL=1) f ({z(i)}Li=1) = Eq({z(i)}Li=1) Gqk[ch(k)]BP[ch(k)] ,

(13)

where ch(k) denotes the children variables of k, and with zv  ch(k),

Gqk[ch(k )]

=

[·

·

·

,

g q (zv
k

|pa(zv

))

,

·

·

· ],

BP[ch(k)] = [· · · , BP[zv], · · · ]T ,

and BP[zv] is iteratively calculated as BP[zv] = Gzqv [ch(zv)]BP[ch(zv)], until leaf variables where BP[zv] = Dzv [f ({z(i)}iL=1)].

Statistical back-propagation in Theorem 3 is relevant to hierarchical variational inference (HVI) (Ranganath et al., 2016b; Hoffman & Blei, 2015; Mnih & Gregor, 2014) (see Appendix G), greatly generalizing GO gradients to the inference of directed acyclic probabilistic graphical models. In HVI variational distributions are specified as hierarchical graphical models constructed by neural networks. Using statistical back-propagation, one may rely on GO gradients to perform HVI with low variance, while greatly broadening modeling flexibility.

6 RELATED WORK
There are many methods directed toward low-variance gradients for expectation-based objectives. Attracted by the generalization of REINFORCE, many works try to improve its performance via efficient variance-reduction techniques, like control variants (Mnih & Gregor, 2014; Titsias & LázaroGredilla, 2015; Gu et al., 2015; Mnih & Rezende, 2016; Tucker et al., 2017; Grathwohl et al., 2017). Most of this research focuses on discrete random variables, likely because Rep (if it exists) works well for continuous random variables but it may not exist for discrete random variables. Other efforts are devoted to continuously relaxing discrete variables, to combine both REINFORCE and Rep for variance reduction (Jang et al., 2016; Maddison et al., 2016; Tucker et al., 2017; Grathwohl et al., 2017).
Inspired by the low variance of Rep, there are methods that try to generalize its scope. The Generalized Rep (GRep) gradient (Ruiz et al., 2016) employs an approximate reparameterization whose transformed distribution weakly depends on the parameters of interest. Rejection sampling variational inference (RSVI) (Naesseth et al., 2016) exploits highly-tuned transformations in mature rejection sampling simulation to better approximate Rep for non-reparameterizable distributions. Compared to the aforementioned methods, the proposed GO gradient, containing Rep as a special case for continuous random variables, applies to both continuous and discrete random variables with the same low-variance as the Rep gradient. Implicit Rep gradients (Figurnov et al., 2018) and pathwise derivatives (Jankowiak & Obermeyer, 2018) are recent low-variance methods that exploit a similar idea as GO, namely use of gradient information of the expected function, although they are restricted to continuous variables.
The idea of gradient backpropagation through random variables has been exploited before (Grathwohl et al., 2017; Rezende et al., 2014; Fan et al., 2015; Schulman et al., 2015). RELAX, employing neuralnetwork-parametrized control variants to assist REINFORCE for that goal, has a variance potentially as low as the Rep gradient (Grathwohl et al., 2017). Stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015), focusing mainly on reparameterizable Gaussian random variables and deep latent Gaussian models, exploits the product rule for an integral to derive gradient backpropagation through

6

Under review as a conference paper at ICLR 2019

several continuous random variables. By comparison, the proposed statistical back-propagation based on the GO gradient is applicable to most distributions for continuous random variables. Further, it also flexibly generalizes to hierarchical probabilistic graphical models with continuous internal variables and continuous/discrete leaf ones.
7 EXPERIMENTS
We examine the proposed GO gradients and statistical back-propagation with four experiments: (i) simple one-dimensional (gamma and negative binomial) examples are presented to verify the GO gradient in Theorem 1, corresponding to nonnegative and discrete random variables; (ii) the discrete variational autoencoder experiment from Tucker et al. (2017) and Grathwohl et al. (2017) is reproduced to compare GO with the state-of-the-art variance-reduction methods; (iii) a multinomial GAN, generating discrete observations, is constructed to demonstrate the deep GO gradient in Theorem 2; (iv) hierarchical variational inference (HVI) for two deep non-conjugate Bayesian models is developed to verify statistical back-propagation in Theorem 3.
Many mature machine learning frameworks, like TensorFlow (Abadi et al.) and PyTorch (Paszke et al., 2017), are optimized for implementation of methods like back-propagation. Fortunately, all gradient calculations in the proposed theorems obey the chain rule in expectation, enabling convenient incorporation of the proposed approaches into existing frameworks. Experiments presented below were implemented in TensorFlow or PyTorch with a Titan Xp GPU. Code for all experiments can be found at github.com/author/GO.
Notation Gam(, ) denotes the gamma distribution with shape  and rate , NB(r, P ) the negative binomial distribution with number of failures r and success probability P , Bern(P ) the Bernoulli distribution with probability P , Mult(n, P ) the multinomial distribution with number of trials n and event probabilities P , Pois() the Poisson distribution with rate , and Dir() the Dirichlet distribution with concentration parameters .
7.1 GAMMA AND NB ONE-DIMENSIONAL SIMPLE EXAMPLES

Grad-Variance Grad-Variance
ELBO r Grad-Variance

GRep 102 RSVI
GRep-Stick RSVI-Stick GO
100

10-2 0

200 400 600
Iteration
(a)

800 1000

105 GRep
RSVI
GRep-Stick
RSVI-Stick 100 GO

10-5

10-10 0

200 400 600
Iteration
(b)

800 1000

0
-0.05
-0.1
-0.15
-0.2 0

10-3 -2

-4

-6

GRep

-8 400 600 800 1000

RSVI

GRep-Stick

RSVI-Stick

GO

200 400 600 800 1000
Iteration

(c)

12 REINFORCE
10 GO
8
6
4
2
0 0 200 400 600 800 1000
Iteration
(d)

Figure 2: Gamma (a-c) and NB (d) toy experimental results. (a) The gradient variance of gamma shape  versus iterations, with posterior parameters 0 = 1, 0 = 0.5. (b)-(c) The gradient variance of  and ELBOs versus iterations respectively, when 0 = 0.01, 0 = 0.5. (d) The gradient variance of NB r versus iterations with r0 = 10, p0 = 0.2. In each iteration, gradient variances are estimated with 20 Monte Carlo samples (each sample corresponds to one gradient estimate), among which the last one is used to update parameters.

We first consider illustrative one-dimensional "toy" problems, to examine the GO gradient for both continuous and discrete random variables. The optimization objective is expressed as

max


ELBO()

=

Eq(z)[log

p(z|x)

-

log

q(z)]

+

log

p(x),

where for continuous z we assume p(z|x) = Gam(z; 0, 0) for given set (0, 0), with q(z) = Gam(z; , ) and  = {, }; for discrete z we assume p(z|x) = NB(z; r0, p0) for given set (r0, p0), with q(z) = NB(z; r, p) and  = {r, p}. Stochastic gradient ascent with onesample-estimated gradients is used to optimize the objective, which is equivalent to minimizing KL(q(z) p(z|x)).
Figure 2 shows the results (see Appendix H for additional details). For the nonnegative continuous z associated with the gamma distribution, we compare our GO gradient with GRep (Ruiz et al., 2016), RSVI (Naesseth et al., 2016), and their modified version using the "sticking" idea (Roeder et al.,

7

Under review as a conference paper at ICLR 2019

-110

-110

-115

-115

ELBO ELBO

-120

REBAR train

REBAR valid

-125

RELAX train RELAX valid

GO train

GO valid

-130 0 2 4 6 8 10

Iteration

105

-120
-125
-130
-135 0

REBAR train REBAR valid RELAX train RELAX valid GO train GO valid
2000 4000 6000 8000
Time (seconds)

Figure 3: Training curves for the discrete VAE experiment with 1-layer linear model (see Appendix I) on the stochastically binarized MNIST dataset(Salakhutdinov & Murray, 2008). All methods are run with the same learning rate for 1, 000, 000 iterations. The black line represents the best training ELBO of REBAR and RELAX. ELBOs are calculated using all training/validation data.

2017), denoted as GRep-Stick and RSVI-Stick, respectively. For RSVI and RSVI-Stick, the shape augmentation parameter is set as 5 by default. The only difference between GRep and GRep-Stick (also RSVI and RSVI-Stick) is the latter does not analytically express the entropy Eq(z)[- log q(z)]. Figure 2(a) clearly shows the utility of employing sticking to reduce variance; without it, GRep and RSVI exhibit high variance, that destabilizes the optimization for small gamma shape parameters, as shown in Figures 2(b) and 2(c). We adopt the sticking approach hereafter. Among methods with sticking, GO exhibits the lowest variance in general, as shown in Figures 2(a) and 2(b). GO empirically provides more stable learning curves, as shown in Figure 2(c). For the discrete case corresponding to the NB distribution, GO is compared to REINFORCE (Williams, 1992). It is apparent from Figure 2(d) that, thanks to analytically removing the "0" terms in (8), the GO gradient has much lower variance, even in this simple one-dimensional case.

7.2 DISCRETE VARIATIONAL AUTOENCODER
To demonstrate the low variance of the proposed GO gradient, we consider the discrete variational autoencoder (VAE) experiment from REBAR (Tucker et al., 2017) and RELAX (Grathwohl et al., 2017), to make a direct comparison with state-of-the-art variance-reduction methods. Since the statistical back-propagation in Theorem 3 cannot handle discrete internal variables, we focus on the single-latent-layer settings (1 layer of 200 Bernoulli random variables), i.e.,
p(x, z) : x  Bern NNP x|z (z) , z  Bern P z q(z|x) : z  Bern NNP z|x (x)
where P z is the parameters of the prior p(z), NNP x|z (z) means using a neural network to project the latent binary code z to the parameters P x|z of the likelihood p(x|z), and NNP z|x (x) is similarly defined for q(z|x). The objective is given in (1). See Appendix I for more details.

Table 1: Best obtained ELBOs for discrete variational autoencoders. Results of REBAR and RELAX are obtained by running the released code1 from Grathwohl et al. (2017). All methods are run with the same learning
rate for 1, 000, 000 iterations.

Dataset
MNIST Omniglot

Model
Linear 1 layer Nonlinear Linear 1 layer Nonlinear

REBAR
-112.16 -96.99 -122.19 -79.51

Training
RELAX
-112.89 -95.99 -122.17 -80.67

GO
-110.21 -82.26 -119.03 -54.96

Validation

REBAR
-114.85 -112.96 -124.81 -129.00

RELAX
-115.36 -112.42 -124.95 -128.99

GO
-114.27 -111.48 -123.84 -126.59

1github.com/duvenaud/relax

8

Under review as a conference paper at ICLR 2019

Figure 3 (also Figure 9 of Appendix I) shows the training curves versus iteration and running time for the compared methods. Even without any variance-reduction techniques, GO provides better performance, faster convergence rate, and better running efficiency (about ten times faster in achieving the best training ELBO of RERAR/RELAX in this experiment). We believe GO's better performance originates from: (i) its inherent low-variance nature; (ii) GO has less parameters compared to REBAR and RELAX (no control variant is adopted for GO); (iii) efficient batch processing methods (see Appendix I) are adopted to benefit from parallel computing. Table 1 presents the best training/validation ELBOs under various experimental settings for the compared methods. GO provides the best performance in all situations. Additional experimental results are given in Appendix I.
Many variance-reduction techniques can be used to further reduce the variance of GO, especially when complicated models are of interest. Compared to RELAX, GO cannot be directly applied when f (y) is not computable or where the interested model has discrete internal variables. For the latter issue, we present in Appendix B.4 a procedure to assist GO (or statistical back-propagation in Theorem 3) in handling discrete internal variables.
7.3 MULTINOMIAL GAN
To demonstrate the deep GO gradient in Theorem 2, we adopt multinomial leaf variables x and construct a new multinomial GAN (denoted as MNGAN-GO) for generating discrete observations with a finite alphabet. The corresponding generator p(x) is expressed as
 N (0, I), x  Mult(1, NNP( )).
For brevity, we integrate the generator's parameters  into the NN notation, and do not explicitly express them. Details for this example are provided in Appendix J.
We compare MNGAN-GO with the recently proposed boundary-seeking GAN (BGAN) (Hjelm et al., 2018) on 1-bit (1-state, Bernoulli leaf variables x), 1-bit (2-state), 2-bit (4-state), 3-bit (8-state) and 4-bit (16-state) discrete image generation tasks, using quantized MNIST datasets (LeCun et al., 1998). Table 2 presents inception scores (Salimans et al., 2016) of both methods. MNGAN-GO performs better in general. Further, with GO's assistance, MNGAN-GO shows more potential to benefit from richer information coming from more quantized states. For demonstration, Figure 4 shows the generated samples from the 4-bit experiment, where better image quality and higher diversity are observed for the samples from MNGAN-GO.

Table 2: Inception scores on quantized MNIST. BGAN's results are run with the author-released code https://github.com/rdevon/BGAN.

bits (states) 1 (1) 1 (2) 2 (4) 3 (8) 4 (16)

BGAN 8.31 ± .06 8.56 ± .04 7.76 ± .04 7.26 ± .03 6.29 ± .05

MNGAN-GO 9.10 ± .06 8.40 ± .07 9.02 ± .06 9.26 ± .07 9.27 ± .06

Figure 4: Generated samples from BGAN (top) and MNGAN-GO (bottom) trained on 4-bit quantized MNIST.

7.4 HVI FOR DEEP EXPONENTIAL FAMILIES AND DEEP LATENT DIRICHLET ALLOCATION
To demonstrate statistical back-propagation in Theorem 3, we design variational inference nets for two nonconjugate hierarchical Bayesian models, i.e., deep exponential families (DEF) (Ranganath et al., 2015) and deep latent Dirichlet allocation (DLDA) (Zhou et al., 2015b;a; Cong et al., 2017).
DEF : x  Pois(W(1)z(1)), z(l)  Gam  , /z z W(l+1)z(l+1) , W(l)  Gam(0, 0) DLDA : x  Pois((1)z(1)), z(l)  Gam (l+1)z(l+1), c(l+1) , (l)  Dir(0). For demonstration, we design the inference nets q(z|x) following the first-order Markov chain construction in Section 5, namely q(z|x) : z(1)  Gam NN(1)(x), NN(1)(x) , z(l)  Gam NN(l)(z(l-1)), NN(l)(z(l-1)) .

9

Under review as a conference paper at ICLR 2019

Further details are provided in Appendix K. One might also wish to design inference nets that have structure beyond the above first-order Markov chain construction, as in Zhang et al. (2018); we do not consider that here, but Theorem 3 is applicable to that case.

-0.5

-1

ELBO

-1.5
-2 0

GO

0.5 1 1.5 2

Iteration

104

Figure 5: ELBOs of HVI for a 128-64 DEF on MNIST. Except for different ways to calculate gradients, all other experimental settings are the same for compared methods, including the sticking idea and one-MC-sample gradient estimate.

Figure 6: HVI results for a 128-64-32 DLDA on MNIST. Upperleft is the training ELBOs. The remaining subfigures are learned dictionary atoms from (1) (top-right), (1)(2) (bottom-left), and (1)(2)(3) (bottom-right).

HVI for a 2-layer DEF is first performed, with the ELBO curves shown in Figure 5. GO enables faster and more stable convergence. Figure 6 presents the HVI results for a 3-layer DLDA, for which stable ELBOs are again observed. More importantly, with the GO gradient, one can utilize pure gradient-based methods to efficiently train such complicated nonconjugate models for meaningful dictionaries (see Appendix K for more implementary details).

8 CONCLUSIONS
For expectation-based objectives, we propose a General and One-sample (GO) gradient that applies to continuous and discrete random variables. We further generalize the GO gradient to cases for which the underlying model is deep and has a marginal distribution corresponding to the latent variables of interest, and to cases for which the latent variables are hierarchical. The GO-gradient setup is demonstrated to yield the same low-variance estimation as the reparameterization trick, which is only applicable to continuous random variables. Alongside the GO gradient, we constitute a means of propagating the chain rule through distributions. Accordingly, we present statistical back-propagation, to flexibly integrate deep neural networks with general classes of random variables.

REFERENCES
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. TensorFlow: Large-scale machine learning on heterogeneous systems. URL https://www.tensorflow.org/. Software available from tensorflow.org.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. In ICLR, 2017.
C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
Y. Cong, B. Chen, H. Liu, and M. Zhou. Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC. In ICML, 2017.
K. Fan, Z. Wang, J. Beck, J. Kwok, and K. Heller. Fast second order stochastic backpropagation for variational inference. In NIPS, pp. 1387­1395, 2015.
Michael Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. arXiv preprint arXiv:1805.08498, 2018.
Z. Gan, L. Chen, W. Wang, Y. Pu, Y. Zhang, H. Liu, C. Li, and L. Carin. Triangle generative adversarial networks. In NIPS, pp. 5253­5262, 2017.

10

Under review as a conference paper at ICLR 2019
K. Geddes, M. Glasser, R. Moore, and T. Scott. Evaluation of classes of definite integrals involving elementary functions via differentiation of special functions. Applicable Algebra in Engineering, Communication and Computing, 1(2):149­165, 1990.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pp. 2672­2680, 2014.
W. Grathwohl, D. Choi, Y. Wu, G. Roeder, and D. Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. arXiv:1711.00123, 2017.
S. Gu, S. Levine, I. Sutskever, and A. Mnih. MuProp: Unbiased backpropagation for stochastic neural networks. arXiv:1511.05176, 2015.
J. M. Hernández-Lobato, Y. Li, M. Rowland, D. Hernández-Lobato, T. Bui, and R. E. Turner. Black-box -divergence minimization. In ICML, 2016.
R. Hjelm, A. Jacob, T. Che, A. Trischler, K. Cho, and Y. Bengio. Boundary seeking GANs. In ICLR, 2018. URL https://openreview.net/forum?id=rkTS8lZAb.
M. Hoffman and D. Blei. Stochastic structured variational inference. In AISTATS, pp. 361­369, 2015.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv:1611.01144, 2016.
Martin Jankowiak and Fritz Obermeyer. Pathwise derivatives beyond the reparameterization trick. arXiv preprint arXiv:1806.01851, 2018.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
C. Li, H. Liu, C. Chen, Y. Pu, L. Chen, R. Henao, and L. Carin. Alice: Towards understanding adversarial learning for joint distribution matching. In NIPS, pp. 5501­5509, 2017.
C. Li, J. Li, G. Wang, and L. Carin. Learning to sample with adversarially learned likelihood-ratio. 2018.
Y. Li and R. E. Turner. Rényi divergence variational inference. In NIPS, pp. 1073­1081, 2016.
C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv:1611.00712, 2016.
A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML, pp. 1791­1799, 2014.
A. Mnih and D. Rezende. Variational inference for monte carlo objectives. In ICML, pp. 2188­2196, 2016.
C. A. Naesseth, F. J. R. Ruiz, S. W. Linderman, and D. M. Blei. Rejection sampling variational inference. arXiv:1610.05683, 2016.
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. 2017.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv:1511.06434, 2015.
R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. In AISTATS, 2014.
R. Ranganath, L. Tang, L. Charlin, and D. M. Blei. Deep exponential families. In AISTATS, 2015.
R. Ranganath, D. Tran, J. Altosaar, and D. Blei. Operator variational inference. In NIPS, pp. 496­504, 2016a.
R. Ranganath, D. Tran, and D. Blei. Hierarchical variational models. In ICML, pp. 324­333, 2016b.
11

Under review as a conference paper at ICLR 2019
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.
G. Roeder, Y. Wu, and D. K. Duvenaud. Sticking the landing: Simple, lower-variance gradient estimators for variational inference. In NIPS, pp. 6928­6937, 2017.
F. J. R. Ruiz, M. K. Titsias, and D. Blei. The generalized reparameterization gradient. In NIPS, pp. 460­468, 2016.
D. Rumelhart and G. Hinton. Learning representations by back-propagating errors. Nature, 323(9), 1986.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In Proceedings of the 25th international conference on Machine learning, pp. 872­879. ACM, 2008.
T. Salimans, D. A. Knowles, et al. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4):837­882, 2013.
T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In NIPS, pp. 2234­2242, 2016.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 3528­3536. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/ 5899-gradient-estimation-using-stochastic-computation-graphs. pdf.
M. Titsias and M. Lázaro-Gredilla. Local expectation gradients for black box variational inference. In NIPS, pp. 2638­2646, 2015.
Michalis K Titsias. Local expectation gradients for doubly stochastic variational inference. arXiv preprint arXiv:1503.01494, 2015.
G. Tucker, A. Mnih, C. Maddison, J. Lawson, and J. Sohl-Dickstein. REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models. In NIPS, pp. 2624­2633, 2017.
R. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
H. Zhang, B. Chen, D. Guo, and M. Zhou. WHAI: Weibull hybrid autoencoding inference for deep topic modeling. 2018.
J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In ICLR, 2016. M. Zhou, Y. Cong, and B. Chen. Gamma belief networks. arXiv:1512.03081, Dec. 2015a. M. Zhou, Y. Cong, and B. Chen. The Poisson gamma belief network. In NIPS, pp. 3025­3033,
2015b.
12

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 1

We first prove (7) in the main manuscript, followed by its discrete counterpart, i.e., (8) in the main manuscript. Then, it is easy to verify Theorem 1.

A.1 PROOF OF EQUATION (7) IN THE MAIN MANUSCRIPT

Similar proof in one-dimension is also given in the supplemental materials of Ranganath et al. (2016b).

We want to calculate

 Eq (y)[f (y)] =  E v q (yv)[f (y)] = Ev q (y-v) f (y) q (yv)dyv ,

where y-v denotes y with yv excluded. Without loss of generality, we assume yv  (-, ).

Let v (yv) =  q (yv), and we have

yv

v(yv) =

 q (t)dt = 

-

yv
q (t)dt =  Q (yv),
-

where Q (yv) is the cumulative distribution function (CDF) of q (yv). Further define u(yv) = f (yv, y-v), we then apply integration by parts (or partial integration) to get

 Eq (y)[f (y)] = = =

Ev q (y-v) u(yv)v (yv)dyv Ev q (y-v) u(yv)v(yv)|- - u (yv)v(yv)dyv Ev q (y-v) f (y) Q (yv)|- - [ Q (yv)][yv f (y)]dyv .
"0" "Key"

(14)

With Q () = 1 and Q (-) = 0, it's straightforward to verify that the first term is always zero for any Q (yv), thus named the "0" term.

A.2 PROOF OF EQUATION (8) IN THE MAIN MANUSCRIPT

For discrete variables y, we have

 Eq (y)[f (y)] =

Ev q (y-v )

N
f (y) q (yv) ,
yv =0

where yv  {0, 1, · · · , N } and N is the size of the alphabet.

To handle the summation of products of two sequences and develop discrete counterpart of (7), we

first introduce Abel transformation.

Abel B0 =

transformation. Given two

b0 and Bn =

n k=0

bn

for

n

sequences {an} and  1. Accordingly, we

{bn}, have

with

n



{0, · · ·

, N },

we

define

SN =

N
n=0 anbn = a0b0 +

N
n=1 an(Bn - Bn-1)

= a0B0 +

N
n=1 anBn -

N -1
n=0 an+1Bn

= aN BN +

N -1
anBn -
n=0

N -1
an+1Bn
n=0

= aN BN -

N -1
(an+1 - an)Bn.
n=0

Substituting n = yv, an = f (y), bn =  q (yv), and Bn =  Q (yv) into the above equation, we have

 Eq (y)[f (y)] = Ev q (y-v) f (y-v, yv = N ) Q (yv = N ) -

N -1
yv=0[f (y-v, yv + 1) - f (y)] Q (yv) .

"0" "Key"

13

Under review as a conference paper at ICLR 2019

Note the first term equals zero for both finite alphabet, i.e., N < , and infinite alphabet, i.e., N = . When N = , we get (8) in the main manuscript. With the above proofs for Eqs. (7) and (8), one could straightforwardly verify Theorem 1.
Table 3: Variable-nabla examples. Note y is a scalar random variable.

q (y) Delta (y - µ)

Bernoulli(p)

Normal(µ, 2)

Log-Normal(µ, 2)

Logit-Normal(µ, 2)

Cauchy(µ, )

Gamma

,

1 

Beta , 

NB r, p
Exponential()
Student's t(v)
Weibull(, k) Laplace(µ, b) Poisson() Geometric(p) Categorical(p)

gq (y)

gµq (y) = 1

gpq (y) =

1/(1 - p) 0

y=0 y=1

gµq (y) = 1 gµq (y) = y gµq (y) = y(1 - y) gµq (y) = 1

gq (y)

=

y-µ 

gq (y)

=

y

log

y-µ 

gq (y)

=

y(1

-

y)

logit(y)-µ 

gq (y)

=

y-µ 

gq (y)

=

[log(y)-()](,y)+yT (3,,y)   y -1 e- y

gq (y)

=

-

y 

gq (y) =

log y - () + ( + ) - y-1(1 - y)-1 B(y; , )
y + 2(1 - y)-1 · 3F2(, , 1 - ;  + 1,  + 1; y)

 log(1 - y) - () + ( + )



B(1 - y; , )

gq (y) = 

y-1(1 - y)-1 1-y



- 2y-1 · 3F2(, , 1 - ;  + 1,  + 1; 1 - y)

 log(1 - p) - (r) + (r + y + 1)



- (y + r)B(1 - p; r, y + 1)

grq (y) =  y + r

(1 - p)r py



+ py r2 ·3F2(r, r, -y; r + 1, r + 1; 1 - p)

gpq (y)

=

y+r 1-p

gq (y)

=

-

y 

1

v

v+1

+

-

v2

2

1 v + 1 3 y2

· 2F1 ,

; ;-

222 v



gvq (y)

=

y 2

1

+

y2 v

v+1 · 2

2y(v + 1)

3 v + 3 5 y2

+

· 2F1 ,

; ;-

3v 2 2 2 v

  3 v+3

v+1



 +

y2 3v

, ; 1; 1,

·

F220112

 

2



5

2 v+3

2, ; ;

;

2

;

-

y2

,

-

y2

 

v v

gq (y)

=

y 

22

gkq (y)

=

y k

log

 y

gµq (y) = 1

gbq (y)

=

y-µ b

gq (y) = 1

gpq (y)

=

-

y+1 p

gpq (y)

=

-

1 py

[10y

,

11y

,

·

·

·

, 1(N-1)y, 0]T

for y = {0, 1, · · · , N }, p = [p0, p1, · · · , pN ]T

     

(x) is the digamma function. (x, y) is the upper incomplete gamma function. B(x; , ) is the incomplete beta function. pFq(a1, · · · , ap; b1, · · · , bp; x) is the generalized hypergeometric function. T (m, s, x) is a special case of Meijer G-function (Geddes et al., 1990).

B WHY DISCRETE Internal VARIABLES ARE CHALLENGING?
For simpler demonstration, we first use a 2-layer model to show why discrete internal variables are challenging. Then, we present an importance-sampling proposal that might be useful under specific situations. Finally, we present a strategy to learn discrete internal variables with the statistical back-propagation in Theorem 3 of the main manuscript. Assume q (y) being the marginal distribution of the following 2-layer model
q (y) = Eq ()[qy (y|)] where  = {y, }, q (y, ) = qy (y|)q () = v qy (yv|) · k q (k), and both the leaf variable y and the internal variable  could be either continuous or discrete. Accordingly, the objective becomes
Eq (y)[f (y)] = Eq (y,)[f (y)] = Eqy (y|)q ()[f (y)].
14

Under review as a conference paper at ICLR 2019

For gradient wrt y, using Theorem 1, it is straight to show

y Eq (y,)[f (y)] = Eq () y Eqy (y|)[f (y)]

= Eq (y,)

q y
Gy

(y|)

Dy

[f

(y)]

.

(15)

For gradient wrt , we first have

 Eq (y,)[f (y)] =  Eq () Eqy (y|)[f (y)] .

With f^() = Eqy (y|)[f (y)], we then apply Theorem 1 and get

 Eq (y,)[f (y)] = Eq ()

Gq


()

D

[f^()]

,

where D[f^()] = [· · · , Dk [f^()], · · · ]T , and

Dk [f^()]

k f^(),

Continous k

f^(-k, k + 1) - f^(), Discrete k

(16)

Next, we separately discuss the situations where k is continuous or discrete.

B.1 FOR CONTINUOUS k

One can directly apply Theorem 1 again, namely

Dk [f^()] = k Eqy (y|)[f (y)] = Eqy (y|)

q y
Gk

(y|)

Dy

[f

(y)]

.

Substituting (17) into (16), we have

 Eq (y,)[f (y)] = Eq (y,)

Gq


()

q
G

y

(y|)

Dy

[f

(y)]

.

(17) (18)

B.2 FOR DISCRETE k

In this case, we need to calculate Dk [f^()] = f^(-k, k + 1) - f^(). The keys are again partial integration and Abel transformation. For simplicity, we first assume one-dimensional y, and separately discuss y being continuous and discrete.
For continuous y, we apply partial integration to f^() and get

f^() = Eqy (y|)[f (y)] = qy (y|)f (y)dy

=

f (y)Qy (y|)

 -

-

Accordingly, we have Dk [f^()] = f^(-k, k + 1) - f^()

Qy (y|)yf (y)dy.

= Qy (y|) - Qy (y|-k, k + 1) yf (y)dy

+ f (y)Qy (y|-k, k + 1)

 -

-

f (y)Qy (y|)

 -

.

Removing the "0" term, we have

"0"

Dk [f^()] = Qy (y|) - Qy (y|-k, k + 1) yf (y)dy.

(19)

For discrete y, by similarly exploiting Abel transformation, we have f^() = Eqy (y|)[f (y)] = y qy (y|)f (y) = f ()Qy (|) - y Qy (y|) f (y + 1) - f (y) .

15

Under review as a conference paper at ICLR 2019

Accordingly, we get Dk [f^()] =

y Qy (y|) - Qy (y|-k, k + 1) f (y + 1) - f (y) .

(20)

Unifying (19) for continuous y and (20) for discrete y, we have

Dk [f^()] = f^(-k, k + 1) - f^() = Eqy (y|-k,k+1)[f (y)] - Eqy (y|)[f (y)] = Eqy (y|) Dy[f (y)] · g¯qky (y|)

(21)

where we define

g¯qky (y|)

Qy (y|) - Qy (y|-k, k + 1) . qy (y|)

Multi-dimensional y. Based on the above one-dimensional foundation, we next move on to multidi-
mensional situations. With definitions y:i {y1, · · · , yi} and yi: {yi, · · · , yV }, where V is the dimensionality of y, we have

Dk [f^()] = f^(-k, k + 1) - f^() = Eqy (y|-k,k+1)[f (y)] - Eqy (y|)[f (y)]

= Eqy (y2:|-k,k+1) Eqy (y1|-k,k+1)[f (y)] - Eqy (y1|)[f (y)]

(22)

+ Eqy (y1|)[f (y)] - Eqy (y|)[f (y)]

Apply (21) and we have

Dk [f^()] = Eqy (y2:|-k,k+1) Eqy (y1|) Dy1 [f (y)] · g¯qky (y1|)
+ Eqy (y2:|-k,k+1) Eqy (y1|)[f (y)] - Eqy (y|)[f (y)] = Eqy (y2:|-k,k+1)qy (y1|) Dy1 [f (y)] · g¯qky (y1|) + f (y) - Eqy (y|)[f (y)]

Similarly, we add extra terms to the above equation to enable applying (21) again as

Dk [f^()] = Eqy (y2:|-k,k+1)qy (y1|) Dy1 [f (y)] · g¯qky (y1|) + f (y) - Eqy (y3:|-k,k+1)qy (y:2|) Dy1 [f (y)] · g¯qky (y1|) + f (y) + Eqy (y3:|-k,k+1)qy (y:2|) Dy1 [f (y)] · g¯qky (y1|) + f (y) - Eqy (y|)[f (y)].
Accordingly, we apply (21) to the first two terms and have

Dk [f^()] = Eqy (y3:|-k,k+1)qy (y:2|) Dy2 Dy1 [f (y)]g¯qky (y1|) + f (y) g¯qky (y2|)

+ Eqy (y3:|-k,k+1)qy (y:2|) Dy1 [f (y)]g¯qky (y1|) + f (y) - Eqy (y|)[f (y)]

=

Eqy

(y3: |-k ,k +1)q y

(y :2 |)

 

Dy2 Dy1 [f (y)]g¯qky (y1|) + Dy1 [f (y)]g¯qky (y1|) +

+ f (y) f (y)

g¯qky (y2|) 

- Eqy (y|)[f (y)]

So forth, we summarize the pattern into the following equation as

Dk [f^()] = Eqy (y|) [Ak (y, V ) - f (y)] ,

(23)

16

Under review as a conference paper at ICLR 2019

where Ak (y, V ) is iteratively calculated as
Ak (y, 0) = f (y) Ak (y, v) = Dyv Ak (y, v - 1) g¯qky (yv|) + Ak (y, v - 1) Ak (y, V ) = DyV Ak (y, V - 1) g¯qky (yV |) + Ak (y, V - 1).
Despite elegant structures within (23), to calculate it, one must iterate over all dimensions of y, which is computational expensive in practice. More importantly, it is straightforward to show that deeper models will have similar but much more complicated expressions.

B.3 AN IMPORTANCE-SAMPLING PROPOSAL TO HANDLE DISCRETE Internal VARIABLES

Next, we present an intuitive proposal that might be useful under specific situations.

The key idea is to use different extra items to enable "easy-to-use" expression for Dk [f^()], namely Dk [f^()] = f^(-k, k + 1) - f^() = Eqy (y|-k,k+1)[f (y)] - Eqy (y|)[f (y)] = Eqy (y|-k,k+1)[f (y)] - Eqy (y:V -1|-k,k+1)qy (yV |)[f (y)] + Eqy (y:V -1|-k,k+1)qy (yV |)[f (y)] ··· - Eqy (y:i-1|-k,k+1)qy (yi:|)[f (y)] + Eqy (y:i-1|-k,k+1)qy (yi:|)[f (y)] ··· - Eqy (y1|-k,k+1)qy (y2:|)[f (y)] + Eqy (y1|-k,k+1)qy (y2:|)[f (y)] - Eqy (y|)[f (y)].
Apply (21) to the adjacent two terms for V times, we have

Dk [f^()] = Eqy (y:V -1|-k,k+1)qy (yV |) DyV [f (y)]g¯qky (yV |) ···

+ Eqy (y:i-1|-k,k+1)qy (yi:|) Dyi [f (y)]g¯qky (yi|) ···

+ Eqy (yV |) Dy1 [f (y)]g¯qky (y1|) ,

where we can apply the idea of importance sampling and modify the above equation to

Dk [f^()] = Eqy (y|)

DyV

[f (y)]g¯qky (yV

|)

qy (y:V -1|-k, k qy (y:V -1|)

+

1)

···

+ Eqy (y|)

Dyi [f (y)]g¯qky (yi|)

qy (y:i-1|-k, k qy (y:i-1|)

+

1)

···

(24)

+ Eqy (yV |) Dy1 [f (y)]g¯qky (y1|)

= Eqy (y|)

V
Dyv
v=1

f (y)

g¯qky (yv|)

qy (y:v-1|-k, k qy (y:v-1|)

+

1)

.

17

Under review as a conference paper at ICLR 2019

Note that importance sampling may not always work well in practice (Bishop, 2006).

We further define the generalized variable-nabla as

g

gqy k

(yv

|)


   

-1 qy (yv|)

k

Qy

(yv |),

Continuous k

   

Qy

(yv |)

- Qy (yv|-k, qy (yv|)

k

+

1)

qy

(y:v-1|-k, k qy (y:v-1|)

+

1)

,

Discrete k

(25)

With the generalized variable-nabla, we unify (24) for discrete k and (17) for continuous k and get

Dk [f^()] = Eqy (y|)

Dyv [f (y)] · ggqk(yv|) ,

v

which apparently obeys the chain rule. Accordingly, we have the gradient for  in (16) as

 Eq (y,)[f (y)] = Eq (y,)
k

Dyv [f (y)] · ggqk(yv|) · ggq (k) .
v

One can straightforwardly verify that, with the generalized variable-nabla defined in (25), the chain rule applies to  Eq(y(L))[f (y(L))], where one can freely specify both leaf and internal variables to be either continuous or discrete. The only problem is that, for discrete internal variables, the
importance sampling trick used in (24) may not always work as expected.

B.4 STRATEGY TO LEARN DISCRETE INTERNAL VARIABLES WITH STATISTICAL BACK-PROPAGATION
Practically, if one has to deal with a q (y(1), · · · , y(L)) with discrete internal variables y(l), l < L, we suggest the strategy in Figure 7, with which one should expect a close performance but enjoy much easier implementation with statistical back-propagation in Theorem 3 of the main manuscript. In fact, one can always add additional continuous internal variables to the graphical models to remedy the performance loss or even boost the performance.

(a) (b)
Figure 7: A strategy for discrete internal variables. Blue and red circles denote continuous and discrete variables, respectively. The centered dots represent the corresponding distribution parameters. (a) Practically, one uses a neural network (black arrow) to connect the left variable to the parameters of the center discrete one, and then uses another neural network to propagate the sampled value to the next. (b) Instead, we suggest "extracting" the discrete variable as a leaf one and propagate its parameters to the next.

C PROOF OF LEMMA 1

First, a marginal distribution q (y) with reparameterization y =   ( ),  q( ) can be expressed as a joint distribution, namely

q (y) = q (y, ) = q (y| )q( ),
where q (y| ) = (y -   ( )), (·) is the Dirac delta function, and   ( ) could be flexibly specified as a injective, surjective, or bijective function.
Next, we align notations and rewrite (10) as

 Eq (y)[f (y)] = Eq (y,

)

q (y|
G

)
Dy

[f

(y

)]

= Eq(

)

Eq (y|

)

q (y|
G

)Dy[f (y)]

,

(26)

where

q (y|
G

)

=

· · · , gq (yv| ), · · ·

and gq (yv| )

-1 q (yv |

)  Q (yv|

).

18

Under review as a conference paper at ICLR 2019

With q (yv| ) = (yv - [  ( )]v), we have

1 Q (yv| ) = 0

[  ( )]v  yv [  ( )]v > yv

Accordingly, we have

 Q (yv| ) = [  ( )]v Q (yv| ) ·  [  ( )]v = -([  ( )]v - yv) ·  [  ( )]v

and

gq (yv|

)=

-1 q (yv|

)  Q (yv|

) =  [  (

)]v .

Substituting the above equations into (26), we get

 Eq (y)[f (y)] = Eq( ) [   ( )][yf (y)]|y=  ( ) , which is the multi-dimensional Rep gradient in (5) of the main manuscript.

(27)

D PROOF OF THEOREM 2
Firstly, with the internal variable  being continuous, (10) and (11) in the main manuscript are proved by (15) and (18) in Section B, respectively. Then, by iteratively generalizing the similar derivations to deep models and utilizing the fact that the GO gradients with variable-nablas in expectation obey the chain rule for models with continuous internal variables, Theorem 2 could be readily verified.

E PROOFS FOR COROLLARY 1

When all q(i) y(i)|y(i-1) s are specified as Dirac delta functions, namely

q(i) y(i)|y(i-1) = (y(i) - ((i), y(i-1))),

where ((i), y(i-1)) denotes the activated values after activation functions, the objective becomes

Eq (y(L))[f (y(L))] = f (y(L)) = f (((L), y(L-1))) = f (((L), ((L-1), y(L-1)))) = f (((L), ((L-1), · · · , ((1)))) = f (),

(28)

where  = {(1), · · · , (N)}.

Back-Propagation. For the objective in (28), the Back-Propagation is expressed as

(i) f () = [(i) y(i)][y(i) f (·)],

where

y(i) f (·) = [y(i) y(i+1)][y(i+1) f (·)].

(29)

Deep GO Gradient. We consider the continuous special case, where Dy(L) f (y(L)) y(L) f (y(L)).

With q(i+1) y(i+1)|y(i) s being Dirac delta functions, namely,

q(i+1) yk(i+1)|y(i) =

, 0,

[((i+1), y(i))]k = yk(i+1) [((i+1), y(i))]k = yk(i+1)

we have

Q(i+1) yk(i+1)|y(i) =

1, 0,

[((i+1), y(i))]k  yk(i+1) [((i+1), y(i))]k > yk(i+1)

=

19

Under review as a conference paper at ICLR 2019

Taking derivative wrt yv(i), we got

 Qyv(i) (i+1) yk(i+1)|y(i) =  Q[((i+1),y(i))]k (i+1) yk(i+1)|y(i) · yv(i) [((i+1), y(i))]k = - [((i+1), y(i))]k - yk(i+1) · yv(i) [((i+1), y(i))]k
Accordingly, we have

gq 

(i+1)

(yk(i+1)

|y(i)

)

yv(i)

=

q

(i+1)

-1 (yk(i+1)

|y

(i)

)

yv(i)

Q

(i+1)

(yk(i+1)

|y

(i)

)

= yv(i) [((i+1), y(i))]k = yv(i) yk(i+1)

By substituting the above equation into (12) in Theorem 2, and then comparing it with(29), one can

easily verify Corollary 1.

F PROOF FOR THEOREM 3
Based on the proofs for Theorem 1 and Theorem 2, it is clear that, if one constrains all internal variables to be continuous, the GO gradients in expectation obey the chain rule. Therefore, one can straightforwardly utilizing the chain rule to verify Theorem 3. Actually, Theorem 3 may be seen as the chain rule generalized with random variables, among which the internal ones are only allowed to be continuous.

G DERIVATIONS FOR HIERARCHICAL VARIATIONAL INFERENCE

In Hierarchical Variational Inference, the objective is to maximize the evidence lower Bound (ELBO)

ELBO(, ; x) = Eq(z|x)[log p(x, z) - log q(z|x)].

(30)

For the common case with z = {z(1), · · · , z(L)}, it is obvious that Theorem 3 of the main manuscript

can be applied when optimizing .

Practically, there are situations where one might further put a latent variable  in reference q(z|x), namely q(z|x) = qz (z|)q ()d with  = {z, }. Following Ranganath et al. (2016b), we briefly discuss this situation here.

We first show that there is another unnecessary variance-injecting "0" term.

ELBO(, ; x) = [q(z|x)][log p(x, z) - log q(z|x)]dz

- q(z|x) log q(z|x)dz,

(31)

"0"
where the second "0" term is straightly verified as
q(z|x) log q(z|x)dz = q(z|x)dz =  q(z|x)dz = 1 = 0.

Eliminating the "0" term from (31), one still has another problem, that is, log q(z|x) is usually non-trivial when q(z|x) is marginal. For this problem, we follow Ranganath et al. (2016b) to use
another lower bound ELBO2 of the ELBO in (30).

- log q(z|x) =

q(|z, x)[- log q(z|x)]d =

q(|z, x)

-

log

q(z, |x) q(|z, x)

d

= Eq(|z,x)  Eq(|z,x)

- log q(z, |x) + log q(|z, x)

r(|z, x)

r(|z, x)

- log q(z, |x) r(|z, x)

20

Under review as a conference paper at ICLR 2019

where r(|z, x), evaluable, is an additional variational distribution to approximate the variational posterior q(|z, x). Accordingly, we get the ELBO2 for Hierarchical Variational Inference as
ELBO2(, ; x) = Eq(z,|x) log p(x, z) - log q(z, |x) + log r(|z, x) .
Note similar to (31), the unnecessary "0" term related to log q(z, |x) should also be removed. Accordingly, we have
ELBO2(, ; x) = q(z, |x) log p(x, z)-log q(z, |x)+log r(|z, x) ddz.
Obviously, Theorem 3 is readily applicable to provide GO gradients.

H DETAILS OF GAMMA AND NB ONE-DIMENSIONAL SIMPLE EXAMPLES

We first consider illustrative one-dimensional "toy" problems, to examine the GO gradient in Theorem 1 for both continuous and discrete random variables.

The optimization objective is expressed as

max


ELBO()

=

Eq(z)[log

p(z|x)

-

log

q(z)]

+

log

p(x),

where for continuous z we assume p(z|x) = Gam(z; 0, 0) for set (0, 0), with q(z) = Gam(z; , ) and  = {, }; for discrete z we assume p(z|x) = NB(z; r0, p0) for set (r0, p0), with q(z) = NB(z; r, p) and  = {r, p}. Stochastic gradient ascent with one-sample-estimated gradients is used to optimize the objective, which is equivalent to minimizing KL(q(z) p(z|x)).
Figure 8 shows the experimental results. For the nonnegative continuous z associated with the gamma distribution, we compare our GO gradient with GRep (Ruiz et al., 2016), RSVI (Naesseth et al., 2016), and their modified version using the "sticking" idea (Roeder et al., 2017), denoted as GRep-Stick and RSVI-Stick respectively. For RSVI and RSVI-Stick, the shape augmentation parameter is set as 5 by default. The only difference between GRep and GRep-Stick (also RSVI and RSVI-Stick) is the latter does NOT analytically express the entropy Eq(z)[- log q(z)]. One should apply sticking because (i) Figures 8(a)-8(c) clearly show its utility in reducing variance; and (ii) without it, GRep and RSVI exhibit high variance that unstabilizes the optimization for small gamma shape parameters, as shown in Figures 8(d)-8(f). We adopt the sticking approach hereafter. Since the gamma rate parameter  is reparameterizable, its gradient calculation is the same for all sticking methods, including GO, GRep-Stick, and RSVI-Stick. Therefore, similar variances are observed in Figures 8(b) and 8(e). Among methods with sticking, GO exhibits the lowest variance in general, as shown in Figures 8(a) and 8(d). Note it is high variance that causes optimization issues. As a result, GO empirically provides more stable learning curves, as shown in Figures 8(c) and 8(f). For the discrete case corresponding to the NB distribution, GO is compared to REINFORCE (Williams, 1992). It is apparent from Figures 8(g)-8(l) that, thanks to analytically removing the "0" terms, the GO gradient has much lower variance and thus faster convergence, even in this simple one-dimensional case.

I DETAILS OF THE DISCRETE VAE EXPERIMENT

Complementing the discrete VAE experiment of the main manuscript, we present below its experimental settings, implementary details, and additional results.

Since the presented statistical back-propagation in Theorem 3 of the main manuscript cannot handle discrete internal variables, we focus on the single-latent-layer settings (1 layer of 200 Bernoulli random variables) for fairness, i.e.,

p(x, z) : x  Bern NNP x|z (z) , z  Bern P z q(z|x) : z  Bern NNP z|x (x) .

(32)

Referring to the experimental settings in Grathwohl et al. (2017), we consider

21

Under review as a conference paper at ICLR 2019

Grad-Variance

Grad-Variance

102 100 10-2
0

GRep RSVI GRep-Stick RSVI-Stick GO
200 400 600 800 1000
Iteration
(a)

Grad-Variance

102 GRep RSVI GRep-Stick RSVI-Stick
100 GO
10-2
0 200 400 600 800 1000
Iteration
(b)

ELBO

0

-0.05

-0.1
-0.15
-0.2 0

GRep RSVI GRep-Stick RSVI-Stick GO
200 400 600 800 1000
Iteration
(c)

105 GRep
RSVI
GRep-Stick
RSVI-Stick 100 GO

10-5

10-10 0

200 400 600 800 1000
Iteration
(d)

Grad-Variance

1020 1010 100 10-10 10-20
0

GRep RSVI GRep-Stick RSVI-Stick GO
200 400 600 800 1000
Iteration
(e)

ELBO

0
-0.05
-0.1
-0.15
-0.2 0

10-3 -2

-4

-6

GRep

-8 400 600 800 1000

RSVI

GRep-Stick

RSVI-Stick

GO

200 400 600 800 1000
Iteration
(f)

12 REINFORCE
10 GO
8
6
4
2
0 0 200 400 600 800 1000
Iteration
(g)

p Grad-Variance

30 REINFORCE
25 GO
20
15
10
5
0 0 200 400 600 800 1000
Iteration
(h)

ELBO

0 -0.2 -0.4 -0.6 -0.8
0

REINFORCE GO
200 400 600 800 1000
Iteration
(i)

12

REINFORCE

REINFORCE

0

GO GO 0.8 1.5 -0.05

p Grad-Variance

ELBO

0.6 -0.1

1 -0.15
0.4 -0.2

0.5 0.2 -0.25
REINFORCE

0 0 200 400 600 800 1000

0 0 200 400 600 800 1000

-0.3 0

GO 200 400 600 800 1000

Iteration

Iteration

Iteration

(j) (k) (l)

r Grad-Variance

r Grad-Variance

Figure 8: Gamma (a-f) and NB (g-l) toy experimental results. Columns show the gradient variance for the first parameter (gamma  or NB r), that for the second parameter (gamma  or NB p), and the ELBO, respectively. The first two rows correspond to the gamma toys with posterior parameters 0 = 1, 0 = 0.5 and 0 = 0.01, 0 = 0.5, respectively. The last two rows show NB toy results with r0 = 10, p0 = 0.2 and r0 = 0.5, p0 = 0.2, respectively. In each iteration, gradient variances are estimated with 20 Monte Carlo samples (each sample corresponds to one gradient estimate), among which the last one is used to update parameters. 100 Monte Carlo samples are used to calculate the ELBO in the NB toys.

· 1-layer linear model:

NNP x|z (z) = (WpT z + bp)

NNP z|x (x) = (WqT x + bq)

where (·) is the sigmoid function.

· Nonlinear model:

NNP x|z (z) = (WpT2hp(2)+bp2), h(p2) = tanh(WpT1hp(1)+bp1), hp(1) = tanh(WpT z+bp)

NNP z|x (x) = (WqT2h(q2)+bq2), hq(2) = tanh(WqT1hq(1)+bq1), hq(1) = tanh(WqT x+bq) where tanh(·) is the hyperbolic-tangent function.

22

Under review as a conference paper at ICLR 2019

The used datasets and other experimental settings, including the hyperparameter search strategy, are the same as those in Grathwohl et al. (2017).

For such single-latent-layer settings, it is obvious that Theorem 3 (also Theorem 1) can be straightforwardly applied. However, since Bernoulli distribution has finite support, as mentioned in the main manuscript, we should analytically express some expectations for lower variance, as detailed below. Notations of (8) and (9) of the main manuscript are used for clarity and also for generalization.

In fact, we should take a step back and start from (8) of the main manuscript, which is equivalent to analytically express an expectation in (9), namely

 Eq (y)[f (y)] = Ev q (y-v) - yv [ Q (yv)][f (y-v, yv + 1) - f (y)] ,

(33)

where Q (yv) =

1 - Pv() 1

yv yv

= =

0 1

with

Pv ( )

being

the

Bernoulli

probability

of

Bernoulli

random variable yv. Accordingly, we have

 Eq (y)[f (y)] = - Ev q (y-v) [ Q (yv = 0)][f (y-v, yv = 1) - f (y-v, yv = 0)]

= Ev q (y-v) [ Pv()][f (y-v, yv = 1) - f (y-v, yv = 0)]

= Eq (y)

v[ Pv()][f (y-v, yv = 1) - f (y-v, yv = 0)]

(34)

For better understanding only, with abused notations  P = [· · · ,  Pv(), · · · ]T , P y = I,

and yf (y) = [· · · , {f (y-v, yv = 1) - f (y-v, yv = 0)}, · · · ]T , one should observe a chain rule

within the above equation.

To assist better understanding of how to practically cooperate the presented GO gradients with deep learning frameworks like TensorFlow or PyTorch, we take (34) as an example, and present for it the following simple algorithm.

Algorithm 1 An algorithm for (34) as an example to demonstrate how to practically cooperate GO gradients with deep learning frameworks like TensorFlow or PyTorch. One sample is assumed for clarity. Practically, an easy-to-use trick for changing gradients of any function h(x) is to define h^(x) = xT StopGradient[g] + StopGradient[h(x) - xT g] with g the desired gradients.

# Forward-Propagation

1.   P (): Calculate Bernoulli probabilities P ( )

2. P ()  y: Sample y  Bern(P ())

Change gradient: P ()y = I

3. y  f (y): Calculate the loss f (y)

# Back-Propagation

Change gradient: [yf (y)]v = f (y-v, yv = 1) - f (y-v, yv = 0)

1. Rely on the mature auto-differential software for back-propagating gradients

For efficient implementation of Dyf (y), one should exploit the prior knowledge of function f (y). For example, f (y)s are often neural-network-parameterized. Under that settings, one should be able
to exploit tensor operation to design efficient implementation of Dyf (y). Again we take (34) as an example, and assume f (y) has the special structure

f (y) = r(T h + c), h = (WT y + b)

(35)

where (·) is an element-wise nonlinear activation function, and r(·) is a function that takes in a vector and outputs a scalar. One can easily modify the above f (y) for the considered discrete VAE experiment.

Since yvs are now Bernoulli random variables with support {0, 1}, we have that

Dyf (y) v = f (y-v, yv = 1) - f (y-v, yv = 0)

=

f (y-v, yv + 1) - f (y) f (y) - f (y-v, yv - 1)

yv = 0 yv = 1

= av[f (y-v, yv + av) - f (y)],

(36)

23

Under review as a conference paper at ICLR 2019

1 where av = - 1

yv = 0 is the v-th element of vector a. yv = 1

ELBO

ELBO

-110

-115

-120

REBAR train

REBAR valid

-125

RELAX train RELAX valid

GO train

GO valid

-130 0 2 4 6 8 10

Iteration

105

(a) MNIST Linear Iteration

-80

-100

-120

REBAR train

REBAR valid

-140

RELAX train RELAX valid

GO train

GO valid

-160 0 2 4 6 8 10

Iteration

105

(c) MNIST Nonlinear Iteration

-120

-125

-130

-135

REBAR train

-140

REBAR valid RELAX train

-145

RELAX valid GO train

GO valid

-150 0 2 4 6 8 10

Iteration

105

(e) Omniglot Linear Iteration

-50

-100

-150

-200 -250 -300

-125 -130 -135 -140

1

-350 0

2

23 105
46
Iteration

REBAR train REBAR valid RELAX train RELAX valid GO train GO valid
8 10 105

(g) Omniglot Nonlinear Iteration

ELBO

ELBO

-110

-115

-120

-125
-130
-135 0

REBAR train REBAR valid RELAX train RELAX valid GO train GO valid
2000 4000 6000 8000
Time (seconds)

(b) MNIST Linear Running-Time

-80

-90

-100

-110

-120
-130
-140
-150 0

REBAR train REBAR valid RELAX train RELAX valid GO train GO valid
2000 4000 6000 8000 10000
Time (seconds)

(d) MNIST Nonlinear Running-Time

-120

-130

ELBO

-140
-150
-160 0

REBAR train REBAR valid RELAX train RELAX valid GO train GO valid
2000 4000 6000 8000
Time (seconds)

(f) Omniglot Linear Running-Time

-50

-100

ELBO

-150

-200 -250

-125 -130 -135 -140 -145

500

1500 2500

REBAR train REBAR valid RELAX train RELAX valid GO train GO valid

-300 0

2000 4000 6000 8000

Time (seconds)

(h) Omniglot Nonlinear Running-Time

ELBO

ELBO

Figure 9: Training/Validation ELBOs for the discrete VAE experiments. Rows correspond to the experimental results on the MNIST/Omniglot dataset with the 1-layer-linear/nonlinear model, respectively. Shown in the first/second column is the ELBO curves as a function of iteration/running-time. All methods are run with the same learning rate for 1, 000, 000 iterations. The black line represents the best training ELBO of REBAR and RELAX. ELBOs are calculated using all training/validation data. Note GO does not suffer more from over-fitting, as clarified in the text.

24

Under review as a conference paper at ICLR 2019

Table 4: Average running time per 100 iterations for discrete variational autoencoders. Results of REBAR and RELAX are obtained by running the released code2 from Grathwohl et al. (2017). The same computer with one
Titan Xp GPU is used.

Dataset MNIST Omniglot

Model
Linear 1 layer Nonlinear
Linear 1 layer Nonlinear

REBAR
0.94s 0.99s
0.89s 0.97s

RELAX
0.95s 1.02s
0.92s 0.98s

GO
0.25s 0.54s
0.21s 0.45s

Then to efficiently calculate the f (y-v, yv + av)s, we use the following batch processing procedure to benefit from parallel computing.
· Step 1: Define yh as the matrix whose element [yh]vj represents the "new" hj when input {y-v, yv + av} in (35). Then, we have
[yh]vj = (yT W:j + bj + avWvj )
where W:j is the j-th column of the matrix W. Note the vth row of yh, i.e., [yh]v:, happens to be the "new" h when input {y-v, yv + av}. · Step 2: Similarly, we define yf as the vector whose element [yf ]v = f (y-v, yv + av). Utilizing yh obtained in Step 1, we have
yf = r([yh] + cT ),
where r(·) is applied to each row of the matrix ([yh] + cT ).
Note the above batch processing procedure can be easily extended to deeper neural networks. Accordingly, we have
Dyf (y) = a [yf - f (y)],
where represents the matrix element-wise product.
Now we can rely on Algorithm 1 to solve the problem whose objective has its gradient expressed as (34), for example the inference of the single-latent-layer discrete VAE in (32).
All training curves versus iteration/running-time are given in Figure 9, where it is apparent that GO provides better performance, a faster convergence rate, and a better running efficiency in all situations. The average running time per 100 iterations for the compared methods are given in Table 4, where GO is 2 - 4 times faster in finishing the same number of training iterations. We also quantify the running efficiency of GO by considering its running time to achieve the best training ELBO (within 1, 000, 000 training iterations) of RERAR/RELAX, referring to the black lines shown in the second-column subfigures of Figure 9. It is clear that GO is approximately 5 - 10 times more efficient than REBAR/RELAX in the considered experiments.
As shown in the second and fourth rows of Figures 9, for the experiments with nonlinear models all methods suffer from over-fitting, which originates from the redundant complexity of the adopted neural networks and appeals for model regularizations. We detailedly clarify these experimental results as follows.
· All the compared methods are given the same and only objective, namely to maximize the training ELBO on the same training dataset with the same model; GO clearly shows its power in achieving a better objective.
· The "level" of over-fitting is ultimately determined by the used dataset, model, and objective; it is independent of the adopted optimization method. Different optimization methods just reveal different optimizing trajectories, which show different sequences of training objectives and over-fitting levels (validation objectives).
2github.com/duvenaud/relax
25

Under review as a conference paper at ICLR 2019
· Since all methods are given the same dataset, model, and objective, they have the same over-fitting level. Because GO has a lower variance, and thus more powerful optimization capacity, it gets to the similar situations much faster than REBAR/RELAX. Note this does not mean GO suffers more from over-fitting. In fact, GO provides better validation ELBOs in all situations, as shown in Figure 9 and also Table 1 of the main manuscript. In practice, GO can benefit from the early-stopping trick to get a better generalization ability.
J DETAILS OF THE MULTINOMIAL GAN
Complementing the multinomial GAN experiment in the main manuscript, we present more details as follows. For a quantitative assessment of the computational complexity, our PyTorch code takes about 30 minutes to get the most challenging 4-bit task in Figs. 4 and 11, with a Titan Xp GPU. Firstly, recall that the generator p(x) of the developed multinomial GAN (denoted as MNGAN-GO) is expressed as
 N (0, I), x  Mult(1, NNP( )), where NNP( ) denotes use of a neural network to project to distribution parameters P. For brevity, we integration the generator's parameters  into the NN notation and do not explicitly express them. Multinomial leaf variables x is used to describe discrete observations with a finite alphabet. To train MNGAN-GO, the vanilla GAN loss (Goodfellow et al., 2014) is used. A deconvolutional neural network as in Radford et al. (2015) is used to map to P in the generator. The discriminator is constructed as a multilayer perceptron. Detailed model architectures are given in Table 5. Figure 10 illustrates the pipeline of MNGAN-GO. Note MNGAN-GO has a smaller number of parameters, compared to BGAN (Hjelm et al., 2018). For clarity, we briefly discuss the employed data preprocessing. Taking MNIST for an example, the original data are 8-bit grayscale images, with pixel intensities ranging from 0 to 255. For the n-bit experiment, we obtain the real data, such as the 2-bit one in Figure 10, by rescaling and quantizing the pixel intensities to the range [0, 2n - 1], having 2n different states (values). For the 1-bit special case, the multinomial distribution reduces to the Bernoulli distribution. Of course, one could intuitively employ the redundant multinomial distribution, which is denoted as 1-bit (2-state) in Table 2. An alternative and popular approach is to adopt the Bernoulli distribution to remove the redundancy by only modeling its probability parameters; we denote this case as 1-bit (1-state, Bernoulli) in Table 2. Figure 11 shows the generated samples from the compared models on different quantized MNIST. It is obvious that MNGAN-GO provides images with better quality and wider diversity in general.
Figure 10: Illustration of MNGAN-GO.
K DETAILS OF HVI FOR DEF AND DLDA
Complementing the HVI experiments in the main manuscript, we present more details as follows.
26

Under review as a conference paper at ICLR 2019

Table 5: Model Architectures of the Multinomial GAN.

Generator
Gaussian noise (100 dimension) 4 × 4 conv. 256 lReLU, stride 1, zero-pad 0, BN 4 × 4 conv. 128 lReLU, stride 2, zero-pad 1, BN 4 × 4 conv. 64 lReLU, stride 2, zero-pad 2, BN 4 × 4 conv. 2bit SoftMax, stride 2, zero-pad 1, BN
Multinomial Sampling. Output: 28 × 28

Discriminator
Input Image Linear output 512, lReLU, SN Linear output 256, lReLU, SN Linear output 128, lReLU, SN
Linear output 1, Sigmoid

Table 6: Inception scores on quantized MNIST. BGAN's results are ran with the author released code https: //github.com/rdevon/BGAN.

bits (states) 1 (1, Bernoulli)
1 (2) 2 (4) 3 (8) 4 (16)

BGAN 8.31 ± .06 8.56 ± .04 7.76 ± .04 7.26 ± .03 6.29 ± .05

MNGAN-GO 9.10 ± .06 8.40 ± .07 9.02 ± .06 9.26 ± .07 9.27 ± .06

For a quantitative assessment of the computational complexity, the TensorFlow code used takes about 0.08 seconds per iteration (including > 40, 000 Meijer-G function calculations, with an approximate algorithm coded also with TensorFlow).

Deep exponential families (DEF) (Ranganath et al., 2015) is expressed as

x  Pois(W(1)z(1)), z(l)  Gam  , /z z W(l+1)z(l+1) , W(l)  Gam(0, 0),

where z = 0.1, 0 = 0.3, and 0 = 0.1 following Ruiz et al. (2016); Naesseth et al. (2016). Since the variables of interest, W(l) and z(l), are gamma-distributed, we design the variational
approximations as

qz (z|x) : z(1)  Gam NN(1)(x), NN(1)(x) , z(l)  Gam NN(l)(z(l-1)), NN(l)(z(l-1)) ,
qW (W) : W(l)  Gam W(l) , (Wl) ,
where NN(l)(·)s are set to the same shapes as the corresponding z(l), (Wl) and W(l) also have the shape of W(l). We employ a simple two-layer DEF for demonstration, with z(1) and z(2) having 128 and 64 components, respectively. The mini-batch size is set to 200. One-sample gradient estimates are used to train the model for the compared methods. For RSVI (Naesseth et al., 2016), the shape augmentation parameter B is set to 5. All experimental settings, except for different ways to calculate gradients, are the same for the compared methods.

Deep latent Dirichlet allocation (DLDA) (Zhou et al., 2015b;a; Cong et al., 2017) is expressed as

x Pois((1)z(1)), z(l)  Gam (l+1)z(l+1), c(l+1) , z(L)  Gam r, c(L+1) , (l)  Dir(0), c(l)  Gam(e0, f0), r  Gam(0/K, c0),

(37)

where hyperparameters are chosen following Cong et al. (2017). Compared to DEF, DLDA is more complicated: 1) model is constructed via the highly nonlinear Gamma shape parameters; 2) dictionaries are described by more challenging Dirichlet distributions; 3) more interested random variables.

The variational approximations for DLDA are designed as

q(z, c|x) :z(1)  Gam NN(1z)(x), NN(1z)(x) , c(2)  Gam NN(2c)(z(1)), NN(2c)(z(1)) , z(l)  Gam NN(l)z (z(l-1)), NN(lz) (z(l-1)) , c(l+1)  Gam NN(l+c 1)(z(l)), NN(lc+1)(z(l)) ,
q() :(l)  Dir((l)), q(r) :r  Gam(r, r),

27

Under review as a conference paper at ICLR 2019
(a) (b) (c) (d) (e)
(f) (g) (h) (i) (j) Figure 11: Generated images from BGAN (top) and MNGAN-GO (bottom). Columns correspond to 1bit(Bernoulli), 1-bit, 2-bit, 3-bit, 4-bit tasks, respectively. where motivated by the original upward-downward Gibbs sampler developed in Zhou et al. (2015b), we specify NN(lz) (·) as a scaler to mimic the Gibbs conditional posteriors. The other NN(·)s and also (l), r, r have the same shapes of the corresponding random variables. A three-layer DLDA, having 128, 64, 32 components for latent z(1), z(2), z(3) respectively, is trained on MNIST with mini-batches of size 200. With the learned variational inference nets, one could efficiently project the observed x to its latent variables z, c during testing. For applications requiring realtime processing, this is a clear advantage. For demonstration, Figure 12 shows test data samples x and their reconstruction
x^ = (1)z^(1), z^(1)  Gam NN(1z)(x), NN(1z)(x) , where one Monte Carlo sample is used to calculate z^(1).
(a) (b) Figure 12: (a) Test data samples and (b) their reconstruction via the learned 128-64-32 DLDA.
28

Under review as a conference paper at ICLR 2019 Note for the challenging DLDA task in (37), we find it tricky to naively apply pure-gradient-based learning methods. The main reason is: the latent code z(l)s and their gamma shape parameters (l+1)z(l+1)s are usually extremely sparse, meaning most elements are almost zero; a gamma distribution z  Gam(, ) with almost-zero  has an increasingly steep slope when z approaches zero, namely the gradient wrt z shall have an enormous magnitude that unstablize the learning procedure. Even though it might not be sufficient to just use the first-order gradient information, empirically the following tricks help us get the presented reasonable results.
· Let z(l)  Tz, where Tz = 1e-5 is used in the experiments; · Let c(l)  Tc, where Tc = 1e-5; · Let (l+1)z(l+1)  T with T = 0.2; · Use a factor to compromise the likelihood and prior for each z(l). For more details, please refer to our released code. We are working on exploiting higher-order information (such as Hessian) to help remedy this issue.
29

