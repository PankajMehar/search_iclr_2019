Under review as a conference paper at ICLR 2019
PROVABLE DEFENSES AGAINST SPATIALLY TRANSFORMED ADVERSARIAL INPUTS: IMPOSSIBILITY AND POSSIBILITY RESULTS
Anonymous authors Paper under double-blind review
ABSTRACT
One intriguing property of neural networks is their inherent vulnerability to adversarial inputs, which are maliciously crafted samples to trigger target networks to misbehave. The state-of-the-art attacks generate adversarial inputs using either pixel perturbation or spatial transformation. Thus far, several provable defenses have been proposed against pixel perturbation-based attacks; yet, little is known about whether such solutions exist for spatial transformation-based attacks. This paper bridges this striking gap by conducting the first systematic study on provable defenses against spatially transformed adversarial inputs. Our findings convey mixed messages. On the impossibility side, we show that such defenses may not exist in practice: for any given networks, it is possible to find legitimate inputs and imperceptible transformations to generate adversarial inputs that force arbitrarily large errors. On the possibility side, we show that it is still feasible to construct adversarial training methods to significantly improve the resilience of networks against adversarial inputs over empirical datasets. We believe our findings provide insights for designing more effective defenses against spatially transformed adversarial inputs.
1 INTRODUCTION
Despite their tremendous success in computer vision and pattern recognition (LeCun et al., 2015), neural networks are inherently vulnerable to adversarial inputs ­ those maliciously crafted samples to trigger target networks to misbehave ­ which hinders their application in security-critical domains (Szegedy et al., 2014; Goodfellow et al., 2015; Sharif et al., 2016; Eykholt et al., 2017). This serious vulnerability has spurred intensive research effort, leading to an arms race between developing more powerful attacks (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2015; Papernot et al., 2016a; Carlini & Wagner, 2017; Meng & Chen, 2017; Xu et al., 2018) and designing more robust defenses (Papernot et al., 2016b; Madry et al., 2018; Trame`r et al., 2018; Kannan et al., 2018). For example, defensive distillation (Papernot et al., 2016b), originally considered as an effective defense, was later shown to be ineffective against stronger attacks (Carlini & Wagner, 2017).
To end this constant arms race, several provable defense methods (Bastani et al., 2016; Raghunathan et al., 2018; Wong & Zico Kolter, 2018) have been proposed recently. Motivated by that most existing attacks generate adversarial inputs by directly modifying the pixel values of benign inputs, where the perturbation "imperceptibility" is often quantified by its Lp norm, such provable defenses, by reducing the upper bound of the worst-case loss that can be caused by norm-bounded perturbation, provides the following guaranteed protection: for a given network and test input, no attack is able to force the error to exceed a certain threshold.
The effectiveness of these provable defenses hinges on modeling the upper bound of the worstcase loss under norm-bounded perturbation. While most existing attacks indeed use Lp norm to quantify the perturbation imperceptibility, it has long been recognized that this is not an ideal metric (Johnson et al., 2016; Isola et al., 2016). Recently, spatial transformation has been proposed as a new approach for generating adversarial inputs (Xiao et al., 2018; Engstrom et al., 2017b): instead of directly modifying pixel values, it changes their spatial positions (e.g., via rotation and shifting), which is shown to better preserve the identity and structure of the original image (Zhou et al., 2016). It is observed that spatial transformation-based adversarial inputs are often perceptually less distinguishable from pixel perturbation-based counterparts, as shown in Figure1. However, for spatial transformation, it is insensible to measure the perturbation imperceptibility using Lp norm; rather, alternative metrics such as the total variation (Rudin et al., 1992) are used to quantify
1

Under review as a conference paper at ICLR 2019

(a) (b) (c) (d)

Figure 1: (a) benign input, (b) pixel perturbation-based adversarial input, (c) spatial transformationbased adversarial input, and (d) flow vector of spatial transformation.
the magnitude of spatial deformation. In other words, spatially transformed adversarial inputs are inherently not norm-bounded, which thus raises an important and intriguing question:
Does there exist provable defenses that provide a given network with guaranteed protection against spatially transformed adversarial inputs?
In this paper, we present a systematic investigation to answer this question. Under the setting of a neural network with one hidden layer, we conduct both analytical and empirical studies on the existence of provable defenses. Our findings convey mixed messages. The impossibility results indicate that such defenses may not exist in practice. We show that for any given networks, it is possible to find benign inputs and imperceptible transformations to produce adversarial inputs that force arbitrarily large errors. Meanwhile, the possibility results imply that while the worst case can be arbitrarily bad, it is still possible to construct adversarial training methods to significantly improve the robustness of given networks over empirical datasets.
In summary, to our best knowledge, this work represents the first in-depth study on provable defenses against spatial transformation-based adversarial attacks. We provide both impossibility and possibility results regarding the existence of such defenses. We believe that our findings will highlight the fundamental difference of spatial transformation- and pixel perturbation-based attacks from the defender's perspective, and inspire designing more effective defenses against spatially transformed adversarial inputs.

2 BACKGROUND

2.1 SPATIAL TRANSFORMATION

We model a network as a function f . For simplicity, we focus on a binary classification task (`+' and `-') (the extension to the multi-class setting given in the appendix), in which the network predicts the class of a given input x as f (x) > 0 as `+' and f (x) < 0 as `-'. In adversarial attacks, the adversary crafts an adversarial input x~ by applying perturbation r to a benign input x: x~ = g(r; x), with the objective of forcing x~ to be misclassified. To maximize the attack evasiveness, the adversary also desires to preserve x's perceptual quality in x~.
According to their perturbation types, existing adversarial attacks can be categorized as either pixel perturbation-based attacks or spatial transformation-based attacks. Figure 1 compares two versions of adversarial inputs with respect to the same benign input.
In spatial transformation attacks, instead of directly modifying x's pixel values, the adversary applies a spatial transformation (e.g., shifting and rotation) over x to generate x~. Specifically, we use the per-pixel flow field (displacement) r to synthesize x~ using the pixels of x. Let x~i be x~'s i-th pixel and (u~i, v~i) be its position in x~. We optimize the amount of displacement with a flow vector ri := (ui, vi), which goes from x~ to x. The position of x~i's corresponding pixel in x is derived as: (ui, vi) = (u~i + ui, v~i + vi). As (ui, vi) may be fractional numbers and do not necessarily lie on the integer grid, we use the differentiable bilinear interpolation to compute x~i's pixel value:

x~i =

xj max (0, 1 - |u~i + ui - uj|) max (0, 1 - |v~i + vi - vj|)

jN (i)

(1)

2

Under review as a conference paper at ICLR 2019

where j iterates over N (i): the set of pixels adjacent to (ui, vi) (top-left, top-right, bottom-left, and bottom-right) in x. Under this setting, diff can be instantiated with the total variation Rudin et al.
(1992), which compares the spatial movement distance for any two adjacent pixels:

diff (r) =

(||ui - uj||22 + ||vi - vj||22)
i jN (i)

(2)

where i iterates over all the pixels.

We now derive its matrix form. Let r  Rd×2: r = [r(u), r(v)] be the flow field, where r(u) and r(v)
respectively represent the transformation along the horizontal and vertical directions. We assume r(u) = r(v) and will justify this assumption in § 3. With a little abuse of notations, let r = r(u) = r(v). Let M (t) be the top neighbor matrix, with the i-th row being the top neighbor j of the i-th pixel (i.e., the one-hot encoding of j). Similarly, we define M (b), M (l), and M (r) respectively as
the bottom, left, and right neighbor matrices. We have the proposition (proof in appendix):

Proposition 1. The magnitude of spatial transformation can be specified as: 
diff (r) = 2r M M r

where M M = i{t,b,l,r} I - M (i) I - M (i) .

Therefore, the constraint on the spatial transformation magnitude can be given as: diff (r) = M r 

(3)

2.2 ADVERSARIAL LOSS

Let g(r; x) be the adversarial attack that applies the transformation r to x to generate the adversarial input x~, i.e., x~ = g(r; x) and x = g(0; x). Therefore, for a given benign input x, we define the margin between f (x~) and f (x) as the adversarial loss. Formally,

adv(x, r)

d=ef

f (x~) - f (x) 2

=

f

 g(r; x) - f (x) 2

Note that adv(x, r) is a function of r.

In the case of bilinear interpolation as defined in Eqn (1), g is differentiable with respect to r. Let

gj be the j-th element of g and ri be r's i-th element. Note that in Eqn (1), x~i only depends on ri.

Thus,

gj ri

= 0 for i = j.

Meanwhile,

gi ri

=

j

xj max (0, 1 - |v~i + ri - vj|)

0 |u~i + ri - uj|  1 1 u~i + ri  uj -1 u~i + ri < uj

where sub-gradients are used (Jaderberg et al., 2015) due to the discontinuities of interpolation.

The

Jacobian

Dg

of

g

is

a

diagonal

matrix

with

its

i-th

diagonal

element

given

by

 

gi ri

.

Further,

the

gradient of f with respect to g is given as: f =

f g1

,

f g2

,

.

.

.

,

f gd

.

Putting everything together using the chain rule, we have the gradient of f  g with respect to r as:

(f  g)(r; x) = (Dg) f

We now compute adv(r; x) using the integration along the line from x to x~ = f  g(r; x):
1
adv(x, r) = ((f  g)(zr; x)) rdz
0

(4)

3 IMPOSSIBILITY RESULTS

Next we present the negative results for provable defenses against spatially transformed adversarial inputs. Intuitively, we show that given a classifier f and a threshold  > 0, if the perturbation magnitude is allowed to be reasonably large, the adversary is able to find an input x and its corresponding

3

Under review as a conference paper at ICLR 2019

spatial transformation r such that the adversarial input causes f to misclassify the adversary input g(r; x) with a large margin adv(x, r)  .
Specifically, we consider the maximum influence of spatial transformation on the adversarial loss via the following supremum:
sup adv(x, r)
x 1, M r 
We then find a particular input-transformation pair (x, r) such that f g(r; x)-f (x) represents a lower bound of this supremum. Apparently, if f  g(r; x) - f (x)  2, the supremum of adv(x, r) must exceed . Next we present the concrete attack for a network with one hidden layer.

3.1 LOWER BOUND ATTACK

For a network with one hidden layer, we have f (x) = v (W x) where W and v respectively

represent the weights of the first and second layers of the network, and  is the activation function

(e.g., ReLU). In this case, we have f = W

diag(v), where ()i

=

  (yi ) yi

with yi

=

(W g(r; x))i. Substituting it into Eqn (4), we have:

1
adv(x, r) = () diag(v)W Dg(zr)rdz
0

(5)

We consider a linear approximation of adv(x, r):

~adv(x, r) = ((W x)) diag(v)W diag(x)r

The following theorem finds a lower bound of the supremum of ~adv(x, r) (proof in appendix). Theorem 1. For a given network f , assuming W has rank d and Wd is a rank-d submatrix of W ,

sup ~adv(x, r)  M -1 diag(x)W diag(v)(W x)
x 1, M r 

where x = (Wd-11) (1 is an all-one vector). The lower bound is attained when

r = M -12(M -1 diag(x)W diag(v)(W x)).

(6)

Theorem 1 gives the initial setting of (x, r). We further increase the adversarial loss by improving

both

x

and

r.

Let

r(i)

=

(2i-1) 2n

r.

We

compute

the

midpoint

approximation

of

Eqn (5):

adv(x, r)



1 n

n



Wg

r(i); x

diag(v)W Dg r(i); x r

i=1

(7)

Summation
where we divide the interval [0, 1] into n intervals.

Updating x To update x, we consider x as a symbolic vector. Yet, it is difficult to optimize Eqn (7) with respect to x directly as  also depends on x. Instead, we update  and x alternatively.

Let xk be the current setting of x. We update x as follows. Let i be the coefficient of x's i-th element in Eqn (7) with  computed based on xk. We set xk+1 = [sign(1), sign(2), . . . , sign(d)] , which maximizes Eqn (7) under fixed r, fixed  and the constraint of x   1. We repeat this
procedure until convergence.

Updating r Meanwhile, for fixed x, we may also increase the adversarial loss by refining r using the integration approximation. Let rk be the current setting of r. We update r using the rule of:

rk+1 = M -12

n

M -1

Dg

rk(i); x

W diag(v)

Wg

r(i) k

;

x

i=1

(8)

Intuitively, we find rk+1 that aligns with the summation in Eqn (7) under M rk+1  .

Algorithm 1 sketches the attack against a given network. After initialization (line 1-3), it updates x and r alternatively until convergence (line 4-8).

4

Under review as a conference paper at ICLR 2019

Algorithm 1: Lower Bound Attack.
Input: v, W : f 's weights; : threshold of perturbation magnitude; M : neighboring matrix; : update step; n: parameter of midpoint approximation
Output: x: genuine input; r: spatial transformation // initialization 1 Wd  rank-d submartrix of W ; 2 x  (Wd-11); 3 initialize r according to Eqn (6); 4 while not converged do
// refinement of x 5 while not converged do 6 compute Eqn (7) with symbolic x;
// i: coefficient of x's i-th element in Eqn (7) 7 x  [sign(1), sign(2), . . . , sign(d)] ;
// refinement of r 8 update r according to Eqn (8);
9 return (x, r);
3.2 EMPIRICAL EVALUATION

Adversarial loss

Next we empirically validate the above analytical result. We mainly use MNIST (LeCun et al., 1998) as the benchmark dataset and a network with one hidden layer (with 512 hidden neurons) as the target model f . We consider three variants of f : (i) fnt ­ f is normally trained, (ii) frd ­ f is randomly initialized, and (iii) fat ­ f is adversarially trained (Madry et al., 2018).
100 (a) 300 (b) 60 (c)
80 250 50
200 40 60
150 30 40
100 20
20 50 10
00 2 4 6 8 0 0 2 4 6 8 00 2 4 6 8 Transformation threshold
Figure 2: Lower bounds of adversarial loss by Algorithm 1 (w.r.t. adv) and Theorem 1 (w.r.t. ~adv).

Benign Input

Adversarial Input

Flow Vector

Figure 3: Samples of benign and adversarial inputs found by Algorithm 1. 5

Under review as a conference paper at ICLR 2019

(a) 100

(b) 300

(c) 50

Adversarial loss

80 250 40

200 60 30

150

0 0 5

40 20 100

20 50 10

0 0

5 10 15 20 25 00

5 10 15 20 25 00

5 10 15 20 25

Iteration

Figure 4: Convergence of estimation of adversarial loss by Algorithm 1.

We first measure the quality of lower bounds given by Algorithm 1. With respect to each of the
networks, under different setting of , we measure the estimation by Algorithm 1 (with respect to adv) and that by Theorem 1 (with respect to ~adv). Figure 2 shows the results. It is clear that as varies from 0 to 7.84, the adversarial loss increases in an exponential manner, indicating that with reasonably transformation magnitude, it is possible to find (x, r) that forces arbitrarily large error. Also note that Algorithm 1 provides much higher-quality estimation of adversarial loss compared
with the initial estimation by Theorem 1. Figure 3 shows a set of samples of benign and adversarial
inputs (and their associated flow fields).

Figure4 shows that Algorithm 1 typically converges fast under varied setting of network and , which validates our analysis of the performance of Algorithm 1.

4 POSSIBILITY RESULTS

Despite the impossibility results for provable defenses against spatial transformation-based attacks, in this section, we show that it is still possible to construct adversarial training methods that significantly improve DNN resilience on empirical datasets.

4.1 ADVERSARIAL TRAINING

To this end, we establish an upper bound on adv(x, r) and reduce this upper bound in training f to improve its robustness. Specifically, we consider the integration form of adv(x, r) in Eqn (4).

adv(x, r) =

1
((f  g)(zr)) rdz
0

 max ((f  g)(zr; x)) r
z[0,1], M r 

In the following, we derive its upper bound form for a network with one hidden layer.

We have the following derivation regarding adv(x, r):

adv(x, r)



max () diag(v)W Dgr
,Dg,r

(i)
 max s diag(v)W Dgr
||s|| 1,Dg ,r

(ii)

 2 max

s diag(v)W diag()t

||s|| 1,||t|| 1

In (i), we use the assumption that the non-linear activation function applied element-wise has bounded gradients   [0, 1], which holds in many settings (e.g., for ReLU,   [0, 1]; for sigmoid,   [0, 1/4]). In (ii), we use the following results. As M r = defines an ellipsoid E centered at 0, diag(z)r with z   1 corresponds to the union of the set of ellipsoids mirrored to E along the axises. We can thus bound diag(z)r using the axis aligned bounding box (AABB) of these ellipsoids. We have the following proposition (proof in appendix):
Proposition 2. The AABB of the ellipsoid defined by M r = is given by [- i, i] (1  i  d), where 2i is the i-th diagonal element of the matrix (M M )-1.

6

Under review as a conference paper at ICLR 2019

Because diag(v)W diag() is not necessarily positive (negative) semidefinite, the above formulation is in general a non-convex optimization problem, which is similar to the NP-hard MAXCUT problem. We thus resort to the Semidefinite Programming (SDP) relaxation.

We first re-parameterize the variables as:

z d=ef

s t

P d=ef

0 diag(v)W diag()

diag()W diag(v)

0

we obtain the formulation:

adv(x, r)  max z P z
||z || 1

Using the fact that z P z = tr(zz P ), we first have:

max z P z = max tr(ZP )

||z || 1

Z=zz , z 1

By relaxing Z = zz and z   1 with Z 0 and diag(Z)  1 (Boyd & Vandenberghe, 2004), we have the following convex SDP problem:

max tr(ZP )

s.t.

diag(Z)  1 Z0

which is efficiently solvable using off-the-shelf SDP optimizers.

0/1 Adversarial loss

4.2 EMPIRICAL EVALUATION
Next we empirically validate the above analytical results. Following the same setting as in § 3, we mainly use MNIST (LeCun et al., 1998) as the benchmark dataset and a two layer neural network as the target network model f . We consider four variants of f : (i) fnt ­ f is normally trained, (ii) fat ­ f is adversarially trained (Madry et al., 2018), (iii) f is trained with regularized bounds, and (iv) fstn ­ f is trained with spatial transformer network. For (iii), we consider three types of bounds: Frobenius ­ ffro, spectral ­ fspe, and SDP fsdp (our proposed method). The implementation details are presented in the appendix.
1
0.8 Frobenius
0.6 Spectral
0.4 SDP
0.2
00 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Transformation threshold
(a) (b) (c)
Figure 5: 0/1 adversarial loss for different variants of f on MNIST based on Frobenius, spectral and SDP upper bound.
Upper Bounds We evaluate the different upper bounds on adversarial loss. For each of the networks described above, we compute the 0/1 loss based on the Frobenius, spectral, and SDP bounds respectively, with results shown in Figure 5. It is observed that all the bounds are fairly loose: the estimated adversarial loss goes to 1 as reaches 0.1. This again echos the impossibility results found in § 3. However, our proposed SDP bound is slightly tighter than alternative bounds under fsdp.
Model Robustness We also observe that fsdf is more robust than alternative models under varying perturbation budget. Specifically, we plot both the success rates of spatial transformation attacks and the test accuracy of different models under varying  in Figure 6. Here  controls the allowed magnitude of spatial transformation, with  = 0.05 used in (Xiao et al., 2018). For fsdp, it is observed that there are the reduction of around 0.1 for attack success rates and the gain of around 0.08 for adversarial test accuracy, compared with other training objectives.

7

Under review as a conference paper at ICLR 2019

11

Attack success rate Classification accuracy

0.8 0.8

0.6 0.6

0.4 0.4

0.2 0.2

0 0.025

0.05 0.075 (a)

0.1 0 0.025 0.05 0.075

Hyper-parameter

(b)

0.1



Figure 6: (a) Spatial transformation attack success rate versus  under different training objectives. (b) Test accuracy of spatial transformed adversarial inputs for different training objectives and  ( =  corresponds to benign cases).

5 RELATED WORK

Adversarial Inputs Due to their increasing use in security-critical domains, machine learning models are becoming the targets of malicious attacks (Barreno et al., 2010; Biggio & Roli, 2018). Compared with simple models (e.g., decision tree, support vector machine, and logistic regression), securing deep neural networks deployed in adversarial settings poses even more challenges due to their significantly higher model complexity (LeCun et al., 2015). A variety of adversarial attacks have been proposed, including both white-box attacks (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2015; Papernot et al., 2016a; Kurakin et al., 2016; Carlini & Wagner, 2017; Meng & Chen, 2017; Xu et al., 2018) and black-box attacks (Papernot et al., 2016; Liu et al., 2016; Reddy Mopuri et al., 2017).
Spatial Transformation While most state-of-the-art attacks directly modify the pixel values of benign images, recent work (Xiao et al., 2018; Engstrom et al., 2017b) has proposed to use spatial deformation as an alternative to generate adversarial inputs. Compared with pixel perturbation, spatial transformation tends to better preserve the perceptual quality of original images. Yet, besides their perceptual superiority, thus far little is known about the security properties of spatially transformed adversarial inputs.
Provable Defenses Another line of research has focused on improving model resilience against adversarial attacks by developing new training and inference strategies (Goodfellow et al., 2015; Huang et al., 2015; Papernot et al., 2016b; Ji et al., 2018; Madry et al., 2018; Trame`r et al., 2018; Kannan et al., 2018). Yet, the fundamental challenges of defending against adversarial attacks stem from their adaptive nature. Existing defenses, once deployed, can be easily circumvented by adaptive attacks. This arms race between attacks and defenses has motivated the development of provable defenses (Bastani et al., 2016; Raghunathan et al., 2018; Wong & Zico Kolter, 2018), which ensure that for a given network, no attack is able to force the error to exceed a certain threshold. However, existing provable defenses all assume norm-bounded perturbation. It is an open question whether such defenses exist for spatial transformation-based attacks.

6 CONCLUSION
This work represents the first systematic study on provable defenses against spatial transformationbased adversarial attacks. Our findings include both possibility and impossibility results. We show that such provable defenses do not exist in practice. For any given networks, the adversary is able to find legitimate inputs and imperceptible transformations to generate adversarial inputs that force arbitrarily large errors. Yet, we also show that it is still possible to construct adversarial training methods to significantly improve the robustness of given networks against adversarial inputs over empirical datasets. We hope this work may inspire designing more effective defenses against spatial transformation-based attacks and adversarial attacks in general.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D. Tygar. The Security of Machine Learning. Mach. Learn., 81(2):121­148, 2010.
O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, and A. Criminisi. Measuring Neural Net Robustness with Constraints. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2016.
Battista Biggio and Fabio Roli. Wild Patterns: Ten Years after the Rise of Adversarial Machine Learning. Pattern Recognition, 84:317­331, 2018.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim S rndic´, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Proceedings of European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD), 2013.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, USA, 2004.
N. Carlini and D. Wagner. Towards Evaluating the Robustness of Neural Networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P), 2017.
Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. Adversarial classification. In Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD), 2004.
L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. ArXiv e-prints, 2017a.
L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. ArXiv e-prints, 2017b.
K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song. Robust Physical-World Attacks on Deep Learning Models. ArXiv e-prints, 2017.
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples. In Proceedings of International Conference on Learning Representations (ICLR), 2015.
R. Huang, B. Xu, D. Schuurmans, and C. Szepesvari. Learning with a Strong Adversary. ArXiv e-prints, 2015.
P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-Image Translation with Conditional Adversarial Networks. ArXiv e-prints, 2016.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2015.
Y. Ji, X. Zhang, and T. Wang. EagleEye: Attack-Agnostic Defense against Adversarial Inputs. ArXiv e-prints, 2018.
J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual Losses for Real-Time Style Transfer and SuperResolution. ArXiv e-prints, 2016.
H. Kannan, A. Kurakin, and I. Goodfellow. Adversarial Logit Pairing. ArXiv e-prints, 2018.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. ArXiv e-prints, 2016.
Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
9

Under review as a conference paper at ICLR 2019
D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Math. Program., 45(3):503­528, 1989.
Y. Liu, X. Chen, C. Liu, and D. Song. Delving into Transferable Adversarial Examples and Blackbox Attacks. ArXiv e-prints, 2016.
Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD), 2005.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. In Proceedings of International Conference on Learning Representations (ICLR), 2018.
Dongyu Meng and Hao Chen. Magnet: A two-pronged defense against adversarial examples. In Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2017.
S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. DeepFool: a simple and accurate method to fool deep neural networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
Blaine Nelson, Benjamin I. P. Rubinstein, Ling Huang, Anthony D. Joseph, Steven J. Lee, Satish Rao, and J. D. Tygar. Query strategies for evading convex-inducing classifiers. J. Mach. Learn. Res., 13:1293­1332, 2012.
N. Papernot, P. McDaniel, and I. Goodfellow. Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. ArXiv e-prints, 2016.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swamil. The limitations of deep learning in adversarial settings. In Proceedings of IEEE European Symposium on Security and Privacy (Euro S&P), 2016a.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P), 2016b.
A. Raghunathan, J. Steinhardt, and P. Liang. Certified Defenses against Adversarial Examples. In Proceedings of International Conference on Learning Representations (ICLR), 2018.
K. Reddy Mopuri, U. Garg, and R. Venkatesh Babu. Fast Feature Fool: A data independent approach to universal adversarial perturbations. ArXiv e-prints, 2017.
Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Phys. D, 60(1-4):259­268, 1992.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of ACM SAC Conference on Computer and Communications (CCS), 2016.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing Properties of Neural Networks. In Proceedings of International Conference on Learning Representations (ICLR), 2014.
F. Trame`r, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble Adversarial Training: Attacks and Defenses. In Proceedings of International Conference on Learning Representations (ICLR), 2018.
E. Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In Proceedings of IEEE Conference on Machine Learning (ICML), 2018.
C. Xiao, J.-Y. Zhu, B. Li, W. He, M. Liu, and D. Song. Spatially Transformed Adversarial Examples. In Proceedings of International Conference on Learning Representations (ICLR), 2018.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. In Proceedings of Network and Distributed System Security Symposium (NDSS), 2018.
10

Under review as a conference paper at ICLR 2019

T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View Synthesis by Appearance Flow. ArXiv e-prints, 2016.

APPENDIX

6.1 NOTATIONS AND SYMBOLS

Symbol Description

x genuine input

x~ adversarial input

f (·) classifier

r flow vector

g(r; x) spatial transformation r on x

f (x, r) margin between f  g(r; x) and f (x)

f (·) gradient of f

Dg(·) Jacobian matrix of g

2(·)

L2 projection 2(z) =

z z

(·)

L projection (z) =

z z

Table 1: Symbols and notations.

6.2 PROOFS OF PROPOSITIONS, LEMMAS, AND THEOREMS (Unless noted otherwise, we use · to indicate L2 norm.)

PROOF OF PROPOSITION 3 Proposition 3. The extreme values of Eqn (4) are achieved when r(u) = r(v).

Proof. (Proposition 3) Consider the first-order approximation of Eqn (4). Recall that r = [r(u), r(v)] and notice that g(0) = [x, x] according to Eqn (2.2). We can rewrite the above expression as:

adv(x, r) = tr (diag(f (x)) [x, x]) r(u), r(v)



s.t.

r(u) M 2r(u) + r(v) M 2r(v)  x 1

Due to the symmetry of r(u) and r(v), the extreme values of f (x, r) is attained when r(u) = r(v).

Proof. (Proposition 1)

We derive the following matrix form for Eqn (2): 



diff (r) =

2r 

I - M (i)

i{t,b,l,r}

I - M (i)  r

(9)

where I denotes the identity matrix, and tr is the trace operator. We have the following proposition to simplify the notation (proof in Appendix).

We first prove that i{l,r,t,b}(I - M (i)) (I - M (i)) is diagonally dominant. It suffices to show that each matrix (I - M (i)) (I - M (i)) is diagonally dominant. Without loss of generality, assume M (i) corresponds to the top neighbor matrix.

Let A = I - M (i). Consider the i-th pixel. We differentiate three cases: (i) if i is on the top boundary, it must be the top neighbor of another pixel j; (ii) if i is on the bottom boundary, it must be the bottom neighbor of another pixel k; (iii) otherwise, i is the top neighbor of some j and the bottom neighbor of some k.

11

Under review as a conference paper at ICLR 2019

Consider the i-th column of A. We have: (i) Aii = 0, Aji = -1, and Ari = 0 for r = i, j; (ii) Aii = 1, Ari = 0 for r = i; (iii) Aii = 1, Aji = -1, and Ari = 0 for r = i, j. Now we compute Z = A A. First consider the diagonal element Zii: (i) Zii = 1; (ii) Zii = 1; and (iii) Zii = 2. Then consider the off-diagonal element Zij: (i) if i is j's top neighbor, then Zij = -1, and Zij = 0 otherwise; (ii) if i is j's bottom neighbor, Zij = -1, and Zij = 0 otherwise; (iii) if i is j's top neighbor or bottom neighbor, Zij = -1, and Zij = 0 otherwise.
In all the cases, Zii  0 and |Zii|  j=i |Zij|, i.e., Z is diagonally dominant with real nonnegative diagonal elements. Thus, i{l,r,t,b}(I -M (i)) (I -M (i)) is positive semidefinite. Using the spectral decomposition, we can rewrite it as: i{l,r,t,b}(I -M (i)) (I -M (i)) = M M , where M is a square matrix of the same rank as i{l,r,t,b}(I - M (i)) (I - M (i)).

PROOF OF THEOREM 2

Proof. Recall that M r = defines an ellipsoid. It is straightforward to see that the supremum of ~ f (x, r) = v diag(x)r is attained only if r lies on the ellipsoid boundary. Otherwise, suppose r0 = arg maxr ~ f (x, r) and M r0 < . Note that f (x, r)  0. We may extend r0 to the ellipsoid boundary to further increase ~ f (x, r). Thus, we only need to consider the case of M r = . For
convenience, we rewrite the ellipsoid in the following form: r = M -1s with s = .

Thus, supx,r ~adv(x, r) = supx,s v diag(x)M -1s, which represents the maximum projection of s on M -1 diag(x)v. Note that s lies on the ball of s = and that M -1 diag(x)v = M -1 diag(v)x.
We have the formulation as follows:

sup ~adv(x, r) = sup M -1 diag(v)x

x 1, M r 

x 1

(10)

which is attained by s = 2(M -1 diag(v)x).

It is noted that without the constraint of x   1, the supremum above is attained by assigning x the primary eigenvector v of diag(v)M -2 diag(v). We thus set x = (v), which leads to:

sup ~adv(x, r)  M -1 diag(v)(v)
x 1, M r 

PROOF OF THEOREM 1

Proof. Similar to the proof of Theorem 2, it is easy to see that the supremum of ~ f (x, r) is attained only if r lies on the ellipsoid boundary of M r = . For convenience, we redefine the ellipsoid as: r = M -1s with s = .

According to the definition, we have:

sup

~adv(x, r) =

sup () diag(v)W diag(x)M -1s

x 1, M r 

s  , x 1

As s lies on the ball of s = , for fixed x, we have:

sup ~adv(x, r) = sup M -1 diag(x)W diag(v)

x 1, M r 

x 1

which is achieved by s = 2(M -1 diag(x)W diag(v)).

We then define the following auxiliary function:

(x)

=

1 2

M -1 diag(x)W

diag(v) 2

Recall that in the case that  is ReLU, ()i = 1 if (W x)i > 0 or ()i = 0 otherwise. Observe that (x) has the absolute lower bound of 0, which can be achieved when x = 0, i.e., all the neurons in the first layer are inactive. We attempt to increase (x) by maximizing the number of active
neurons.

12

Under review as a conference paper at ICLR 2019

For a practical neural network f , we may consider each column of W as a basis function. We thus assume W has full column rank, i.e., rank(W ) = d. Therefore, by properly setting x, we may make at least d dimensions of W x larger than 0. Let Wd be the submatrix of W that consists of d independent rows of W . We consider x = (Wd-11) where 1 is an all-one vector. It is easy to verify that (i) Wdx > 0, i.e., (Wdx) = 1 and x0   1. That is, x is a valid input and activates at least d neurons in the first layer of the network. We have the following lower bound:
sup ~adv(x, r)  M -1 diag(x)W diag(v)(W x) .
x 1, M r 

6.3 RESULTS FOR LINEAR CLASSIFIERS

6.4 IMPOSSIBILITY RESULTS

In the case of single-layer neural network (i.e., linear classifier), we have f (x) = v x, where v is the weights of the network. In this case, we have the following result:

1
adv(x, r) = v Dg(zr; x)rdz
0

(11)

To find x and r, we consider a linear approximation of adv(r; x):

~adv(x, r) = v Dg(0; x)r = v diag(x)r

where Dg(0; x) = diag(x) according to Eqn (1). Theorem 2 finds a lower bound of the supremum of ~adv(x, r) (proof in appendix).
Theorem 2. For a single-layer network f , we have

sup ~adv(x, r)  M -1 diag(v)x
x 1, M r 

where x = (v) and v represents the primary eigenvector of diag(v)M -2 diag(v). The bound

is attained when

r = M -12(M -1 diag(v)x).

(12)

While Theorem 2 finds high-quality setting of (x, r), we may further increase the adversarial loss

by

approximating

Eqn (11).

Let

r(i)

=

2i-1 2n

r.

We

compute

the

midpoint

approximation

of

Eqn (11):

adv(x, r)



1 n

n

v

Dg

r(i); x

r

i=1

(13)

Summation (i)

where we divide the interval [0, 1] into n subintervals.

Updating x
We first update x to maximize Eqn (13) With fixed r. To this end, we consider x as a symbolic vector in Eqn (13). Let i be the coefficient of x's i-th element in Eqn (13). We set x = [sign(1), sign(2), . . . , sign(d)] , which maximizes Eqn (13) under fixed r and the constraint of x   1.

Updating r

Let rk be the setting of r at the k-th iteration. With x fixed, we may update r as follows:

n

rk+1 = M -12 M -1

Dg

r(i) k

;

x

v

i=1

(14)

Intuitively, to maximize Eqn (13), we find rk+1 that aligns with the direction of the summation (i) in Eqn (13) under M rk+1  .

13

Under review as a conference paper at ICLR 2019

Mr   r=

1 2

r=

-1 -2

M

-1 0 01

r 

Figure 7: Bounds of diag(s)r, where the dashed area corresponds to diag(s)r that satisfies ||M r||  and ||s||  1.

Attack

Algorithm 2 sketches the attack against a given single-layer network. After initialization (line 1-3), it updates x and r in an interleaving manner (line 4-7).
Algorithm 2: Attack against single-layer networks.
Input: v: f 's weights; : threshold of perturbation magnitude; M : neighboring matrix; n: parameter of midpoint approximation
Output: x: genuine input; r: spatial transformation // initialization 1 v  primary eigenvector of diag(v)M -2 diag(v); 2 x  (v); 3 set r according to Eqn (12); 4 while not converged do
// refinement of x 5 compute Eqn (13) with symbolic x;
// i: coefficient of x's i-th dimension in Eqn (13) 6 x  [sign(1), sign(2), . . . , sign(d)] ;
// refinement of r 7 update r according to Eqn (14);
8 return (x, r);

6.5 POSSIBILITY RESULTS

In the case of single-layer network (i.e., linear classifier), we have:

adv(x, r) 

max v Dg(zr; x)r

z[0,1], M r 

 2 max v diag(s)r
||s||1, M r 

(15)

where the last inequality follows from the fact that as defined in Eqn (1), under bilinear interpolation, the diagonal elements of Dg(zr; x) vary within the interval of [-2, 2].
Let  = [1, 2, . . . , d] . We have the following derivation of Eqn (15) (note that we reparameterize r with t):

adv(x, r)  2 max v diag()t
t 1
= 2 v diag() 1

(16)

where the upper-bound is attained when t = sign(v diag()).
Note that this upper bound is irrelevant to the original input x and solely dependent on the network's properties (i.e., v and ). Thus, it only needs to be computed once for a given network.

14

Under review as a conference paper at ICLR 2019

6.6 EXTENSION TO MULTI-CLASS CASES (IMPOSSIBILITY)

We can extend Algorithm 2 and Algorithm 1 for a K-class classifier f (·). We considerthe CW

attack and take  = 0:

adv (r)

=

max
y=y~

y

(g(r;

x))

-

y~(g(r;

x))

(17)

For the most popular gradient-based adversarial attack methods, their update rules of r at t-th itera-

tion are:

rt+1

=

rt

-



 adv r

(r)

(18)

where  is learning rate. Let y such that arg maxy=y~ y(g(r; x)) = y , then according to the chain

rule,

we

have



maxy=y~ r

y

g

(r;

x)

=



y 

r

g

(r;

x).

Then we

derive

a

simple

work

around:

at

each

iteration, we only update based on the two classes case between class y (-) and y~(+).

6.7 EXTENSION TO MULTI-CLASS CASES (POSSIBILITY)

We extend the proposed SDP training objective to multiple classes case. Here we only consider a two-layer neural network, it is straightforward to apply the extension to a one-layer network. Similar to Eqn (9), we define:

P ij d=ef

0 diag(vi - vj)W diag()

diag()W diag(vi - vj)

0

for all pair of classes i(+), j(-). Then we have

ij adv

(x,

r)



max
diag(Z)1,Z

tr(ZP ij ) d=ef qij (x)
0

For an image x with class y, the attack is failed if and only if maxi=y

iy adv

(x)



0.

Therefore,

we

take maxi=y qiy(x) as the upper bound.

6.8 ADDITIONAL EXPERIMENTAL RESULTS
IMPLEMENTATION DETAILS
For the empirical evaluation (ii), we follow the experimental setup in Raghunathan et al. (2018) and evaluate the part with a two-layer network f with 500 hidden units. We construct a set of variants of f using different training methods. We report our training objectives and details below:
1. fnt ­ f is normally trained, with cross-entropy loss and without explicit regularization.
2. ffro ­ f is trained with Hinge loss Frobenius norm regularization (Fro-NN) and  ( W F + v 2) with  = 0.08.
3. fspe ­ f is trained with hinge loss and a regularizer  ( W 2 + v 2) with  = 0.09.
4. fstn­ f is trained with hinge loss and a three layer spatial transformer network with 100 hidden units processes input images before feeding them to classification network.
5. fat ­ f is trained with cross-entropy loss with an adversarial training regularization against spatial transformed adversarial examples. We take regularization coefficient r = 0.3 for training and  = 0.05 for regularizing the diff . Due to Xiao et al. (2018) taking L-BFGS for solving the optimization problem, it is very slow to generate adversarial examples on the fly. Instead, we generate a pool of adversarial examples every k = 3 epochs, and sample adversarial example from pools during training.
6. fsdp ­ f is the upper bound we derived in Eqn (9), and we follow the implementation of SDP-NN in Raghunathan et al. (2018). In particular, we also optimize the dual form of Eqn (9) so that we can backpropagate through both the classification loss and the SDP regularization term.
The other naive bounds used in the § 4:

15

Under review as a conference paper at ICLR 2019

10

8

Adversarial loss

6

4

2

00 2 4 6 8 Transformation threshold

Figure 8: Lower bounds estimated with respect to different networks.

1. Spectral bound 2. Frobenius bound

diff (x, r)  W 2 v 2 diff (x, r)  W F v 2

Benign Input

Adversarial Input

Flow Vector

Figure 9: Samples of benign and adversarial inputs found by Algorithm 1 under frd. 16

Under review as a conference paper at ICLR 2019

Benign Input

Adversarial Input

Flow Vector

Figure 10: Samples of benign and adversarial inputs found by Algorithm 1 under fat.

0/1 Adversarial loss

1

0.8

0.6 Frobenius Spectral
0.4

0.2

0 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1

Transformation threshold (a)

(b)

Figure 11: Upper bounds on 0/1 adversarial loss for different networks on MNIST.

17

