Under review as a conference paper at ICLR 2019
STATISTICAL VERIFICATION OF NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We present a new approach to neural network verification based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. This permits classic verification as a special case, for which one considers only the question of whether this expectation is exactly zero or not. When the property can be violated, our approach provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than classical formal verification approaches. Key to achieving this is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical verification framework. We demonstrate that our approach is able to emulate existing verification procedures on benchmark problems, while scaling to larger networks and providing reliable additional information in the form of accurate estimates of the violation probability.
1 INTRODUCTION
The robustness of deep neural networks must be guaranteed in mission-critical applications where their failure could have severe real-world implications. This motivates the study of neural network verification, in which one wishes to assert whether certain inputs in a given subdomain of the network might lead to important properties being violated (Zakrzewski, 2001; Bunel et al., 2018). For example, in a classification task, one might want to ensure that small perturbations of the inputs do not lead to incorrect class labels being predicted (Szegedy et al., 2013; Goodfellow et al., 2015). The classic approach to such verification has focused on answering the binary question of whether there exist any counterexamples that violate the property of interest. We argue that this approach has two major drawbacks. Firstly, it provides no notion of how robust a network is whenever a counterexample can be found. Secondly, it creates a computational problem whenever no counterexamples exist, as formally verifying this can be very costly and does not currently scale to the size of networks used in many applications. To give a demonstrative example, consider a neural network for classifying objects in the path of an autonomous vehicle. It will almost certainly be infeasible to train such a network that is perfectly robust to misclassification. Furthermore, because the network will most likely need to be of significant size to be effective, it is unlikely to be tractable to formally verify the network is perfectly robust, even if such a network exists. Despite this, it is still critically important to assess the robustness of the network, so that manufacturers can decide whether it is safe to deploy. To address the shortfalls of the classic approach, we develop a new measure of intrinsic robustness of neural networks based on the probability that a property is violated under an input distribution model. Our measure is based on two key insights. The first is that for many, if not most, applications, full formal verification is neither necessary nor realistically achievable, such that one actually desires a notion of how robust a network is to a set of inputs, not just a binary answer as to whether it is robust or not. The second is that most practical applications have some acceptable level of risk, such that it is sufficient to show that the probability of a violation is below a certain threshold, rather than confirm that this probability is exactly zero. By providing a probability of violation, our approach is able to address the needs of applications such as our autonomous vehicle example. If the network is not perfectly robust, it provides an explicit measure of exactly how robust the network is. If the network is perfectly robust, it is still able to tractability assert that a violation event is "probably-unsatisfiable". That is it is able to statistically
1

Under review as a conference paper at ICLR 2019
conclude that the violation probability is below some tolerance threshold to true zero, even for large networks for which formal verification would not be possible.
Calculating the probability of violation is still itself a computationally challenging task, corresponding to estimating the value of an intractable integral. In particular, in most cases, violations of the target property constitute (potentially extremely) rare events. Consequently, the simple approach of constructing a direct Monte Carlo estimate by sampling from the input model and evaluating the property will be expensive and only viable when the event is relatively common. To address this, we adapt an algorithm from the Monte Carlo literature, adaptive multi-level splitting (AMLS) (Guyader et al., 2011; Nowozin, 2015), to our network verification setting. AMLS is explicitly designed for prediction of rare events and our adaptation means that we are able to reliably predict the probability of violation, even when the true value is extremely small.
Our resulting framework is easy to implement, scales linearly in the cost of the forward operation of the neural network, and is agnostic both to the network architecture and input model. Assumptions such as piecewise linearity, Lipschitz continuity, or a specific network form are not required. Furthermore, it produces a diversity of samples which violate the property as a side-product. To summarize, our main contributions are:
· Reframing neural network verification as the estimation of the probability of a violation, thereby providing a more informative robustness metric for non-verifiable networks;
· Adaptation of the AMLS method to our verification framework to allow the tractable estimation of our metric for large networks and rare events;
· Validation of our approach on several models and datasets from the literature.
2 RELATED WORK
The literature on neural network robustness follows two main threads. In the optimization community, researchers seek to formally prove that a property holds for a neural network by framing it as a satisfiability problem (Zakrzewski, 2001). Such methods have only been successfully scaled beyond one hidden layer networks for linear piecewise networks (Cheng et al., 2017; Katz et al., 2017), and even then these solutions do not scale to, for example, common image classification architectures with input dimensions in the hundreds, or apply to networks with nonlinear activation functions (Bunel et al., 2018). Other work has sought approximate solutions in the same general framework but still does not scale to larger networks (Pulina & Tacchella, 2010; Xiang et al., 2018; Huang et al., 2017). As the problem is NP-hard (Katz et al., 2017), it is unlikely that an algorithm exists with runtime scaling polynomially in the number of network nodes.
In the deep learning community, research has focused on constructing and defending against adversarial attacks, and by estimating the robustness of networks to such attacks. Weng et al. (2018) recently constructed a measure for robustness to adversarial attacks estimating a lower bound on the minimum adversarial distortion, that is the smallest perturbation required to create an adversarial example. Though the approach scales to large networks, the estimate of the lower bound is often demonstratively incorrect: it is often higher than an exact upper bound on the minimum adversarial distortion (Goodfellow, 2018). Other drawbacks of the method are that it cannot be applied to networks that are not Lipschitz continuous, it requires an expensive gradient computation for each class per sample, does not produce adversarial examples, and cannot be applied to non-adversarial properties. The minimum adversarial distortion is also itself a somewhat unsatisfying metric for many applications, as it conveys little information about the density of adversarial examples.
3 MOTIVATING EXAMPLES
To help elucidate our problem setting, we consider the ACASXU dataset (Katz et al., 2017) from the formal verification literature. A neural network is trained to predict one of five correct steering decisions, such as "hard left," "soft left," etc., for an unmanned aircraft to avoid collision with a second aircraft. The inputs x describe the positions, orientations, velocities, etc. of the two aircraft. Ten interpretable properties are specified along with corresponding constraints on the inputs, for which violations correspond to events causing collisions. Each of these properties is encoded in a function, s, such that it is violated when s(x)  0. The formal verification problem asks the question, "Does there exist an input x  E  X in a constrained subset, E, of the domain such that the property is violated?" If there exists a counterexample violating the property, we say that the property is satisfiable (SAT), and otherwise, unsatisfiable (UNSAT).
2

Under review as a conference paper at ICLR 2019

Another example is provided by adversarial properties from the deep learning literature on datasets
such as MNIST. Consider a neural network f(x) = Softmax(z(x)) that classifies images, x, into C classes, where the output of f gives the probability of each class. Let  be a small perturbation
in an lp-ball of radius , that is,  p < . Then we say that x = x +  is an adversarial example for x if arg maxi z(x)i = arg maxi z(x )i, that is, the perturbation changes the predicted label. In this case, the property function is s(x) = maxi=c (z(x)i - z(x)c), where c = arg maxj z(x )j and s(x)  0 indicates that x is an adversarial example. Our approach subsumes adversarial properties
as a specific case.

4 ROBUSTNESS METRIC

The framework for our robustness metric is very general, requiring only a) a neural network f, b) a property function s(x; f, ), and c) an input model p(x). Together these define an integration problem, with the main practical challenge being the estimation of this integral. Consequently, the method can be used for any neural network. The only requirement is that we can evaluate the property function, which typically involves a forward pass of the neural network.
The property function, s(x; f, ), is a deterministic function of the input x, the trained network f, and problem specific parameters . For instance, in the MNIST example,  = arg maxi f(x )i is the true output of the unperturbed input. Informally, the property reflects how badly the network is performing with respect to a particular property. More precisely, the event

E {s(x; f, )  0}

(1)

represents the property being violated. Predicting the occurrence of these, typically rare, events will be the focus of our work. We will omit the dependency on f and  from here on for notional conciseness, noting that these are assumed to be fixed and known for verification problems.

The input model, p(x), is a distribution over the subset of the input domain that we are considering for counterexamples. For instance, for the MNIST example we could use p(x; x ) 
1 ( x - x p  ) to consider uniform perturbations to the input around an lp-norm ball with ra-
dius . More generally, the input model can be used to place restrictions on the input domain and
potentially also to reflect that certain violations might be more damaging than others.

Together, the property function and input model specify the probability of failure through the integral

I [p, s] PXp(·) (s(X)  0) = 1{s(x)0}[x]p(x)dx.
X

(2)

This integral forms our measure of robustness. The integral being equal to exactly zero corresponds to the classical notion of a formally verifiable network. Critically though, it also provides a measure for how robust a non-formally-verifiable network is.

5 METRIC ESTIMATION

Our primary goal is to estimate (2) in order to obtain a measure of robustness. Ideally, we also wish to generate example inputs which violate the property. Unfortunately, the event E is typically very rare in verification scenarios. Consequently, the simple approach of estimating the integral directly by the Monte Carlo estimate,

P^X p(·)

(s(X )



0)

=

1 N

1N
n=1 {s(xn)0}[xn],

where xn i.i.d. p(·)

(3)

is typically not feasible for real problems, requiring an impractically large number of samples to achieve a reasonable accuracy. Even when E is not a rare event, we desire to estimate the probability using as few forward passes of the neural network as possible to reduce computation. Furthermore, the dimensionality of x is typically large for practical problems, such that it is essential to employ a method that scales well in dimensionality. Consequently many of the methods commonly employed for such problems, such as the cross-entropy method (Rubinstein, 1997; De Boer et al., 2005), are inappropriate due to relying on importance sampling, which is well known to scale poorly.

As we will demonstrate empirically, a less well known but highly effective method from the statistics literature, adaptive multi-level splitting (AMLS) (Kahn & Harris, 1951; Guyader et al., 2011), can be readily adapted to address all the aforementioned computational challenges. Specifically, AMLS

3

Under review as a conference paper at ICLR 2019

is explicitly designed for estimating the probability of rare events and our adaptation is able to give highly accurate estimates even when the E is very rare. Furthermore, as will be explained later, AMLS also allows the use of MCMC transitions, meaning that our approach is able to scale effectively in the dimensionality of x. A further desirable property of AMLS is that it produces property-violating examples as a side product, namely, it produces samples from the distribution

(x) p(x | E) = p(x)1{s(x)0}[x] I [p, s] .

(4)

Such samples could in theory be used to perform robust learning, in a similar spirit to Goodfellow et al. (2015) and Madry et al. (2017).

5.1 MULTI-LEVEL SPLITTING

The high-level idea of multi-level splitting (Kahn & Harris, 1951) is to divide the problem of predicting the probability of a rare event into several simpler ones. Specifically, we construct a sequence of intermediate targets,

k(x) p(x | {s(x)  Lk})  p(x)1{s(x)Lk}[x], k = 0, 1, 2, . . . , K,
for levels, - = L0 < L1 < L2 < · · · < LK = 0, to bridge the gap between the input model p(x) and the target (x). For any choice of the intermediate levels, we can now represent equation (2) through the following factorization,

KK
PXp(·) (s(X)  0) = k=1 P (s(X)  Lk | s(X)  Lk-1) := k=1 Pk.

(5)

Provided consecutive levels are sufficiently close, we will be able to reliably estimate each Pk by making use of the samples from one level to initialize the estimation of the next.

Our approach starts by first drawing N samples, {x(n0)}nN=1, from 0(·) = p(·), noting that this can be done exactly because the perturbation model is known. These samples can then be used to
estimate P1 using simple Monte Carlo

P1  P^1

1 N

1N
n=1

{s(xn)L1}[x(n0)]

where

xn  0(·).

In other words, P1 is the fraction of these samples whose property is greater than L1. Critically, by ensuring the value of L1 is sufficiently small for {s(xn)  L1} to be a common event, we can ensure P^1 is a reliable estimate for moderate numbers of samples N .

To estimate the other Pk, we need to be able to draw samples from k-1(·). For this we note that if {xn(k-1)}Nn=1 are distributed according to k-1(·), then the subset of these samples for which s(x(nk-1))  Lk are distributed according to k(·). Furthermore, setting Lk up to ensure this event
is not rare means a significant proportion of the samples will satisfy this property.

To avoid our set of samples shrinking from one level to the next, it is necessary to carry out a rejuve-
nation step to convert this smaller set of starting samples to a full set of size N for the next level. To
do this, we first carry out a uniform resampling with replacement from the set of samples satisfying s(x(nk-1))  Lk to generate a new set of N samples which are distributed according to k(·), but with a large number of duplicated samples. Starting with these samples, we then successively apply M Metropolis­Hastings (MH) transitions targeting k(·) separately to each sample to produce a fresh new set of samples {x(nk)}nN=1 (see Appendix A for full details). These samples can then in turn be used to form a Monte Carlo estimate for Pk

Pk  P^k

1 N

1N
n=1

{s(xn )Lk } [xn(k) ]

where

x(nk)  k(·),

(6)

along with providing the initializations for the next level. Running more MH transitions decreases the correlations between the set of samples, improving the performance of the estimator.

We have thus far omitted to discuss how the levels Lk are set, other than asserting the need for the levels to be sufficiently close to allow reliable estimation of each Pk. Presuming that we are also free to choose the number of levels K, there is inevitably a trade-off between ensuring that each {s(X)  Lk} is not rare given {s(X)  Lk-1}, and keeping the number of levels small to reduce computational costs and avoid the build-up of errors. AMLS (Guyader et al., 2011) builds

4

Under review as a conference paper at ICLR 2019

Algorithm 1 Adaptive multi-level splitting with termination criterion

1: Input: Input model p(x), sample quantile , MH proposal g(x |x), number of MH steps M , termination

threshold log(Pmin)

2: Sample {xn}nN=1 i.i.d. from p(·)

3: Initialize L  -, Lprev  -, log(I)  0

4: while L < 0 do

5: Evaluate and sort {s(xn)}Nn=1 in descending order

Updating the level

6: L  min{0, s(x) N }

7: log(I)  log(I) + log(#{xn | s(x)  L}) - log(N )

Updating integral estimate

8: if log(I) < log(Pmin) then return (, -) end if

Final estimate will be less than log(Pmin)

9: Uniformly resample with replacement N particles from {xn | s(x)  L} to initialize {x(nk)}nN=1

10: Apply M MH updates separately to each xn(k) using g(x |x)

11: [Optional] Adapt g(x |x) based on MH acceptance rates

12: end while 13: return ({xn}nN=1, log(I))

on the basic multi-level splitting process, providing an elegant way of controlling this trade-off by adaptively selecting the level to be the minimum of 0 and some quantile of the property under the current samples. The approach terminates when the level reaches zero, such that LK = 0 and K is a dynamic parameter chosen implicitly by the adaptive process.
Choosing the th quantile of the values of the property results in discarding a fraction (1 - ) of the chains at each step of the algorithm. This allows explicit control of the rarity of the events to keep them at a manageable level. We note that if the all the sample property values are distinct, then this approach gives Pk = , k < K. To give intuition to this, we can think about splitting up log(I) into chunks of size log(). For any value of log(I), there is always a unique pair of values {K, PK} such that log(I) = K log() + log(PK), K  0 and PK < . Therefore the problem of estimating I is equivalent to that of estimating K and PK.

5.2 TERMINATION CRITERION
The application of AMLS to our verification problem presents a significant complicating factor in that the true probability of our rare event might be exactly zero. Whenever this is the case, the basic AMLS approach outlined in (Guyader et al., 2011) will never terminate as the quantile of the property will never rise above zero; the algorithm simply produces closer and closer intermediate levels as it waits for the event to occur.
To deal with this, we introduce a termination criterion based on the observation that AMLS's running estimate for I monotonically decreases during running. Namely, we introduce a threshold probability, Pmin, below which the estimates will be treated as being numerically zero. We then terminate the algorithm if I < Pmin and return I = 0, safe in the knowledge that even if the algorithm would eventually generate a finite estimate for I, this estimate is guaranteed to be less than Pmin.
Putting everything together, gives the complete method as shown in Algorithm 1. See Appendix B for low-level implementation details.

6 EXPERIMENTS
6.1 EMULATION OF FORMAL VERIFICATION
In our first experiment, we aim to test whether our statistical verification framework is able to effectively emulate formal verification approaches, while providing additional robustness information for SAT properties. In particular, we want to test whether it reliably identifies properties as being UNSAT, for which I = 0, or SAT, for which I > 0. We note that the method still provides a formal demonstration for SAT properties because having a non-zero estimate for I indicates that at least one counterexample has been found. Critically, it further provides a measure for how robust SAT properties are, through its estimate for I.
We used the COLLISIONDETECTION dataset introduced in the formal verification literature by (Ehlers, 2017). It consists of a neural network with six inputs that has been trained to classify

5

Under review as a conference paper at ICLR 2019

log10 (I )  log10 I  log10 I

0.0 -2.5 -5.0 -7.5 -10.0 -12.5 -15.0 -17.5 -20.0

0

naive MC M = 1000,  = 0.1

50 100 property id

150

M = 1000 0.20  = 0.5
 = 0.25 0.15  = 0.1
0.10
0.05
0.00
50 100 150 property id

0.075 0.050 0.025 0.000 -0.025 -0.050 -0.075

 = 0.1

M = 100 M = 250 M = 1000

50 100 150 property id

(a) (b) (c)
Figure 1: (a) Estimate of I for all SAT properties of COLLISIONDETECTION problem. Error bars indicating ± three standard errors from 30 runs are included here and throughout, but the variance of the estimates was so small that these are barely visible. We can further conclude low bias of our method for the properties where naive MC estimation was feasible, due to the fact that naive MC produces unbiased (but potentially high variance) estimates. (b) Mean AMLS estimate relative to naive MC estimate for different  holding M = 1000 fixed, for those properties with log10 I > -6.5 such that they could be estimated accurately. The bias decreases both as  and the rareness of the event decrease. (c) As per (b) but with varying M and holding  = 0.1 fixed.

two car trajectories as colliding or non-colliding. The architecture has 40 linear nodes in the first layer, followed by a layer of max pooling, a ReLU layer with 19 hidden units, and an output layer with 2 hidden units. Along with the dataset, 500 properties are specified for verification, of which 172 are SAT and 328 UNSAT. This dataset was chosen because the model is small enough so that the properties can be formally verified. These formal verification methods do not calculate the value of I, but rather confirm the existence of a counterexample for which s(x) > 0.
We ran our approach on all 500 properties, setting  = 0.1, N = 104, M = 1000 (the choice of these hyperparameters will be justified in the next subsection), and using a uniform distribution over the input constraints as the perturbation model, along with a uniform random walk proposal. We compared our metric estimation approach against the naive Monte Carlo estimate using 1010 samples. The generated estimates of I for all SAT properties are shown in Figure 1a.
Both our approach and the naive MC baseline correctly identified all of the UNSAT properties by estimating I as exactly zero. However, despite using substantially more samples, naive MC failed to find a counterexample for 8 of the rarest SAT properties, thereby identifying them as UNSAT, whereas our approach found a counterexample for all the SAT properties. As shown in Figure 1a, the variances in the estimates for I of our approach were also very low and matched the unbiased MC baseline estimates for the more commonly violated properties, for which the latter approach still gives reliable, albeit less efficient, estimates. Along with the improved ability to predict rare events, our approach was also significantly faster than naive MC throughout, with a speed up of several orders of magnitude for properties where the event is not rare--a single run with naive MC took about 3 minutes, whereas a typical run of ours took around 3 seconds.

6.2 SENSITIVITY TO PARAMETER SETTINGS
As demonstrated by Bre´hier et al. (2015), AMLS is unbiased under the assumption that perfect sampling from the targets, {k}kK=-11, is possible, and that the cumulative distribution function of s(X) is continuous. In practice, finite mixing rates of the Markov chains and the dependence between the initialization points for each target means that sampling is less than perfect, but improves with larger values of M and N . The variance, on the other hand, theoretically strictly decreases with larger values of N and  (Bre´hier et al., 2015).
In practice, we found that while larger values of M and N were always beneficial, setting  too high introduced biases into the estimate, with  = 0.1 empirically providing a good trade-off between bias and variance. Furthermore, this provides faster run times than large values of , noting that the smaller values of  lead to larger gaps in the levels.
To investigate the effect of the parameters more formally, we further ran AMLS on the SAT properties of COLLISIONDETECTION, varying   {0.1, 0.25, 0.5}, N  {103, 104, 105} and

6

Under review as a conference paper at ICLR 2019

MNIST

log10 (I )

0 M = 1000 naive MC
-5
-10
-15
-20 0.25 0.30 0.35 0.40 0.45

 log10(I)

1.0 0.5 0.0 -0.5 -1.0 -1.5

M = 100 M = 250
0.25 0.30 0.35 0.40 0.45

CIFAR­10

log10 (I )

0.0 -2.5 -5.0 -7.5 -10.0 -12.5 -15.0

M = 2000 naive MC

0.04 0.05 0.06 0.07 0.08

 log10(I)

6 M = 100
5 M = 250 4 M = 500
M = 1000 3
2
1
0
-1
0.04 0.05 0.06 0.07 0.08

Figure 2: [Left] Estimates for I on adversarial properties of a single datapoint with N = 10000 and  = 0.1. As in Figure 1, the error bars from 30 runs are barely visible, highlighting a very low variance in the estimates, while the close matching to the naive MC estimates when is large enough to make the latter viable, indicate a very low bias. [Right] The difference in the estimate for the other values of M from M = 1000/M = 2000 for MNIST/CIFAR­10, respectively. The estimate steadily converges as M increases, with larger M more important for rarer events.
M  {100, 250, 1000}, again comparing to the naive MC estimate for 1010 samples. We found that the value of N did not make a discernible difference in this range regardless of the values for  and M , and thus all presented results correspond to setting N = 104. As shown in Figure 1b, we found that the setting of  made a noticeable difference to the estimates for the relatively rarer events. All the same, these differences were small relative to the differences between properties. As shown in Figure 1c, the value of M made little difference when  = 0.1,. Interesting though, we found that the value of M was important for different values of , as shown in Appendix C.1, with larger values of M giving better results as expected.

6.3 CONVERGENCE WITH HIGHER-DIMENSIONAL INPUTS
To validate the algorithm on a higher-dimensional problem, we tested adversarial properties on the MNIST and CIFAR­10 datasets using a dense ReLU network with two hidden-layer of size 256. An l-norm ball perturbation around the data point with width was used as the uniform input model, with = 1 representing an l-ball filling the entire space (the pixels are scaled to [0, 1]), together with a uniform random walk MH proposal. After training the classifiers, multilevel splitting was run on ten samples from the test set at multiple values of , with N = 10000 and  = 0.1, and M  {100, 250, 1000} for MNIST and M  {100, 250, 500, 1000, 2000} for CIFAR­10. The results for naive MC were also evaluated using 5 × 109 samples--less than the previous experiment as the larger network made estimation more expensive--in the cases where the event was not too rare.
As the results were similar across datapoints, we present the result for a single example in Figure 2. As desired, a smooth curve is traced out as decreases, for which the event E becomes rarer. For MNIST, acceptable accuracy is obtained for M = 250 and high accuracy results for M = 1000. For CIFAR­10, which has about four times the input dimension of MNIST, larger values of M were required to achieve comparable accuracy. The magnitude of required to give a certain value of log(I) is smaller for CIFAR­10 than MNIST, reflecting that adversarial examples for the former are typically more perceptually similar to the datapoint.
7

log10 (I )

Under review as a conference paper at ICLR 2019
0
-10
-20
-30 = 0.3 = 0.2
-40 = 0.1
0 20 40 60 80 100 epoch
Figure 3: Variation in I during the robustness training of on a CNN model for MNIST for three different perturbation sizes . Epoch 0 corresponds to the network after conventional training, with further epochs corresponding to iterations of robustness training. The solid line indicates the median over 40 datapoints, and the limits of the shaded regions the 25 and 75 percentiles. Our measure is capped at Pmin = exp(-100). We see that while training improves robustness for = 0.2, the initial network is already predominantly robust to perturbations of size = 0.1, while the robustness to perturbations of size = 0.3 actually starts to decrease after around 20 epochs.
6.4 ROBUSTNESS OF PROVABLE DEFENSES DURING TRAINING
In this experiment, we examine how our robustness metric varies for a ReLU network as that network is trained to be more robust against norm bounded perturbations to the inputs using the method of Wong & Kolter (2018). Roughly speaking, their method works by approximating the set of outputs resulting from perturbations to an input with a convex outer bound, and minimizing the worst case loss over this set. The motivation for this experiment is threefold. Firstly, this training provides a series of networks with ostensibly increasing robustness, allowing us to check if our approach produces robustness estimates consistent with this improvement. Secondly, the networks in question are larger than those that those that can be practically verified to be UNSAT with formal verification approaches, allowing us to further verify the scaling of our method. Thirdly, it allows us to investigate whether the training to improve robustness for one type of adversarial attack helps to protect against others. Specifically, we check if training for robustness against a small perturbation size improves robustness to larger perturbations. We train a CNN model on MNIST for 100 epochs with the standard cross-entropy loss, then train the network for a further 100 epochs using the robust loss of Wong & Kolter (2018), saving a snapshot of the model at each epoch. The architecture is the same as in (Wong & Kolter, 2018), containing two strided convolutional layers with 16 and 32 channels, followed by two fully connected layers with 100 and 10 hidden units, and ReLU activations throughout. The robustification phase trains the classifier to be robust in an l -ball around the inputs, where
is annealed from 0.01 to 0.1 over the first 50 epochs. At a number of epochs during the robust training, we calculate our robustness metric with  {0.1, 0.2, 0.3} on 40 samples from the test set. The results are summarized in Figure 3 with additional per-sample results in Appendix C.2. We see that our approach is able to capture variations in the robustnesses of the network.
7 DISCUSSION
We have introduced a new measure for the intrinsic robustness of a neural network, and have validated its utility on several datasets from the formal verification and deep learning literatures. Our approach was able to exactly emulate formal verification approaches for satisfiable properties and provide high confidence, accurate predictions for properties which were not. The two key advantages it provides over previous approaches are: a) providing an explicit and intuitive measure for how robust networks are to satisfiable properties; and b) providing improved scaling over classical approaches for identifying unsatisfiable properties. Going forward, an interesting avenue that could be exploited further is the fact that our approach has the highly desirable property of producing a diverse set of counterexamples as a by-product. In many cases these are likely to be of notable interest in themselves, for example, potentially paving the way for new approaches to training robust networks and protecting against adversarial attacks.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Charles-Edouard Bre´hier, Tony Lelie`vre, and Mathias Rousset. Analysis of adaptive multilevel splitting algorithms in an idealized case. ESAIM: Probability and Statistics, 19:361­394, 2015.
Rudy Bunel, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, and M. Pawan Kumar. A unified view of piecewise linear neural network verification. arXiv preprint arXiv:1711.00455v3 [cs.AI], 2018.
Chih-Hong Cheng, Georg Nu¨hrenberg, and Harald Ruess. Verification of binarized neural networks. arXiv preprint arXiv:1710.03107, 2017.
Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of operations research, 134(1):19­67, 2005.
Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium on Automated Technology for Verification and Analysis, pp. 269­286. Springer, 2017.
Ian Goodfellow. Gradient masking causes clever to overestimate adversarial perturbation size. arXiv preprint arXiv:1804.07870, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2015.
Arnaud Guyader, Nicolas Hengartner, and Eric Matzner-Løber. Simulation and estimation of extreme quantiles and extreme probabilities. Applied Mathematics & Optimization, 64(2):171­196, 2011.
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In International Conference on Computer Aided Verification, pp. 3­29. Springer, 2017.
H. Kahn and T.E. Harris. Estimation of particle transmission by random sampling. National Bureau of Standards applied mathematics series, 12:27­30, 1951.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pp. 97­117. Springer, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Sebastian Nowozin. Multilevel splitting. http://www.nowozin.net/sebastian/blog/ multilevel-splitting.html, 2015.
Luca Pulina and Armando Tacchella. An abstraction-refinement approach to verification of artificial neural networks. In International Conference on Computer Aided Verification, pp. 243­257. Springer, 2010.
Gareth O Roberts, Andrew Gelman, Walter R Gilks, et al. Weak convergence and optimal scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110­120, 1997.
Reuven Y Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operational Research, 99(1):89­112, 1997.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. arXiv preprint arXiv:1801.10578, 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pp. 5283­5292, 2018.
9

Under review as a conference paper at ICLR 2019 Weiming Xiang, Hoang-Dung Tran, and Taylor T Johnson. Output reachable set estimation and
verification for multilayer neural networks. IEEE transactions on neural networks and learning systems, (99):1­7, 2018. Radosiaw R Zakrzewski. Verification of a trained neural network accuracy. In Neural Networks, 2001. Proceedings. IJCNN'01. International Joint Conference on, volume 3, pp. 1657­1662. IEEE, 2001.
10

Under review as a conference paper at ICLR 2019

APPENDIX

A METROPOLIS­HASTINGS

Metropolis­Hastings is an MCMC method that allows for sampling when one only has access to an unnormalized version of the target distribution. The unnormalized targets distributions of interest for our problem are k(x~) where
k(x)  k(x) = p(x)1{s(x~)Lk}[x], k = 1, 2, . . . , K.
A MH transition comprises of proposing a new sample using a proposal x  g(x | x), calculating an acceptance probability,

Ak(x | x)

min 1, k(x )g(x | x ) , k(x)g(x | x)

(7)

and accepting the new sample with probability Ak(x | x), returning the old sample if the new one is rejected. Successive applications of this transition process generates samples which converge in distribution to the target k(x) and whose correlation with the starting sample diminishes to zero.
In our approach, these MH steps are applied independently to each sample in the set, while the only samples used for the AMLS algorithm are the last samples produced from the resulting Markov chains.

B IMPLEMENTATION DETAILS
Algorithm 1 has computational cost O(N M K), where the number of levels K will depend on the rareness of the event, with more computation required for rarer ones. Parallelization over N is possible provided that the batches fit into memory, whereas the loops over M and K must be performed sequentially.
One additional change we make from the approach outlined by Guyader et al. (2011) is that we perform MH updates on all chains in Lines 10, rather than only those that were previously killed off. This helps reduce the build up of correlations over multiple levels, improving performance.
Another is that we used an adaptive scheme for g(x |x) to aid efficiency. Specifically, our proposal takes the form of a random walk, the radius of which, , is adapted to keep the acceptance ratio roughly around 0.234 (see Roberts et al. (1997)). Each chain has a separate acceptance ratio that is average across MH steps, and after M MH steps, for those chains whose acceptance ratio is below 0.234 it is halved, and conversely for those above 0.234, multiplied by 1.02.

C ADDITIONAL RESULTS
C.1 VARYING M FOR FIXED  ON COLLISIONDETECTION
Whereas the exact value of M within the range considered proved to not be especially important when  = 0.1, it transpires to have a large impact in the quality of the results for larger values of  as shown in Figure 4.

C.2 PER-SAMPLE ROBUSTNESS MEASURE DURING ROBUST TRAINING
Figure 5 illustrates the diverse forms that the per-sample robustness measure can take on the 40 datapoints averaged over in Experiment §5.3. We see that different datapoints have quite varying initial levels of robustness, and that the training helps with some points more than others. In one case, the datapoint was still not robust add the end of training for the target perturbation size = 0.1.

11

Under review as a conference paper at ICLR 2019

 log10 I  log10 I

0.05 0.00 -0.05 -0.10 -0.15 -0.20

 = 0.25
M = 100 M = 250 M = 1000 50 100 150 property id

0.2 0.0 -0.2 -0.4 -0.6

 = 0.5
M = 100 M = 250 M = 1000 50 100 150 property id

Figure 4: Mean AMLS estimate relative to naive (unbiased) MC estimate for different M = holding  fixed to 0.25 (left) and 0.5 (right), for those properties whose naive MC estimate was greater than log10 I = -6.5 such that they could be estimated accurately.

12

Under review as a conference paper at ICLR 2019

0 -10 -20 -30 -40
0 0 -10 -20 -30 -40
0 0 -10 -20 -30 -40
0 0 -10 -20 -30 -40
0 0 -10 -20 -30 -40
0 0 -10 -20 -30 -40
0 0 -10 -20 -30 -40
0 0 -10 -20 -30 -40
0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

0

-10

-20

-30

-40

20 40 60 80 100

0

20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100

Figure 5: Convergence of individual datapoints used in forming Figure 3.

13

