Under review as a conference paper at ICLR 2019
TOWARDS MORE THEORETICALLY-GROUNDED PARTICLE OPTIMIZATION SAMPLING FOR DEEP LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Many deep-learning based methods such as Bayesian deep learning (DL) and deep reinforcement learning (RL) have heavily relied on the ability of a model being able to efficiently explore via Bayesian sampling. Particle-optimization sampling (POS) is a recently developed technique to generate high-quality samples from a target distribution by iteratively updating a set of interactive particles, with a representative algorithm the Stein variational gradient descent (SVGD). Though obtaining significant empirical success, the non-asymptotic convergence behavior of SVGD remains unknown. In this paper, we generalize POS to a stochasticity setting by injecting random noise in particle updates, called stochastic particle-optimization sampling (SPOS). Notably, for the first time, we develop non-asymptotic convergence theory for the SPOS framework, characterizing convergence of a sample approximation w.r.t. the number of particles and iterations under both convex- and noncovexenergy-function settings. Interestingly, we provide theoretical understanding of a pitfall of SVGD that can be avoided in the proposed SPOS framework, i.e., particles tend to collapse to a local mode in SVGD under some particular conditions. Our theory is based on the analysis of nonlinear stochastic differential equations, which serves as an extension and a complementary development to the asymptotic convergence theory for SVGD such as (Liu, 2017). With such theoretical guarantees, SPOS can be safely and effectively applied on both Bayesian DL and deep RL tasks. Extensive results demonstrate the effectiveness of our proposed framework.
1 INTRODUCTION
Recent development of deep-learning techniques have required the ability of an algorithm to efficiently explore some particular space (e.g., parameter space) via Bayesian sampling, due to high model complexity of modern deep models. Meanwhile, recent years have seen the development of scalable sampling methods such as stochastic gradient MCMC (SG-MCMC) (Welling & Teh, 2011; Chen et al., 2014; Ding et al., 2014; Chen et al., 2015) and Stein variational gradient descent (SVGD) (Liu & Wang, 2016) to facilitate big-data analysis. SG-MCMC is a family of scalable Bayesian sampling algorithms relying on Itó diffusions, linear stochastic differential equations (SDE) with appropriately designed coefficients such that the corresponding stationary distributions match a target distribution. One potential issue is that samples might be highly correlated partially due to the nature of Markov chains, leading to low sample efficiency, an undesirable property of SG-MCMC.
SVGD, on the other hand, belongs to the family of particle-based sampling methods, which optimize a set of interacting particles to minimize some distance metric between the target distribution and the distribution induced by the particles. By optimizing the distance measure, one maintains an optimal set of particles at each time. Recent development of SVGD has shown that the underlying mathematical principle is based on a family of nonlinear SDEs, in the sense that coefficients of the SDE depend on the current density of the particles. Though achieving numerous practical successes (Liu & Wang, 2016; Feng et al., 2017; Liu et al., 2017; Haarnoja et al., 2017; Liu & Zhu, 2018), little theory has been developed to understand the convergence property of the algorithm. A recent theoretical development has interpreted SVGD as a special type of gradient flow in the space of probability measures, and developed theory to disclose its asymptotic convergence behavior (Liu, 2017).
1

Under review as a conference paper at ICLR 2019

Recently, Chen et al. (2018) unified SG-MCMC and SVGD by proposing a particle-optimization sampling framework that interprets both as Wasserstein gradient flows (WGFs). Generally speaking, a WGF is a partial differential equation (PDE) defined on the space of probability measures, describing the evolution of a density function over time. In (Chen et al., 2018), the authors define a WGF by combining the corresponding Fokker-Planck equations for both SG-MCMC and SVGD, and solve it with deterministic particle approximations. However, due to the diffusion nature, deterministicparticle approximation leads to a hard-to-control error, challenging for theoretical analysis.
Based on the unified framework in (Chen et al., 2018), we propose to solve WGFs with stochastic particle approximation, leading to stochastic particle-optimization sampling (SPOS). The idea is instead of solving the WGF with an uncontrollable deterministic approximation for a diffusion term, we solve it stochastically by injecting random noise to the particle-update equations. Remarkably, for the first time, we develop nonasymptotic convergence theory for the family of SPOS algorithms, considering both convex- and nonconvex-energy functions. Different from existing theory for SGMCMC based algorithms (Teh et al., 2016; Vollmer et al., 2016; Chen et al., 2015; Raginsky et al., 2017; Zhang et al., 2017; Xu et al., 2018), our development relies on the theory of nonlinear SDEs, which is more involved and less explored in literature. Particularly, we adopt tools from granular media equations (Malrieu, 2003; Cattiaux et al., 2008) to develop nonasymptotic error bounds in terms of the 1-Wasserstein distance. Please refer to Section M in the Supplementary Material (SM) for detailed distinctions of our work to existing work. Within our theoretical framework, we provide a formal theoretical understanding of a pitfall of SVGD, i.e., particles tend to collapse to one point under some particular conditions; whereas this can be avoided in the proposed SPOS framework due to the injected random noise. Our theory is general enough to be applied for various deep-learning tasks, including Bayesian deep learning and Bayesian exploration in reinforcement learning. Our extensive experimental results well suggest advantages of our framework compared to existing methods.

2 PRELIMINARIES
This section introduces necessary preliminaries, along with notations used in this paper. For the sake of clarity, through out the paper, we use bold letters to denote variables in continuous-time diffusions and model definitions, e.g., t in (1) defined below (indexed by "time" t). By contrast, normal unbold letters are used to denote parameters in algorithms (discrete solutions of continuous-time diffusions), e.g., k(i) in (3) below (indexed by "iteration" k). For conciseness, all the proofs as well as some extra experimental results are presented in the SM. Discussion on the complexity of our algorithm is also included in Section L of the SM.

2.1 STOCHASTIC GRADIENT MCMC

In Bayesian sampling, we aim to generate random samples from a posterior distribution p(|X ) 

p(X |)p(), where   Rd represents the model parameter with a prior distribution p(), and X

{xq}qN=1 represents the observed data with likelihood p(X |) = q p(xq |). Define the potential en-

ergy as: U ()

- log p(X |) - log p() = -

N q=1

log p(xq

|)

+

1 N

log p()

N q=1

Uq ().

SG-MCMC algorithms belong to diffusion-based sampling methods, where a continuous-time diffu-

sion process is designed such that its stationary distribution matches the target posterior distribution.

The diffusion process is driven by a specific stochastic differential equation (SDE). For example, in

stochastic gradient Langevin dynamic (SGLD), the SDE endows the following form:

dt = --1F (t)dt + 2/dWt ,

(1)

where F ()

U () =

N q=1

 Uq ()

N q=1

Fq

();

t

is

the

time

index;



>

0

is

the

temperature parameter; and Wt  Rd is a d-dimensional Brownian motion. More instances of SDEs

corresponding to other SG-MCMC algorithms can be defined by specifying different forms of F and

potentially other diffusion coefficients. We focus on SGLD and (1) in this paper, and refer interested

readers to (Ma et al., 2015) for more detailed description of general SG-MCMC algorithms. Denote

the probability density function of t in (1) as t, and a · b a b for two vectors a and b. It is

known that t is characterized by the following Fokker-Planck (FP) equation (Risken, 1989):

tt =  · (-1tF () + -1t) .

(2)

where the stationary distribution  equals to our target distribution p(|X ) according to Chiang & Hwang (1987). SGLD generates samples from p(|X ) by numerically solving the SDE (1). For

2

Under review as a conference paper at ICLR 2019

scalability, it replaces F (k) in each iteration with an unbiased evaluation by randomly sampling

a subset of X , i.e. F (k) is approximated by: Gk

N Bk

qIk Fq(k), where Ik is a random

subset of [1, 2, · · · , N ] with size Bk in each iteration. Based on the above settings, SGLD uses the

Euler method with stepsize hk to numerically solve (1) and obtains the update equation: k+1 =

k - -1Gkhk + 2-1hkk, k  N (0, I).

2.2 STEIN VARIATIONAL GRADIENT DESCENT

Different from SG-MCMC, SVGD is a deterministic particle-optimization algorithm that is able to

generate samples from a target distribution. In the algorithm, a set of particles interact with each

other, driving them to high density locations in the parameter space while keeping them far away

from each other with repulsive force. The update equations of the particles follow the fastest descent

direction of the KL-divergence between current empirical distribution of the particles and the target distribution, on an RKHS induced by a kernel function (·, ·) (Liu & Wang, 2016). Formally, Liu &

Wang (2016) derived the following updating rules for the particles {k(i)}Mi=1 at the k-th iteration with

stepsize hk and G(ki)

N Bk

qIk Fq(k(i)):

k(i+) 1

=

k(i)

+

hk M

M

(k(j), k(i))Gk(i) + k(j) (k(j), k(i)) , i

j=1

(3)

where the first term in the bracket encourages particles to locate on high density modes, and the

second term serves as repulsive force that pushes away different particles. Different from SG-MCMC, only particles at the current iteration, {k(i)}Mi=1, are used to approximate the target distribution.

2.3 PARTICLE-OPTIMIZATION BASED SAMPLING METHODS

SG-MCMC and SVGD, though look closely related, behave very differently in terms of algorithms,

e.g., stochastic and noninteractive versus deterministic and interactive particle updates. Recently,

Chen et al. (2018) proposed a deterministic particle-optimization framework that unifies both SG-

MCMC and SVGD. Specifically, the authors viewed both SG-MCMC and SVGD as Wasserstein

gradient flows (WGFs) on the space of probabilistic measures, and derived several deterministic

particle-optimization techniques for particle evolutions, like what SVGD does. For SG-MCMC, the

FP equation (2) for SGLD is a special type of WGFs. Together with an interpretation of SVGD as a

special case of the Vlasov equation in nonlinear PDE literature, Chen et al. (2018) proposed a general

form of PDE to characterize the evolution of the density for the model parameter , denoted as t at time t with  matching our target (posterior) distribution, i.e.,

tt =  · t-1F () + t (K  t()) + -1t ,

(4)

where K is a function controlling the interaction of particles in the PDE system. For example, in

SVGD, Chen et al. (2018) showed that K and K  t() endow the following forms:

K(,  ) F ( )( , ) -  ( , ) and K  t()

K(,  )t( )d

(5)

where (·, ·) is a kernel function such as the RBF kernel. In the following, we introduce a new unary

function K() = exp(-

2 2

),

thus

(,



)

can

be

rewritten

as

(,



)

=

K (

-



).

Hence,

(4)

with K defined in (5) can be equivalently rewritten as:

tt =  · t-1F () + t (EY t K( - Y )F (Y ) - K  t()) + -1t , (6)

where Y is a random sample from t but independent of . Importantly,

Proposition 1 (Chen et al. (2018)) The stationary distribution of (6) equals to our target distribution, which means () = p(|X ).

Chen et al. (2018) proposed to solve (4) numerically with deterministic particle-optimization algo-

rithms such as the blob method. Specifically, the continuous density t is approximated by a set of

M

particles

{t(i)}Mi=1

that

evolve over

time t,

i.e.

t



1 M

M i=1

t(i) (),

where

t(i) ()

=

1

if

 = t(i) and 0 otherwise. Note t in (4) is no longer a valid definition when adopting particle

approximation for t. Consequently, t needs nontrivial approximations, e.g., by discrete gradient

flows or blob methods proposed in (Chen et al., 2018). We omit the details here for simplicity.

3

Under review as a conference paper at ICLR 2019

3 STOCHASTIC PARTICLE-OPTIMIZATION SAMPLING ALGORITHMS

The deterministic particle-approximation methods proposed by Chen et al. (2018) to approximately

solve the WGF problem (4) introduce approximation errors for t that are hard to control analytically. To overcome this problem, we propose to solve (4) stochastically to replace the t term with a Brownian motion. Specifically, first note that the term -1 · t is contributed

from Brownian motion, i.e., solving the SDE, dt = 2-1dWt, is equivalent to solving the corresponding FP equation: t = -1 · t. Consequently, we decompose RHS of (4) into two parts: F1  · t-1F (t) + (K  t)t and F2 -1 · t. Our idea is to solve F1

deterministically under a PDE setting, and solve F2 stochastically based on its corresponding SDE.

When adopting particle approximation for the density t, both solutions of F1 and F2 are represented
in terms of particles {t(i)}. Thus we can combine the solutions from the two parts directly to approximate the original exact solution of (4). Similar to the results of SVGD in Section 3.3 in

(Liu, 2017), we first formally show in Theorem 2 that when approximating t with particles, i.e.,

t



1 M

M i=1

t(i) (),

the

PDE

can

be

transformed

into

a

system

of

deterministic

differential

equations with interacting particles.

Theorem 2 When approximating t in (4) with particles {t(i)}i, the PDE tt = F1 reduces to the following system of differential equations describing evolutions of the particles over time: i

dt(i)

=

--1F (t(i))dt

-

1 M

M

K (t(i)

-

t(j))F (t(j))dt

+

1 M

M

K(t(i) - t(j))dt

(7)

j=1

j=1

On the other hand, by solving tt = F2 stochastically with its equivalent SDE counterpart, we

arrive at the following differential equation system, describing evolutions of the particles {t(i)} over time t: i

dt(i) =

--1F (t(i))

-

1 M

M

K (t(i)

-

t(j))F (t(j))

+

1 M

M

K(t(i) - t(j))

dt +

2-1dWt(i)

j=1

j=1

Our intuition is that if

(8)

the particle evolution

(8) can be solved ex-

actly, the solution of

(6) t will be well-

approximated by the

particles {t(i)}iM=1. In Figure 1: Comparison of SPOS (left) and SVGD (right) on a multi-mode our theory, we show distribution. The circles with different colors are the resulting 100 particles,

this intuition is actu- which are able to spread over all modes for SPOS.

ally true. In practice, however, solving (8) is typically infeasible, and thus numerical methods

are adopted. Furthermore, in the case of big data, following SG-MCMC, F (k(i)) is typically replaced

by a stochastic version G(ki)

N Bk

qIk Fq(k(i)) evaluated with a minibatch of data of size Bk for

computational feasibility. Based on the Euler method (Chen et al., 2015) with a stepsize hk, (8) leads

to the following updates for the particles at the k-th iteration

k(i+) 1

=

k(i) -hk  -1 G(ki)

-

hk M

M

K(k(i) - k(j))Gk(j)

j=1

+ hk M

M

K(k(i) - k(j)) +

j=1

2-1hkk(i), k(i)  N (0, I) i .

(9)

We called the algorithm with particle update equations (9) stochastic particle-optimization sampling (described in Algorithm 3), in the sense that particles are optimized stochastically with extra random Gaussian noise. Intuitively, the added Gaussian noise would enhance the ability of the algorithm to jump out of local modes, leading to better ergodic properties compared to standard SVGD. This serves as one of our motivations to generalize SVGD to SPOS. To illustrate the advantage of introducing the noise term, we compare SPOS and SVGD on sampling a difficult multi-mode distribution, with the density function given in Section A of the SM. The particles are initialized on a local mode close to zero. Figure 1 plots the final locations of the particles along with the true density, which shows that particles in SPOS are able to reach different modes, while they are all trapped at one mode in SVGD. This pitfall of SVGD will be studied formally in Section 4.4.

4

Under review as a conference paper at ICLR 2019

4 NON-ASYMPTOTIC CONVERGENCE ANALYSIS: THE CONVEX CASE

In this section, we develop nonasymptotic convergence theory

Algorithm 1 Stochastic Particle-Optimization Sampling

for the proposed SPOS when the Input: Initial particles {0(i)}iM=1 with 0(i)  Rd, step size hk,

energy function U () is convex. batch size Bk

The nonconvex case is discussed 1: for iteration k= 0,1,...,T do

in Section 5. We prove nonasymptotic convergence rates for SPOS algorithm under the 1Wasserstein metric W1, a special

2: Update k(i+) 1 with (9) for i. 3: end for Output:{T(i)}iM=1

case of p-Wasserstein metric de-

1/p
fined as Wp(µ, ) = inf(µ,) Rd×Rd Xµ - X pd(Xµ, X ) , where (µ, ) is the set of joint

distributions on Rd × Rd with marginal distribution µ and . Note that SPOS reduces to SVGD

when   , thus our theory also sheds light on the convergence behavior of SVGD, where

non-asymptotic theory is currently missing, despite the asymptotic theory developed recently (Liu,

2017; Lu et al., 2018). It is worth noting that part of our proofs are generalization of techniques for

analyzing granular media equations in (Malrieu, 2003; Cattiaux et al., 2008).

4.1 BASIC SETUP AND ASSUMPTIONS
Due to the exchangeability of the particle system {t(i)}Mi=1 in (8), if we initialize all the particles t(i) with the same distribution 0, they would endow the same distribution for each time t. We denote the distribution of each t(i) as t. Similar arguments hold for the particle system {k(i)}Mi=1 in (9), and thus we denote the distribution of each k(i) as µk. To this end, our analysis aims at bounding W1(µT , ) since  equals to our target distribution p(|X ) according to Proposition 1. Before proceeding to our theoretical results, we first present the following basic assumptions.
Assumption 1 Assume F and K satisfy the following conditions:

· There exist positive mF and mK , such that F () - F ( ),  -   mF  -  2 and K() - K( ),  -   -mK  -  2.
· F is bounded by HF and LF -Lipschitz continuous i.e., F ()  HF and F (1) - F (2)  LF 1 - 2 ; K are LK -Lipschitz continuous.
· K is an even function, i.e., K(-) = K().

Note the first bullet indicates U to be a convex function and W to be a concave function. For an RBF kernel, the later could be achieved by setting the bandwidth large enough and only considering the concave region for simplicity. This assumption is used for revealing some undesired property of SVGD developed below. We do not need such an assumption when analyzing under a nonconvex energy function U in Section 5.

The high-level idea of bounding W1(µT , ) in this section is to decompose it as follows:

W1(µT , )  W1(µT , 

T -1 k=0

hk

)

+

W1(

, T -1
k=0

hk

T -1 k=0

hk

)

+

W1(

T -1 k=0

hk ,

)

.

(10)

4.2 BOUNDS WITH STOCHASTIC PARTICLE APPROXIMATION

We firstly bound W1(

, T -1
k=0

hk

T -1 k=0

hk

)

and

W1(

T -1 k=0

hk

,

)

with

the

following

theorems.

Theorem 3 Under Assumption 1 and let 0 = 0, there exist some positive constants c1 and c2 independent of (M, d) and satisfying c2 < -1 such that

W1(t, t)  c1(-1 - c2)-1M -1/2, t.

(11)

Remark 1 According to

W ( ,  )1

T -1 k=0

hk

T -1 k=0

hk

Theorem 
M

3,
c1 (-1

we
-c2

)

can .

bound the W1(

,T -1
k=0

hk

Furthermore, by letting t



)T -1
k=0

hk

 ,

term as we have

W1(, )



, c1
M (-1-c2)

which

is

an

important

intermediate

result

to

prove

the

following

theorem.

5

Under review as a conference paper at ICLR 2019

Theorem 4 Under Assumption 1, the following holds: W1(t, )  c3e-21t, where 1 =

-1mF - 3HF LW - 2LF and c3 is some positive constant independent of (M, d). Hence, the

W1(

T k=0

hk

,

)

term

in

(10)

can

be

bounded

as:

T -1

W1(

T -1 k=0

hk

,

)



c3

exp

-21(

hk) .

(12)

k=0

To ensure W1(

T -1 k=0

hk ,

)

to

decrease

over

time,

one

needs

to

choose



small

enough

such

that

1 > 0. This also sheds light on a failure case of SVGD (where   ) discussed in Section 4.4.

4.3 BOUNDS WITH A NUMERICAL SOLUTION

To bound the W1(µT , 

T -1 k=0

hk

)

term,

we

adopt

techniques

from

(Raginsky

et

al.,

2017;

Xu

et

al.,

2018) on analyzing the behaviors of SGLD, and derive the following results for our SPOS algorithm:

Theorem 5 Under Assumptions 1, for a fixed step size hk = h that is small enough, the correspond-

ing W1(µT , T h) is bounded as

W1(µT

,

T

h)



c4M

d

3 2

-3(c52B-1

+

c6h)

1 2

T

1 2

h

1 2

(13)

where B is the fixed size of the minibatch in each iteration and (c4, c5, c6) are some positive constants.

The dependence of T in the bound above makes the bound relatively loose. Fortunately, we can improve the bound to make it independent of T by considering a decreasing-stepsize SPOS algorithm, stated in Theorem 6.

Theorem 6 Under Assumptions 1, for a decreasing step size hk = h0/(k + 1), and let the minibatch

size in each iteration k be Bk = B0 + [log(k + 1)]100/99, the corresponding W1(µT , 

)T -1
k=0

hk

term is bounded, for some  small enough, as

W1(µT , 

T -1 k=0

hk )



c4



-3

M

d

3 2

(c7h03

+

c83h0/B0

+

c9h202)1/2

,

(14)

where B0 is the initial minibatch size, and (c4, c7, c8, c9) are some positive constants.

Note Bk increases at a very low speed, e.g., only by 15 after 105 iterations, thus it would not

affect algorithm efficiency. According to Theorem 6, W1(µT , 

T -1 k=0

hk

)

would

approach

zero

when

h10/2M  0. By directly combining results from Theorem 3­6, one can easily bound the target

W1(µT , ). Detailed statements are given in Theorem 15­16 in Section H of the SM.

4.4 A PITFALL OF SVGD
Based on the above analysis, we now formally show a pitfall of SVGD, i.e., particles in SVGD tend to collapse to a local mode under some particular conditions. Inspired by the work on analyzing the granular media equations by Malrieu (2003); Cattiaux et al. (2008), we measure this by calculating the expected distance between particles, called expected particle distance (EPD). Firstly, we bound the EPD for the proposed SPOS algorithm in Theorem 7.

Theorem 7 Under Assumption 1, further assuming every {t(i)} with the same initial probability

law 0 and 

E0, 0 [  - 

2]

<

.

Choose

a

such that 

=

mF 

+ mK

- HF LK

>

0.

Then the EPD of SPOS is bounded as: EPD

M
i,j E

t(i) - t(j)

2



C1e-2t + 4

d 

M 

,

where C1 = M (M - 1) - 4

d

-1

M 

.

Remark 2 There are two interesting cases: i) When C1 > 0, the EPD would decrease to the bound 4 d-1M/ along time t. This represents the phenomenon of an attraction force between particles; ii) When C1 < 0, the EPD would increase to the same bound, which represents the phenomenon of a repulsive force between particles, e.g., when particles are initialized with the same value ( = 0),
they would be pushed away from each other until the EPD increases to the aforementioned bound.

Intuitively, the EPD for SVGD can be obtained by taking the    limit. Corollary 8 formally characterizes the particle-degeneracy phenomenon of SVGD, which has been empirically studied in (Zhuo et al., 2018).

6

Under review as a conference paper at ICLR 2019

Corollary 8 Under the same conditions of Theorem 7, the EPD in SVGD is bounded as: EPD

M i,j

t(i) - t(j) 2  C0e-2t, where C0 =

M i,j

0(i) - 0(j) 2 and  = mK - HF LK .

Remark 3 We would like to emphasize two points: 1) In the case of   0, Corollary 8 indicates that particles in SVGD would collapse to a point when t  . In practice, we usually find that particles are trapped in a local mode instead of collapsing. This is due to two reasons: i) numerical errors inject noise into the particles; ii) some particles are out of the concave region of K stated in Assumption 1 in SVGD, which is required for the theory to hold. All these make the empirical EPD not exactly the same as the true particle distance. 2) Corollary 8 also applies when the energy function is nonconvex. Our proof in the SM considers the nonconvex case as well. Consequently, this serves as a strong theoretical motivation to apply SPOS instead of SVGD in deep learning.

5 NON-ASYMPTOTIC CONVERGENCE ANALYSIS: THE NONCONVEX CASE

Since the non-convex case is much more complicated than the convex case, we reply on different assumptions and adopt another distance metric, denoted as B~, to characterize the convergence behavior of SPOS under the non-convex case. Specifically, B~(µ, ) is defined as B~(µ, ) |Eµ[f ()] - E[f ()]| for a known Lf -continuous function f satisfying Assump-
tion 2 below. Note such metric has also been adopted in (Vollmer et al., 2016; Chen et al., 2015). Our
analysis considers (T, M, hk) as variables in B~. In addition, we use {^k(i)}iM=1 to denote the particles when full gradients are adopted in (9). The distribution of the particles is denoted as µ^k.

Our high-level idea of bounding B~(µT , ) is to decompose it as follows: B~(µT , )  B~(µT , µ^T ) + B~(µ^T , µ^) + B~(µ^, ) + B~(, )

(15)

Our second idea is to concatenate the particles at each time into a single vector representation, i.e.

defining the new parameter at time t as t [t(1), · · · , t(M)]  RMd. Consequently, the nonlinear SDE system (8) can be turned into a linear SDE ,which means t is driven by the following linear

SDE:

dt = -F(t)dt + 2-1dWt(Md) ,

(16)

where F(t)

[-1F (t(1))

-

1 M

M j=1

K

(t(1)

-

t(j))

+

1 M

M j=1

K

(t(1)

-

t(j))F (t(j)), ·

·

·

,

-1F (t(M))-

1 M

M j=1

K

(t(M

)

-

t(j)

)

+

1 M

M j=1

K

(t(M

)

-

t(j))F

(t(j

)

)]

is a vector function RMd  RMd, and Wt(Md) is Brownian motion of dimension M d. Similarly,

we can define ^k [^k(1), · · · , ^k(M)]  RMd for the full-gradient case. Hence, it can be seen that

through such a decomposition in (15), the bound related to a nonlinear SDE system (8) reduces to that

of a linear SDE. The second term B~(µ^T , µ^) reflexes the geometric ergodicity of a linear dynamic

system with a numerical method. It is known that even if a dynamic system has an exponential

convergence rate to its equilibrium, its corresponding numerical method might not. Our bound for

B~(µ^T , µ^) is essentially a specification of the result of Mattingly et al. (2002), which has also been applied by Xu et al. (2018). The third term B~(µ^, ) reflects the numerical error of a linear SDE,

which has been studied in related literature such as (Chen et al., 2015). To this end, we adopt standard

assumptions used in the analysis of SDEs (Vollmer et al., 2016; Chen et al., 2015), rephrased in

Assumption 2.

Assumption 2 (Assumption in (Vollmer et al., 2016; Chen et al., 2015)) For the linear SDE (16)

and a Lipschitz function f , let  be the solution functional of the Poisson equation: G(^k) =

1 M

M i=1

f

(^k(i)

)

-

Ep(|D)[f ()],

where

G

denotes

the

infinite

generator

of

the

linear

SDE

(16). Assume  and its up to 4th-order derivatives, Dk, are bounded by a function V, i.e.,

Dk  HkVpk for k = (0, 1, 2, 3, 4), Hk, pk > 0. Furthermore, the expectation of V on {t} is bounded: supl EVp(t) < , and V is smooth such that sups(0,1) Vp (s + (1 - s)  )  H (Vp () + Vp ( )), ,  , p  max{2pk} for H > 0.

Assumption 3 i) F , K and K are LF , LK and Lk Lipschitz; ii) F satisfies the dissipative property, i.e., F (),   m  2 - b for some m, b > 0; iii) Remark 1 applies to the nonconvex
setting, i.e. sup f Lip1 |Eµ [f ()] - E [f ()]| = W1(, ) = O(M -1/2).

7

Under review as a conference paper at ICLR 2019

Remark 4 Assumption 2 is necessary to control the gap between a numerical solution and the exact solution of an SDE. Specifically, it is used to bound the B~(µ^, ) term and the B~(µT , µ^T ) term above. Purely relying on the dissipative assumption in Assumption 3 as in non-convex optimization
with SG-MCMC (Raginsky et al., 2017; Xu et al., 2018) would induce a bound increasing linearly w.r.t. time t. Thus it is not suitable for our goal. Finally, iii) in Assumption 3 is a mild condition and
reasonable because we expect particles to be able to approximate all distributions equally well in the asymptotic limit of t   by ergodicity due to the injected noise. How to remove/replace this
assumption is an interesting future work.

Based on the assumptions above, the bounds for B~(µ^T , µ^) and B~(µ^, ) are summarized below.

Theorem 9 Under Assumption 2­3, if we set the stepsize hk = h, we can have the following results:

B~(µ^T , µ^)  C2-Md/2(1 + emh) exp -2mT hMd/ log()

(17)

B~(µ^, )  C3h/,

(18)

 where  = 2L(M b + m + M d)/m, L = 2-1LF + l , m = -1m - m , and

(, C2, C3, l , m ) are some positive constants independent of (T, M, h) and   (0, 1)

Remark 5 It is seen that in order to make the B~(µ^T , µ^) term asymptotically decrease to zero, the number of running iteration T should increase at a rate faster enough to compensate the effect of
increasing M . We believe there is room for improving this bound, which is an interesting future work.

Next we bound the B~(µT , µ^T ) term related to stochastic gradients. By adapting results from linear SDE (Xu et al., 2018), B~(µT , µ^T ) can be bounded with Theorem 10.
Theorem 10 Under Assumptions 2­3, if we set Bk = B and hk = h, B~(µT , µ^T ) is bounded as
B~(µT , µ^T )  C5T h(L + M C4) ((6 + 2 )/(BM ))1/2 .
where  = 2(1 + 1/m)(M b + 2M 2C42 + M d/) and , (C4, C5) is some positive constant independent of (T, M, h)

Finally, by combining the results from Theorem 9, 10 and iii) in Assumption 3, we arrive at a bound for our target B~(µT , ), summarized in Theorem 11.

Theorem 11 Under Assumptions 2­3, there exist some positive constants (C2, C3, C4, C5, C6) such that: B~(µT , )  C2-Md/2(1 + emh) exp -2mT hMd/ log()
+ C3h/ + C5T h(L + M C4) ((6 + 2 )/(BM ))1/2 + C6M -1/2 ,
where ,  and  are the same as those in Theorem 9­10.

6 EXPERIMENTS

In this section, we illustrate the effectiveness of our proposed Bayesian sampling framework on several deep-learning models, including Bayesian learning of deep neural network and Bayesian exploration in deep reinforcement learning. We start with a simple illustrative example.

10 0 10 -1 10 -2

6.1 BOUNDS ILLUSTRATION WITH A SIMPLE GAUSSIAN EXAMPLE
We follow Chen et al. (2015) and consider a standard Gaussian model where xi  N (, 1),   N (0, 1). 1000 data samples

10 -3 10 1

10 2 Iterations

10 3

10 4

Figure 2: The estimation errors with different particles.

8

Under review as a conference paper at ICLR 2019

{xi} are generated, and every minibatch in the stochastic gradient is of size 10. The test function is defined as f () 2, with explicit expression for the posterior average. To evaluate the expectations in the bias and MSE, we average over 200 runs with random initializations. The estimation errors are plotted in Figure 2. It is seen from the figure that at the beginning, the errors for the ones with less particles decrease faster than those with more particles. This is reflected in the bounds in Theorem 15, which are dominated by the bound in Theorem 5 (indicating larger M results in larger errors at the beginning). When more running time/iterations are given, the increase of error in Theorem 5 by increasing M is essentially canceled out the exponentially-decay term in Theorem 4. According to Theorem 3, the error would eventually decreasing with increasing number of particles.

6.2 BAYESIAN NEURAL NETWORKS FOR REGRESSION

We conduct experiments for Bayesian learning of DNNs, where we Bayesian DNNs are used to model weight uncertainty of neural networks, an important topic that has been well explored (HernándezLobato & Adams, 2015; Blundell et al., 2015; Li et al., 2016; Louizos & Welling, 2016). We assign simple isotropic Gaussian priors to

Table 1: Averaged predictions with standard deviations in terms of RMSE

and log-likelihood on test sets.

Dataset Boston_Housing
Concrete Energy Kin8nm (0.4) Naval (0.4) CCPP Winequality Yacht (0.4) Protein YearPredict

Test RMSE

SVGD

SPOS

2.957 ± 0.099 2.805 ± 0.141

5.324 ± 0.104 5.053 ± 0.150

1.374 ± 0.045 0.746 ± 0.054

0.090 ± 0.001 0.079 ± 0.001

0.006 ± 0.000 0.004 ± 0.000

4.033 ± 0.033 3.931 ± 0.097

0.609 ± 0.010 0.596 ± 0.028

0.864 ± 0.052 0.823 ± 0.066

4.606 ± 0.013 4.254 ± 0.009

8.684 ± NA 8.681 ± NA

Test Log likelihood

SVGD

SPOS

-2.504 ± 0.029 -2.423 ± 0.075

-3.082 ± 0.018 -3.060 ± 0.036

-1.152 ± 0.071 -1.888 ± 0.072

0.984 ± 0.008 1.093 ± 0.013

4.089 ± 0.012 4.147 ± 0.020

-2.815 ± 0.008 -2.791 ± 0.024

-0.925 ± 0.014 -0.909 ± 0.041

-1.225 ± 0.042 -1.436 ± 0.118

-2.947 ± 0.003 -2.876 ± 0.009

-3.580 ± NA -3.576 ± NA

the weights, and perform posterior sampling with different methods. For all methods, we use a

RBF kernel K(,  ) = exp(-  -  22/2), with the bandwidth set to  = med2/ log M . Here med is the median of the pairwise distance between particles. We use a single-layer BNN for regres-

sion tasks. Following Li et al. (2015), 10 UCI public datasets are considered: 100 hidden units for 2

large datasets (Protein and YearPredict), and 50 hidden units for the other 8 small datasets. Following

Zhang et al. (2018b), we repeat the experiments 20 times with batchsize 100 for all datasets except

for Protein and YearPredict, which we repeat 5 times and once with batchsize 1000. The datasets are

randomly split into 90% training and 10% testing. We adopt the root mean squared error (RMSE)

and test log-likelihood as the evaluation criteria. The experimental results are shown in Table 1, from

where we can see the proposed SPOS outperforms other methods, achieving state-of-the-art results.

6.3 BAYESIAN EXPLORATION IN DEEP REINFORCEMENT LEARNING

Following Liu et al. (2017); Zhang et al. (2018a), we define policies in RL with Bayesian neural networks. This naturally introduces uncertainty into action selections under a specific state-action pair, rendering Bayesian explorations to make policy learning more effective.
Specifically, denote the policy as (a | s) parameterized by  with prior distribution p(), where a represent the action variable, and s the state variable. According to Liu et al. (2017), learning the optimal policy corresponds to calculating the following posterior distribution for :

q()  exp

1 J ()

p(),



(19)

where J() denotes the expected cumulative reward under the policy with parameter  and  a hyperparameter. Consequently,  could be updated by drawing samples from (19) with the proposed SPOS. We denote this method as SPOS-PG. In addition, when drawing samples with SVGD, the resulting algorithm is called Stein variational policy gradient (SVPG) (Liu et al., 2017). Note in implementation, the term J() can be approximated with REINFORCE (Williams, 1992) or advantage actor critic (Schulman et al., 2015), which we will investigate in our experiments.

We follow the same setting as in (Liu et al., 2017), and conduct experiments on three classical continuous control tasks are considered: Cartpole Swing-Up, Double Pendulum, and Cartpole. Specifically, the policy is parameterized as a two-layer (25-10 hidden units) neural network with tanh as the activation function. The maximal length of horizon is set to 500. We use a sample size of 10000 for policy gradient estimation, and M = 16,  = 10. For the simplest task, Cartpole, all

9

Under review as a conference paper at ICLR 2019

Average Reward

Cartpole (A2C)
5000 SVPG SPOSPG
4000
3000
2000
1000
0 0 20 40 60
Episodes
Cartpole
5000 SVPG SPOSPG
4000
3000
2000
1000
0 0 20 40 60
Episodes

80 80

100 100

Average Reward

Average Reward

Cartpole Swing Up (A2C) 200
150
100
50
0
50 SVPG 100 SPOSPG
0 200 400 600 800 1000
Episodes
Cartpole Swing Up
300 SVPG 250 SPOSPG
200 150 100 50
0 50 100
0 200 400 600 800 1000
Episodes

Average Reward

Average Reward

400
600
800
1000
1200 0
400 500 600 700 800 900 1000 1100 1200
0

Double Pendulum (A2C)
SVPG SPOSPG
200 400 600 800 1000
Episodes Double Pendulum
SVPG SPOSPG
200 400 600 800 1000
Episodes

Average Reward

Figure 3: Policy learning with Bayesian exploration in policy-gradient methods on six scenarios with SVPG and SPOS-PG.

agents are trained for 100 episodes. For the other two complex tasks, all agents are trained up to 1000 episodes. The average reward versus number of episodes are plotted in Figure 3. It is observed that our proposed SPOS-PG obtains much larger average rewards as well as smaller variance compared to SVPG, though the convergence behaviors are similar in the simplest task Carpole.
7 CONCLUSION
Motivated by the need of effective and efficient Bayesian sampling techniques in modern deep learning, we propose a probability approach for particle-optimization-based sampling that unifies SG-MCMC and SVGD. Notably, for the first time, by analyzing the corresponding nonlinear SDE, we develop non-asymptotic convergence theory for the proposed SPOS framework, a missing yet important theoretical result since the development of SVGD. Within our theoretical framework, a pitfall of SVGD, which has been studied empirically (Zhuo et al., 2018), is formally analyzed. Our theory also indicates the convergence of SPOS to the true posterior distribution in the asymptotic limit of infinite particles and iterations under appropriate conditions. Our theory is of great practice value, as for the first time it provides nonasymptotic theoretical guarantees for the recently proposed particleoptimization-based algorithms such as the SVGD, whose advantages have also been extensively evaluated by experiments on Bayesian learning of DNNs and Bayesian exploration of DRL. There are a number of interesting future works. For example, one might explore more recently developed techniques such as (Cheng et al., 2018) to improve the convergence bound; one can also adopt the SPOS framework for non-convex optimization like what SG-MCMC is used for, and develop corresponding theory to study the convergence property of the algorithm to the global optimum.

10

Under review as a conference paper at ICLR 2019
REFERENCES
C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks. In ICML, 2015.
F. Bolley and C. Villani. Weighted Csiszár-Kullback-Pinsker inequalities and applications to transportation inequalities. Annales de la Facultédes sciences de Toulouse : Mathématiques, 14(6): 331­352, 2005.
J. A. Carrillo, K. Craig, and F. S. Patacchini. A blob method for diffusion. (arXiv:1709.09195), 2017.
P. Cattiaux, A. Guillin, and F. Malrieu. Probabilistic approach for granular media equations in the non-uniformly convex case. Probability Theory and Related Fields, 140(1­2):19­40, 2008.
C. Chen, N. Ding, and L. Carin. On the convergence of stochastic gradient MCMC algorithms with high-order integrators. In Neural Information Processing Systems (NIPS), 2015.
C. Chen, R. Zhang, W. Wang, B. Li, and L. Chen. A unified particle-optimization framework for scalable Bayesian sampling. In UAI, 2018.
T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In International Conference on Machine Learning (ICML), 2014.
X. Cheng, N. S. Chatterji, Y. Abbasi-Yadkori, P. L. Bartlett, and M. I. Jordan. Sharp convergence rates for Langevin dynamics in the nonconvex setting. In arXiv:1805.01648, 2018.
Tzuu-Shuh Chiang and Chii-Ruey Hwang. Diffusion for global optimization in rn. SIAM J. Control Optim., 25(3):737­753, May 1987. ISSN 0363-0129. doi: 10.1137/0325042. URL http: //dx.doi.org/10.1137/0325042.
N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven. Bayesian sampling using stochastic gradient thermostats. In Neural Information Processing Systems (NIPS), 2014.
A. Durmus, A. Eberle, A. Guillin, and R. Zimmer. An Elementary Approach To Uniform In Time Propagation Of Chaos. ArXiv e-prints, May 2018.
A. Durmus, A. Eberle, A. Guillin, and R. Zimmer. An elementary approach to uniform in time propagation of chaos. In arXiv:1805.11387, 2018.
Y. Feng, D. Wang, and Q. Liu. Learning to draw samples with amortized stein variational gradient descent. In UAI, 2017.
C. R. Givens and R. M. Shortt. A class of wasserstein metrics for probability distributions. Michigan Math. J., 31, 1984.
T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based policies. In ICML, 2017.
J. M. Hernández-Lobato and R. P. Adams. Probabilistic backpropagation for scalable learning of Bayesian neural networks. In ICML, 2015.
C. Li, C. Chen, D. Carlson, and L. Carin. Preconditioned stochastic gradient Langevin dynamics for deep neural networks. In AAAI, 2016.
Y. Li, J. Hernández-Lobato, and R. E. Turner. Stochastic expectation propagation. In NIPS, 2015.
C. Liu and J. Zhu. Riemannian Stein variational gradient descent for Bayesian inference. In AAAI, 2018.
Q. Liu. Stein variational gradient descent as gradient flow. In NIPS, 2017.
Q. Liu and D. Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm. In Neural Information Processing Systems (NIPS), 2016.
Y. Liu, P. Ramachandran, Q. Liu, and J. Peng. Stein variational policy gradient. In UAI, 2017.
11

Under review as a conference paper at ICLR 2019
C. Louizos and M. Welling. Structured and efficient variational deep learning with matrix Gaussian posteriors. In ICML, 2016.
J. Lu, Y. Lu, and J. Nolen. Scaling limit of the Stein variational gradient descent part I: the mean field regime. In arXiv:1805.04035, 2018.
Y. A. Ma, T. Chen, and E. Fox. A complete recipe for stochastic gradient MCMC. In NIPS, 2015. F. Malrieu. Convergence to equilibrium granular media equations and their euler schemes. The
Annnals of Applied Probability, 13(2):540­560, 2003. J. C. Mattingly, A. M. Stuartb, and D. J. Higham. Ergodicity for SDEs and approximations: locally
Lipschitz vector fields and degenerate noise. Stochastic Processes and their Applications, 101(2): 185­232, 2002. M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis. In COLT, 2017. D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. In ICML, 2015. H. Risken. The Fokker-Planck equation. Springer-Verlag, New York, 1989. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Consistency and fluctuations for stochastic gradient Langevin dynamics. JMLR, 17(1):193­225, 2016. C. Villani. Optimal transport: old and new. Springer Science & Business Media, 2008. S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. (exploration of the (Non-)asymptotic bias and variance of stochastic gradient Langevin dynamics. JMLR, 1:1­48, 2016. M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML, 2011. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 1992. P. Xu, J. Chen, D. Zou, and Q. Gu. Global convergence of Langevin dynamics based algorithms for nonconvex optimization. In NIPS, 2018. R. Zhang, C. Chen, C. Li, and L. Carin. Policy optimization as wasserstein gradient flows. In ICML, 2018a. R. Zhang, C. Li, C. Chen, and L. Carin. Learning structural weight uncertainty for sequential decision-making. In AISTATS, 2018b. Y. Zhang, P. Liang, and M. Charikar. A hitting time analysis of stochastic gradient Langevin dynamics. In COLT, 2017. J. Zhuo, C. Liu, J. Shi, J. Zhu, N. Chen, and B. Zhang. Message passing stein variational gradient descent. In ICML, 2018.
12

Under review as a conference paper at ICLR 2019

A DENSITY FUNCTION OF THE MULTI-MODE DISTRIBUTION IN SECTION 3

The negative log-density function of the multi-mode distribution in Section 3 is defined as:

U ()

e ( ) ,3 4

2

-

3 2

10 i=1

ci

sin

1 4

i(+4)

where c = (-0.47, -0.83, -0.71, -0.02, 0.24, 0.01, 0.27, -0.37, 0.87, -0.37) is a vector, ci is the i-th element of c.

B GRONWALL LEMMA

The Gronwall Lemma plays an important role in parts of our proofs, which is stated in Lemma 12.
Lemma 12 (Gronwall Lemma) Let I denotes an interval of the form [a, +) for some a  R. If v(t), defined on I, is differentiable in I and satisfies the following inequality:
v (t)  (t)v(t) ,
where (t) is a real-value continuous function defined on I. Then v(t) can be bounded as:
t
v(t)  v(a) (s)ds
a

C PROOF OF THEOREM 2

To prove Theorem 2, we rely on the definition of generalized derivative in Definition 1.
Definition 1 (Generalized Derivative) Let g and  be locally integrable functions on an open set   Rd, that is, Lebesgue integrable on any closed bounded set F  . Then  is the generalized derivative of g with respect to  on , written as  = g, if for any infinitely-differentiable function u with compact support in , we have

g()u()d = - ()u()d .

Proof The proof relies on further expansions on the definition of generalized derivative on specific functions. Specifically, let the function g in Definition 1 be in a form of g Gf for two functions G and f . The generalized derivative of (Gf )(v, t), written as (Gf )(, t), satisfies

(Gf )(, t)u()d = - Gf (, t)u()d

(20)

for all differentiable function u(·). In Theorem 2, we want to prove a particle representation of the following PDE:
tµt = F1 = - · (µtF (t) + (K  µt)µt) -(Gf )(, t)  tµtu()d = - (Gf )(, t)u()d ,

where f (, t) = µt. Consequently, we have

tf (, t)u()d = - (Gf )(, t)u()d

(21)

By applying (20) in (21), we have tf (, t)u()d = - (Gf )(, t)u()d = Gf (, t)u()d .

13

Under review as a conference paper at ICLR 2019

Since f d µ(d, t), we have

tµ(d, t)u() = G · µ(d, t)u()

d dt

µ(d, t)u() =

G · µ(d, t)u()



d dt

Eµ(t)[u()]

=

Eµ(t)[G

·

 u()]

.

(22)

In

particle

approximation,

we

have

µ(t)

=

1 M

M i=1

(t(i))().

For

each

particle,

let

u()

=

,

(22)

reduces to the following equation:

This completes the proof.

dt(i) = G(t(i))dt .

D PROOF OF THEOREM 3

Note that one challenge in our analysis compared with the analysis for linear SDEs, such as those for SG-MCMC Vollmer et al. (2016); Chen et al. (2015), is how to bound the gap between the original nonlinear PDE (4) and the reduced nonlinear SDE (8). Based on the techniques on analyzing granular media equations in Malrieu (2003); Cattiaux et al. (2008); Durmus et al. (2018), we introduce a nonlinear SDE as an element in-between (6) and (8) like :

d¯t = --1F (¯t) - EY t K(¯t - Y )F (Y ) + K  t(¯t) + 2-1dW¯ t L(¯t) = td

(23)

where L(¯t) denotes the probability law of ¯t, W¯t  Rd is a d-dimensional Brownian motion and Y is a random variable independent of ¯t and just used here for the sake of clarity. In order to match the
SDE system (8) of the particles {t(i)}Mi=1, we duplicate (23) M times, each endowing with an exact solution ¯t(i) indexed by i. The distribution of each particles {¯t(i)}Mi=1 is t and the corresponding W¯ t(i) can be set exactly the same as the Wt(i) in (8):

d¯t(i) = --1F (¯t(i)) - EYit K(¯t(i) - Yi)F (Yi) + K  t(¯t(i)) + L(¯t(i)) = td

2-1dW¯ t(i)

(24)

where Yi is a random variable independent of ¯t(i) and just used here for the convenience of the proof. Proof [Proof of Theorem 3] Firstly we have

d t(i) - ¯t(i)

= - -1

F (t(i)) - F (¯t(i))

1 dt +
M

M

K(t(i) - t(j)) - K  t(¯t(i)) dt

j

-1 M M
j

F (t(j))W (t(i) - t(j)) - EYjt F (Yj )W (¯t(i) - Yj ) dt

d

M

t(i) - ¯t(i) 2

2M

= M

(Aij(t) + Bij(t) + Cij(t) + Fij(t) + Gij(t) + Hij(t))dt

i i,j

(25)

where Aij (t) = --1 F (t(i)) - F (¯t(i)) · t(i) - ¯t(i)

Bij (t) = K(t(i) - t(j)) - K(¯t(i) - ¯t(j)) · t(i) - ¯t(i)

14

Under review as a conference paper at ICLR 2019

Cij (t) = K(¯t(i) - ¯t(j)) - K  t(¯t(i)) · t(i) - ¯t(i) Fij (t) = - F (t(j))K(t(i) - t(j)) - F (¯t(j))K(t(i) - t(j)) · t(i) - ¯t(i) Gij (t) = - F (¯t(j))K(t(i) - t(j)) - F (¯t(j))K(¯t(i) - ¯t(j)) · t(i) - ¯t(i) Hij (t) = - F (¯t(j))K(¯t(i) - ¯t(j)) - EYjt F (Yj )K(¯t(i) - Yj ) · t(i) - ¯t(i)

For the Aij(t) term, according to the i) in Assumption 1 for F , we have

Aij (t) = - -1 F (t(i)) - F (¯t(i)) ·

ij ij

 --1mF M
i

t(i) - ¯t(i) 2

t(i) - ¯t(i)

For the Bij(t) term, applying the concave condition for K and the oddness of K in Assumption 1, we have

M
Bij(t) =
ij ij

K(t(i) - t(j)) - K(¯t(i) - ¯t(j)) · t(i) - ¯t(i)

1M =
2
ij

K(t(i) - t(j)) - K(¯t(i) - ¯t(j)) · t(i) - ¯t(i) - (t(j) - ¯t(j))



-

1 2

mK

M

ij

t(i) - ¯t(i) - (t(j) - ¯t(j))

2
0

For the Cij(t) term, we have



E

(1)
Cij(t) 

E

t(i) - ¯t(i) 2

1/2
E

K(¯t(i) - ¯t(j)) - K  t(¯t(i))

21/2
 

jj



(2)
=

E

t(i) - ¯t(i) 2

1/2


1/2

E

K(¯t(i) - ¯t(j)) - K  t(¯t(i))

2


j

(3)   HK 2M

E t(i) - ¯t(i) 2 1/2

where the (1) is obtained by applying the Cauchy-Schwarz inequality and (2) by the fact that
E K(¯t(i) - ¯t(j)) - K  t(¯t(i)) = 0. We can tune the bandwidth of the RBF kernel to make
K  HK, which is omitted in the Assumption due to the space limit. Hence (3) is obtain by the boundedness of K().

Similarly, since K  1, we have the following result for Hij(t) term,

E Hij(t)

j





E t(i) - ¯t(i) 2

1/2
E

F (¯t(j))K(¯t(i) - ¯t(j)) - EYjt F (Yj )K(¯t(i) - Yj )

21/2
 

j



=

E t(i) - ¯t(i) 2

1/2


1/2

E

F (¯t(j))K(¯t(i) - ¯t(j)) - EYjt F (Yj )K(¯t(i) - Yj )

2


j

15

Under review as a conference paper at ICLR 2019

 HF 2M

E t(i) - ¯t(i) 2 1/2

For the Fij(t) and Gij(t) terms, we have:

Fij(t) = -

F (t(j))K(t(i) - t(j)) - F (¯t(j))K(t(i) - t(j)) · t(i) - ¯t(i)

ij ij

 LF
ij
 2LF M

t(j) - ¯t(j) t(i) - ¯t(i)
t(i) - ¯t(i) 2 .
i

Gij(t) =

F (¯t(j))K(t(i) - t(j)) - F (¯t(j))K(¯t(i) - ¯t(j)) · t(i) - ¯t(i)

ij ij

 HF LK
ij
 3HF LK M

t(i) - ¯t(i) - (t(j) - ¯t(j))
t(i) - ¯t(i) 2
i

t(i) - ¯t(i)

We denote i(t)

E

t(i) - ¯t(i)

2
. Due to the exchangeability of the particles, i(t) are the same

for all the particles. Then according to (25), we have

  (t)  -21(t) + HK 2 + HF 2
M where 1 = -1mF - 3HF LK - 2LF

(t) .



(

(t) -  (HK + HF )/ 2

)

M (-1 - 3HF LK - 2LF )

 -1(

(t) -  (HK + HF )/ 2

)

M (-1 - 3HF LK - 2LF )

Note that t(i) and ¯t(i) are initialized with the same initial distribution µ0 = 0 and we can also set 0(i) to be independent of ¯0(i), we can have (0) = 0. Then according to the Gronwall Lemma, we have
 (t)   (HK + HF )/ 2
M (-1 - 3HF LK - 2LF )

Hence, there exist some positive constant (c1, c2) such that:

(1)
W1(t, t)  W2(t, t)

(2)


E

t(i) - ¯t(i)

2 (3)


 c1

,

M (-1 - c2)

(26)

where (2) holds due to the relationship between W1 and W2 metric Givens & Shortt (1984), (2) due to the definition of W2 and (3) due to the result from the previous proof.

16

Under review as a conference paper at ICLR 2019

E PROOF OF THEOREM 4

Proof [Proof of Theorem 4] Firstly, what we aim at is W1(t, )  c3 exp (-21t) in this theorem. According to the relationship between W1 and W2 metric Givens & Shortt (1984), once we bound W2(t, ) as W2(t, )  c3 exp (-21t), we will finish our proof.

Next, look at the equation (8):
If we set the initial distribution of each particle to be 0, which means 0 = L(0(i)) = 0, we will derive M particles denoted as {t(,i1)}iM=1. We denote the distribution of each t(,i1) at t as t,1.
If we set the initial distribution of each particle to be , which means 0 = L(0(i)) = , we will derive M particles denoted as {t(,i2)}iM=1. We denote the distribution of each t(,i2) at t as t,2.

Since we need to bound W2(t, ), we make the following decomposition:

W2(t, )  W2(t, t,1) + W2(t,1, t,2) + W2(t,2, ) .

(27)

Note that 0,1 = 0 and 0,2 = . Then, according to (26), we have

W2(t, t,1)



 c1 M (-1

- c2)

and

W2(t,2, )



 c1 M (-1

-

c2)

Now we need to focus on the term W2(t,1, t,2). Since W2(t,1, t,2)  E t(,i1) - t(,i2) 2 r(t), we will derive a bound for E t(,i1) - t(,i2) 2 in the following. We have

d t(,i1) - t(,i2) = - -1 F (t(,i1)) - F (t(,i2)) dt

1M +
M
j

K(t(,i1) - t(,j1)) - K(t(,i2) - t(,j2)) dt

1M -
M
j

F (t(,j1))K(t(,i1) - t(,j1)) - F (t(,j2))K(t(,i2) - t(,j2)) dt

d

M t(,i1) - t(,i2) 2
i

2 =
M

M
(i1j (t) + i2j (t) + i3j (t) + i4j (t))dt

i,j

where i1j (t) = --1 F (t(,i1)) - F (t(,i2)) · t(,i1) - t(,i2)

i2j (t) = K(t(,i1) - t(,j1)) - K(t(,i2) - t(,j2)) · t(,i1) - t(,i2)

i3j (t) = - F (t(,j1))K(t(,i1) - t(,j1)) - F (t(,j2))K(t(,i1) - t(,j1)) · t(,i1) - t(,i2)

i4j (t) = - F (t(,j2))K(t(,i1) - t(,j1)) - F (t(,j2))K(t(,i2) - t(,j2)) · t(,i1) - t(,i2)

For the i1j(t) terms, according to the i) in Assumption 1 for F , we have

i1j (t) = - -1 F (t(,i1)) - F (t(,i2)) ·
ij ij

 --1mF M

t(,i1) - t(,i2)

2
.

i

t(,i1) - t(,i2)

17

Under review as a conference paper at ICLR 2019

For the i2j(t) term, applying the concave condition for K and the oddness of K in Assumption 1, we have

M
i2j (t) =
ij ij

K(t(,i1) - t(,j1)) - K(t(,i2) - t(,j2)) · t(,i1) - t(,i2)

1M =
2
ij

K(t(,i1) - t(,j1)) - K(t(,i2) - t(,j2)) · t(,i1) - t(,i2) - (t(,j1) - t(,j2))



-

1 2

mK

M

ij

t(,i1) - t(,i2) - (t(,j1) - t(,j2))

2
0.

For the i3j(t) terms, after applying the LF -Lipschitz property for F and K  1, we have

i3j (t) =

- F (t(,j1))K(t(,i1) - t(,j1)) - F (t(,j2))K(t(,i1) - t(,j1)) · t(,i1) - t(,i2)

ij ij

 LF t(,j1) - t(,j2)
ij

t(,i1) - t(,i2)

 2LF M
i

t(,i1) - t(,i2) 2 .

For the i4j(t) terms :

i4j (t) = -

F (t(,j2))K(t(,i1) - t(,j1)) - F (t(,j2))K(t(,i2) - t(,j2)) · t(,i1) - t(,i2)

ij ij

 HF LK
ij

t(,i1) - t(,i2) - (t(,j1) - t(,j2))

t(,i1) - t(,i2)

 3HF LK M
i

t(,i1) - t(,i2) 2 .

Now we have

r (t)  -2(-1mF - 3HF LK - 2LF )r(t) .

According to the Gronwall lemma, r(t)  r(0)e-21t,
where 1 = -1mF - 3HF LW - 2LF .

Consequently, there exists some positive constant c3 such that

W2(t,1, t,2)  c3e-21t

Then we have

W2(t,

)



c3e-21t

+

 c1 M (-1

-

c2)

+

 c1 M (-1

-

c2)

However, it worth noting that t is the solution of (6) which has nothing to do with the number of particles, M . Then let M  , we can derive that W2(t, )  c3e-21t. Now we finish our proof.

18

Under review as a conference paper at ICLR 2019

F PROOF OF THEOREM 5

To bound the W1(µT , 

T -1 k=0

hk

)

term,

note

the

original

SDE

driving

the

particles

{t(i)}

in

(8)

is

a nonlinear SDE, which is hard to deal with. Fortunately, (8) can be turned into a linear SDE by

concatenating the particles at each time into a single vector representation, i.e., by defining the new

parameter at time t as t [t(1), · · · , t(M)]  RMd. Consequently, t is driven by the following linear SDE:

dt = -F(t)dt + 2-1dWt(Md) ,

(28)

where F(t)

[-1F (t(1))

-

1 M

M j=1

K

(t(1)

-

t(j))

+

1 M

M j=1

K

(t(1)

-

t(j))F (t(j)), ·

·

·

,

-1F (t(M))-

1 M

M j=1

K

(t(M

)

-

t(j)

)

+

1 M

M j=1

K

(t(M

)

-

t(j))F

(t(j

)

)]

is a vector function RMd  RMd, and Wt(Md) is Brownian motion of dimension M d.

Now we define the F(q)(t)

[-1Fq

(t(1)

)

-

1 MN

M j=1

K (t(1)

-

t(j)) +

1 M

M j=1

K

(t(1)

-

t(j))Fq(t(j)), · · · , -1Fq(t(M))

-

1 MN

M j=1

K

(t(M

)

-

t(j))

+

1 M

M j=1

K

(t(M

)

-

t(j))Fq(t(j))]. We can verify that F(t) =

N q=1

F(q)

(t).

Then we define k [k(1), · · · , k(M)] and GIk following result holds:

N Bk

qIk F(q)(k). We can verify that the

k+1 = k - -1GIk hk + 2-1hkk ,

(29)

where k  N (0, IMd×Md). Now we reach the conclusion that k of (29) is accutually the numerical solution of the SDE (28) via stochastic gradients.

We denote the distribution of k as µk and the distribution of t as t . Before proceeding to our theoretical results, we need to present the following Lemmas which is very important in our proof.

Lemma 13

W1(µk, t) 

1 M

W1

(µk

,

t

)

Proof [Proof of Lemma 13] Let us recall the definition of W1 metric and its Kantorovich-Rubinstein

duality Villani (2008), i.e. W1(µ, ) sup g lip1 |Eµ[g()] - E [g()]|. We can prove the

fact that if g() : Rd  R is a Lg-Lipschitz function in Rd, the g(), defined as g() =

1 M

M i

g((i)),

is

a

Lg -Lipschitz

function

in

RM d ,

where



[(1), · · · , (M)]:

g(1) - g(2)

 1 M M i=1

g(1(i)) - g(2(i))

 Lg M M i=1

1(i) - 2(i)

 Lg

 M

M

M
1(i) - 2(i) 2 = Lg 1 - 2
i=1

Then we have:

1 M

M i=1

Ek(i)µk [g(k(i))] - Et(i)t [g(t(i))]

(=1) 1 M

= 1 M

|Ekµk [g(k)] - Ett [g(t)]|

1 M

M
(Ek(i)µk [g(k(i))] - Et(i)t [g(t(i))])
i=1

The (1) holds since Ek(1)µk [g(k(1))] = · · · = Ek(M)µk [g(k(M))] for all the particles k(i) and Et(1)t [g(t(1))] = · · · = Et(M)t [g(t(M))] for all the particles t(i). Then according to the

19

Under review as a conference paper at ICLR 2019

definition of W1 metric, we derive that

W1(µk, t) =

1M sup g Mlip1 i=1

Ek(i)µk [g(k(i))] - Et(i)t [g(t(i))]

=

1 M

sup |Ekµk [g(k)] - Ett [g(t)]|
g lip1

=

1 M

sup |Ekµk [g(k)] - Ett [g(t)]|
g lip1



1 M

W1(µk , t )

Lemma 14 Assuming F (0) = 0. If F in (9) is Lipschitz with constant LF , and satisfies the

dissipative property constant 2-1LF

that +l

F (),   and satisfies

mF  2 - b. Then F in (28) is F(),   (-1mF - m )

lipschitz-continuous with  2 - -1M b, where l

and m are some positive constants.

Proof [Proof of Lemma 14]

F(1) - F(2) =

M
i1 + i2 + i3 2 
i

M
( i1 + i2 + i3 )2
i

where

i1 = -1F (1(i)) - -1F (2(i))  -1LF 1(i) - 2(i)

i2

=

1M (
M

K(1(i) - 1(j))F (1(j)) -

M

K(2(i) - 2(j))F (2(j)))

jj



1M (
M

K(1(i) - 1(j))F (1(j)) -

M

K(2(i) - 2(j))F (1(j)))

jj

+

1M (
M

K(2(i) - 2(j))F (1(j)) -

M

K(2(i) - 2(j))F (2(j)))

jj

 (2LK HF + LF ) 1(i) - 2(i)

i3 =

-

1M (

M

K(1(i) - 1(j)) -

M

K(2(i) - 2(j)))

jj

 LK ( M M
j

1(i) - 2(i) - (1(j) - 2(j)) )

 LK ( 1(i) - 2(i)

1 +
M

M

j

1(j) - 2(j) )

It is easy to verify that there exits some positive constant l such that

F(1) - F(2) =

M
i1 + i2 + i3 2
i

This is reasonable, as F in our setting corresponds to an unnormalized log-posterior, which can be shifted so that F (0) = 0 for a specific problem.

20

Under review as a conference paper at ICLR 2019

MM



2(-1LF + 2LK HF + LF + LK )2 1(i) - 2(i) 2 + 2

1(j) - 2(j) 2

ij

M

 2(-1LF + 2LK HF + LF + LK )2 + 2

1(i) - 2(i) 2

i

  ( 2-1LF + l )

M
1(i) - 2(i) 2

i = ( 2-1LF + l ) 1 - 2

Next, we have

F(), 



= M -1F ((i))(i) + 1

M K((i) - (j))F ((j))(i) - 1

M
K((i) - (j))(i)

MM

ij

j

Notice :

MM

-1F ((i))(i)  -1mF

(i) 2 - -1-1M b

ii

= -1mF  2 - -1M b

Since we have assumed F (0) = 0, we have:

M 1 M K((i) - (j))F ((j))(i)  - 1 M MM

M
LF (i)

ij

ij

(j)

M

 - 2LF

(i) 2 = -2LF  2

i=1

Since K is an odd function, we have:

M

1 M

M

M
K((i) - (j))(i)  -

1 M

M
Lk

(i) - (j)

ij

ij

(i)

M
 -3LK
i

(i) 2 = -3LK  2

As a result, we can derive the following result: F(),   (-1m - 2LF - 3LK )  2 - -1M b

Now it is ready to prove Theorem 5. It worth noting that after assuming F (0) = 0, the first bullet in Assumption 1 recovers the dissipative assumption as F (),   mF  2.

Proof Next we use Lemma C.5 in Xu et al. (2018) to verify that F satisfies the assumptions in

Raginsky et al. (2017) by setting  =

a B

with a

a positive constant and B the size of the random set

I. Let µk := L(k) and t := L(t). Now we can borrow the result of Lemma 3.6 in Raginsky

et al. (2017). The relative entropy DKL(µk kh) satisfies:

DKL(µk

kh)



(A0

a B

+ A1h)kh

21

Under review as a conference paper at ICLR 2019

with

A0 =

2( LF + l )2 

a2

+

2(1



1 -1mF

-

m

)(2a12

+

Md )


A1

=6( LF 

+ l )2(A0 + M d)

+ a12

and a1, a2 are some positive constants. When the  is small enough, there exist some positive constants a3, a4 such that

A0



a3

Md 3 , A1



a4

Md 4

Similar to the proof of Lemma 14, it is easy to verify that there exists some positive constant a5 such that F(1) - F(2), 1 - 2  (-1mF - a5) 1 - 2 2. Notice, when  is small enough, (28) satisfies the conditions of Proposition 4.2 in Cattiaux et al. (2008). Hence, there exits

some positive constant C such that W1(µk, kh)  C DKL(µk kh) .

According to Corollary 4 and Lemma 8 in Bolley & Villani (2005), we can derive an explicit expression for C :
C  a6-1M d ,
when  is small enough and a6 is some positive constant.

Applying the Lemma 13, we have

W1(µk, kh)



1 M

W1

(µk

,

kh)



a6

M

d

3 2



-3

(a3

a

2B-1

111
+ a4h) 2 k 2 h 2

Let k = T and we can finish the poof.

G PROOF OF THEOREM 6

Proof Our proof is based on the proof of Lemma 3.6 in Raginsky et al. (2017) with some modifications. Firstly, adopt the same notations in the Section F and we get the following update:

k+1 = k - -1GIk hk + 2-1hkk ,

(30)

where

k



N (0, IMd×Md)

and

hk

=

h0 k+1

.

We

assume

E(GIk )

=

F (k ),

  RMd , which

is a general assumption due to the way that we choose the minibatch Ik. We need to define q(t),

which will be used in the following proof:

k-1

k

q(t) = {k  R| hi  t < hi} .

i=0 i=0

Furthermore, we define

-1 i=0

hi

0 and

0 i=0

hi

h0 here just used for the convenience of

statement in the following.

Now we focus on the following continuous-time interpolation of k:

t  q(s)-1 

(t) =0 - G~I(s) (

hi) ds +

0 i=0

2 

t
Ws(M d) ,
0

where I(s)  Ik for t 

k-1 i=0

hi

,

k i=0

hi

, G~I(s)()

N B(s)

qI(s) F(q)() and B(s) is

the size of the minibatch I(s). And for each k, (

k-1 i=0

hi)

and

k

have

the

same

probability

law

k . Since (t) is not a Markov process, we define the following Itô process which has the same

one-time marginals as (t)

t
(t) = 0 - Gs ((s)) ds +
0

2 

t
Ws(M d)
0

22

Under review as a conference paper at ICLR 2019

  q(t)-1 



where Gt(x) := E G~I(t) (

hi) |(t) = x .

i=0

Let Pt := L ((s) : 0  s  t) and Pt := L ((s) : 0  s  t). According to the proof of lemma 3.6 in Raginsky et al. (2017), we can derive a similar result for the relative entropy of Pt and Pt :

DKL(Pt Pt ) = -

d

Pt

log

d d

Pt Pt

 =
4

t
E F((s)) - Gs ((s)) 2ds
0

 =
4

t
E F((s)) - Gs ((s)) 2ds
0

The last line follows that L((s)) = L((s)), s.

In the following proof, we will let t =

k-1 i=0

hi

for

some

k



R.

Now

we

can

use

the

martingale

property of Itô integral to derive:

D (P P )KL

k-1 i=0

hi



k-1 i=0

hi



 k-1 =
4
j=0

j i=0

hi

j-1 i=0

hi

E

F((s)) - Gs ((s))

2ds

  k-1 2
j=0
 k-1 +
2
j=0

j i=0

hi

q(s)-1

E F((s)) - F((

hi)) 2ds

j-1 i=0

hi

i=0

j i=0

hi

E

j-1 i=0

hi

q(s)-1

 q(s)-1 

F ((

hi)) - G~I(s) (

hi)

i=0 i=0

2ds

 L2F k-1 2
j=0

j i=0

hi

q(s)-1

E (s) - (

hi) 2ds

j-1 i=0

hi

i=0

 k-1 +
2
j=0

j i=0

hi

E

j-1 i=0

hi

q(s)-1

 q(s)-1 

F ((

hi)) - G~I(s) (

hi)

i=0 i=0

2ds

(31) (32)

For the first part (31), we consider some s  [

j-1 i=0

hi,

j-1

j-1

(s) - ( hi) = - (s - hi)GIj +

i=0 i=0

j i=0

hi).

The

following

equation

holds:

2/ (Ws(M d)

-

W (Md) j-1 i=0

)
hi

j-1

j-1

= -(s - hi)GIj + (s - hi)(F(j ) - GIj )
i=0 i=0

+

2/ (Ws(M d)

-

W (Md) j-1 i=0

)
hi

Thus, we can use Lemma 3.1 and 3.2 in Raginsky et al. (2017), and Lemma C.5 in Xu et al. (2018) to get the following result:

j-1
E (s) - ( hi) 2

i=0

3

(j

h02 + 1)2

E

GIj

2

+

3

(j

h0 +

2
1)2

E

F(j ) - GIj

2 + 6h0M d (j + 1)

12 (j

h02 + 1)2

0mjakx-1(LF2  E

j

2

+

b1)

+

6h0M d (j + 1)

23

Under review as a conference paper at ICLR 2019

where b1 is some positive constant. Consequently, the first part above (31) can be bounded as:

L2F k-1 2
j=0

j i=0

hi

q(s)-1

E (s) - (

hi) 2ds

j-1 i=0

hi

i=0

 LF2  k-1 2

12 (j

h03 + 1)3

0mjaKx-1(L2F E

j

2

+

b1)

+

6h02M d (j + 1)2

j=0

2L2F h03 0mjaKx-1(LF2  E

j

2

+

b1)

+

2L2F h02M d 2

,

where the last line follows from the fact that

k-1 1 (j + 1)3



k-1

(j

1 +

1)2





1 (j + 1)2

2 =
6

.

j=0

j=0

j=0

According to Lemma C.5 in Xu et al. (2018), the second part (32) can be bounded as follows:

 k-1 2
j=0

j i=0

hi

E

j-1 i=0

hi

q(s)-1

 q(s)-1 

F ((

hi)) - G~I(s) (

hi)

i=0 i=0

2ds

=

k-1

h0 2(j +

1)

E

F(j ) - GIj

2

j=0



h0 0mjakx-1(LF2  E

j

2

4 + b1)  B0

k-1
+
j=1

(j

+ 1)(B0

4

+

log

100 99

(j

 + 1))



h0 0mjakx-1(L2F E

j

2

4 + b1)  B0

k-1
+
j=1

(j

4

+

1)log

100 99

(j

 + 1)

(b2

+

4 B0 )h0

0mjakx-1(L2F E

j

2 + b1) ,

where the last line follows from the fact that when r > 1,

k-1 4 (j + 1) logr(j + 1)





4 (j + 1) logr(j + 1)

 4 log1-r 2 . r-1

j=1

j=1

Denote µk := L(k) and t := L(t). Due to the data-processing inequality for the relative entropy, we have

DKL(µk

 )  D (P

k-1 i=0

hi

KL

k-1 i=0

hi



P )k-1 i=0

hi



 2LF2  h03 0mjakx-1(L2F E

j

2

+

b1)

+

2LF2  h02M d 2

+

(b2

+

4 B0 )h0

0mjakx-1(LF2  E

~ j

2 + b1)



(2L2F h03

+

b2h0

+

4 B0

h0)

0mjakx-1(L2F E

j

2

+

b1)

+

2LF2  h02M d 2

.

Lemma 3.2 in Raginsky et al. (2017) has provided a uniform bound to max0jk-1(L2F E j 2 +

b1).

Hence

we

can

tell

that

DKL(P

k-1 i=0

hi

P

k-1 i=0

hi )

would

not

increase

w.r.t.

k.

This

is

a

nice

24

Under review as a conference paper at ICLR 2019

 property that the fixed-step-size SPOS does not endow. Since LF = 2-1LF + l , it is easy to verify that when  is small enough, there exists some positive constants b3, b4, b5 and b6 such that:

DKL(µk

 )

k-1 i=0

hi

(2L2F h03

+

b2h0

+

4h0 B0

)

0mjaKx-1(LF2  E

~ j

2

+

b1)

+

2LF2  h02M d 2

(b3h03

+

b43h0 B0

+

b5h02

2

)

Md 4

.

Similar to the proof of Theorem 5, we can bound the W1(µk



k-1 i=0

)
hi

term

with

Corollary

4,

Lemma 8 in Bolley & Villani (2005) and Proposition 4.2 in Cattiaux et al. (2008). Specifically, when

 is small enough, there exist some positive constant a6 such that:

W1(µk



k-1 i=0

)
hi



Md a6( 

)

DKL(µk

 )

k-1 i=0

hi



a6-3M

3 2

d

3 2

(b3h03

+

b43h0 B0

+

b5h022)

1 2

.

According to Lemma 13, we have

W1(µk, kh)



1 M

W1(µk

 )

k-1 i=0

hi

=

a6



-3

M

d

3 2

(b3

h03

+

b43h0 B0

+

b5

h02



2

)

1 2

Let k = T and we can finish the proof.

H DETAILED STATEMENTS OF NONASYMPTOTIC CONVERGENCE UNDER THE CONVEX CASE

We give detailed statements of non-asymptotic convergence of SPOS under the convex case, which can be proved by directly combining results from Theorem 3­6, thus they are omitted for simplicity. We consider a fixed-stepsize and a decreasing-stepsize cases in Theorem 15 and Theorem 16, respectively.

Theorem 15 (Fixed Stepsize) Under Assumption 1 and F (0) = 0, if we set hk = h0 and Bk = B0, we can bound the W1(µT , ) as

W1(µT

,

)

  c1 M (-1

-

c2)

+

c3

exp

-2

-1mF - 3HF LK - 2LF

+

c3M

d

3 2

-3(c42B-1

+

c5h)

1 2

T

1 2

h

1 2

.

Th

where (c1, c2, c3, c4, c5, c6, ) are some positive constants such that

1 

> c2 and

mF 

> 3HF LK -

2LF .

Theorem 16 (Decreasing Stepsize) Under Assumption 1 and F (0) = 0, if we set hk = h0/(k + 1) and Bk = B0 + [log(k + 1)]100/99, we can bound the W1(µT , ) as

W1(µT ,

)

  c1 M (-1

-

c2)

+

c3

exp

-2 -1mF - 3HF LK - 2LF

+

c3-3M

d

3 2

(c6h03

+

c73h0 B0

+

c8h20

2)

1 2

.

T -1
( hk)
k=0

where (c1, c2, c3, c6, c7, c8, ) are some positive constants such that

1 

> c2 and

mF 

> 3HF LK -

2LF , as

25

Under review as a conference paper at ICLR 2019

I PROOF OF THEOREM 7 AND COROLLARY 8

Proof [Proof of Theorem 7]

d t(i) - t(j) = --1 F (t(i)) - F (t(j)) dt

1M +
M

K(t(i) - t(q)) - K(t(j) - t(q)) dt

q

1M -
M
q

F (t(q))K(t(i) - t(q)) - F (t(q))K(t(j) - t(q))) dt

+

2 

(dWt(i)

-

dWt(j))


M
d E
ij



t(i) - t(j)

2M
 = -E2

-1

F (t(i)) - F (t(j))

ij

t(i) - t(j) dt

M 2M +E M

K(t(i) - t(q)) - K(t(j) - t(q))

ij q

t(i) - t(j) dt

M 2M -E M
ij q

F (t(q))K(t(i) - t(q)) - F (t(q))K(t(j) - t(q)))

t(i) - t(j) dt

M
+ E2
ij

2 

(dWt(i)

-

dWt(j))

t(i) - t(j)

M
 -2-1mF E
ij

t(i) - t(j)

2
dt - 2mK E

M

ij

t(i) - t(j)

2
dt

M
+ 2HF LK E
ij

t(i) - t(j)

2
dt

 1/2 

+2

2  E

M

(dWt(i) - dWt(j))2

M
E

ij ij

1/2

t(i) - t(j)

2


.

Denote z(t)= E

M ij

t(i) - t(j)

2
. We have

z(t)  -(2-1mF + 2mK - 2HF LK )z(t) + 4M

d z(t)


We can finish our proof by applying Gronwall Lemma on (33).

(33)

Proof [Proof for Corollary 8]

d t(i) - t(j) =

1M +
M
q
1M -
M
q

K(t(i) - t(q)) - K(t(j) - t(q)) dt F (t(q))K(t(i) - t(q)) - F (t(q))K(t(j) - t(q))) dt

26

Under review as a conference paper at ICLR 2019


M
d E
ij



t(i) - t(j)

2
=

M 2M +E M

K(t(i) - t(q)) - K(t(j) - t(q))

ij q

t(i) - t(j) dt

M
-E

2 M

M

ij q

F (t(q))K(t(i) - t(q)) - F (t(q))K(t(j) - t(q)))

t(i) - t(j) dt

M
 -2mK E
ij

t(i) - t(j)

2M
dt + 2HF LK E

ij

t(i) - t(j)

2
dt

Denote z(t)= E

M ij

t(i) - t(j)

2
. We have

z(t)  -(2mK - 2HF LK )z(t) We can finish our proof by applying Gronwall Lemma on (34).

(34)

J PROOF OF THEOREM 9

Proof [Proof of Theorem 9] Our conclusion for B~(µ^T , µ^) is essentially a specification of the result in Mattingly et al. (2002), which has also been applied in Xu et al. (2018).

Specifically, we rely on the following lemma, which is essentially Theorem 7.3 in Mattingly et al. (2002) and Lemma C.3 in Xu et al. (2018), considering the SDE (16):
dt = -F(t)dt + 2-1dWt(Md)
As mentioned in Section 5, we firstly denote the distribution of t as t. Then we define the ^k [^k(1), · · · , ^k(M)]  RMd, which is actually the numerical solution of (16) using full gradient with Euler method. And we denote the distribution of ^k as µ^k.

Lemma 17 Let F be Lipschitz-continuous with constant L, and satisfy the dissipative property that F(),   m  2 - b. Define V() = C0 + L/2  2. The Euler method for (16) has a unique invariant measure µ^, and for all test function f such that |f|  V(), we have

E[f(^k))] - E^µ^ [f (^)]  C-Md/2(1 + emh) exp

- 2mkhMd log()

,

where   (0, 1), C > 0 are positive constants, and  = 2L(b + m + M d)/m.

Now

we adopt

the

f

:

RM d



R

defined as f()

=

1 M

M i

f ((i)),

where

f

:

Rd



R is

a Lf -Lipschitz function and satisfies our Assumption 2 and [(1), · · · , (M)]. Similar to the

proof of Lemma 12, we can find that f : RMd is a Lf / M -Lipschitz function. Furthermore, according to the Lemma 14, it is easily check that F is L-Lipschitz where L = 2-1LF + l . Hence, when the  is small enough, we have Lf / M  2-1LF + l . Then we can set the C0

large enough to force f to satisfy the condition in Lemma 17 that |f|  V(). Then, according

to the exchangeability of the particle system {^k(i)} and Lemma 14, we can bound the B~(µ^T , µ^) as

B~(µ^T , µ^)  E[f(^T ))] - E^µ^ [f (^)]  C2-Md/2(1 + emh) exp -2mT hMd/ log()

27

Under review as a conference paper at ICLR 2019

 where  = 2L(M b + m + M d)/m, L = 2-1LF + l , m = -1m - m , and (, C2, l , m ) are some positive constants independent of (T, M, h) and   (0, 1).
To prove the bound for B~(µ^, ) Since ^k = (^k(1), · · · , ^k(M)) can be considered as a solution to the SDE (16), standard results from linear FP equation can be applied. Specifically, for the B~(µ^, ) term, we rely on the following lemma adapted from Lemma C.4 in Xu et al. (2018)Chen et al. (2015), and is essentially the work of Chen et al. (2015) when taking K  .

Lemma 18 Under the same assumption as in Lemma 17, for the Lipschitz-continuous function

f ()

=

1 M

M i

f ((i))

mentioned

above,

the

following

bound

is

satisfied

for

some

positive

constant C:

1 K

K -1
E[f (^k )]

-

E [f ()]



h C(

+



).

 Kh

k=1

The uniqueness of invariant measure of the Euler method from Lemma 17 implies the numerical
solution ^k to be ergodic. Then similar to the proof of Lemma 4.2 in Xu et al. (2018), we consider the case where K  . Take average over the {^k}Kk==01, we have

E^µ^ [f(^)]

=

lim
K 

1 K

K -1
E[f (^k )]

k=1

Now according to the exchangeability of the particle system {^k(i)} and {t(i)}, we can bound the B~(µ^, ) as :

B~(µ^, )  E^µ^ [f(^)] - E [f()]  C3h/ where C3 are some positive constant.

K PROOF OF THEOREM 10

Proof [Proof of Theorem 10] Adopting the same notation used in the proof of the Theorem 5, we

define k

[k(1), · · · , k(M)] and GIk

N Bk

qIk F(q)(k). We denote the distribution of k

as µk.

k+1 = k - -1GIk hk + 2-1hkk ,

We firstly give a bound to the W2(µk, µ^k ) (the definition of the µ^k has been mention in the last section). According to the proof of the Lemma 4.4 in Xu et al. (2018)

W2(µk, µ^k )  kh(L + M C4) ((6 + 2 )/B)1/2

where of (T,

 M,

= 2(1 + h). Note

1/m)(M b the fact that

+ 2M 2C42 + W1(µk, µ^k)

M 

d/) and C4 is some positive W2(µk , µ^k ) and W1(µk, µ^k

constant independent

)



1 M

W1

(µk

,

µ^k

)

(see the proof of Lemma 13, similar result holds here), We get

W1(µT , µ^T )  T h(L + M C4) ((6 + 2 )/(BM ))1/2 .

Let us compare the definition of W1(µ, ) and B~(µ, )

W1(µ, ) sup |Eµ[g()] - E [g()]|
g lip1
B~(µ, ) |Eµ[f ()] - E [f ()]|

and we can derive the result that B~(µT , µ^K )  Lf W1(µT , µ^T ). Now we finish our proof.

28

Under review as a conference paper at ICLR 2019

L DISCUSSION ON THE COMPLEXITY OF OUR METHOD
The complexity of an algorithm mainly refers to its time complexity (corresponding to the number of iterations in our method i.e. T) and space complexity (corresponding to the number of particles used in our method i.e. M). Hence the complexity of our method can be well explored with our work, since our non-asymptotic convergence theory was developed w.r.t. the number of particles i.e. M and iterations i.e. T. Their relationship (tradeoff) was even discussed further in Experiment 6.1. Moreover, comparing (9) with (3) , one can easily find that our space complexity is exactly the same as SVGD and our computational time in each iteration is almost the same as SVGD with an extra addition operation. However, it worth noting that our method havs much better performance in practice and no "pitfall" verified by both our theory and our experiments. .

M COMPARISON WITH RELATED WORK

Firstly, our proposed framework SPOS is different from the recently proposed particle-optimization sampling framework (Chen et al., 2018), in the sense that we solve the nonlinear PDE (6) stochastically. For example they solve the equation in (6) t = -1 · t approximately using blob method adopted from (Carrillo et al., 2017). Secondly, our method is also distinguishable to existing work on granular media equations such as (Durmus et al., 2018). Their work about the Granular media equations focuses on the following PDE:

tt =  · t-1F () + t (K  t()) + -1t ,

(35)

whereas our framework focuses on the following one:

tt =  · t-1F () + t (EY t K( - Y )F (Y ) - K  t()) + -1t . (36)

The extra term t (EY t K( - Y )F (Y )) in our framework makes the analysis much more challenging. The main differences are summarized below:

· Formulations are different. The extra term EY µt K( - Y )F (Y ) cannot be combined with the F () term in their (35). This is because function F () itself is a function independent
of t; while EY µt K( - Y )F (Y ) depends on both  and t. This makes our problem much more difficult.

· Assumptions are different. For example, the analysis on granular media equations in (Cattiaux et al., 2008) requires that F satisfies a special condition C(A, ), which is a strong condition impractical to be satisfied in our case; And Durmus et al. (2018) adopts different assumptions from ours with a different goal.

· For the Euler integrator, Durmus et al. (2018) does not consider an Euler solution. Further-

more, our sampling method needs "stochastic gradient" i.e. Gk(i)

N Bk

qIk Fq(k(i)) in

(9) for computational feasibility, which is quite different from the former work on particle-

SDE such as (Malrieu, 2003; Cattiaux et al., 2008). Few of the former work on particle-SDE

considered the stochastic gradient issue.

To sum up, the main purpose of our paper is to provide a non-asymptotic analysis of our method instead of improving the former work on a certain type of PDE. This is also the reason why we said that part of our proof techniques are based on those for analyzing granular media equations.

N EXTRA EXPERIMENTS
N.1 TOY EXPERIMENTS
We compare the proposed SPOS with other popular methods such as SVGD and standard SGLD on four mutil-mode toy examples. We aim to sample from four unnormalized 2D densities p(z)/exp{U (z)}, with the functional form provided in Rezende & Mohamed (2015). We optimize/sample 50 and 2000 particles to approximate the target distributions. The results are illustrated in Figure 4 and Figure 5, respectively.

29

Under review as a conference paper at ICLR 2019
Figure 4: Illustration of different algorithms on toy distributions. Dots are the final particles; the blue regions represent ground true densities. Each column is a distribution case. First row: standard SGLD; Second row: SVGD; Third row: SPOS.
Figure 5: Illustration of different algorithms on toy distributions. Dots are the final particles; the blue regions represent densities estimated by the particles. Each column is a distribution case. First row: ground true densities; Second row: standard SGLD; Third row: SVGD; Fourth row: SPOS. N.1.1 BAYESIAN NEURAL NETWORKS FOR MNIST CLASSIFICATION We perform the classification tasks on the standard MNIST dataset. A two-layer MLP 784-X-X-10 with ReLU activation function is used, with X being the number of hidden units for each layer. The training epoch is set to 100. The test errors are reported in Table 2. Surprisingly, the proposed SPOS outperforms other algorithms such as SVGD at a significant level, though it is just a simple modification of SVGD by adding in random Gaussian noise. This is partly due to the fact that our SPOS algorithm can jump out of local modes efficiently, as explained in Section 4.4.
30

Under review as a conference paper at ICLR 2019

Table 2: Classification error of FNN on MNIST.

Method
SPOS SVGD SGLD RMSprop RMSspectral SGD BPB, Gaussian SGD, dropout

Test Error 400-400 800-800 1.32% 1.24% 1.56% 1.47% 1.64% 1.41% 1.59% 1.43% 1.65% 1.56% 1.72% 1.47% 1.82% 1.99% 1.51% 1.33%

31

