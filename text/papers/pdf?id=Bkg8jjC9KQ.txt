Under review as a conference paper at ICLR 2019
Optimistic Mirror Descent in Saddle-Point Problems: Going the Extra(-Gradient) Mile
Anonymous authors Paper under double-blind review
Abstract
Owing to their connection with generative adversarial networks (GANs), saddlepoint problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convexconcave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality ­ a property which we call coherence. We first show that ordinary, "vanilla" MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an "extra-gradient" step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. [2018] for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets).

Figure 1: Mirror descent (MD) in the non-monotone saddle-point problem f (x1, x2) = (x1 - 1/2)(x2 - 1/2) +

1 3

exp(-(x1

-

1/4)2

-

(x2

-

3/4)2).

Left:

vanilla

MD

spirals

outwards;

right:

optimistic

MD

converges.

1 Introduction

The surge of recent breakthroughs in artificial intelligence (AI) has sparked significant interest in solving optimization problems that are universally considered hard. Accordingly, the need for an effective theory has two different sides: first, a deeper theoretical understanding would help demystify the reasons behind the success and/or failures of different training algorithms; second, theoretical advances can inspire effective algorithmic tweaks leading to concrete performance gains.
Deep learning has been an area of AI where theory has provided a significant boost. As a functional class, deep learning involves non-convex loss functions for which finding even local optima is NPhard; nevertheless, elementary techniques such as gradient descent (and other first-order methods) seem to work fairly well in practice. For this class of problems, recent theoretical results have indeed provided useful insights: using tools from the theory of dynamical systems, Lee et al. [2016, 2017] and Panageas & Piliouras [2017] showed that a wide variety of first-order methods (including gradient

1

Under review as a conference paper at ICLR 2019
descent and mirror descent) almost always avoid saddle points. More generally, the optimization and machine learning communities alike have dedicated significant effort in understanding the geometry of non-convex landscapes by searching for properties which could be leveraged for efficient training. For example, the well-known "strict saddle" property was shown to hold in a wide range of salient objective functions ranging from low-rank matrix factorization [Ge et al. 2016, 2017, Bhojanapalli et al. 2016] and dictionary learning [Sun et al. 2017b,a], to principal component analysis [Ge et al. 2015], phase retrieval [Sun et al. 2016], and many other models.
On the other hand, adversarial deep learning is nowhere near as well understood, especially in the case of generative adversarial networks (GANs) [Goodfellow et al. 2014]. Despite an immense amount of recent scrutiny, our theoretical understanding cannot boast similar breakthroughs as in the case of "single-agent" deep learning. To make matters worse, GANs are notoriously hard to train and standard optimization methods often fail to converge to a reasonable solution. Because of this, a considerable corpus of work has been devoted to exploring and enhancing the stability of GANs, including techniques as diverse as the use of Wasserstein metrics [Arjovsky et al. 2017], critic gradient penalties [Gulrajani et al. 2017], different activation functions in different layers, feature matching, minibatch discrimination, etc. [Radford et al. 2015, Salimans et al. 2016].
A key observation in this context is that first-order methods may fail to converge even in toy, bilinear zero-sum games like Rock-Paper-Scissors and Matching Pennies [Piliouras & Shamma 2014, Papadimitriou & Piliouras 2016, Daskalakis et al. 2018, Mescheder et al. 2018, Mertikopoulos et al. 2018, Bailey & Piliouras 2018]. This is a critical failure of descent methods, but one which Daskalakis et al. [2018] showed can be overcome through "optimism", interpreted in this context as a momentum adjustment that pushes the training process one step further along the incumbent gradient. In particular, Daskalakis et al. [2018] showed that optimistic gradient descent (OGD) succeeds in cases where vanilla gradient descent (GD) fails (specifically, unconstrained bilinear saddle-point problems), and leveraged this theoretical result to improve the training of GANs.
A common theme in the above is that, to obtain a principled methodology for training GANs, it is beneficial to first establish improvements in a more restricted setting, and then test whether these gains carry over to more demanding learning environments. Following these theoretical breadcrumbs, we focus on a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality, a property which we call coherence. Then, motivated by the success of mirror descent (MD) methods in online/stochastic convex programming, and hoping to overcome the shortcomings of ordinary gradient descent by exploiting the problem's geometry, we examine the convergence of MD in coherent problems. On the positive side, we show that if a problem is strictly coherent (a condition that is satisfied by all strictly monotone problems), MD converges almost surely, even in stochastic problems (Theorem 3.1). However, under null coherence (the "saturated" opposite to strict coherence), MD spirals outwards from the problem's solutions and may cycle in perpetuity, even with perfect gradient feedback. The null coherence property covers all bilinear models, so this result generalizes and extends the recent analysis of Daskalakis et al. [2018] and Bailey & Piliouras [2018] for gradient descent and follow-the-regularized-leader (FTRL) respectively (for a schematic illustration, see Figs. 1 and 5). Thus, in and by themselves, gradient/mirror descent methods do not suffice for training convoluted, adversarial deep learning models.
To mitigate this deficiency, we introduce an extra-gradient step which allows the algorithm to look ahead and take an "optimistic" mirror step along a "future" gradient. Following Rakhlin & Sridharan [2013], this method is known as optimistic mirror descent (OMD), and was first studied under the name "mirror-prox" by Nemirovski [2004]. In convex-concave problems, Nemirovski [2004] showed that the so-called "ergodic average" of the algorithm's iterates enjoys an O(1/n) convergence rate. In the context of GAN training, Gidel et al. [2018] further introduced a "gradient reuse" mechanism to minimize the computational overhead of back-propagation and proved convergence in stochastic convex-concave problems. However, beyond the monotone regime, averaging offers no tangible benefits because Jensen's inequality no longer applies; as a result, moving closer to GANs requires changing both the algorithm's output structure as well as the accompanying analysis.
Our first result in this direction is that the last iterate of OMD converges in all coherent problems, including null-coherent ones. As a special case, this generalizes and extends the results of Daskalakis et al. [2018] for OGD in bilinear problems, and also settles in the affirmative an issue left open by the authors concerning the convergence of the algorithm in nonlinear problems. In addition, under the OMD algorithm, the (Bregman) distance to a solution decreases monotonically, so each iterate is better than the previous one (Theorem 4.1). Finally, under strict coherence, we also show that
2

Under review as a conference paper at ICLR 2019

OMD converges with probability 1 in stochastic saddle-point problems (Theorem 4.3). These results suggest that a straightforward, extra-gradient add-on can lead to significant performance gains when applied to existing state-of-the-art first-order methods (such as Adam). This theoretical prediction is validated experimentally in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-10 datasets) in Section 5.

2 Problem setup and preliminaries

Saddle-point problems. Consider a saddle-point problem of the general form

min
x1 X1

max
x2 X2

f (x1,

x2),

(SP)

where each feasible region Xi, i = 1, 2, is a compact convex subset of a finite-dimensional normed space Vi  di , and f : X  X1 × X2  denotes the problem's value function.1 From a gametheoretic standpoint, (SP) can be seen as a zero-sum game between two optimizing agents (or players):
Player 1 (the minimizer) seeks to incur the least possible loss, while Player 2 (the maximizer) seeks to obtain the highest possible reward ­ both given by f (x1, x2).

To obtain a solution of (SP), we will focus on incremental processes that exploit the individual loss/reward gradients of f (assumed throughout to be at least C1-smooth). Since the individual
gradients of f will play a key role in our analysis, we will encode them in a single vector as

g(x) = (g1(x), g2(x)) = (x1 f (x1, x2), -x2 f (x1, x2)),

(2.1)

and, following standard conventions, we will treat g(x) as an element of Y  V, the dual of the ambient space V  V1 × V2, assumed to be endowed with the product norm x 2 = x1 2 + x2 2.

Variational inequalities and coherence. Most of the literature on saddle-point problems has

focused on the monotone case, i.e., when f is convex-concave. In such problems, it is well known that

solutions of (SP) can be characterized equivalently as solutions of the associated (Minty) variational

inequality:

g(x), x - x  0 for all x  X .

(VI)

Importantly, this equivalence extends well beyond the realm of monotone problems: it trivially includes all bilinear problems ( f (x1, x2) = x1 Mx2), quasi-convex-concave objectives (where Sion's minmax theorem applies), etc. For a concrete non-monotone example, consider the problem

min
x1 [-1,1]

max (
x2 [-1,1]

x14

x22

+

x12

+

1)(x12 x24

-

x22

+

1).

(2.2)

The only saddle-point of f is x = (0, 0): it is easy to check that x is also the unique solution of the corresponding problem (VI), despite the fact that f is not even (quasi-)monotone.2 This shows that
the equivalence between (SP) and (VI) encompasses a wide range of phenomena that are innately incompatible with convexity/monotonicity, even in the lowest possible dimension; for an in-depth
discussion of the links between (SP) and (VI), we refer the reader to Facchinei & Pang [2003].

Motivated by this equivalence, we introduce below the notion of coherence:

Definition 2.1. We say that (SP) is coherent if every saddle-point of f is a solution of the associated variational inequality problem (VI) and vice versa. If (VI) holds as a strict inequality whenever x is not a saddle-point of f , (SP) will be called strictly coherent; by contrast, if (VI) holds as an equality for all x  X , we will say that (SP) is null-coherent.

The notion of coherence will play a central part in our considerations, so a few remarks are in order. First, to the best of our knowledge, its first antecedent is a gradient condition examined by Bottou [1998] in the context of nonlinear programming; we borrow the term "coherence" from the more recent paper of Zhou et al. [2017] (who actually used the term to describe strict coherence). We should also note that it is possible to relax the equivalence between (SP) and (VI) by positing that only some of the solutions of (SP) can be harvested from (VI). Our analysis still goes through in this case but, to keep things simple, we do not pursue this relaxation here.

1Compactness is assumed chiefly to streamline our presentation. The convex-closed framework can be dealt
with via a coercivity assumption; however, this would take us too far afield, so we do not pursue this direction. 2To see this, simply note that f (x1, x2) is multi-modal in x2 for certain values of x1.

3

Under review as a conference paper at ICLR 2019

Finally, regarding the distinction between coherence and strict coherence, we show in Appendix A that (SP) is strictly coherent when f is strictly convex-concave. At the other end of the spectrum, typical examples of problems that are null-coherent are bilinear objectives with an interior solution: for instance, f (x1, x2) = x1 x2 with x1, x2  [-1, 1] has g(x), x = x1 x2 - x2 x1 = 0 for all x1, x2  [-1, 1], so it is null-coherent. Finally, neither strict, nor null coherence imply a unique solution to (SP), a property which is particularly relevant for GANs.

3 Mirror descent

The method. Motivated by its prolific success in convex programming, our starting point will be the well-known mirror descent (MD) method of Nemirovski & Yudin [1983], suitably adapted to our saddle-point context; for a survey, see Hazan [2012] and Bubeck [2015].
The basic idea of mirror descent is to generate a new state variable x+ from some starting state x by taking a "mirror step" along a gradient-like vector y. To do this, let h : X  be a continuous and K-strongly convex distance-generating function (DGF) on X , i.e.,

h(tx

+

(1

-

t)x

)



th(x)

+

(1

-

t)h(x

)

-

1 Kt(1

-

t)

x

-x

2,

2

(3.1)

for all x, x  X and all t  [0, 1]. In terms of smoothness (and in a slight abuse of notation), we
also assume that the subdifferential of h admits a continuous selection, i.e., a continuous function h : dom h  Y such that h(x)  h(x) for all x  dom h.3 Then, following Bregman [1967], h
generates a pseudo-distance on X via the relation

D(p, x) = h(p) - h(x) - h(x), p - x for all p  X , x  dom h.

(3.2)

This pseudo-distance is known as the Bregman divergence. As we show in Appendix B, we have

D(p, x) 

1 2

K

x-

p

2, so the convergence of a sequence Xn to some target point

p can be verified by

showing that D(p, Xn)  0. On the other hand, D(p, x) typically fais to be symmetric and/or satisfy

the triangle inequality, so it is not a true distance function per se. Moreover, the level sets of D(p, x)

may fail to form a neighborhood basis of p, so the convergence of Xn to p does not necessarily imply that D(p, Xn)  0; we provide an example of this behavior in Appendix B. For technical reasons, it will be convenient to assume that such phenomena do not occur, i.e., that D(p, Xn)  0 whenever
Xn  p. This mild regularity condition is known in the literature as "Bregman reciprocity" [Chen &

Teboulle 1993, Kiwiel 1997], and it will be our standing assumption in what follows (note also that it

holds trivially for both Examples 3.1 and 3.2 below).

Now, as with standard Euclidean distances, the Bregman divergence generates an associated proxmapping defined as

Px(y) = arg min{ y, x - x + D(x , x)} for all x  dom h, y  Y.
x X

(3.3)

In analogy with the Euclidean case (discussed below), the prox-mapping (3.3) produces a feasible point x+ = Px(y) by starting from x  dom h and taking a step along a dual (gradient-like) vector y  Y. In this way, we obtain the mirror descent (MD) algorithm

Xn+1 = PXn (-ng^n),

(MD)

where n is a variable step-size sequence and g^n is the calculated value of the gradient vector g(Xn) at the n-th stage of the algorithm (for a pseudocode implementation, see Section 3).

For concreteness, two widely used examples of prox-mappings are as follows:

Example 3.1 (Euclidean projections). When X is endowed with the L2 norm · 2, the archetypal

prox-function is the (square of the) norm itself, i.e., h(x) =

1 2

x

2 2

.

In

that

case,

D( p,

x)

=

1 2

x- p

2

and the induced prox-mapping is

Px(y) = (x + y),

(3.4)

with (x) = arg minx X x - x 2 denoting the ordinary Euclidean projection onto X .

3Recall here that the subdifferential of h at x  X is defined as h(x)  {y  Y : h(x )  h(x) + y, x - x for all x  V}, with the standard convention h(x) =  for all x  V \ X .

4

Under review as a conference paper at ICLR 2019

Algorithm 1: mirror descent (MD) for saddle-point problems

Require: K-strongly convex regularizer h : X  1: choose X  dom h 2: for n = 1, 2, . . . do 3: oracle query at X returns g 4: set X  PX(-ng) 5: end for
6: return X

, step-size sequence n > 0 # initialization
# gradient feedback # new state

Example 3.2 (Entropic regularization). When X is a d-dimensional simplex, a widely used DGF

is the (negative) Gibbs­Shannon entropy h(x) =

d j=1

xj

log

xj.

This function is 1-strongly con-

vex with respect to the L1 norm [Shalev-Shwartz 2011] and the associated pseudo-distance is the

Kullback­Leibler divergence DKL(p, x) =

d j=1

p

j

log( p

j/x

j);

in

turn,

this

yields

the

prox-mapping

Px(y) =

(x j exp(y j))dj=1

d j=1

xj

exp(y j)

for all x  X , y  Y.

(3.5)

The update rule x  Px(y) is known in the literature as the multiplicative weights (MW) algorithm [Arora et al. 2012], and is one of the centerpieces for learning in zero-sum games [Freund & Schapire
1999, Mertikopoulos et al. 2018, Daskalakis et al. 2018], adversarial bandits [Auer et al. 1995], etc.

Regarding the gradient input sequence g^n of (MD), we assume that it is obtained by querying a first-order oracle which outputs an estimate of g(Xn) when called at Xn. This oracle could be either perfect, returning g^n = g(Xn) for all n, or imperfect, providing noisy gradient estimations.4 By that token, we will make the following blanket assumptions for the gradient feedback sequence g^n:

a) Unbiasedness:

¾[g^n | Fn] = g(Xn).

b) Finite mean square:

¾[

g^ n

2 

|

Fn]



G2

for some finite G  0.

(3.6)

In the above, y   sup{ y, x : x  V, x  1} denotes the dual norm on Y while Fn represents

the history (natural filtration) of the generating sequence Xn up to stage n (inclusive). Since g^n is

generated randomly from Xn at stage n, it is obviously not Fn-measurable, i.e., g^n = g(Xn) + Un+1,

where

Un

is

an

adapted

martingale

difference

sequence

with

¾[

Un+1

2 

|

Fn

]



2

for some

finite

  0. Clearly, when  = 0, we recover the exact gradient feedback framework g^n = g(Xn).

Convergence analysis. When (SP) is convex-concave, it is customary to take as the output of (MD)

the so-called ergodic average

X¯n =

n k=1

k

Xk

n k=1

k

,

(3.7)

or some other average of the sequence Xn where the objective is sampled. The reason for this is

that convexity guarantees ­ via Jensen's inequality and gradient monotonicity ­ that a regret-based analysis of (MD) can lead to explicit rates for the convergence of X¯n to the solution set of (SP)
[Nemirovski 2004, Nesterov 2007]. Beyond convex-concave problems however, this is no longer the

case: averaging provides no tangible benefits in a non-monotone setting, so we need to examine the

convergence properties of the generating sequence Xn of (MD) directly. With all this in mind, our

main result for (MD) may be stated is as follows:

Theorem 3.1. Suppose that (MD) is run with a gradient oracle satisfying (3.6) and a variable

step-size sequence n such that

 n=1

n

=

.

Then:

a) If f is strictly coherent and

 n=1

n2

<

,

Xn

converges

(a.s.)

to

a

solution

of

(SP).

b) If f is null-coherent, the sequence ¾[D(x, Xn)] is non-decreasing for every solution x of (SP).

This result establishes an important dichotomy between strict and null coherence: in strictly coherent
problems, Xn is attracted to the solution set of (SP); in null-coherent problems, Xn drifts away and cycles without converging. In particular, this dichotomy leads to the following immediate corollaries:

4The reason for this is that, depending on the application at hand, gradients might be difficult to compute directly e.g., because they require huge amounts of data, the calculation of an unknown expectation, etc.

5

Under review as a conference paper at ICLR 2019

Algorithm 2: optimistic mirror descent (OMD) for saddle-point problems

Require: K-strongly convex regularizer h : X 
1: choose X  dom h
2: for n = 1, 2, . . . do 3: oracle query at X returns g 4: set X+  PX(-ng) 5: oracle query at X+ returns g+ 6: set X  PX(-ng+) 7: end for
8: return X

, step-size sequence n > 0 # initialization
# gradient feedback # waiting state
# gradient feedback # new state

Corollary 3.2. Suppose that f is strictly convex-concave. Then, with assumptions as above, Xn converges (a.s.) to the (necessarily unique) solution of (SP).
Corollary 3.3. Suppose that f is bilinear and admits an interior saddle-point x  X . If X1 x and (MD) is run with exact gradient input ( = 0), we have limn D(x, Xn) > 0.
Since bilinear models include all finite two-player, zero-sum games, Corollary 3.3 encapsulates both the non-convergence results of Daskalakis et al. [2018] and Bailey & Piliouras [2018] for gradient descent and FTRL respectively (for a more comprehensive formulation, see Proposition C.3 in Appendix C). This failure of (MD) is due to the fact that, witout a mitigating mechanism in place, a "blind" first-order step could overshoot and lead to an outwards spiral, even with a vanishing step-size. This phenomenon becomes even more pronounced in GANs where it can lead to mode collapse and/or cycles between different modes. The next two sections address precisely these issues.

4 Optimistic mirror descent

The method. In convex-concave problems, taking an average of the algorithm's generated samples as in (3.7) may resolve cycling phenomena by inducing an auxiliary sequence that gravitates towards the "center of mass" of the driving sequence Xn (which orbits interior solutions). However, this technique cannot be employed in non-monotone problems because Jensen's inequality does not hold there. In view of this, we replace averaging with an optimistic "extra-gradient" step which uses the obtained information to "amortize" the next prox step (possibly outside the convex hull of generated states). The seed of this "extra-gradient" idea dates back to Korpelevich [1976] and Nemirovski [2004], and has since found wide applications in optimization theory and beyond ­ for a survey, see Bubeck [2015] and references therein.

In a nutshell, given a state x, the extra-gradient method first generates an intermediate, "waiting" state x^ = Px(-g(x)) by taking a prox step as usual. However, instead of continuing from x^, the method samples g(x^) and goes back to the original state x in order to generate a new state x+ = Px(-g(x^)). Based on this heuristic, we obtain the optimistic mirror descent (OMD) algorithm

Xn+1/2 = PXn (-ng^n) Xn+1 = PXn (-ng^n+1/2)

(OMD)

where, in obvious notation, g^n and g^n+1/2 represent gradient oracle queries at the incumbent and intermediate states Xn and Xn+1/2 respectively (for a pseudocode implementation, see Algorithm 2).

Convergence analysis. In his original analysis, Nemirovski [2004] considered the ergodic average (3.7) of the algorithm's iterates and established an O(1/n) convergence rate in monotone problems. However, as we explained above, even though this kind of averaging is helpful in convex-concave problems, it does not provide any tangible benefits beyond this class: in more general problems, Xn appears to be the most natural solution candidate. Our first result below justifies this choice in the class of coherent problems:
Theorem 4.1. Suppose that (SP) is coherent and g is L-Lipschitz continuous. If (OMD) is run with exact gradient input ( = 0) and n such that 0 < infn n  supn n < K/L, the sequence Xn converges monotonically to a solution x of (SP), i.e., D(x, Xn) decreases monotonically to 0.
Corollary 4.2. Suppose that f is bilinear. If (OMD) is run with assumptions as above, the sequence Xn converges monotonically to a solution of (SP).

6

Under review as a conference paper at ICLR 2019

(a) Vanilla versus optimistic RMS (top and bottom respectively;  = 3 × 10-4 in both cases).

(b) Vanilla versus optimistic Adam (top and bottom respectively;  = 4 × 10-5 in both cases).
Figure 2: Different algorithmic benchmarks (RMSprop and Adam): adding an extra-gradient step allows the training method to accurately learn the target data distribution and eliminates cycling and oscillatory instabilities.

Theorem 4.1 includes as a special case the analysis of Facchinei & Pang [2003, Theorem 12.1.11] for optimistic gradient descent and, in turn, the corresponding asymptotic result of Daskalakis et al. [2018] for bilinear saddle-point problems. As in the case of Daskalakis et al. [2018], Theorem 4.1 shows that optimism (i.e., the extra-gradient add-on) plays a crucial role in stabilizing (MD): not only does (OMD) converge in problems where (MD) provably fails (e.g., in zero-sum finite games), but this convergence is, in fact, monotonic. In other words, at each iteration, (OMD) comes closer to a solution of (SP), whereas (MD) may spiral outwards, towards higher and higher values of the Bregman divergence, ultimately converging to a limit cycle. This phenomenon can be seen very clearly in Fig. 1, and also in the detailed analysis we provide in Appendix C.

Of course, except for very special cases, the monotonic convergence of Xn cannot hold when the gradient input to (OMD) is imperfect: a single "bad" sample of g^n would suffice to throw Xn off-track.
In this case, we have:

Theorem 4.3. Suppose that (SP) is strictly coherent and (OMD) is run with a gradient oracle

satisfying (3.6) and a variable step-size sequence n such that

 n=1

n

=



and

 n=1

n2

<

.

Then,

with probability 1, Xn converges to a solution of (SP).

It is worth noting here that the step-size policy in Theorem 4.3 is different than that of Theorem 4.1.

This is due to a) the lack of randomness (which obviates the summability requirement

 n=1

n2

<



in

Theorem 4.1); and b) the lack of Lipschitz continuity assumption (which, in the case of Theorem 4.1

guarantees monotonic decrease at each step, provided the step-size is not too big). Importantly, the

maximum allowable step-size is also controlled by the strong convexity modulus of h, suggesting

that the choice of distance-generating function can be fine-tuned further to allow for more aggressive

step-size policies ­ a key benefit of mirror descent methods.

5 Experimental results
Gaussian mixture models. For the experimental validation of our theoretical results, we began by evaluating the extra-gradient add-on in a highly multi-modal mixture of 16 Gaussians arranged in a 4 × 4 grid as in Metz et al. [2017]. The generator and discriminator have 6 fully connected layers with 384 neurons and Relu activations (plus an additional layer for data space projection), and the

7

Under review as a conference paper at ICLR 2019
Figure 3: Left: Inception score (left) and Fréchet distance (right) on CIFAR-10 when training with Adam (with and without an extra-gradient step). Results are averaged over 8 sample runs with different random seeds.
Figure 4: Samples generated by Adam with an extra-gradient step on CelebA (left) and CIFAR-10 (right). generator generates 2-dimensional vectors. The output after {4000, 8000, 12000, 16000, 20000} iterations is shown in Fig. 2. The networks were trained with RMSprop [Tieleman & Hinton 2012] and Adam [Kingma & Ba 2014], and the results are compared to the corresponding extra-gradient variant (for an explicit pseudocode representation in the case of Adam, see Daskalakis et al. [2018] and Appendix E). Learning rates and hyperparameters were chosen by an inspection of grid search results so as to enable a fair comparison between each method and its look-ahead version. Overall, the different optimization strategies without look-ahead exhibit mode collapse or oscillations throughout the training period (we ran all models for at least 20000 iterations in order to evaluate the hopping behavior of the generator). In all cases, the extra-gradient add-on performs consistently better in learning the multi-modal distribution and greatly reduces occurrences of oscillatory behavior. Experiments with standard datasets. In our experiments with Gaussian mixture models (GMMs), the most promising training method was Adam with an extra-gradient step (a concrete pseudocode implementation is provided in Appendix E). Motivated by this, we trained a Wasserstein-GAN on the CelebA and CIFAR-10 datasets using Adam, both with and without an extra-gradient step. The architecture employed was a standard DCGAN; hyperparameters and network architecture details may be found in Appendix E. Subsequently, to quantify the gains of the extra-gradient step, we employed the widely used inception score and Fréchet distance metrics, for which we report the results in Fig. 3. Under both metrics, the extra-gradient add-on provides consistently higher scores after an initial warm-up period (and is considerably more stable). For visualization purposes, we also present in Fig. 4 an ensemble of samples generated at the end of the training period. Overall, the generated samples provide accurate feature representation and low distortion (especially in CelebA).
6 Conclusions
Our results suggest that the implementation of an optimistic, extra-gradient step is a flexible add-on that can be easily attached to a wide variety of GAN training methods (RMSProp, Adam, SGA, etc.), and provides noticeable gains in performance and stability. From a theoretical standpoint, the dichotomy between strict and null coherence provides a justification of why this is so: optimism eliminates cycles and, in so doing, stabilizes the method. We find this property particularly appealing because it paves the way to a local analysis with provable convergence guarantees in multi-modal settings; we intend to examine this question in future work.
8

Under review as a conference paper at ICLR 2019
References
Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 214­223, 2017. URL http://proceedings.mlr.press/v70/arjovsky17a. html.
Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: A meta-algorithm and applications. Theory of Computing, 8(1):121­164, 2012.
Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, 1995.
James P Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In Proceedings of the 2018 ACM Conference on Economics and Computation, pp. 321­338. ACM, 2018.
Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, New York, NY, USA, 2 edition, 2017.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873­3881, 2016.
Léon Bottou. Online learning and stochastic approximations. On-line learning in neural networks, 17(9):142, 1998.
Lev M. Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Mathematical Physics, 7(3):200­217, 1967.
Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231­358, 2015.
Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization algorithm using Bregman functions. SIAM Journal on Optimization, 3(3):538­543, August 1993.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. In ICLR '18: Proceedings of the 2018 International Conference on Learning Representations, 2018.
Francisco Facchinei and Jong-Shi Pang. Finite-Dimensional Variational Inequalities and Complementarity Problems. Springer Series in Operations Research. Springer, 2003.
Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29:79­103, 1999.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pp. 797­842, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pp. 2973­2981, 2016.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.
Gauthier Gidel, Hugo Berard, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial networks. https://arxiv.org/pdf/1802.10551.pdf, February 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 5769­5779. Curran Associates, Inc., December 2017. URL https://papers.nips.cc/paper/ 7159-improved-training-of-wasserstein-gans. arxiv: 1704.00028.
P. Hall and C. C. Heyde. Martingale Limit Theory and Its Application. Probability and Mathematical Statistics. Academic Press, New York, 1980.
Elad Hazan. A survey: The convex optimization approach to regret minimization. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright (eds.), Optimization for Machine Learning, pp. 287­304. MIT Press, 2012.
Anatoli Juditsky, Arkadi Semen Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17­58, 2011.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 12 2014. Krzysztof C. Kiwiel. Free-steering relaxation methods for problems with strictly convex costs and linear
constraints. Mathematics of Operations Research, 22(2):326­349, 1997. G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Èkonom. i Mat.
Metody, 12:747­756, 1976.
9

Under review as a conference paper at ICLR 2019
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pp. 1246­1257, 2016.
Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin Recht. First-order methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406, 2017.
Panayotis Mertikopoulos, Christos H. Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. In SODA '18: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms, 2018.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? https://arxiv.org/abs/1801.04406, 2018.
Lars M. Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In NIPS, pp. 1823­1833, 2017.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. ICLR Proceedings, 2017.
Arkadi Semen Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229­251, 2004.
Arkadi Semen Nemirovski and David Berkovich Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley, New York, NY, 1983.
Arkadi Semen Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574­1609, 2009.
Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems. Mathematical Programming, 109(2):319­344, 2007.
Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update with constant step-size in congestion games: Convergence, limit cycles and chaos. In NIPS '17: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017.
Ioannis Panageas and Georgios Piliouras. Gradient descent only converges to minimizers: Non-isolated critical points and invariant regions. In Innovations of Theoretical Computer Science (ITCS), 2017.
Christos Papadimitriou and Georgios Piliouras. From nash equilibria to chain recurrent sets: Solution concepts and topology. In ITCS, 2016.
Georgios Piliouras and Jeff S Shamma. Optimization despite chaos: Convex relaxations to complex limit sets via poincaré recurrence. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pp. 861­873. SIAM, 2014.
A. Radford, L. Metz, and S. Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. ArXiv e-prints, November 2015.
Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In NIPS '13: Proceedings of the 26th International Conference on Neural Information Processing Systems, 2013.
Ralph Tyrrell Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, 1970. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. In NIPS, pp. 2234­2242, 2016. Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine
Learning, 4(2):107­194, 2011. Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory (ISIT), 2016
IEEE International Symposium on, pp. 2379­2383. IEEE, 2016. Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and the geometric
picture. IEEE Transactions on Information Theory, 63(2):853­884, 2017a. Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere ii: Recovery by riemannian
trust-region method. IEEE Transactions on Information Theory, 63(2):885­914, 2017b. T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning. 2012. Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W. Glynn. Stochastic
mirror descent for variationally coherent optimization problems. In NIPS '17: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017.
10

Under review as a conference paper at ICLR 2019

A Coherent saddle-point problems
We begin our discussion with some basic results on coherence: Proposition A.1. If f is convex-concave, (SP) is coherent. In addition, if f is strictly convex-concave, (SP) is strictly coherent.

Proof. Let x be a solution point of (SP). Since f is convex-concave, first-order optimality gives

g1(x1, x2), x1 - x1 = x1 f (x1, x2), x1 - x1  0,

(A.1a)

and

g2(x1, x2), x2 - x2 = -x2 f (x1, x2), x2 - x2  0. Combining the two, we readily obtain the (Stampacchia) variational inequality

(A.1b)

g(x), x - x  0 for all x  X .

(A.2)

In addition to the above, the fact that f is convex-concave also implies that g(x) is monotone in the sense that

g(x ) - g(x), x - x  0

(A.3)

for all x, x  X Bauschke & Combettes [2017]. Thus, setting x  x in (A.3) and invoking (A.2),

we get

g(x), x - x  g(x), x - x  0,

(A.4)

i.e., (VI) is satisfied.

To establish the converse implication, focus for concreteness on the minimizer, and note that (VI) implies that

g1(x), x1 - x1  0 for all x1  X1.

(A.5)

Now, if we fix some x1  X1 and consider the function (t) = f (x1 + t(x1 - x1), x2), the inequality (A.5) yields

 (t) = g(x1 + t(x1 - x1), x2), x1 - x1

=

1 t

g(x1 + t(x1 - x1), x2), x1 + t(x1 - x1 - x1

 0,

(A.6)

for all t  [0, 1]. The maximizing

This implies that  is nondecreasing, component follows similarly, showing

so that

f (x1, x is

x2) = (1) a solution

 of

(0) (SP)

=f and,

(x1, x2). in turn,

establishing that (SP) is coherent.

For the strict part of the claim, the same line of reasoning shows that if g(x), x - x = 0 for some x that is not a saddle-point of f , the function (t) defined above must be constant on [0, 1], indicating
in turn that f cannot be strictly convex-concave, a contradiction.

We proceed to show that the solution set of a coherent saddle-point problem is closed (we will need this regularity result in the convergence analysis of Appendix C):
Lemma A.2. Let X  denote the solution set of (SP). If (SP) is coherent, X  is closed.

Proof. Let xn, n = 1, 2, . . . , be a sequence of solutions of (SP) converging to some limit point x  X . To show that X  is closed, it suffices to show that x  X . Indeed, given that (SP) is coherent, every solution thereof satisfies (VI), so we have g(x), x - xn  0 for all x  X . With xn  x as n  , it follows that

g(x), x - x

=

lim
n

g(x),

x

-

xn

0

for all x  X ,

(A.7)

i.e., x satisfies (VI). By coherence, this implies that x is a solution of (SP), as claimed.

11

Under review as a conference paper at ICLR 2019

B Properties of the Bregman divergence

In this appendix, we provide some auxiliary results and estimates that are used throughout the convergence analysis of Appendix C. Some of the results we present here (or close variants thereof) are not new [see e.g., Nemirovski et al. 2009, Juditsky et al. 2011]. However, the hypotheses used to obtain them vary wildly in the literature, so we provide all the necessary details for completeness.

To begin, recall that the Bregman divergence associated to a K-strongly convex distance-generating function h : X  is defined as

D(p, x) = h(p) - h(x) - h(x), p - x

(B.1)

with h(x) denoting a continuous selection of h(x). The induced prox-mapping is then given by

Px(y) = arg min{ y, x - x + D(x , x)}
x X
= arg max{ y + h(x), x - h(x )}
x X

(B.2)

and is defined for all x  dom h, y  Y (recall here that Y  V denotes the dual of the ambient

vector space V). In what follows, we will also make frequent use of the convex conjugate h : Y 

of h, defined as

h(y) = max{ y, x - h(x)}.
xX

(B.3)

By standard results in convex analysis [Rockafellar 1970, Chap. 26], h is differentiable on Y and its

gradient satisfies the identity h(y) = arg max{ y, x - h(x)}.

(B.4)

xX

For notational convenience, we will also write

Q(y) = h(y)

(B.5)

and we will refer to Q : Y  X as the mirror map generated by h. All these notions are related as follows:
Lemma B.1. Let h be a distance-generating function on X . Then, for all x  dom h, y  Y, we have:

a) x = Q(y)  y  h(x). b) x+ = Px(y)  h(x) + y  h(x+)  x+ = Q(h(x) + y).

(B.6a) (B.6b)

Finally, if x = Q(y) and p  X , we have

h(x), x - p  y, x - p .

(B.7)

Remark. By (B.6b), we have h(x+) , i.e., x+  dom h. As a result, the update rule x  Px(y) is well-posed, i.e., it can be iterated in perpetuity.

Proof of Lemma B.1. For (B.6a), note that x solves (B.3) if and only if y - h(x) 0, i.e., if and only if y  h(x). Similarly, comparing (B.2) with (B.3), it follows that x+ solves (B.2) if and only if h(x) + y  h(x+), i.e., if and only if x+ = Q(h(x) + y).
For (B.7), by a simple continuity argument, it suffices to show that the inequality holds for interior p  X . To establish this, let

(t) = h(x + t(p - x)) - [h(x) + y, x + t(p - x) ].

(B.8)

Since h is strongly convex and y  h(x) by (B.6a), it follows that (t)  0 with equality if and
only if t = 0. Since (t) = h(x + t(p - x)) - y, p - x is a continuous selection of subgradients of  and both  and  are continuous on [0, 1], it follows that  is continuously differentiable with  =  on [0, 1]. Hence, with  convex and (t)  0 = (0) for all t  [0, 1], we conclude that  (0) = h(x) - y, p - x  0, which proves our assertion.

We continue with some basic bounds on the Bregman divergence before and after a prox step. The basic ingredient for these bounds is a generalization of the (Euclidean) law of cosines which is known in the literature as the "three-point identity" [Chen & Teboulle 1993]:

12

Under review as a conference paper at ICLR 2019

Lemma B.2. Let h be a distance-generating function on X . Then, for all p  X and all x, x 

dom h, we have

D(p, x ) = D(p, x) + D(x, x ) + h(x ) - h(x), x - p .

(B.9)

Proof. By definition, we have: D(p, x ) = h(p) - h(x ) - h(x ), p - x D(p, x) = h(p) - h(x) - h(x), p - x D(x, x ) = h(x) - h(x ) - h(x ), x - x .
Our claim then follows by adding the last two lines and subtracting the first.

(B.10)

With this identity at hand, we have the following series of upper and lower bounds:

Proposition B.3. Let h be a K-strongly convex distance-generating function on X , fix some p  X , and let x+ = Px(y) for x  dom h, y  Y. We then have:

D(p, x)

K 

x - p 2.

2

D(p, x+)  D(p, x) - D(x+, x) + y, x+ - p

(B.11a) (B.11b)

 D(p, x) +

y, x - p

+1 2K

y

2 

(B.11c)

Proof of (B.11a). By the strong convexity of h, we get

h(p)  h(x) + h(x), p - x + K p - x 2 2

(B.12)

so (B.11a) follows by gathering all terms involving h and recalling the definition of D(p, x).

Proof of (B.11b) and (B.11c). By the three-point identity (B.9), we readily obtain D(p, x) = D(p, x+) + D(x+, x) + h(x) - h(x+), x+ - p .

(B.13)

In turn, this gives

D(p, x+) = D(p, x) - D(x+, x) + h(x+) - h(x), x+ - p

 D(p, x) - D(x+, x) + y, x+ - p ,

(B.14)

where, in the last step, we used (B.7) and the fact that x+ = Px(y), so h(x) + y  h(x+). The above is just (B.11b), so the first part of our proof is complete.

For (B.11c), the bound (B.14) gives D(p, x+)  D(p, x) + y, x - p + y, x+ - x - D(x+, x).

(B.15)

Therefore, by Young's inequality [Rockafellar 1970], we get

y, x+ - x

K 
2

x+ - x 2 + 1 2K

y 2,

and hence

D(p, x+)  D(p, x) +

y, x - p

+1 2K

y

2 

+

K 2

x+ - x 2 - D(x+, x)

 D(p, x) +

y, x - p

+1 2K

y

2 

,

with the last step following from Lemma B.1 applied to x in place of p.

(B.16) (B.17)

The first part of Proposition B.3 shows that Xn converges to p if D(p, Xn)  0. However, as
we mentioned in the main body of the paper, the converse may fail: in particular, we could have lim infn D(p, Xn) > 0 even if Xn  p. To see this, let X be the L2 ball of d and take h(x) =

- 1 - x 22. Then, a straightforward calculation gives

D(p, x) = 1 - p, x

1-

x

2 2

(B.18)

13

Under review as a conference paper at ICLR 2019

whenever p 2 = 1. The corresponding level sets Lc(p) = {x  d : D(p, x) = c} of D(p, ·) are given by the equation

1 - p, x = c 1 - x 22,

(B.19)

which admits p as a solution for all c  0 (so p belongs to the closure of Lc(p) even though D(p, p) = 0

by definition). As a result, under this distance-generating function, it is possible to have Xn  p even

when lim infn D(p, Xn) > 0 (simply take a sequence Xn that converges to p while remaining on the

same level set of D). As we discussed in the main body of the paper, such pathologies are discarded

by the Bregman reciprocity condition

D(p, Xn)  0 whenever Xn  p.

(B.20)

This condition comes into play at the very last part of the proofs of Theorems 3.1 and 4.1; other than

that, we will not need it in the rest of our analysis.

Finally, for the analysis of the OMD algorithm, we will need to relate prox steps taken along different directions:
Proposition B.4. Let h be a K-strongly convex distance-generating function on X and fix some p  X , x  dom h. Then:

a) For all y1, y2  Y, we have: i.e., Px is (1/K)-Lipschitz.

Px(y2) - Px(y1)



1 K

y2 - y1

,

b) In addition, letting x1+ = Px(y1) and x2+ = Px(y2), we have: D(p, x2+)  D(p, x) + y2, x1+ - p + [ y2, x2+ - x1+ - D(x2+, x)]

 D(p, x) +

y2, x1+ - p

+1 2K

y2 - y1

2 

-

K 2

x1+ - x 2.

(B.21)
(B.22a) (B.22b)

Proof. We begin with the proof of the Lipschitz property of Px. Indeed, for all p  X , (B.7) gives

h(x1+) - h(x) - y1, x1+ - p  0,

(B.23a)

and

h(x2+) - h(x) - y2, x2+ - p  0. Therefore, setting p  x2+ in (B.23a), p  x1+ in (B.23b) and rearranging, we obtain
h(x2+) - h(x1+), x2+ - x1+  y2 - y1, x2+ - x1+ .

(B.23b) (B.24)

By the strong convexity of h, we also have

K x2+ - x1+ 2  h(x2+) - h(x1+), x2+ - x1+ .

(B.25)

Hence, combining (B.24) and (B.25), we get

K x2+ - x1+ 2  y2 - y1, x2+ - x1+  y2 - y1  x2+ - x1+ ,

(B.26)

and our assertion follows.

For the second part of our claim, the bound (B.11b) of Proposition B.3 applied to x2+ = Px(y2) readily gives

D(p, x2+)  D(p, x) - D(x2+, x) + y2, x2+ - p

= D(p, x) + y2, x1+ - p + [ y2, x2+ - x1+ - D(x2+, x)] thus proving (B.22a). To complete our proof, note that (B.11b) with p  x2+ gives
D(x2+, x1+)  D(x2+, x) + y1, x1+ - x2+ - D(x1+, x),

(B.27) (B.28)

or, after rearranging, We thus obtain

D(x2+, x)  D(x2+, x1+) + D(x1+, x) + y1, x2+ - x1+ .

(B.29)

y2, x2+ - x1+ - D(x2+, x)  y2 - y1, x2+ - x1+ - D(x2+, x1+) - D(x1+, x)



y2 - y1

2 

+

K

2K 2

x2+ - x1+

2-

K 2

x2+ - x1+

2-

K 2

x1+ - x 2

1 2K

y2 - y1

2 

-

K 2

x1+ - x 2,

(B.30)

where we used Young's inequality and (B.11a) in the second inequality. The bound (B.22b) then

follows by substituting (B.30) in (B.27).

14

Under review as a conference paper at ICLR 2019

C Convergence analysis of mirror descent

We begin by recalling the definition of the mirror descent algorithm. With notation as in the previous section, the algorithm is defined via the recursive scheme

Xn+1 = PXn (-ng^n),

(MD)

where n is a variable step-size sequence and g^n is the calculated value of the gradient vector g(Xn) at the n-th stage of the algorithm. As we discussed in the main body of the paper, the gradient input sequence g^n of (MD) is assumed to satisfy the standard oracle assumptions

a) Unbiasedness:

¾[g^n | Fn] = g(Xn).

b) Finite mean square:

¾[ g^n

2 

|

Fn]



G2

for some finite G  0.

where Fn represents the history (natural filtration) of the generating sequence Xn up to stage n (inclusive).

With this preliminaries at hand, our convergence proof for (MD) under strict coherence will hinge on the following results:

Proposition C.1. Suppose that (SP) is coherent and (MD) is run with a gradient oracle satisfying

(3.6) and a divergence

variable D(x, Xn

step-size n ) converges

such (a.s.)

that to a

 n=1

n2

<

.

If

x



random variable D(

X x)

is a solution of with ¾[D(x)]

(SP), < .

the

Bregman

Proposition C.2. Suppose that (SP) is strictly coherent and (MD) is run with a gradient oracle

satisfying (3.6) and a step-size n such that

 n=1

n

=



and

 n=1

n2

<

.

Then,

with

probability

1,

there exists a (possibly random) solution x of (SP) such that lim infn D(x, Xn) = 0.

Proposition C.1 can be seen as a "dichotomy" result: it shows that the Bregman divergence is an asymptotic constant of motion, so (MD) either converges to a saddle-point x (if D(x) = 0) or to some nonzero level set of the Bregman divergence (with respect to x). In this way, Proposition C.1 rules out more complicated chaotic or aperiodic behaviors that may arise in general ­ for instance, as in the analysis of Palaiopanos et al. [2017] for the long-run behavior of the multiplicative weights algorithm in two-player games. However, unless this limit value can be somehow predicted (or estimated) in advance, this result cannot be easily applied. This is the main role of Proposition C.2: it shows that (MD) admits a subsequence converging to a solution of (SP) so, by (B.20), the limit of D(x, Xn) must be zero.
With all this at hand, our first step is to prove Proposition C.1:

Proof of Proposition C.1. Let Dn = D(x, Xn) for some solution x of (SP). Then, by Proposition B.3, we have

Dn+1 = D(x, PXn (-ng^n))  D(x, Xn) - n g^n, Xn - x

+ n2 2K

g^ n

2

= Dn - n g(Xn), Xn - x

- n Un+1, Xn - x

+ n2 2K

g^ n

2 



Dn

+

nn+1

+

n2 2K

g^ n

2 

,

(C.1)

where, in the last line, we set n+1 = - Un+1, Xn - x and we invoked the assumption that (SP) is coherent. Thus, conditioning on Fn and taking expectations, we get

¾[Dn+1 | Fn]



Dn

+ ¾[n+1 | Fn] +

n2 2K

¾[

g^ n

2 

|

Fn]



Dn

+

G2 2K

n2

,

(C.2)

where we used the oracle assumptions (3.6) and the fact that Xn is Fn-measurable (by definition).

Now, letting Rn = Dn + (2K)-1G2

 k=n

k2

,

the

estimate

(C.1)

gives

¾[Rn+1 | Fn]

=

¾[Dn+1 | Fn] +

G2 2K


k2
k=n+1



Dn

+

G2 2K

 k=n

k2

=

Rn,

(C.3)

i.e., Rn is an Fn-adapted supermartingale. Since

 n=1

n2

<

,

it

follows

that

¾[Rn]

=

¾[¾[Rn | Fn-1]]



¾[Rn-1]



···



¾[R1]



¾[D1] +

G2 2K


n2
n=1

< ,

(C.4)

15

Under review as a conference paper at ICLR 2019

i.e., Rn is uniformly bounded in L1. Thus, by Doob's convergence theorem for supermartingales [Hall
& Heyde 1980, Theorem 2.5], it follows that Rn converges (a.s.) to some finite random variable R with ¾[R] < . In turn, by inverting the definition of Rn, this shows that Dn converges (a.s.) to some random variable D(x) with ¾[D(x)] < , as claimed.

We now turn to the proof of existence of a convergent subsequence of (MD) under strict coherence (Proposition C.2):

Proof of Proposition C.2. We begin with the technical observation that the solution set X  of (SP) is
closed ­ and hence, compact (cf. Lemma A.2 in Appendix A). Clearly, if X  = X , there is nothing to show; hence, without loss of generality, we may assume in what follows that X  X .

Assume now ad absurdum that, with positive probability, the sequence Xn generated by (MD) admits no limit points in X . Conditioning on this event, and given that X  is compact, there exists a (nonempty) compact set C  X such that C  X  =  and Xn  C for all sufficiently large n. Moreover, given that (SP) is strictly coherent, we have g(x), x - x > 0 whenever x  C and x  X . Therefore, by the continuity of g and the compactness of X  and C, there exists some a > 0 such that

g(x), x - x  a for all x  C, x  X .

(C.5)

To proceed, fix some x  X  and let Dn = D(x, Xn). Then, telescoping (C.1) yields the estimate

n
Dn+1  D1 - k g(Xk), Xk - x
k=1

+

n
k k+1
k=1

+

n k=1

k2 2K

g^ k

2 

,

(C.6)

where, as in the proof of Proposition C.1, we set n+1 = Un+1, Xn - x . Subsequently, letting

n =

n k=1

k

and

using

(C.5),

we

obtain

 Dn+1  D1 - na -

n k=1

k k+1

-

(2K)-1

n

n k=1

k2

n

g^ k

2  .

(C.7)

By the unbiasedness hypothesis of (3.6) for Un, we have ¾[n+1 | Fn] = ¾[Un+1 | Fn], Xn - x = 0 (recall that Xn is Fn-measurable by construction). Moreover, since Un is bounded in L2 and n is 2 summable (by assumption), it follows that



n2 ¾[n2+1 | Fn] 

n2

Xn - x

2 ¾[

Un+1

2 

|

Fn]

n=1 n=1



 diam(X )22 n2 < .

n=1

(C.8)

Therefore, by the law of large numbers for martingale difference sequences [Hall & Heyde 1980,

Theorem 2.18], we conclude that -n1

n k=1

k k+1

converges

to

0

with

probability

1.

Finally, for the last term of (C.6), let S n+1 =

n k=1

k2

g^ k

2 

.

Since g^k is Fn-measurable for all

k = 1, 2, . . . , n - 1, we have

 n-1



¾[S n+1 | Fn] = ¾ k=1 k2

g^ k

2 

+

n2

g^ n

2 

Fn = S n + n2 ¾[ g^n

2 

|

Fn]



S

n,

(C.9)

i.e., S n is a submartingale with respect to Fn. Furthermore, by the law of total expectation, we also

have

n

¾[S n+1] = ¾[¾[S n+1 | Fn]]  G2 n2  G2 n2 < ,

(C.10)

k=1 k=1

so S n is bounded in L1. Hence, by Doob's submartingale convergence theorem [Hall & Heyde 1980,
Theorem 2.5], we conclude that S n converges to some (almost surely finite) random variable S  with ¾[S ] < , implying in turn that limn S n+1/n = 0 (a.s.).

Applying all of the above, the estimate (C.6) gives Dn+1  D1 - an/2 for sufficiently large n, so D(x, Xn)  -, a contradiction. Going back to our original assumption, this shows that, with probability 1, at least one of the limit points of Xn must lie in X , as claimed.

16

Under review as a conference paper at ICLR 2019

With all this at hand, we are finally in a position to prove our main result for (MD):

Proof of Theorem 3.1(a). Proposition C.2 shows that, with probability 1, there exists a (possibly random) solution x of (SP) such that lim infn Xn - x = 0 and, hence, lim infn D(x, Xn) = 0 (by Bregman reciprocity). Since limn D(x, Xn) exists with probability 1 (by Proposition C.1), it follows that limn D(x, Xn) = lim infn D(x, Xn) = 0, i.e., Xn converges to x.

We proceed with the negative result hinted at in the main body of the paper, namely the failure of (MD) to converge under null coherence:

Proof of Theorem 3.1(b). The evolution of the Bregman divergence under (MD) satisfies the identity

D(x, Xn+1) = D(x, Xn) + D(Xn, Xn+1) + n g^n, Xn - x = D(x, Xn) + D(Xn, Xn+1) + Un+1, Xn - x

(C.11)

where, in the last line, we used the null coherence assumption g(x), x - x = 0 for all x  X . Since D(Xn, Xn+1)  0, taking expecations above shows that D(x, Xn) is nondecreasing, as claimed.

With Theorem 3.1 at hand, the proof of Corollary 3.2 is an immediate consequence of the fact that strictly convex-concave problems satisfy strict coherence (Proposition A.1). As for Corollary 3.3, we provide below a more general result for two-player, zero-sum finite games.

To state it, let Ai = {1, . . . , Ai}, i = 1, 2, be two finite sets of pure strategies, and let Xi = (Ai) denote the set of mixed strategies of player i. A finite, two-player zero-sum game is then defined by a matrix M  A1×A2 so that the loss of Player 1 and the reward of Player 2 in the mixed strategy profile x = (x1, x2)  X are concurrently given by

f (x1, x2) = x1 Mx2

(C.12)

Then, writing   (A1, A2, M) for the resulting game, we have:

Proposition C.3. Let  be a two-player zero-sum game with an interior Nash equilibrium x. If

X1 x and (MD) is run with exact gradient input (2 = 0), we have limn D(x, Xn) > 0. If, in

addition,

 n=1

n2

<

,

limn

D(x,

Xn)

is

finite.

Remark. Note that non-convergence does not require any summability assumptions on n.

In words, Proposition C.3 states that (MD) does not converge in finite zero-sum games with a unique
interior equilibrium and exact gradient input: instead, Xn cycles at positive Bregman distance from the game's Nash equilibrium. Heuristically, the reason for this behavior is that, for small   0, the incremental step V(x) = Px(-g(x)) - x of (MD) is essentially tangent to the level set of D(x, ·) that passes through x.5 For finite  > 0, things are even worse because V(x) points noticeably away
from x, i.e., towards higher level sets of D. As a result, the "best-case scenario" for (MD) is to orbit x (when   0); in practice, for finite , the algorithm takes small outward steps throughout its runtime, eventually converging to some limit cycle farther away from x.

We make this intuition precise below (for a schematic illustration, see also Fig. 1 above):

Proof of Proposition C.3. Write v1(x) = -Mx2 and v2(x) = x1 M for the players' payoff vectors under the mixed strategy profile x = (x1, x2). By construction, we have g(x) = -(v1(x), v2(x)). Furthermore, since x is an interior equilibrium of f , elementary game-theoretic considerations show that v1(x) and v2(x) are both proportional to the constant vector of ones. We thus get

g(x), x - x = v1(x), x1 - x1 + v2(x), x2 - x2 = -x1 M x2 + (x1) M x2 + x1 M x2 - x1 M x2 = 0,

(C.13)

where, in the last line, we used the fact that x is interior. This shows that f satisfies null coherence, so our claim follows from Theorem 3.1(b).

5This observation was also the starting point of Mertikopoulos et al. [2018] who showed that FTRL in continuous time exhibits a similar cycling behavior in zero-sum games with an interior equilibrium.

17

Under review as a conference paper at ICLR 2019

 


  + -


 

 

 

 

 

       

       

Figure 5: Trajectories of vanilla and optimistic mirror descent in a zero-sum game of Matching Pennies (left and right respectively). Colors represent the contours of the objective, f (x1, x2) = (x1 - 1/2)(x2 - 1/2).

For our second claim, arguing as above and using (B.11c), we get

D(x, Xn+1)  D(x, Xn) + n g(Xn), Xn - x

+ n2 2K

g(Xn)

2 



D(x,

Xn)

+

n2G2 2K

(C.14)

with G = maxx1X1,x2X2 (-Mx2, x1 M) . Telescoping this last bound yields

sup D(x, Xn)
n



D(x, X1) +

 k=1

n2G2 2K

<

,

(C.15)

so D(x, Xn) is also bounded from above. Therefore, with D(x, Xn) nondecreasing, bounded from above and D(x, X1) > 0, it follows that limn D(x, Xn) > 0, as claimed.

D Convergence analysis of optimistic mirror descent

We now turn to the optimistic mirror descent (OMD) algorithm, as defined by the recursion

Xn+1/2 = PXn (-ng^n) Xn+1 = PXn (-ng^n+1/2)

(OMD)

with X1 initialized arbitrarily in dom h, and g^n, g^n+1/2 representing gradient oracle queries at the incumbent and intermediate states Xn and Xn+1/2 respectively.

The heavy lifting for our analysis is provided by Proposition B.4, which leads to the following crucial lemma:

Lemma D.1. Suppose that (SP) is coherent and g is L-Lipschitz continuous. With notation as above and exact gradient input ( = 0), we have

D(x,

Xn+1)



D(x,

Xn)

-

1 2

K-

n2 L2 K

Xn+1/2 - Xn 2,

(D.1)

for every solution x of (SP).

Proof. Substituting x  Xn, y1  -ng(Xn), and y2  -ng(Xn+1/2) in Proposition B.4, we obtain the estimate:

D(x, Xn+1)  D(x, Xn) - n g(Xn+1/2), Xn+1/2 - x

+ n2 2K

g(Xn+1/2)

- g(Xn)

2 

-

K 2

Xn+1/2 - Xn

2

18

Under review as a conference paper at ICLR 2019



D(x,

Xn)

+

n2 L2 2K

Xn+1/2 - Xn

2- K 2

Xn+1/2 - Xn

2,

(D.2)

where, in the last line, we used the fact that x is a solution of (SP)/(VI), and that g is L-Lipschitz.

We are now finally in a position to prove Theorem 4.1 (reproduced below for convenience):

Theorem. Suppose that (SP) is coherent and g is L-Lipschitz continuous. If (OMD) is run with exact gradient input and a step-size sequence n such that

0 < limn n  supn n < K/L,

(D.3)

the sequence Xn converges monotonically to a solution x of (SP), i.e., D(x, Xn) is non-increasing and converges to 0.

Proof. Let x be a solution of (SP). Then, by the stated assumptions for n, Lemma D.1 yields

D(x, Xn+1)



D(x, Xn) -

1 K(1 - 2) 2

Xn+1/2

- Xn

2,

(D.4)

where   (0, 1) is such that n2 < K/L for all n (that such an  exists is a consequence of the assumption that supn n < K/L). This shows that D(x, Xn) is non-decreasing for every solution x of (SP).

Now, telescoping (D.1), we obtain

D(x, Xn+1)



D(x, X1) -

1 2

n
K
k=1

-

k2 L2 K

 

Xk+1/2 - Xk

2,

(D.5)

and hence:

n
1 -
k=1

k2 L2 K2

 

Xk+1/2

- Xk

2



2 K

D(

x,

X1

).

(D.6)

With supn n < K/L, the above estimate readily yields

 n=1

Xn+1/2 - Xn

2

< , which in turn implies

that Xn+1/2 - Xn  0 as n  .

By the compactness of X , we further infer that Xn admits an accumulation point x^, i.e., there exists
a subsequence nk such that Xnk  x^ as k  . Since Xnk+1/2 - Xnk  0, this also implies that Xnk+1/2 converges to x^ as k  . Further, by passing to a subsequence if necessary, we may also assume without loss of generality that nk converges to some limit value  > 0. Then, by the Lipschitz continuity of the prox-mapping (cf. Proposition B.4), we readily obtain

x^

=

lim
k

Xnk

+1/2

=

lim
k

PXnk

(Xnk

-

nk g(Xnk ))

=

P x^ ( x^

-

g(x^)),

(D.7)

i.e., x^ is a solution of (VI) ­ and, hence, (SP). Since D(x^, Xn) is nonincreasing and lim infn D(x^, Xn) = 0 (by the Bregman reciprocity requirement), we conclude that lim infn D(x^, Xn) = 0, i.e., Xn converges to x^. Since x^ is a solution of (SP), our proof is com-
plete.

Our last result concerns the convergence of (OMD) in strictly coherent problems with a stochastic gradient oracle:

Proof of Theorem 4.3. Our argument hinges on the inequality

D(x, Xn+1)  D(x, Xn) - n g^n+1/2, Xn+1/2 - x

+ n2/(2K)

g^n+1/2 - g^n

2 

(D.8)

which is obtained from the two-point estimate (B.22b) by substituting x  x, x1  Xn, y1  g^n,

x1+  proof

Xn+1/2 = PXn (-ng^n), y2  g^n+1/2, and x2+  Xn = PXn (-ng^n+1/2). Then, of Proposition C.1, we obtain the following estimate for the sequence Dn =

working as D(x, Xn):

in

the

Dn+1  Dn - n g(Xn+1/2), Xn+1/2 - x

- n Un++1, Xn - x

+ n2 2K

g^n+1/2 - g^n

2 



Dn

+

nn++1

+

n2 K

g^ n

2 

+

g^2n+1/2  ,

(D.9)

19

Under review as a conference paper at ICLR 2019

where Un++1 = g^n+1/2 - g(Xn+1/2) denotes the martingale part of g^n+1/2 and we have set n++1 =

Un++1, Xn+1/2 - x

.

Since ¾[

gn

2 

|

Xn

,

.

.

.

,

X1

]

and

¾[

gn+1/2

2 

|

Xn+1/2

,

.

.

.

,

X1

]

are

both

bounded

by

G2, we get the bound

¾[Dn+1

|

Fn]



Dn

+

G K

n2.

(D.10)

Then, following the same steps as in the proof of Proposition C.1, it follows that Dn converges to some limit value D.

To proceed, telescoping (D.9) also yields

n
Dn+1  D1 - k g(Xk+1/2), Xk+1/2 - x
k=1

+

n k=1

k k++1

+

n k=1

k2 2K

g^k+1/2 - g^k 2.

(D.11)

Each term in the above bound can be controlled in the same way as the corresponding terms in (C.6). Thus, repeating the steps in the proof of Proposition C.2, it follows that there exists a subsequence of Xn+1/2 (and hence also of Xn) which converges to x.
Our claim then follows by combining the two intermediate results above in the same way as in the proof of Theorem 3.1(a); to avoid needless repetition, we omit the details.

E Experimental results
E.1 Adam with extra-gradient step
For most of our experiments, the method that seemed to generate the best results was Adam and its optimistic version [Daskalakis et al. 2018]; for a pseudocode iplementation, see Algorithm 3 below. We also noticed empirically that it was more efficient to use two different sets of moment estimates (mt, vt) and (mt, vt) for the first and the second gradient steps. We used this algorithm for our experiments with both GMMs and the CelebA/CIFAR-10 datasets.

Algorithm 3: Adam with extra-gradient add-on (optimistic Adam)

Compute stochastic gradient: ,t

Update biased estimate of 1st momentum: mt = 1mt-1 + (1 - 1),t

Update biased estimate of 2nd momentum: vt = 2vt-1 + (1 - 2)2,t

Compute Compute

bias bias

corrected corrected

1st moment: m^ t 2nd moment: v^t

= =

mt 1-vtt1 1-t2

Perform:

t

=

t-1

-



m^ t v^t +

Compute stochastic gradient:  ,t

Update biased estimate of 1st momentum: mt = 1mt-1 + (1 - 1) ,t

Update biased estimate of 2nd momentum: vt = 2vt-1 + (1 - 1)2 ,t

Compute

bias

corrected

1st

moment:

m^ t

=

mt 1-t1

Compute

bias

corrected

2nd

moment:

v^t

=

vt 1-t1

Perform: t = t-1 - 

m^ t
v^t +

Return t

E.2 Experiments with standards datasets
In this section we present the results of our image experiments using OMD training techniques. Inception and FID scores obtained by our model during training were reported in Fig. 3: as can be seen there, the extra-gradient add-on improves the performance of GAN training and efficiently stabilizes the model; without the extra-gradient step, performance tends to drop noticeably after approximately 100k steps.
For ease of comparison, we provide below a collection of samples generated by Adam and optimistic Adam in the CelebA and CIFAR-10 datasets. Especially in the case of CelebA, the generated samples are consistently more representative and faithful to the target data distribution.

20

Under review as a conference paper at ICLR 2019
(a) Vanilla versus optimistic Adam training in the CelebA dataset (left and right respectively).
(b) Vanilla versus optimistic Adam training in the CIFAR-10 dataset (left and right respectively). Figure 6: GAN training with and without an extra-gradient step in the CelebA and CIFAR-10 datasets. E.2.1 Network Architecture and hyperparameters For the reproducibility of our experiments, we provide Table 1 and Table 2 the network architectures and the hyperparameters of the GANs that we used. The architecture employed is a standard DCGAN architecture with a 5-layer generator with batchnorm, and an 8-layer discriminator. The generated samples were 32×32×3 RGB images.
21

Under review as a conference paper at ICLR 2019
Table 1: Generator and discriminator architectures for our images experiments Generator
latent space 100 (gaussian noise) dense 4 × 4 × 512 batchnorm ReLU 4×4 conv.T stride=2 256 batchnorm ReLU 4×4 conv.T stride=2 128 batchnorm ReLU 4×4 conv.T stride=2 64 batchnorm ReLU 4×4 conv.T stride=1 3 weightnorm tanh
Discriminator Input Image 32×32×3 3×3 conv. stride=1 64 lReLU 3×3 conv. stride=2 128 lReLU 3×3 conv. stride=1 128 lReLU 3×3 conv. stride=2 256 lReLU 3×3 conv. stride=1 256 lReLU 3×3 conv. stride=2 512 lReLU 3×3 conv. stride=1 512 lReLU
dense 1
Table 2: Image experiments settings
batch size = 64 Adam learning rate = 0.0001 Adam 1 = 0.0 Adam 2 = 0.9 max iterations = 200000 WGAN-GP  = 1.0 WGAN-GP ndis = 1 GAN objective = 'WGAN-GP' Optimizer = 'extra-Adam' or 'Adam'
22

