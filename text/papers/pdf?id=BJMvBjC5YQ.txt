Under review as a conference paper at ICLR 2019
CUTTING DOWN TRAINING MEMORY BY RE-FOWARDING
Anonymous authors Paper under double-blind review
ABSTRACT
Deep Neutral Networks(DNNs) require huge GPU memory when training on modern image/video databases. Unfortunately, the GPU memory as a hardware resource is always finite, which limits the image resolution, batch size, and learning rate that could be used for better DNN performance. In this paper, we propose a novel training approach, called Re-forwarding, that substantially reduces memory usage in training. Our approach automatically finds a subset of layers in DNNs, and stores tensors only at these layers during the first forward. During backward, extra local forwards (called the Re-forwarding process) are conducted to compute the missing tensors between the subset of layers. The total memory cost becomes the sum of (1) the memory cost at the subset of layers and (2) the maximum memory cost among local re-forwards. Re-forwarding trades training time for memory and does not compromise any performance in testing. We propose theories and algorithms that achieve the optimal memory solutions for DNNs with either linear or arbitrary computation graphs. Experiments show that Re-forwarding cuts down up-to 80% of training memory on popular DNNs such as Alexnet, VGG, ResNet, Densenet and Inception net.
1 INTRODUCTION
The standard DNN training process consists of two alternated stages: forward and backward. Fig. 1 (a) illustrates an example of feed-forward neural networks. In the forward stage, the network takes an input tensor, [BatchSize × Channel × W idth × Height], and computes the tensors at each layer until producing the output. In the backward stage, difference between the output and ground truth is passed back along the network to compute the gradients at each layer. The regular training approach saves tensors at all layers during forward, because they are all needed to compute gradients during backward. The total memory cost is the sum of cost over all layers.
In popular backbone DNNs for feature extraction of images, such as AlexNet (Krizhevsky et al. (2012)), VGG (Simonyan & Zisserman (2014)) and ResNet (He et al. (2016)), the memory cost increases quadratically with the input image resolution and network depth. For example, given an median size input tensor of (32, 3, 224, 224), ResNet101 requires around 5000 MB. In more challenging tasks, DNNs that detect small objects and large number of object categories require input image resolution of more than 600 × 600 (Ren et al. (2015); Singh et al. (2017); Redmon & Farhadi (2018)). The memory issue is worse for video-based DNNs, such as CDC (Shou et al. (2017)), C3D (Ji et al. (2013)) and 3D-ResNet (Hara et al. (2017)). To model complex activities in video, the input tensor may contain 64 frames. Moreover, DNN training takes much more memory than testing. In order to train DNNs with large databases and big learning rate, the batch size can be up to 64. In training DNN compositions, such as Generative adversarial networks (GANs), multiple generator and discriminator networks are simultaneously stored in GPU memory.
Existing efforts to address memory issues presented three main approaches: (1) Better single GPUs. Recent GPUs provide larger memory at the expense of exponentially growing price and power consumption. For instance, from TitanXp, Quadro P6000 to Tesla V100, for 1-2.7 times increase in memory, the prices increase 2.8-8.5 times. (2) Parallelization among multiple GPUs (Dean et al. (2012); Shi et al. (2009); Langford et al. (2009); Mcdonald et al. (2009); McDonald et al. (2010); Zinkevich et al. (2010); Agarwal et al. (2014); Agarwal & Duchi (2011)), which requires expensive clusters, introduces substantial I/O cost, and does not reduce the total memory cost. (3) Low-level
1

Under review as a conference paper at ICLR 2019
Figure 1: Regular Training Approach vs. Re-forwarding (our). (a) The regular approach saves all tensors during forward, and uses these tensors to compute gradients during backward. (b) Reforwarding (our) saves a subset of tensors during the first forward, and conducts "Re-forward" to compute tensors for gradients during backward.
heuristic techniques. Optimization of computation graphs (Aho et al. (1986)), which merges inplace operations into non-inplace operations to cut down memory. Liveness analysis (Aho et al. (1986)), which dynamically recycles garbage tensors in training epochs. These approaches are specific to certain DNN structures, data and tasks. To address above issues, we propose a fundamental approach that explores trade-off between memory and computation power of GPUs. Note that recent affordable GPUs, although limited in memory ( 12GB), provide exceptional improvement in GPU cores and FLOPS. Trading computational time for memory is a very attractive solution that make it possible to train very heavy DNNs with finite GPU memory. Our approach only saves tensors at a subset of layers during the first forward, and conduct only extra local forwards to compute the missing tensors needed during backward. We call the extra forward process as Re-forwarding. The total memory cost is the sum of (1) the cost at the subset of layers and (2) the maximum memory cost among local re-forwards. Training with Reforwarding, see Fig. 1 (b), leads to substantial memory reduction. We propose sophisticate theories and efficient algorithms that achieve the optimal memory solution of arbitrary computation graphs.
2 RELATED WORK
To alleviate the memory pressure from a single GPU processor, many researchers utilized the wellestablished techniques for distributed computation (Dean et al. (2012); Shi et al. (2009); Langford et al. (2009); Mcdonald et al. (2009); McDonald et al. (2010); Zinkevich et al. (2010); Agarwal et al. (2014); Agarwal & Duchi (2011)). These techniques distribute memory pressure to possibly infinite GPUs or server clusters, but do not reduce the total memory cost of DNNs. Other researchers reduced the memory on finite hardware by optimizing computation graph of DNN and performing liveness analysis. The computation graph of DNNs describes the dependencies of tensors among layers. Liveness analysis recycles garbage to manage memory. These ideas were originated from compiler optimization (Aho et al. (1986)) and has been widely adopted by deep learning frameworks: Theano (Bastien et al. (2012); Bergstra et al. (2010)), MXNet (Chen et al. (2015)), Tensorflow (Abadi et al. (2016)) and CNTK (Yu et al. (2014)). Some other techniques efficiently swap data between CPU and GPU (Wang et al. (2018); Rhu et al. (2016)). These techniques usually cost extra I/O time and still do not actually reduce the total memory cost. The closest work to our approach, Chen et al.(Chen et al. (2016)), uses the gradient checkpoints (similar to the subset of layers in Re-forwarding). However, (Chen et al. (2016)) only worked on linear computation graph via a heuristic algorithm. Our approach generates optimal solutions for both linear and arbitrary computation graphs. Our algorithm reduces training memory by manipulating high-level tensors, therefore is generalizable to any DNNs and their compositions. All previous techniques are compatible to our approach and can further improve the memory efficiency of DNN training.
2

Under review as a conference paper at ICLR 2019

3 LINEAR COMPUTATION GRAPH (LCG)

Denote a computation graph as G = E, V . E = {ei} and V = {vi} are the edges and vertexes in

the computation graph, respectively. In deep neural networks, the vertexes represent the tensors and

the edges represent operations. Denote function l(·) as a measure of memory cost. VR is the subset

of vertexes saved during the first forward. l(vi) is defined as the memory cost of storing vertex vi.

For two adjacent vertexes vi and vj in set VR, the memory cost during re-forwarding from vi to vj

is defined as l(vi, vj) =

j-1 t=i+1

l(vt),

which

is

the

sum

of

cost

over

all

the

vertexes

between

vi

and

vj. Using these notations, the memory cost of training with re-forwarding is formulated as

min l(vi) + max l(vj, vj+1),

VR i

j

(1)

where the first term is the sum of the memory cost of all the stored tensors, and the second term is the maximal cost among the re-forwards.

For easy illustration, we start by formulating Re-forwarding on Linear Computation Graphs (LCG) (Fig. 2 (a)). For LCGs, Eqn. 1 can be solved in two cases.

Figure 2: (a) Linear Computation Graph (LCG). "s" denotes the start vertex,"t" denotes the end vertex. (b) Arbitrary Computation Graph (ACG). The structure between "s" and "t" vertexes may contain arbitrary branches and connections.

Case(1) LCG with Identical Vertex Cost: Suppose a LCG has n vertexes, each of which has the

same cost l(vi)

=

1 n

and the total cost is 1.

Obviously,

the optimal solution

is reached when vertexes

in is

VR are

then

k N

distributed evenly

+

1 k

.

The

optimal

in the LCG. Suppose thenumber solution of Eqn. 1 is k = n, and

of vertexes the optimal

in VR is k. The

total

cost

is

2 n

.

total

cost

Case (2) LCG with Non-identical Vertex Cost: When the assumption of identical cost does not

hold, the solution to Eqn. 1 does not have an analytic form. Denote the maximal Re-forward cost

max l(vj, vj+1) as a constant C, and the solution to Eqn. 1 is reduced to solving for min
j VR

i l(vi).

Algorithm 1 Linear Computation Graph (LCG) Solver
1: for each vertex pair (vi, vj) in G do 2: Set the maximal term as l(vi, vj) 3: Construct Accessibility Graph 4: Find the shortest path in the Accessibility Graph as the solution 5: Compute the actual total cost of the solution 6: Save the solution if it's better. 7: Suppose the actual max term of this solution is B, and l(vi, vj) = C, skip the loops where
B  l(vi, vj) < C

All the Re-forward costs in an optimal solution satisfy the constraint l(vj, vj+1)  C. We solve Eqn. 1 by constructing a new graph, called Accessibility Graph GA = EA, V . The edges of GA,

called Accessibility Edge eiAj, exists between vertex vi and vj if and only if l(vi, vj)  C. Now the

problem of solving min
VR

i l(vi) is equivalent to finding the shortest path from the source vertex

and the target vertex in the Accessibility Graph. Notice that in the optimal solution, the max term

equal the one maximal term among all l(vi, vi+1) terms. To traverse all possible max terms, we can simply compute the loss of every vertex pair and use it as a possible max term. Given a max term

C, suppose the actual max term of the solution under C is B and B < C. It's obvious that for all

3

Under review as a conference paper at ICLR 2019
the max terms B  max < C, the solution would be the same solution. Therefore, these max terms can be skipped. Algorithm 1 summarizes the process for searching an optimal solution for LCG.
4 ARBITRARY COMPUTATION GRAPH(ACG)
As generalization of DNNs with LCG, we present theory1 and algorithms for DNNs with Arbitrary Computation Graphs (ACG), in particular the acyclic directed graphs(Fig. 2 (b)).
4.1 ASSUMPTION
The optimal solution of Re-forwarding corresponds to an optimal division of ACG, such that memory cost (Eqn. 1) is minimum. We denote that an ACG is divided into end-to-end segments by a set of vertexes. These end-to-end segments can have multiple endpoint vertexes, for example, multiple source vertexes and multiple target vertexes. In this paper, as an assumption and also for simplification, these end-to-end segments are narrowed down to those with only one source vertex and one target vertex. Another assumption in the case of ACG is imposed on the operation that has multiple inputs: one can compute the gradients of output with respect to the gradients of inputs without using the current value of inputs. Examples of operations that meet this assumption are: concatenation (the gradient of output is also the concatenation of the gradient of input), add (the gradient of output equals the gradient of input), etc. An example that breaks this assumption is multiplication (the gradient of input depends on the input). Fortunately, most of the popular networks meet this assumption. A simple way to remove this assumption is to store all the input tensors of this multi-input operation. However, this is not modeled by our loss function and may lead to sub-optimal solution. In summary, there are only two assumptions in our approach: (1) the segment in a solution only has two endpoints (source and target). (2) the multi-input operation can compute the gradients of output without using the current value of input. Under these two assumptions, our approach is optimal for ACGs.
4.2 DEFINITION AND THEOREM
Figure 3: Close Set Examples: (a) Close set in a graph. v2 and v4 cannot form a close set because v3 depends on v1. v1 and v3 can form a close set because v2 doesn't depend on any other vertex. (b) Splittable Close Set (Type 1). v2 is the splitting vertex of s13. (c) Branched Close Set (Type 2). (d) Non-branched Close Set (Type 3). Definition 1. Close Set: A set of vertexes and edges that start from vi and end at vj but doesn't include vi, vj. sij = {v, e} is called a close set such that v1  sij, v1 has no edge to any v2 / sij  {vi, vj}. vi is the ancestor of v  sij. vj is the descendant of v  sij. The edges of sij are all the edges between v1, v2  sij, all the edges between vi and v  sij, all the edges between vj and v  sij. The edges of sij can include or not include the edge between vi and vj. The two situations define valid but different close sets. Definition 2. [sij] = sij  {vi, vj}. [sij) = sij  {vi}. (sij] = sij  {vj}
We define Close Set for independent end-to-end segment in the computation graph. Independence means that the vertexes inside this segment have no connections with other vertexes outside this segment. A close set sij is a set of edges and vertexes, starting from vertex vi and ending at vertex
1All proofs are in the appendix due to space limitation.
4

Under review as a conference paper at ICLR 2019
vj. For convenience, the close set sij doesn't include vi and vj. If vi has an edge to vj, there would be two valid close set: one has this edge, the other has not. We make this special case clear in this definition for the convenience of defining Branched Close Set in Definition 5. We also define notations for convenience to describe whether a set has the starting or ending vertexes in Definition 2. Figure 3 shows examples of Close Set. Definition 3. Splitting Vertex: A vertex vt  sij is a splitting vertex of sij if and only if sit exists, stj exists and sij = sit  stj  {vt} and sit  stj =  Definition 4. Splittable Close Set (Type 1): close set with at least 1 splitting vertex.
The definition of Splitting Vertex is to describe whether a close set can be divided into two linearly arranged close set. A close set is splittable if it has at least 1 splitting vertex and is defined as Close Set Type 1. Definition 5. Branched Close Set (Type 2): A close set is branched if it has 0 splitting vertex and can be divided into branches: sij = s1ij  s2ij and s1ij  si2j =  Definition 6. Non-branched Close Set (Type 3): A close set sij is non-branched if it has 0 splitting vertex and no branch: s1ij sij Among close set with no splitting vertex, we categorize close set with branches as Close Set Type 2, and close set without branches as Close Set Type 3. The examples of different types of close set are shown in Fig. 3.
Figure 4: Example divisions of three types of close sets. Members of a division are colored differently. (a) Division of close set type 1. The division is {[s12], [s23]} (b) Division of close set type 2. The division is {[s112], [s122], [s132]} (c) Division of close set type 3. The division is {[s12], [s13], [s23], [s24], [s34]} Definition 7. Maximal Split: {[spq]} is a maximal split of non-branched sij if [sij] = {[spq]} and sab, scd  {[spq]}, sab  scd =  and {[spq]} {[spq]} such that {[spq]} = [skt] [sij] Definition 8. Division of Close Set: For type 1, its division is the linear segments separated by all its splitting vertexes; for type 2, its division is all its branches, any of which cannot be divided into more branches; for type 3, its division is its maximal split.
For close set type 1, it can be divided into linearly arranged segments. For close set type 2, it can be divided into branches. So here we investigate the division of close set type 3. As we don't want trivial division, for example, division that is formed by every edge in the close set, we define Maximal Split to describe the split such that each member of the split is as large as possible. An example of maximal split is shown in Fig. 4 (c). In the definition of maximal split, the term maximal is implied by saying that any subset of this split cannot be combined into a single close set. If it can, then the maximal split will be formed by this larger close set and all the rest of the previous split. For close set type 3, we use its maximal split as its division. Definition 9. Division Tree: Division tree is a representation of a computation graph, where the root node is the whole computation graph, the leaf nodes are all the single tensors in the computation graph, and for a non-leaf node, its children is the members of its division.
With the division of 3 types of close sets, the computation graph can be reorganized into a division tree (Figure 5) where a non-leaf node would be a close set and its children would be its corresponding division. The root node is the whole computation graph, the largest close set, and the leaf nodes would be single tensors in the computation graph. With division tree, we can apply divide-andconquer to search for optimal solution.
5

Under review as a conference paper at ICLR 2019
Figure 5: In this tree, the root node is the whole computation graph. All the leaf nodes are single tensors. Every other node except root and leaves is a member of the division of its parent.
Theorem 1. The division tree of a computation graph is unique and complete.
The uniqueness of the division tree indicates that the optimal solution of the division tree would also be the optimal solution of the whole computation graph. The completeness indicates that the division tree has included all the possible members of solution and represents the whole search space for the optimal solution. Theorem 1 is proved in the appendix.
4.3 ALGORITHM
We search optimal solutions for ACGs by solving several sub-problems using Algorithm 2-4 respectively. Based on these components, we present our final solver as Algorithm 5. Algorithm 2 judges whether a vertex is a splitting vertex of a close set. This algorithm mainly follows the Definition 3 and uses vertex set to check the property of a splitting vertex. With this algorithm, we can judge whether a close set is type 1 and get its division if it is.
Algorithm 2 Judge whether a vertex vt is a splitting vertex of close set sij 1: Let {vin} be the vertexes of all the vertexes within [sij] that have paths to vt. Let {vout} be the
vertexes of all the vertexes within [sij] that have paths from vt. 2: if {vin}{Vout}{vt} = {v|v  [sij]} and {vin}{Vout} =  and v1  {vin}, v2  {vout},
v1, v2 have connections then 3: Return true 4: else 5: Return False
Algorithm 3 examines whether a close set is branched. It uses a growing algorithm to check whether an independent subpart of this close set can form a close set. If a non-trivial close set sij has an edge from vi to vj, then it's branched because this edge itself can be treated as a close set. Combined with Algorithm 2, we can know the type of a close set and get its division if it's type 2. Algorithm 4 addresses the problem of finding the maximal split, the division of a close set type 3 sij. First get all the possible close sets within sij and use a property of maximal split to judge whether this close set is a member of the maximal split. The property is: there cannot exist another close set sab sij but contains any member of this maximal split. This property is proved in Lemma 6 of the appendix. Algorithm 5 is the solver for ACGs. First, the division tree of the computation graph is built. Similar to the linear solver, a max term list is formed by the cost of all the possible close sets for traverse. Given a max term, we propose a greedy idea: for a close set, never expand it unless the its cost exceed the max term. In other word, if the max term doesn't allow a leap over this close set, we expand it, otherwise, do not expand it. Because once expanded, some cost of other vertexes inside this close set might be introduced, and the cost will never be smaller than unexpanded. If some children of the close set type 1 are expanded, the rest reforms a few linear segments and still can
6

Under review as a conference paper at ICLR 2019
Algorithm 3 Judge whether sij is branched 1: if sij has at least 1 vertex then 2: if sij includes an edge from vi to vj then 3: Return true 4: else 5: Initialize a vertex set s = {vk}. vk  sij is a randomly chosen vertex. 6: while True do 7: For any vt  sij, vt  s that has connection to any vk  s, add vt to s. 8: if No more vertex can be added to s then 9: Break 10: if s = {v  sij} then 11: Return false 12: else 13: Return true 14: else 15: Return false
Algorithm 4 Find the maximal split of a non-branched sij with 0 splitting vertex 1: for each vertex pair (vk, vt) except (vi, vj) in [sij] do 2: For all the vertexes {v} that have paths from vk and have paths to vt. 3: if v2  {v} and v2 = vk, vt, v2 has connection to a v1  {v} then 4: Form a close set skt with all these vertexes. 5: for each formed close set skt do 6: If there doesn't exist a sab such that skt sab sij, put skt into the maximal split.
be solved by the linear solver. If some children of the close set type 2 or 3 are expanded, the other members remain unexpanded and need no changes.
Algorithm 5 Arbitrary Computation Graph (ACG) Solver 1: Get all possible close set and their costs. Use their costs to form the max term list. 2: Reorganize the computation graph into a division tree: from the root node (the computation
graph), build its children from its division, until all the leaf nodes are single tensors. 3: for each possible max term m in max term list {m} do 4: if current close set is type 1 then 5: For all the children that have cost larger than current max term. Expand them and solve the
next level. 6: All the expanded children have separated the current close set to linear segments. Solve all
the linear segments with current max term. 7: else 8: For all the children that have cost larger than current max term. Expand them and solve the
next level. 9: All the other members remain unexpanded. 10: Summarize the total loss, save the current solution if it's better.
5 EXPERIMENT
We evaluated Re-forwarding on two main groups of neural networks (1) networks with linear structures, such as Alexnet (Krizhevsky et al. (2012)) and vgg series (Simonyan & Zisserman (2014)). (2) networks with non-linear structures, such as Resnet series (He et al. (2016)), Densenet series (Huang et al. (2017)) and Inception net (Szegedy et al. (2016)). For each network in Table 5, an computation graph is built such that every vertex is a Float32 tensor, every edge is an operation, and the memory cost of a vertex is its tensor size measured in MB. We compared Re-forwarding with Chen (Chen et al. (2016)) and the regular training approach. Note that Chen et al. (2016) only worked on linear computation graphs. To compare with (Chen et al. (2016)) on non-linear networks, we manually re-organized all the non-linear computation graphs into linear computation graphs with
7

Under review as a conference paper at ICLR 2019

Table 1: Training memory usage of the regular, Chen et al. (2016), Chen et al. (2016) manual and Re-

forwarding (ours) approach on linear and non-linear computation graph. "Memory Usage Ratios"

are used to measure the training memory used by Re-forwarding (ours) against the regular training

approach. All ratios are the lower the better.

Linear network

Regular Chen et al. (2016) Re-forwarding (ours) Memory Usage Ratios Memory Usage Ratios

(MB)

(MB)

(MB)

(ours)(Measured) (ours)(Theoretical)

Alexnet batch size 1024 3550

3108

2620

0.74

0.58

Vgg11 batch size 64 2976

2292

1802

0.61

0.50

Vgg13 batch size 64 4152

2586

2586

0.62

0.53

Vgg16 batch size 64 4470

2894

2586

0.58

0.49

Vgg19 batch size 64 4788

2894

2502

0.52

0.47

Non-linear network

Regular Chen et al. (2016) Re-forwarding (ours) Memory Usage Ratios Memory Usage Ratios

(MB) manual (MB)

(MB)

(ours)(Measured) (ours)(Theoretical)

Resnet18 batch size 256 5402

2898

2898

0.54

0.37

Resnet34 batch size 128 3900

1936

1544

0.40

0.27

Resnet50 batch size 64 5206

2332

1798

0.35

0.25

Resnet101 batch size 32 3812

1216

970

0.25

0.19

Resnet152 batch size 16 2810

636

564

0.20

0.16

Densenet121 batch size 32 3984

1012

776

0.19

0.19

Densenet161 batch size 16 3658

744

616

0.17

0.16

Densenet169 batch size 32 4826

998

848

0.18

0.16

Densenet201 batch size 16 3164

600

582

0.18

0.14

Inceptionv3 batch size 32 2976

1026

910

0.31

0.24

their splitting vertexes, and fed them to Chen et al. (2016) (see Table 5 "Chen et al. (2016) manual (MB)"). Our Re-forwarding approach directly works on arbitrary computation graphs.
All experiments were conducted in Pytorch. To remove irrelevant GPU memory cost, such as model and Pytorch CUDA interface cost, all training memory costs were measured with two different input sizes and compute the difference between two measurements. For example, to measure the memory cost of Alexnet with input size [BatchSize, Channel, W idth, Height]= [16, 3, 224, 224], we first record the training memory of input [16, 3, 224, 224] as r1, and input [32, 3, 224, 224] as r2. The actual memory cost given [16, 3, 224, 224] input is measured as r2 - r1. To see how well the reality matches with the theory, We also compared the "Measured" and "Theoretical" (given by Algorithm 5) memory usage ratio of Re-forwarding divided by the regular approach. To use existing DNN implementations, the input of Inception net is [Batchsize, 3, 300, 300], and the input of all other networks is [Batchsize, 3, 224, 224].
Table. 5 shows that Re-forwarding cuts down huge amount of memory from the regular approach: nearly 30% off for Alexnet, 50% off for Vgg series. For Resnet series, the deeper network, the more memory was cut down. On the deepest Resnet152, 80% off was achieved. For Densenet series, more than 80% off was achieved. Observe that, "Measured" ratios are slightly higher than the "Theoretical" ratios, meaning less memory was cut down. This is because, in implementation, we assume that the whole input tensors of each operation are always stored. In reality, some operations only need to store small tensors for backward. For example, batch-normalization only needs a few statistics for backward and doesn't need the whole input tensor. Moreover, notice that, Chen et al. (2016) only works on linear networks. Its results on non-linear networks were manually synthesized. Re-forwarding directly works on non-linear networks and constantly outperformed Chen et al. (2016) and its "manual" version. This supports our claim that Re-forwarding is optimal.
6 CONCLUSION
Re-forwarding is a fundamental approach that explores trade-off between memory and computation power of GPUs. By saving tensors at a subset of layers during forward, and conducting extra local forwards for backward, Re-forwarding makes it possible to train very heavy DNNs with finite GPU memory. To our knowledge, our theoretical and algorithmic results are the first top-down work that achieve an optimal memory solution for arbitrary computation graphs in DNNs. Re-forwarding can be further embedded and optimized with any low-level techniques such as distributed computing, GPU/CPU swapping, computation graph optimization and liveness analysis.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems, pp. 873­881, 2011.
Alekh Agarwal, Olivier Chapelle, Miroslav Dud´ik, and John Langford. A reliable effective terascale linear learning system. The Journal of Machine Learning Research, 15(1):1111­1133, 2014.
Alfred V Aho, Ravi Sethi, and Jeffrey D Ullman. Compilers, principles, techniques. Addison Wesley, 7(8):9, 1986.
Fre´de´ric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, and Yoshua Bengio. Theano: new features and speed improvements. arXiv preprint arXiv:1211.5590, 2012.
James Bergstra, Olivier Breuleux, Fre´de´ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A cpu and gpu math compiler in python. In Proc. 9th Python in Science Conf, volume 1, 2010.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in neural information processing systems, pp. 1223­1231, 2012.
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Learning spatio-temporal features with 3d residual networks for action recognition. In Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition, volume 2, pp. 4, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, volume 1, pp. 3, 2017.
Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1):221­231, 2013.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
John Langford, Alexander J Smola, and Martin Zinkevich. Slow learners are fast. Advances in Neural Information Processing Systems, 22:2331­2339, 2009.
Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon S Mann. Efficient large-scale distributed training of conditional maximum entropy models. In Advances in Neural Information Processing Systems, pp. 1231­1239, 2009.
Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured perceptron. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 456­464. Association for Computational Linguistics, 2010.
9

Under review as a conference paper at ICLR 2019
Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. CoRR, abs/1804.02767, 2018. URL http://arxiv.org/abs/1804.02767.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91­99, 2015.
Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W Keckler. vdnn: Virtualized deep neural networks for scalable, memory-efficient neural network design. In Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on, pp. 1­13. IEEE, 2016.
Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, Alex Strehl, and Vishy Vishwanathan. Hash kernels. In Artificial intelligence and statistics, pp. 496­503, 2009.
Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. Cdc: convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1417­1426. IEEE, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Bharat Singh, Hengduo Li, Abhishek Sharma, and Larry S. Davis. R-FCN-3000 at 30fps: Decoupling detection and classification. CoRR, abs/1712.01802, 2017. URL http://arxiv.org/ abs/1712.01802.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818­2826, 2016.
Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic gpu memory management for training deep neural networks. arXiv preprint arXiv:1801.04380, 2018.
Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, et al. An introduction to computational networks and the computational network toolkit. Microsoft Technical Report MSR-TR-2014­112, 2014.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In Advances in neural information processing systems, pp. 2595­2603, 2010.
10

Under review as a conference paper at ICLR 2019
A PROOF
A.1 LEMMAS
Lemma 1. If sij  skt =  and sij  skt and skt  sij , then sij  skt = skj or sij  skt = sit
Proof. Let [sij]  [skt] = s = {v, e}. Let vp be the source vertex of s, vq be the target vertex of s. If vp = vi and vp = vk and vq = vj and vq = vt, then vi, vk has path to vp and vj, vt has path from vq. Therefore, vp has at least 2 immediate parents va, vb with va  [sij], va  [skt], vb  [skt], vb  [sij]. If so, the independence of sij and skt is violated. Therefore, vp must be vi or vk. Same on vq, vq must be vj or vt. If vp = vi, vq = vj, then sij  skt. If vp = vk, vq = vt, then skt  sij. Therefore, vp = vi, vq = vt or vp = vk, vq = vj. Suppose vp = vk, vq = vj, let's prove s is a close set. With s  skt, v1  s, v1 has no edge with v2  [skt]. With s  sij, v1  s, v1 has no edge with v2  [sij]. Therefore, v1  s, v1 has no edge with v2  [s]. The independence of s is guaranteed. In the discussion before, we can see the source vertex vp of s must be either vi or vk. If vi and vk are both the source vertexes of s, then vi  [skt] and vk  [sij], vi has path to vk and vk has path to vi, which will force vi = vk because the sij, skt is acyclic. Same on vq, s can only have 1 source vertex and 1 target vertex. Therefore, s is close set. Therefore, sij  skt = skj or sij  skt = sit.
Lemma 2. The intersection of two closets s = si  sj =  is also a closet
Proof. Given the independence of si and sj, the independence of s is obvious. The remaining thing is whether s only has 1 source vertex and 1 target vertex. In the proof of Lemma 1, we can see any source or target vertex of s will eventually become source or target vertex of si and sj. With simple discussion, we can have this lemma.
Lemma 3. If sij  skt = skj = , then vk is the splitting vertex of sij and vj is the splitting vertex of skt
Proof. Let's first prove that vk is the splitting vertex of sij. Let s = sij - [skj). Obviously, sij = s  skj  {vk} and s  skj = . We only need to prove that s is close set. For convenience, let's denote [s] = s  {vi, vk} vi is obviously the only source vertex of [s] because vi is source vertex of [sij]. We discuss the target vertex here. If vk is not the target vertex of [s], as vk  [s], vk must have path to the target vertex v of [s] and v also has path to vj as v  sij. Because v / [skj], in the path from v to vj, there exists an edge that connects a vertex v1  s with a vertex v2  skt which violates the independence of skt. Therefore, the target vertex of [s] can only be vk. As s  [sij), v1  s, v1 has no edge with v2  [sij). As skj is close, v1  s, v1 has no edge with v2  skj. v1  s, v1 can only have edge with v2  [s]. Thus the independence of s is guaranteed. Therefore, s is close set, vk is the splitting vertex of sij. Same on vj, vj is the splitting vertex of skt
Lemma 4. If sij has n splitting vertexes {v1, v2, ..., vn}, then sij = si1  s12  ...  snj  {v1, v2, ..., vn}
Proof. If n = 2, the splitting vertexes are v1, v2, sij = si1  s1j  {v1} = si2  s2j  {v2}. Let v1  si2, v1 = v2, then s1j  si2 = s12 = . According to Lemma 3, v1 is splitting vertex of si2 and v2 is splitting vertex of s1j. Therefore, sij = si1  s12  s2j  {v1, v2}. For n > 2, the lemma can be proved by repetitively using the conclusion in n = 2.
11

Under review as a conference paper at ICLR 2019
Lemma 5. If the non-branched sij has a maximal split {[spq]}, and |{[spq]}| > 2, denote {v} as all the endpoint vertexes of [s]  {[spq]}. Then v  {v}, v = vi, vj, v is the endpoint vertex of at least 3 members of the maximal split.
Proof. If vb is the endpoint vertex of only 2 members of the maximal split, suppose the 2 members are sab and sbc. If so, sab and sbc can be merged into sac. If sac = sij, this violates the definition of maximal split. Otherwise, it violates the condition that sij is non-branched and |{[spq]}| > 2. It is impossible that the 2 members are sab and scb because in this way vb has no path to vj and violates the definition of close set. If vb is the endpoint vertex of only 1 member of the maximal split, then vb must be either vi or vj. Therefore, this lemma is proved.
Lemma 6. Any member of a maximal split can not be the subset of another close set s sij.
Proof. Suppose the source vertex of s is v1 and target vertex is v2, a member sxy of the maximal split is inside s.
Suppose a member sab of the maximal split has its source vertex va inside s and target vertex vb outside s. Then the boundary vertex (the vertex that has edges to the non-overlapping parts of both sets) must be v2, otherwise the independence of s will be violated. Notice that v2 is inside sab and the independence of sab needs to be guaranteed, for vp  s, vp / s  sab, vq  s  sab, vp has no edge with vq. Therefore, va is a splitting vertex of s. Similarly, if sba has its target vertex va inside s and source vertex vb outside s, the boundary vertex must be v1 and va is a splitting vertex of s. For the close set s, from the discussion above, we know that there are at most 2 members of the maximal split that can overlap with s. Other members must be either completely inside s or completely outside s. Let's discuss the number of members that overlaps with s.
If there are 0 member that overlaps with s, s is the union of a subset of members of the maximal split, which violates the definition of maximal split.
If there is 1 member that overlaps with s, suppose the corresponding splitting vertex is vb, and the boundary vertex is actually v2. Then s1b is a close set containing sxy and corresponds to the situation of 0 member overlapping. s1b is the union of a subset of members of the maximal split, and violates the definition of maximal split.
If there are 2 members that overlaps with s, suppose they generate two different splitting vertex va and vb. Then sab is a close set containing sxy and corresponds to the situation of 0 member overlapping. sab is the union of a subset of members of the maximal split, and violates the definition of maximal split.
If they generate the same splitting vertex vb, from lemma 5, vb is also the endpoint vertex of at least 1 other member sab which has to be inside s. Suppose the two overlapping members are scb that contains v1, and sbd that contains v2. As the source vertex of s, v1 has path to vb and v1 has path to va, which implies vb has path to va. As the target vertex of s, v2 has path from vb and v2 has path from va, which implies vb has path from va. This conflicts with the fact that s is acyclic. Therefore, this case is not possible.
Therefore, this lemma is proved.
Lemma 7. If non-branched sij has at least 1 vertex but has 0 splitting vertex, then its maximal split has length > 2
Proof. As sij is not branched, the members of its maximal split cannot have the starting vertex as vi and the ending vertex as vj at the same time. If sij has at least 1 vertex, and its maximal split has length 2, then its maximal split must be {[sik], [skj]}, and vk will be the splitting vertex of sij, which violates that sij has no splitting vertex.
12

Under review as a conference paper at ICLR 2019
If sij has at least 1 vertex without splitting vertex, it has at least 2 edges and cannot have a trivial length 1 maximal split. Therefore, its maximal split has length > 2
A.2 UNIQUENESS OF DIVISION TREE
To prove this uniqueness, we simply discuss the division uniqueness of close set type 1, 2 and 3.
A.2.1 UNIQUENESS OF DIVISION OF CLOSE SET TYPE 1
Proof. By the definition of this division and Lemma 4, the uniqueness of the division is equivalent to the uniqueness of the splitting vertex set of a close set type 1. The splitting vertex set is obviously unique.
A.2.2 UNIQUENESS OF DIVISION OF CLOSE SET TYPE 2
Proof. If there exists another division, there must be a branch member s1ij in division 1 and a branch member si2j in division 2, where si1j  s2ij =  and si1j = s2ij. Denote s = si1j  s2ij. By Lemma 1 and 2, s = si3j is also a close set. As s1ij and s2ij cannot be divided into more branches, s = si1j = si2j. Therefore, the division of close set type 2 is unique.
A.2.3 UNIQUENESS OF DIVISION OF CLOSE SET TYPE 3
Proof. As the close set in the division tree has at least 1 vertex, with Lemma 7, we know that the division, i.e. maximal split of a close set type 3 sij within the division tree will have length > 2. Denote this maximal split as {[spq]}, we only need to prove this maximal split is unique. Suppose there is a another different maximal split {[spq]}, let us only check the difference between {[spq]} and {[spq]}. Denote {[skt]} and {[skt]} with {[spq]} - {[skt]} = {[spq]} - {[skt]} and s  {[skt]}, s  {[skt]}, s = s . As {[spq]} - {[skt]} = {[spq]} - {[skt]}, we have {[skt]} = {[skt]} Obviously, |{[skt]}|  2 and |{[skt]}|  2. Denote {v} as all the endpoint vertexes of [s]  {[skt]}, and {v } for {[skt]}. Obviously {v} =  and {v } = . As sij is non-branched, {v}  {vi, vj} - {vi, vj} =  and {v }  {vi, vj} - {vi, vj} = . Suppose sab, sbc  {[skt]}, according to Lemma 5, there's at least 1 other member that has vb as endpoint vertex. Suppose the other endpoint of this member is vd. Let's discuss whether vb  {v } and whether vd  {[skt]}. If vd  {[skt]}, then vb must occur in {v }. Otherwise, vb would be inside a close set which would be violated by vd. Given vb  {v }, as sab  {[skt]}, suppose seb  {[skt]} and sab  seb = . If va  seb, from Lemma 1, seb cannot be close. If ve  sab, from Lemma 5, sab cannot be close. In this case, there cannot exist another different maximal split. If vd  {[skt]}, then sbd  {[skt]}. If vb  {v }, we can use the same logic above to show this is impossible. Therefore, vb  {v } and sbd is included by a close set s. From Lemma 6, this is impossible. In this case, there cannot exist another different maximal split. In all the cases, there cannot exist another different maximal split. Therefore, the maximal split is unique.
A.3 COMPLETENESS OF DIVISION TREE
Similar with the uniqueness, the completeness of division tree is equivalent to the completeness of the division of a close set. To prove this completeness, we simply discuss the division completeness of close set type 1, 2 and 3. An equivalent statement of the division completeness is: there doesn't exist a close set whose head is in one member of the division and whose tail is in another member of the division.
13

Under review as a conference paper at ICLR 2019
A.3.1 COMPLETENESS OF DIVISION OF CLOSE SET TYPE 1 Proof. Suppose there exists a close set s whose head vp is in one member s1 and whose tail vq is in another member s2. If vp is not an endpoint of s1, then according to Lemma 3, vp is also a splitting vertex in s1 and can break s1 into smaller segments, which makes vp also the splitting vertex of the whole close set. However, vp is not the splitting vertex of the whole close set sij. This also applies to vq. Therefore, the division of close set type 1 is complete. A.3.2 COMPLETENESS OF DIVISION OF CLOSE SET TYPE 2 Proof. Suppose there exists a close set s whose head vp is in one branch s1ij and whose tail vq is in another branch si2j. As s crosses s1ij and s2ij, there exists a boundary vertex v in s, which belongs to [si1j] and has direct connection with a vertex outside [s1ij]. If v is not vi or vj, it will violate the independence of sij. If v = vi, as vi is the head of both si1j and si2j, it cannot be the boundary vertex, same when v = vj. Therefore, there cannot exist such a close set s. The division of close set type 2 is complete. A.3.3 COMPLETENESS OF DIVISION OF CLOSE SET TYPE 3 Proof. Suppose there exists a close set s whose head vp is in one member s1 and whose tail vq is in another member s2. Same with close set type 2, the boundary vertex v has to be the endpoint vertex of s1 or the independence of s1 will be violated. According to Lemma 5, v is the endpoint vertex of at least 3 members, meaning that v will at least have 1 connection with another close set s3. To maintain the independence of s, s has to include s3 as well. However, s3 also has its endpoints. This will propagate until s becomes the whole close set. Therefore, there cannot exist such a close set s. The division of close set type 3 is complete.
14

