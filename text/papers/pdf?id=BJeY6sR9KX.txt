Under review as a conference paper at ICLR 2019
ALIGNING ARTIFICIAL NEURAL NETWORKS TO THE BRAIN YIELDS SHALLOW RECURRENT ARCHITEC-
TURES
Anonymous authors Paper under double-blind review
ABSTRACT
Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks that score any ANN on how brainlike it is, together with an online platform where ANNs can be submitted to receive a Brain-Score and their rank relative to other models. Deploying our framework on dozens of state-of-the-art ANNs, we found that ResNet and DenseNet families of models are the closest models from the Machine Learning community to primate ventral visual stream. Curiously, best current ImageNet models, such as PNASNet, were not the top-performing models on Brain-Score. Despite high scores, these deep models are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: a neural network developed by using Brain-Score as a guide with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas with recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet.
1 INTRODUCTION
In our view, the goal for computational vision systems is to be at the very least as capable as the human visual system. If we could mimic the workings of the visual system ­ both the outputs of the system and its internal representations ­ such an approach would necessarily yield powerful models: for neuroscience, these models would become mechanistic hypotheses of the processes in the brain, and for machine learning, robust models that generalize across datasets and tasks would become available. Historically, this idea of neuroscience-driven machine learning dates back to the earliest days of the field of artificial intelligence with aritficial neurons being directly inspired from biological neurons (McCulloch & Pitts, 1943) and hierarchical convolutional structure of the visual system paving the road towards convolutional neural networks (Hubel & Wiesel, 1962; Fukushima, 1980). In recent years however, aligning artifical neural networks to biological ones has been less successful, partially due to the lack of sufficient large datasets that could constrain machine learning models and robust comparison metrics (but see Yamins et al. (2013; 2014)). Instead, progress in machine learning stemmed from strong benchmarks such as ImageNet (Deng et al., 2009) which lead to impressive models of object recognition (Russakovsky et al., 2015). In neuroscience on the other hand, benchmarking has so far not been clearly established and quantitative model evaluations are less common.
To facilitate the cross-talk between machine learning and neuroscience, we developed Brain-Score, a large-scale benchmark composed of neural recordings and behavioral measurements that enables
1

Under review as a conference paper at ICLR 2019

.55 .50 .45 .40 .35
0

.56
.55
.54
.53
.52 n.s.
70 r = 07.292 74 76 78 80 82 20 40 60 80

Figure 1: Synergizing machine learning and neuroscience through Brain-Score (top). By quantifying brain-likeness of models, we can compare models of the brain and use insights gained to inform the next generation of models. DenseNet, CORnet-S and ResNet architectures are the current winning models on Brain-Score. Correlation to ImageNet is strong at first, but non-significant for high-performance models. CORnet-S area architecture (bottom left). The model consists of four areas which are pre-mapped to cortical areas V1, V2, V4, and IT in the ventral stream. V1COR is feed-forward and acts as a pre-processor to reduce the input complexity. V2COR, V4COR and ITCOR are recurrent (within area) to reduce the need for many layers and incorporate skip-connections, following the observation that ResNets are strong models on Brain-Score.
a quantified comparison between models and brain data. For the machine learning community, Brain-Score aims to supplement existing benchmarks by enabling a quick comparison against an "oracle" model that is known to generalize well across datasets and tasks. On the neuroscience side, we hope to encourage the development of models that are more brain-like and thus easier to map onto the brain, to analyze, and to drive next experiments. For instance, deep neural networks are often criticized or ignored by the neuroscience community due to the complexity in the number of layers and due to missing key elements from the ventral stream, such as recurrent and feedback connections.
To validate the promise of our approach, we show that using this benchmark as a guiding score for building models can yield competitive performance on ImageNet. Our model CORnet-S commits to anatomical structure and is a top model on brain neural and behavioral recordings. At the same time, despite being relatively shallow, CORnet-S retains a strong ImageNet top-1 performance of 73.1%. Taken together, Brain-Score benchmark opens new possibilities for machine learning practitioners and neuroscientists to collaborate and advance both fields together.
2 BRAIN-SCORE: COMPARING MODELS TO BRAIN
To obtain a single scalar for brain-likeness, we built Brain-Score, a composite benchmark that measures how well models can predict (a) the mean neural response of each neural recording site to each and every tested naturalistic image in non-human primate visual areas V4 and IT (data from Majaj et al., 2015) and (b) mean pooled human choices when reporting a target object to each and every tested naturalistic image (data from Rajalingham et al., 2018). Individual benchmarks are briefly outlined below; see Appendix B for more details.
Brain-Score is open-sourced as a platform to score neural networks on brain data through GitHub, a pip package, and a website for online submissions.
2

Under review as a conference paper at ICLR 2019
2.1 NEURAL PREDICTIVITY
A total of 2760 images containing a single object pasted randomly on a natural background were presented centrally to passively fixated monkeys for 100 ms and neural responses were obtained from 88 V4 sites and 168 IT sites. For our analyses, we used normalized time-averaged neural responses in the 70-170 ms window. A regression model was constructed for each neuron using 90% of image responses and tested on the remaining 10% in a 10-fold cross-validation strategy. The median over neurons of the Pearsons r between the predicted and actual response constituted the final neural fit score for each visual area. In our model CORnet-S, we used designated model areas and the best time point to predict corresponding neural data. In comparison models, we used the most predictive layer.
2.2 BEHAVIORAL PREDICTIVITY
A total of 2400 images containing a single object pasted randomly on a natural background were presented to 1472 humans for 100 ms and they were asked to choose from two options which object they saw. 240 of those images with around 60 responses per object-distractor pair were used in further analyses, totalling in over three hundred thousand unique responses. For all models tested, we used the outputs of the layer just prior to transformation into 1000-value ImageNet-specific category vectors to construct a linear (logistic regression) decoder from model features. We used the regressions probabilities for each class to compare model choices against actual human responses. This is a correlational measure, meaning that models that do better on behavioral predictivity are better aligned with human responses, making similar correct choices and commiting similar errors.
2.3 FEEDFORWARD SIMPLICITY
Given equally predictive models, we prefer a simpler one. From a neuroscience point of view, simpler models can be better mapped to cortex and be better analyzed and understood with regard to the brain. Simpler models can also be better made sense of sense in terms of what components constitute a strong model by reducing models to their most essential elements. We considered several alternative metrics to measure model simplicity. One possibility was to use the total number of parameters (weights). However, it did not seem to map well to simplicity in neuroscience terms. For instance, a single convolutional layer with many filter maps could have many parameters yet it seems much simpler than a multilayer branching structure, like the Inception block (Szegedy et al., 2017), that may have less parameters overall.
Moreover, our models are always tested on independent data sampled from different distributions than the train data. Thus, after training a model, all these parameters were fixed for the purposes of brain benchmarks, and the only free parameters are the ones introduced by the linear decoder that is trained on top of the frozen models parameters (see above for decoder details).
We also considered computing the total number of convolutional and fully-connected layers, but some models, like Inception, perform some convolutions in parallel, while others, like ResNeXt (Xie et al., 2017), group multiple convolutions into a single computation.
We therefore computed the number of convolutions and fully-connected layers along the longest path of information flow. For instance, the circuits in each of CORnet-S areas have the length of four since information is passed sequentially through four convolutional layers. Note that we counted recurrent (shared parameter) paths only once. If recurrent paths were counted the number of times information was passed through them, models with shared parameters would be no simpler than those using unique parameters at every time (i.e., feedforward models), which is counterintuitive to us.
We also wanted to emphasize that the difference between a path length of 5 and 10 is much greater than between 105 and 110, so the path length was transformed by a natural logarithm. Finally, we wanted a measure of simplicity, not complexity, so the resulting value was inverted, resulting in the following formulation of Feedforward Simplicity:
1 Feedforward Simplicity = ln (Longest path in model)
3

Under review as a conference paper at ICLR 2019
Given the lack of consensus how many areas primate ventral visual pathway contains, Feedforward Simplicity is currently not included in the composite Brain-Score and is reported separately in this paper.
3 CORNET-S: BRAIN-DRIVEN MODEL ARCHITECTURE
Using Brain-Score and Feedforward Simplicity as our guiding measures, we built CORnet-S. Specifically, our model aims to be (based on Kubilius, 2018): (1) Predictive, so that it is most explanatory of neural and behavioral benchmarks, i.e. the model should obtain strong scores on Brain-Score. We are not only interested in having correct model outputs (behaviors) but also internals that match the brain's anatomical and functional constraints. We prefer neural network models because neurons are the units of online information transmission and models without neurons cannot be obviously mapped to neural spiking data (Yamins & DiCarlo, 2016). (2) Compact, i.e. we prefer simpler models among models with similar scores as they are potentially easier to understand and more efficient to experiment with. The human and non-human primate ventral visual pathway consists of only a handful of areas that process visual inputs: retina, LGN, V1, V2, V4, and a set of areas in the inferior temporal cortex (IT). While the exact number of areas is not yet established, we ask that models have few areas (though each area may perform multiple operations). The model should thus obtain strong scores on Feedforward Simplicity. We further have no strong reason to believe that circuitry should be different across areas in the ventral visual pathway. We therefore prefer models where the operations and connectivity in each model area are the same. (3) Recurrent; while core object recognition was originally believed to be the result of largely feedforward mechanism because of its fast time scale (DiCarlo et al., 2012), it has long been suspected that recurrent connections must be relevant for some aspects of object perception (Lamme & Roelfsema, 2000; Bar et al., 2006; Wagemans et al., 2012), and recent studies have shown a potential role of recurrent processes even at the fast time scale of core object recognition (Kar et al., 2018; Tang et al., 2018; Rajaei et al., 2018). Moreover, even if recurrent processes are not critical to core object recognition, responses in the visual system still have a temporal profile, so a good model at least should be able to produce responses over time.
3.1 CORNET-S MODEL SPECIFICS
CORnet-S (Fig. 1) aims to rival the best models on Brain-Score by transforming very deep feedforward architectures into a shallow recurrent model. Specifically, CORnet-S draws inspiration from ResNets that are some of the best models on our behavioral benchmark Fig. 1 and (Rajalingham et al., 2018). Liao & Poggio (2016) proposed that ResNet structure could be thought of as an unrolled recurrent network and recent studies further demonstrated that weight sharing was indeed possible without a significant loss in CIFAR and ImageNet performance (Jastrzbski et al., 2017; Leroux et al., 2018).
Moreover, CORnet-S takes mapping to brain areas seriously. While in Brain-Score we establish this mapping by searching for the layer in the model that best explains responses in a given brain area, ideally such mapping would already be provided by the model, leaving no free parameters to researchers. Thus, CORnet-S has four computational areas, conceptualized as analogous to the ventral visual areas in cortex: V1, V2, V4, and IT, and a linear category decoder that maps from the population of neurons in the model's last visual area to its behavioral choices. Importantly, this commitment also means that, for example, if model's area V1 is worse than area V4 at predicting neural responses in visual area V1, then this model would be falsified.
Each visual area implements a particular neural circuitry with neurons performing simple canonical computations: convolution, addition, nonlinearity, response normalization or pooling over a receptive field. In the model presented here, the circuitry is identical in each of its visual areas (except for V1), but we vary the total number of neurons in each area.
The details of the layers depicted in Figure 1 are as follows: Due to high computational demands, first area V1COR performs a 7 × 7 convolution with stride 2, 3 × 3 max pooling with stride 2, and a 3 × 3 convolution. Areas V2COR, V4COR and ITCOR perform two 1 × 1 convolutions, a 3 × 3 convolution with stride 2 and a 1 × 1 convolution. To implement recurrence, outputs of an area are passed through it a several times to yield the final output of that area. For instance, after V2COR
4

Under review as a conference paper at ICLR 2019
Figure 2: Comparison of brain scores on several popular models and CORnet-S. CORnet-S is comparable to the state-of-the-art models on neural predictivity and a top model on behavioral predictivity (see Appendix A for numerical results).
processed the input once, that result is passed into V2COR again and treated as a new input. Input changes over time are thus not implemented (denoted as "gate" in Fig. 1). V2COR and ITCOR are repeated twice, V4COR is repeated four times. Batch normalization (Ioffe & Szegedy, 2015) was not shared over time as suggested by Jastrzbski et al. (2017). There are no across-area bypass or acrossarea feedback connections in the current definition of CORnet-S and retinal and LGN processing are omitted (see Section 5). The decoder part of a model implements a simple linear classifier ­ a set of weighted linear sums with one sum for each object category. When training on ImageNet, responses of last model area (and last time step in the case of recurrence) are further passed through a softmax nonlinearity to perform a 1000-way classification. To reduce the amount of neural responses projecting to this classifier, we first average responses over the entire receptive field per feature map.
3.2 IMPLEMENTATION DETAILS
We used PyTorch 0.4.1 and trained the model using ImageNet 2012 (Russakovsky et al., 2015). Images were preprocessed (1) for training, with random crops to 224 x 224 pixels, randomly flipped left and right and normalized by mean subtraction and division by standard deviation of the dataset; (2) for validation, with central crops to 224 x 224 pixels and normalized by mean subtraction and division by standard deviation of the dataset. We used a batch size of 256 images and trained on 2 GPUs (NVIDIA Titan X / GeForce 1080Ti) for 43 epochs. We use similar learning rate scheduling to ResNet with more variable learning rate updates (primarily in order to train faster): 0.1, divided by 10 every 20 epochs. For optimization, we use Stochastic Gradient Descent with momentum .9, a cross-entropy loss between image labels and model predictions (logits). We will open-source our code and weights through GitHub.
4 RESULTS
4.1 BEST MODELS ON BRAIN-SCORE ARE NOT TOP IMAGENET MODELS
We performed a large-scale model comparison, using all most commonly used neural network families: AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2014), ResNet (He et al., 2016), Inception (Szegedy et al., 2015a;b; 2017), SqueezeNet (Iandola et al., 2016), DenseNet (Huang et al., 2017), MobileNet (Howard et al., 2017), and (P)NASNet (Zoph et al., 2017; Liu et al., 2017). To further map out the space of possible architectures and add a baseline of neural, behavioral, and performance scores, we included an in-house-developed family of models with up to moderate ImageNet performance, termed BaseNets: lightweight AlexNet-like architectures with six convolutional layers and a single fully-connected layer, captured at various stages of training. Various hyperparameters were varied between BaseNets, such as the number of filter maps, nonlinearities, pooling, learning rate scheduling, and so on. Figure 1 shows how models perform on Brain-Score and ImageNet. The strongest model under our current set of benchmarks is DenseNet-169 with a Brain-Score of .549, closely followed by CORnet-S with a Brain-Score of .544 and ResNet-101 with a Brain-Score of .542. The current topperforming models on ImageNet from the machine learning community all stem from the DenseNet
5

Under review as a conference paper at ICLR 2019
Figure 3: Feedforward Simplicity versus Brain-Score (left) and ImageNet performance (right). Most simple models perform poorly on Brain-Score and ImageNet, while best models for explaining brain data are complicated. CORnet-S offers the best of both worlds with the best Brain-Score and ImageNet performance and the highest degree of simplicity we could achieve to date. Note that dots were slightly jittered along the x-axis to improve visibility. Most of these jittered datapoints come from either MobileNet v1 (Feedforward Simplicity around .27) or v2 (Feedforward Simplicity around .2), so all models have the same simplicity (due to the same architecture) but varying BrainScore (due to varying numbers of neurons).
and ResNet families of models. DenseNet-169 and ResNet-101 are also among the highest-scoring models on the IT neural predictivity and the behavioral predictivity respectively with scores of .606 on IT (DenseNet-169, layer conv5 block31 concat) and .389 on behavior (ResNet-101, layer avg pool). VGG families win V4 with a score of .672 (VGG-19, layer block3 pool). Several observations for other model families are also worth noting: while ANNs from the Inception architectural family improved on ImageNet performance over subsequent versions, their Brain-Score decreased. Another natural cluster emerged with AlexNet and SqueezeNet at the bottom of the ranking: despite reasonable scores on V4 and IT neural predictivity, their behavioral scores are sub-par. Interestingly, models that score high on brain data are also not the ones ranking the highest on ImageNet performance, suggesting a potential disconnect between ImageNet performance and fidelity to brain mechanisms. For instance, despite its superior performance of 82.90% top-1 accuracy on ImageNet, PNASNet only ranks 13th on the overall Brain-Score. Models with an ImageNet top-1 performance below 70% show a strong correlation with Brain-Score of .92 (p < 10-14) but above 70% ImageNet performance, there was no significant correlation (p .05, cf. Figure 1).
4.2 CORNET-S IS ONE OF THE BEST YET SUBSTANTIALLY SIMPLER BRAIN-PREDICTING
MODELS
CORnet-S is strong at neural as well as behavioral prediction (Fig. 2), making it one of the best models tested on Brain-Score so far. Critically, CORnet-S is substantially simpler than other top-performing models on Brain-Score (Fig. 3, left) and commits to a particular mapping between model and brain areas. However, note that CORnet-S was developed using Brain-Score as a guiding benchmark and although it was never directly used in model search or optimization, testing CORnet-S on Brain-Score is not a completely independent test. Ideally, all models should be further validated on new experimental data sets ­ work that is in progress now.
4.3 CORNET-S IS BEST ON IMAGENET AMONG COMPACT MODELS
Due to anatomical constraints imposed by the brain, CORnet-S's architecture is much more compact than the majority of deep models in computer vision (Fig. 3, right). Compared to similar models with a path length of less than 50, CORnet-S is better in terms of Feedforward Simplicity and outperforms other models on ImageNet top-1 classification accuracy (Table 1). AlexNet and IamNN are simpler models (simplicity .48 (path length 8) and .38 (14) respectively) but suffer on classifica-
6

Under review as a conference paper at ICLR 2019

tion accuracy (57.7 and 69.6 top-1 respectively) ­ CORnet-S provides a trade-off between the two with a Feedforward Simplicity of .37 (path length 15) and top-1 accuracy of 73.1. Several epochs later top-1 accuracy actually climbed to 74.4 but since we are optimizing for the brain, we chose the epoch with maximum Brain-Score. For reference, the state-of-the-art large model (Mahajan et al., 2018) achieves top-1 accuracy of 85.4, but at the cost of a Feedforward Simplicity of only .22 (path length 101). Also note that CORnet-S never used ImageNet as a guiding benchmark ­ the strong performance on ImageNet is a result that independently arose from optimizing for Brain-Score.

Table 1: Comparison of compact models on Feedforward Simplicity and classification accuracy. CORnet-S outperforms comparable models on simplicity and ImageNet top-1 performance.

Network
AlexNet (Krizhevsky et al., 2012) VGG-16 (Simonyan & Zisserman, 2014) VGG-19 (Simonyan & Zisserman, 2014) ResNet-18 (He et al., 2016) SqueezeNet (Iandola et al., 2016) IamNN (Leroux et al., 2018) MobileNet-224 (Howard et al., 2017) CORnet-S

Simplicity (path length) .48 (8) .36 (16) .34 (19) .35 (18) .35 (18) .38 (14) .27 (41) .37 (15)

ImageNet top-1 / 5
57.7 / 79.1 71.5 / 90.4 72.4 / 90.9 69.8 / 89.1 57.5 / 80.3 69.6 / 89.0 70.6 / 89.5 73.1 / 91.1

5 DISCUSSION
We here presented an initial framework for quantitatively comparing any artificial neural network to the brain's neural network for visual processing. With even the relatively small number of brain benchmarks that we have included so far, the framework already reveals interesting patterns: It extends prior work showing that performance correlates with brain similarity, and our analysis of stateof-the-art networks yielded DenseNet-169, CORnet-S and ResNet-101 as the current best models of the primate visual stream. On the other hand, we also find a potential disconnect between ImageNet performance and Brain-Score, with the winning DenseNet-169 not being the best ImageNet model, and even small networks ("BaseNets") with poor ImageNet performance achieving reasonable scores. Importantly, these insights together with anatomical constraints led us into developing a relatively shallow recurrent model CORnet-S that is among the top models on Brain-Score yet is competitive on ImageNet, combining the best of both neuroscience desiderata and machine learning engineering requirements, demonstrating that models that could satisfy both communities could be developed.
While we believe that CORnet-S is a closer approximation to the anatomy of the ventral visual stream than current state-of-the-art deep ANNs because we specifically limit the number of areas and we include recurrence, it is still far from complete in many ways. From a neuroscientist's point of view, on top of the lack of biologically-plausible learning mechanisms (self-supervised or unsupervised), a better model of ventral visual pathway would include more anatomical and circuitry-level details. For instance, retina and lateral geniculate nucleus are not modeled in CORnet-S as current supervised training methods yield V1-like detectors in the first convolutional layer of the model rather than center-surround receptive field properties observed in primate visual system. Similarly, adding a skip connection was not informed by cortical circuit properties but rather proposed by He et al. (2016) as a means to alleviate the degradation problem in very deep architectures (where stacking more layers results in decreased performance). But we note that not just any architectural choices work. We have tested hundreds of architectures before finding CORnet-S circuitry and thus it is possible that the proposed circuits could have a strong relation to biological implementations.
From a machine learning perspective, aligning models to brain data could potentially yield more robust models that, just like human visual system, work well across different datasets and tasks without finetuning. For instance, Kornblith et al. (2018) reported that ImageNet pretrained models do not generalize as well as expected to other datasets. It is possible that under Brain-Score's guidance, models would be more likely to generalize. Indeed, our preliminary testing of CORnet-S on fixed CIFAR-10 and CIFAR-100 features demonstrates that it is close to ResNets in its ability to generalize without finetuning. More in-depth comparison between Brain-Score and such general-
7

Under review as a conference paper at ICLR 2019
ization abilities could be an important direction. However, we note that building replicas of the brain alone is unlikely to be the most efficient path towards building intelligent systems. Nonetheless, we believe that sharing insights, models, and benchmarks between the two communities is likely to lead to synergies on both ends.
Finally, we do not believe that our initial set of chosen metrics for Brain-Score is perfect, and we expect the metrics to evolve in several ways: by obtaining more neural and behavioral data, especially for different image sets and tasks or using other neural recording techniques; by estimating noise ceilings, informing how close models are to the noise limit in the data; and by including improved metrics for comparing brain data to models.
6 CONCLUSION
We here proposed a new approach of using the brain to evaluate models by quantifying goals into a composite neural and behavioral benchmark, which we hope serves as a bridge between the machine learning and neuroscience communities. We further presented our current best model that resulted from this approach: CORnet-S is a model optimized to predict brain recordings, stays close to brain anatomy, and thus is much more compact than the standard models in Computer Vision. CORnet-S is a top model on Brain-Score and beats comparably compact models on ImageNet performance.
Collecting more brain recordings across a variety of datasets (e.g. cartoons (Kubilius et al., 2018) or short videos (Monfort et al., 2018)) will constrain models even more. Further including more brain regions such as early regions V1 and V2 might point out the need for different forms of connections such as feedback. With a growing set of brain constraints, model search could also be automated instead of manually searching for the best architecture. For instance, the brain could act as a teacher that guides network evolution (Bashivan et al., 2018).
To further connect neuroscientists and machine learning researchers, our goal is to make neural network models more accessible to the neuroscience community. While CORnet-S takes first steps towards filling in the biological gaps, many details are omitted (Section 5) and we hope future work can address those shortcomings. Yet we do not expect all details to be implemented in a single model. Different abstractions of computation will necessitate different details of implementations. For instance, while neurotransmitters might be important for a neuron model, they might not make their way into a population neural network model. By further refining and quantifying our goals in neuroscience and connecting to the advances in machine learning, we will get closer to building an in silico model of the best neural network to date: our brain.
REFERENCES
Moshe Bar, Karim S Kassam, Avniel Singh Ghuman, Jasmine Boshyan, Annette M Schmid, Anders M Dale, Matti S Ha¨ma¨la¨inen, Ksenija Marinkovic, Daniel L Schacter, Bruce R Rosen, et al. Top-down facilitation of visual recognition. Proceedings of the national academy of sciences, 103 (2):449­454, 2006.
Pouya Bashivan, Mark Tensen, and James J DiCarlo. Teacher guided architecture search. arXiv preprint arXiv:1808.01405, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 248­255. IEEE, jun 2009. ISBN 978-1-4244-3992-8. doi: 10.1109/CVPR.2009.5206848.
James J DiCarlo, Davide Zoccolan, and Nicole C Rust. How does the brain solve visual object recognition? Neuron, 73(3):415­434, 2012.
K Fukushima. Neocognitron: a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193­202, 1980.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
8

Under review as a conference paper at ICLR 2019
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv preprint arXiv:1704.04861, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.
David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. The Journal of physiology, 160(1):106­154, 1962.
Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and. arXiv preprint arXiv:1602.07360, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Stanisaw Jastrzbski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. arXiv preprint arXiv:1710.04773, 2017.
Kohitij Kar, Jonas Kubilius, Kailyn M Schmidt, Elias B Issa, and James J DiCarlo. Evidence that recurrent circuits are critical to the ventral stream's execution of core object recognition behavior. bioRxiv, pp. 354753, 2018.
Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do Better ImageNet Models Transfer Better? arXiv preprint arXiv:1805.08974, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Jonas Kubilius. Predict, then simplify. NeuroImage, 180:110 ­ 111, 2018.
Jonas Kubilius, Kohitij Kar, Kailyn Schmidt, and James J DiCarlo. Can deep neural networks rival human ability to generalize in core object recognition? In Cognitive Computational Neuroscience, 2018. URL https://ccneuro.org/2018/Papers/ViewPapers.asp?PaperNum= 1234.
Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by feedforward and recurrent processing. Trends in neurosciences, 23(11):571­579, 2000.
Sam Leroux, Pavlo Molchanov, Pieter Simoens, Bart Dhoedt, Thomas Breuel, and Jan Kautz. Iamnn: Iterative and adaptive mobile neural network for efficient image classification. arXiv preprint arXiv:1804.10123, 2018.
Qianli Liao and Tomaso Poggio. Bridging the gaps between residual learning, recurrent neural networks and visual cortex. arXiv preprint arXiv:1604.03640, 2016.
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive Neural Architecture Search. arXiv preprint, 2017. URL https://arxiv.org/pdf/1712.00559.pdfhttp://arxiv. org/abs/1712.00559.
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. arXiv preprint arXiv:1805.00932, 2018.
Najib J Majaj, Ha Hong, Ethan A Solomon, and James J DiCarlo. Simple learned weighted sums of inferior temporal neuronal firing rates accurately predict human core object recognition performance. Journal of Neuroscience, 35(39):13402­13418, 2015.
Warren S. McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, 5(4):115­133, dec 1943. ISSN 00074985. doi: 10. 1007/BF02478259.
9

Under review as a conference paper at ICLR 2019
Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Tom Yan, Alex Andonian, Kandan Ramakrishnan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. arXiv preprint arXiv:1801.03150, 2018.
Karim Rajaei, Yalda Mohsenzadeh, Reza Ebrahimpour, and Seyed-Mahdi Khaligh-Razavi. Beyond core object recognition: Recurrent processes account for object recognition under occlusion. bioRxiv, pp. 302034, 2018.
Rishi Rajalingham, Kailyn Schmidt, and James J DiCarlo. Comparison of object recognition behavior in human and monkey. Journal of Neuroscience, 35(35):12127­12136, 2015.
Rishi Rajalingham, Elias B Issa, Pouya Bashivan, Kohitij Kar, Kailyn Schmidt, and James J DiCarlo. Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. Journal of Neuroscience, pp. 0388­ 18, 2018.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211­252, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), volume 07-12-June, sep 2015a. ISBN 9781467369640. doi: 10.1109/CVPR.2015.7298594. URL http://arxiv.org/abs/1409.4842.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. arXiv preprint, 2015b. ISSN 08866236. doi: 10.1109/CVPR.2016.308. URL https://arxiv.org/pdf/1512. 00567.pdfhttp://arxiv.org/abs/1512.00567.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In AAAI, volume 4, pp. 12, 2017.
Hanlin Tang, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, pp. 201719397, 2018.
Johan Wagemans, James H Elder, Michael Kubovy, Stephen E Palmer, Mary A Peterson, Manish Singh, and Ru¨diger von der Heydt. A century of gestalt psychology in visual perception: I. perceptual grouping and figure­ground organization. Psychological bulletin, 138(6):1172, 2012.
Saining Xie, Ross Girshick, Piotr Dolla´r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pp. 5987­5995. IEEE, 2017.
Daniel L Yamins, Ha Hong, Charles Cadieu, and James J DiCarlo. Hierarchical modular optimization of convolutional networks achieves representations similar to macaque it and human ventral stream. In Advances in neural information processing systems, pp. 3093­3101, 2013.
Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understand sensory cortex. Nature neuroscience, 19(3):356, 2016.
Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of Sciences, 111(23):8619­8624, 2014.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning Transferable Architectures for Scalable Image Recognition. arXiv preprint, jul 2017. ISSN 10659471. doi: 10.1109/CVPR.2018.00907.
10

Under review as a conference paper at ICLR 2019

A NUMERICAL BRAIN-SCORES

Table 2: Brain-Scores and individual performances for state-of-the-art models

Brain-Score .549 .544 .542 .541 .541 .541 .540 .533 .532 .532 .531 .530 .528 .528 .527 .527 .525 .524 .523 .522 .521 .500 .488 .469 .454

model densenet-169 cornet s resnet-101 v2 densenet-201 densenet-121 resnet-152 v2 resnet-50 v2 xception inception v2 inception v1 resnet-18 nasnet mobile pnasnet large inception resnet v2 nasnet large best mobilenet vgg-19 inception v4 inception v3 resnet-34 vgg-16 best basenet alexnet squeezenet1 1 squeezenet1 0

neural predictivity V4 IT .663 .606 .650 .600 .653 .585 .655 .601 .657 .597 .658 .589 .653 .589 .671 .565 .646 .593 .649 .583 .645 .583 .650 .598 .644 .590 .639 .593 .650 .591 .613 .590 .672 .566 .628 .575 .646 .587 .629 .559 .669 .572 .652 .592 .631 .589 .652 .553 .641 .542

behavioral predictivity .378 .382 .389 .368 .369 .377 .377 .361 .357 .362 .364 .342 .351 .352 .339 .377 .338 .371 .335 .378 .321 .256 .245 .201 .180

top-1 accuracy ImageNet 75.90 73.10 77.00 77.00 74.50 77.80 75.60 79.00 73.90 69.80 69.76 74.00 82.90 80.40 82.70 69.80 71.10 80.20 78.00 73.30 71.50 47.64 57.70 57.50 57.50

11

Under review as a conference paper at ICLR 2019

B BRAIN-SCORE BENCHMARK DETAILS

In the following section we outline the benchmarks that models are measured against. A benchmark consists of a metric applied to a specific set of experimental data, which here can be either neural recordings or behavioral measurements.

B.1 NEURAL
The purpose of neural metrics is to establish how well internal representations of a source system (e.g., a neural network model) match the internal representations in a target system (e.g., a primate). Unlike typical machine learning benchmarks, these metrics provide a principled way to prefer some models over others even if their outputs are identical. We outline here one common metric, Neural Predictivity, which is a form of a linear regression.

Neural Predictivity: Image-Level Neural Consistency Neural Predictivity is used to evaluate how well responses X to given images in a source system (e.g., a deep ANN) predict the responses in a target system (e.g., a single neuron's response in visual area IT). As inputs, this metric requires two assemblies of the form stimuli×neuroid where neuroids can either be neural recordings or model activations. First, source neuroids are mapped to each target neuroid using a linear transformation:
y = Xw + ,
where w denotes linear regression weights and is the noise in the neural recordings. This mapping procedure is performed on multiple train-test splits across stimuli. In each run, the weights are fit to map from source neuroids to a target neuroid using training images, and then using these weights predicted responses y are obtained for the held-out images. We used the neuroids from V4 and IT separately to compute these fits.
To obtain a neural predictivity score for each neuroid, we compare predicted responses y with the measured neuroid responses y by computing the Pearson correlation coefficient r:

r=

in=1(yi - y)(yi - y ) in=1(yi - y)2(yi - y )2

(1)

A median over all individual neuroid neural predictivity values (e.g., all measured target sites in a target brain region) is computed to obtain a predictivity score for that train-test split (median is used since responses are typically distributed non-normally). The final neural predictivity score for the target brain region is computed as the mean across all train-test splits.

We further estimate the internal consistency between neural responses by splitting neural responses in half across repeated presentations of the same image and computing Spearman-Brown-corrected Pearson correlation coefficient (Eq. 1) between the two splits across images for each neuroid.

In practice, we found that standard linear regression is comparably slow given a large dimensionality of the source system and not sufficiently robust. Thus, following Yamins et al. (2014), we use a partial least squares (PLS) regression with 25 components. We further optimized this procedure by first projecting source features into a lower-dimensional space using principal components analysis. The projection matrix is obtained for the features of a selection of ImageNet images, so that the projection is constant across train-test splits. This projection matrix is then used to transform source features. Results reported here were obtained by retaining 1000 principal components from the feature responses per layer to 1000 ImageNet validation images that captured the most variance of a source model.

Neural Recordings The neural dataset currently used in both neural benchmarks included in this version of Brain-Score is comprised of neural responses to 2,560 naturalistic stimuli in 88 V4 neurons and 168 IT neurons (cf. Figure 1), collected by Majaj et al. (2015). The image set consists of 2,560 grayscale images in eight object categories (animals, boats, cars, chairs, faces, fruits, planes, tables). Each category contains eight unique objects (for instance, the "face" category has eight unique faces). The image set was generated by pasting a 3D object model on a naturalist background. In each image, the position, pose, and size of an object was randomly selected in order to

12

Under review as a conference paper at ICLR 2019

create a challenging object recognition task both for primates and machines. A circular mask was applied to each image (see Majaj et al. (2015) for details on image generation).
Two macaque monkeys were implanted three arrays each, with one array placed in area V4 and the other two placed on the posterior-anterior axis of IT cortex. The monkeys passively observed a series of images (100 ms image duration with 100 ms of gap between each image) that each subtended approximately 8 deg visual angle. To obtain a stable estimate of the neural responses to each image, each each image was re-tested about 50 times (re-tests were randomly interleaved with other images). In the benchmarks used here, we used an average neural firing rate (normalized to a blank gray image response) in the window between 70 ms and 170 ms after image onset where the majority of object category-relevant information is contained (Majaj et al., 2015).

B.2 BEHAVIORAL
The purpose of behavioral benchmarks it to compute the similarity between source (e.g., an ANN model) and target (e.g., human or monkey) behavioral responses in any given task. For core object recognition tasks, primates (both human and monkey) exhibit behavioral patterns that differ from ground truth labels. Thus, our primary benchmark here is a behavioral response pattern metric, not an overall accuracy metric, and higher scores are obtained by ANNs that produce and predict the primate patterns of successes and failures. One consequence of this is that ANNs that achieve 100% accuracy will not achieve a perfect behavioral similarity score.
Even within the visual behavioral domain of core object recognition, there are many possible behavioral metrics. We here use the metric of the image-by-image patterns of difficulty, broken down by the object choice alternatives (termed I2n), because recent work (Rajalingham et al., 2018) suggests that it has the most power to distinguish among alternative ANNs (assuming that sufficient amounts of behavioral data are available).

I2n: Normalized Image-Level Behavioral Consistency Source data (model features) for a total

of i images are transformed first into a ib × c matrix of c object categories and ib images with

behavioral data available using the following procedure. First, images where behavioral responses

are not available (namely, i - ib images) are used to build a c-way logistic regression from source

data to a c-value probability vector for each image, where each probability is the probability that a

given object is in the image. This regression is then used to estimate probabilities for the held-out

ib images. For each image, all normalized target-distractor pair probabilities are computed from the c-way probability vector. For instance, if an image contains a dog and the distractor is a bear, the

target-distractor

score

is

p(dog) p(dog)+p(bear)

.

In order to compare source and target data, we first transform these raw accuracies in the ib × c response matrix to a d measure for each cell in the ib × c matrix:

d = Z(Hit Rate) - Z(False Alarms Rate),
where Z is the estimated z-score of responses, Hit Rate is the accuracy of a given target-distractor pair while the False Alarms Rate corresponds to how often the observers incorrectly reported seeing that target object in images where another object was presented. For instance, if a given image contains a dog and distractor is a bear, the Hit Rate for the dog-bear pair for that image comes straight from the ib × c matrix, while in order to obtain the False Alarms Rate, all cells from that matrix that did not have dogs in the image but had a dog as a distractor are averaged, and 1 minus that value is used as a False Alarm Rate. All d above 5 were clipped. This transformation helps to remove bias in responses and also to diminish ceiling effects (since many primate accuracies were close to 1), but empirically observed benefits of d in this dataset are small; see Rajalingham et al. (2018) for a thorough explanation.
The resulting response matrix is further refined by subtracting the mean d across trials of the same target-distractor pair (e.g., for dog-bear trials, their mean is subtracted from each trial). Such normalization exposes variance unique to each image and removes global trends that may be easier for models to capture. For instance, dog-bear trials on average could have been harder than dog-zebra trials. Without this normalization, a model might score very well by only capturing this tendency.

13

Under review as a conference paper at ICLR 2019
After normalization, all responses are centered around zero, and thus capturing only global trends but not each image's idiosyncrasies would be insufficient for a model to rank well. After normalization, a Pearson correlation coefficient rst between source and target data is computed using Eq. 1. We further estimate noise ceiling, that is, how well an ideal model could perform given the noise in the measured behavioral responses, by dividing target data in half across trials, computing the normalized d ib × c matrices for each half, and computing the Pearson correlation coefficient rtt between the two halves. If source data is produced by a stochastic process, the same procedure can be carried out on the source data, resulting in the source's reliability rss. The final behavioral predictivity score of each ANN is then computed by:
r =  rst rssrtt
All models that we tested so far produced deterministic responses, thus rss = 1 in our scoring. Primate behavioral data The behavioral data used in the current round of benchmarks was obtained by Rajalingham et al. (2015) and Rajalingham et al. (2018). Here we focus on only the human behavioral data, but the human and non-human primate behavioral patterns are very similar to each other (Rajalingham et al., 2015; 2018). The image set used in this data collection was generated in a similar way as the images for V4 and IT using 24 object categories. In total, the dataset contains 2,400 images (100 per object). For this benchmark, we used 240 (10 per object) of these images for which the most trials were obtained. 1,472 human observers responded to briefly presented images on Amazon Mechanical Turk. At each trial, an image was presented for 100 ms, followed by two response choices, one corresponding to the target object present in the image and the other being one of the remaining 23 objects (i.e., a distractor object). Participants responded by choosing which object was presented in the image. Thus, over three hundred thousand responses for each target-distractor pair were obtained from multiple participants, resulting in a 240 (images)×24 (objects) response matrix when averaged across participants.
B.3 BRAIN-SCORE To evaluate how well a model is doing overall, we computed the global Brain-Score as a composite of neural V4 predictivity score, neural IT predictivity score, and behavioral I2n predictivity score (each of these scores was computed as described above). The Brain-Score presented here is the mean of the three scores. This approach does not normalize by different scales of the scores so it may be penalizing scores with low variance but it also does not make any assumptions about significant differences in the scores, which would be present in ranking.
14

