Under review as a conference paper at ICLR 2019
UNDERSTAND THE DYNAMICS OF GANS VIA PRIMAL-DUAL OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions. In spite of its great success in applications, GAN is known to be notoriously hard to train. The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area. To resolve these issues, we need to first understand how GANs work. Herein, we take a step toward this direction by examining the dynamics of GANs. We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator. By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. The same framework also applies to multi-task learning and distributional robust learning problems. We verify our analysis on numerical examples with both synthetic and real data sets. We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.
1 INTRODUCTION
Since it was first invented by Ian Goodfellow in his seminal work Goodfellow et al. (2014), generative adversarial networks (GANs) have been considered as one of the greatest discoveries in machine learning community. It is an extremely powerful tool to estimate data distributions and generate realistic samples. To train its implicit generative model, GAN uses a discriminator since traditional Bayesian methods that require analytic density functions are no longer applicable. This novel approach inspired by zero sum game theory leads to a significant performance boost; GANs are able to generate samples in a fidelity level that is way beyond traditional Bayesian methods. During the last few years, there have been numerous research articles in this area aiming at improving its performance (Radford et al., 2015; Zhao et al., 2016; Nowozin et al., 2016; Arjovsky et al., 2017; Mao et al., 2017). GANs have now became one of most recognized unsupervised learning techniques and have been widely used in a variety of domains such as image generation (Nguyen et al., 2017), image super resolution (Ledig et al., 2017), imitation learning (Ho & Ermon, 2016).
Despite the great progress of GANs, many essential problems remain unsolved. Why is GAN so hard to train? How to tune the hyper-parameters to reduce instability in GAN training? How to eliminate mode collapse and fake images that show up frequently in training (Arjovsky & Bottou, 2017)? Comparing with many other machine learning techniques, the properties of GANs are far from being well understood. It is quite likely that the theoretical foundation of GANs will become a longstanding problem. The theoretical difficulty of GANs mainly lies in the following several aspects. First, it is a non-convex optimization problem with complicated landscape. It is unclear how to solve such optimization problems efficiently. The first-order method widely used in the literature via updating the generator and discriminator along descent/ascent direction does not seem to converge all the time. In fact, there is no evident that this algorithm guarantees even local optimality. Second, even if there were an efficient algorithm to solve this optimization problem, we do not know how well they generalize. After all, the optimization formulation is based only on the samples generated by the underlying distribution but our goal is to recover this underlying distribution. Of course, this is a problem faced by all machine learning techniques. Last, there is no reliable ways to evaluate the
1

Under review as a conference paper at ICLR 2019
quality of trained models. There are a number of works in this topic (Salimans et al., 2016; Heusel et al., 2017), but human eyes inspection remains the primary approach to judge a GAN model.
In the present work, we focus on the first problem and analyze the dynamics of GANs from an optimization point of view. More precisely, we study the convergence properties of the first-order method in GAN training. Our contributions can be summarized as follows. 1) We formulate a large class of GAN problems as a primal-dual optimization problem with a coupling term that is linear over discriminator (see Section 2 for the exact formulation); 2) We prove that the simple primal-dual first-order algorithm converges to a stationary solution with a sublinear convergent rate; 3) We use an adaptive strategy to select the stepsize of the updates in order to guarantee convergence. Our proof might provide guidance for selecting proper stepsize for related problems.
There have been a number of papers that study the dynamics of GANs from an optimization viewpoint. These works can be roughly divided into three categories. In the first category, the authors focus on high level idea using nonparametric models. This includes the original GAN paper Goodfellow et al. (2014), the Wasserstein GAN papers Arjovsky & Bottou (2017); Arjovsky et al. (2017) and many other works proposing new GAN structures. In the second category, the authors consider the unrolled dynamics (Metz et al., 2016), that is, the discriminator remains optimal or almost optimal during the optimization processes. This is considerably different to the first-order iterative algorithm widely used in GAN training. Recent works Heusel et al. (2017); Li et al. (2017); Sanjabi et al. (2018) provide global convergence analysis for this algorithm.
The last category is on the first-order primal-dual algorithm, in which both the discriminator and the generator update via (stochastic) gradient descent. However, most of the convergence analysis are local (Daskalakis et al., 2017; Mescheder et al., 2017; Nagarajan & Kolter, 2017; Li et al., 2018). Other related work including the following: In Qian et al. (2018) the authors consider a gradient descent/ascent algorithm for a special min-max problem arising from robust learning (min problem is unconstrained, max problem has simplex constraints); In Hajinezhad & Hong (2017) a primal-dual algorithm has been studied for a non-convex linearly constrained problem (which can be reformulated into a min-max problem, with the max problem being linear and unconstrained, and with linear coupling between variables); In Hamedani & Aybat (2018), (Chen et al., 2013) and the references therein, first-order methods have been developed for convex-concave saddle point problems. Compared to these works, our considered problem is more general, allowing non-convexity and non-smoothness in the objective, non-convex coupling between variables, and can further include constraints. Moreover, we provide global convergence rate analysis, which is much stronger than the local analysis mentioned above.
It turns out that the primal-dual framework we study in this paper can also be applied to the distributional robust machine learning problems (Namkoong & Duchi, 2016) and the multi-task learning problems (Qian et al., 2018). In multi-task learning, the goal is to train a single neural network that would work for several different machine learning tasks. Similarly, in distributional robust learning, the purpose is to have a single model that would work for a set of data distributions. In both problems, an adversarial layer is utilized to improve the worst case performance, which leads to a primal-dual optimization structure that falls into the scope of problems we consider.
The rest of the paper is structured as follows. In Section 2 we introduce GAN and its primal-dual formulation. We also mention several other relevant machine learning problems that can be addressed within the same framework. We provide details of the algorithms with proof sketches in Section 3. The full proofs are relegated to the appendix. We highlight our theoretical results in Section 4 via several numerical examples, with both synthetic and real datasets.
2 GENERATIVE ADVERSARIAL NETWORKS AND MIN-MAX PROBLEMS
GAN is a type of deep generative model with implicit density functions. It consists of two critical components: a generator and a discriminator. The generator takes random variables with known distribution as input and outputs fake samples. The discriminator is trained to distinguish real samples and fake samples.
2

Under review as a conference paper at ICLR 2019

2.1 GANS

In the original form of GAN, the generator is a neural network and the discriminator is a standard classifier with cross entropy cost. Denote the underlying probability distributions of the data sample {yi}ni=y1  Rd and the random seed x (a random variable in Rk) of the generative model by Py and Px respectively. The original or vanilla GAN is of the form (Goodfellow et al., 2014)

min max{E(log(F (y))) - E(log(1 - F (G(x))))}.
GG F F

(1)

Here G stands for the set of possible generators and F the set of possible discriminators.

There have been numerous modifications of the original GAN, among which the Wasserstein GAN (Arjovsky et al., 2017) attracts a lot of attention. It has the form

min max{E(F (y)) - E(F (G(x)))},
GG F F

(2)

where G is the set of parameterized generators and F is the set of Lipschitz functions with Lipschitz constant 1. This is a special case of a more general class of GANs with the same structure but different constraint set F. The inner maximization loop defines different notions of integral probability metrics (Mu¨ller, 1997) depending on the choices of F. Other than Wasserstein GAN, one interesting example with this structure is the generative moment matching networks (Li et al., 2015; Arjovsky et al., 2017). Wasserstein GAN can be extended to general optimal transport cost c(x, y), which results in saddle point formulation

min max{E((y)) + E((G(x)) | (y) + (x)  c(x, y), x, y}.
GG ,

(3)

Note that the discriminator now becomes two functions ,  instead of one. When c(x, y) = x - y , the above reduces to the standard Wasserstein GAN in equation 2.

2.2 MIN-MAX PROBLEMS

We observe that, compared to vanilla GAN in equation 1, the Wasserstein GAN in equation 2 and equation 3 have a special structure. In the nonparametric form, the coupling between the generator G and the discriminator F is linear on F in Wasserstein GAN while nonlinear in vanilla GAN. Indeed, replacing F by F1 + F2 in the coupling term E(F (G(x))) yields

E(F (G(x))) = E((F1 + F2)(G(x))) = E(F1(G(x))) + E(F2(G(x))).

This structure motivates us to study the following min-max primal-dual optimization problem

min max h(X) + g(X), Y - l(Y )
XX Y Y

(4)

with l being strictly convex. the other two function h and g can be non-convex. Here Y represents the discriminator F and X represents the generator G. Note that the coupling term g(X), Y is linear over the discriminator Y , but nonlinear in X.In real applications, we need to parameterize the discriminator and generator, which may lead to the loss of the property that the coupling is linear over discriminator. Next we present several cases where this linear structure stays.

2.2.1 WASSERSTEIN GAN IN LQG SETTING

Wasserstein GAN in linear quadratic Gaussian (LQG) setting was proposed in Feizi et al. (2017) to understand GAN. In this simplified case, the data distribution is Gaussian, the cost is quadratic and the generator is linear, namely,

1 c(x, y) =
2

x-y

2, y  N (0, y), x  N (0, Ik), G(x) = x.

It can be shown that it suffice to consider discriminator of the form

1 (x) =

x

2

-

1 xT Ax,

1 (y) =

y

2

-

1 yT By

22

22

with A, B being positive definite.

3

Under review as a conference paper at ICLR 2019

The

constraint

(x)

+ (y)



1 2

x-y

2 implies that B

A-1. Consequently, the discriminator

can be parametrized by a single variable A 0. Additionally, G(x) is a zero mean Gaussian random

variable with covariance T . Therefore, the Wasserstein GAN in equation 3 then reduces to

min max Tr(y) + Tr(T ) - Tr(AT ) - Tr(A-1y),
Rd×k A 0

(5)

which is apparently in the form of equation 4. When only samples {xi}ni=1, {yi}in=1 of x, y are available, this becomes

1n 1

min max (

Rd×k A

0n
i=1

2

yi

2

-

1 2

yiT

A-1

yi

)

+

1 n

n1 ( 2

i=1

xi

2

-

1 2

xiT

T

Axi

).

(6)

2.2.2 GANS WITH DISCRIMINATORS LINEAR ON FEATURES

In general quadratic discriminators are not sufficient to distinguish complicate high dimension

distributions. In order to deal with more general data sets, we consider the setting where the

discriminator is a linear combination of a predefined basis functions. More specifically, let {Fi}in=1 be the basis functions, and F = iFi with some constraint on , then the above formulation

becomes

nn

min max
GG Rn

iE(Fi(y)) - iE(Fi(G(x)) -   2 .

i=1 i=1

(7)

Here the term   2 with  > 0 is used to regularize , or equivalently, the discriminator.

Similarly, for GAN structure in equation 3, we can restrict our discriminators  and  to be linear combinations of basis functions {i}ni=1 and {i}ni=1, that is,  = ii,  = ii. The constraint (x) + (y)  c(x, y) in equation 3 is difficult to impose precisely. Instead, we use a
regularization term l and obtain

nn

min max

iE{i(G(x))} + iE{i(y)} - l(, ) .

GG ,

ii

(8)

2.3 ROBUST MULTI-TASK MACHINE LEARNING

Multi-task machine learning (Qian et al., 2018) aims at learning a single model that would work for

several different machine learning tasks. Let

min E{f1(x1, W )}, · · · , min E{fn(xn, W )}
WW

be n supervised learning problems, then a multi-task formulation is

n
min piE{fi(xi, W )},
W i=1

(9)

where p is a probability vector to weight the tasks. A common choice is the uniform distribution

p¯ = [1/n, . . . , 1/n]. To improve the worst case performance, one can change p adaptively and attain

the max-min formulation

n

min max{
Wp

piE{fi(xi, W )} - D(p, p¯) + h(W )},

i=1

(10)

where D is a distance function to regularize p. Here we have also added an regularization term on W .

A closely related topic is the distributional robustness (Namkoong & Duchi, 2016) problem

n

min max
W pP

pif (xi, W ),

i=1

(11)

where P is a subset of the space of probability vectors. Relaxing the hard constraint on p points to a

regularized version

n

min max
Wp

pif (xi, W ) - D(p, p¯).

i=1

(12)

While p represents weights on different tasks in multi-task learning, it describes data distribution in

distributional robustness. It beautifully incorporates data uncertainties in the learning problems.

4

Under review as a conference paper at ICLR 2019

3 ALGORITHM DESIGN AND CONVERGENCE ANALYSIS

In this work, we consider the following general min-max problem,

min max f (X, Y )
XX Y Y

1 n

n
(hi(X) + gi(X), Y

- li(Y ))

i=1

(13)

where X is a convex and compact set and the size of X is upper bounded by X ; hi(X) : Rd  R, i is a non-convex function and has Lipschitz continuous gradient with constant LX ; li(x) : Rd  R, i are strongly convex with modulus  > 0 and Lipschitz gradient constant LY ; the matrix function g(X) : X  Rd×d can also be non-convex, and it is assumed to be Lipschitz and Lipschitz gradient continuous with constants Lg,1 and Lg,2. We note that regardless of whether Y is a bounded set or not, one can show that for all x  X , the maximizer Y  for the maximization
problem lies in a bounded set; see Lemma 3 in the appendix for proof.

We note that by allowing constraints in the form of x  X , and y  Y, one can also include

nonsmooth regularizers in the formulation. As an example, if we add  X 1 into the objective

function, one can introduce a new variable z, and consider an equivalent problem with the constraints

X

1



z,

with

the

objective

function

changed

to

1 n

n i=1

(hi(X

)

+

gi(X), Y

- li(Y )) + z.

The above formulation is quite general. Compared with the existing convex-concave saddle point literature (Hamedani & Aybat, 2018), (Chen et al., 2013), our formulation allows non-convexity in the minimization, which is essential to modelling the neural network structure of generators in GANs and general non-convex supervised tasks in multi-task learning; Compared with the non-convex linearly constrained problems considered in (Hajinezhad & Hong, 2017), it further allows non-linear and non-convex function g(x) to couple with y. Compared with the robust learning formulation given in equation 10, equation 13 can further include constraints and nonsmooth objective functions (thus can include nonsmooth regularizers such as 1 norm).

It is important to note that due to the generality of problem equation 13, developing performance guaranteed first-order method, which only utilizes the gradient information about functions hi, gi, li is very challenging. To the best of our knowledge, there has been no such algorithm that can provably compute even first-order stationary solutions for problem equation 13.

3.1 ALGORITHM DESCRIPTION

Our proposed gradient primal-dual algorithm of solving equation 13 is listed below, in which we alternatingly perform first-order optimization to update X and Y :

Xr+1 = arg min
X X

1 n

n

(X hi(Xr) + X Tr(gi(Xr)Y r)) , X - Xr

i=1

 +

X - Xr

2,

2

(14)

Y r+1 = arg max
Y Y

-1 n

n

Y li(Y r) + gi(Xr+1), Y - Y r

- 1 Y - Y r 2. 2

i=1

To be consistent with the optimization literature, we will refer to the X-step the "primal step" and the Y -step as the "dual step". We note that 1/ and  are two positive parameters, that represent stepsizes of the two updates, and both of them should be small. A few remarks are ready.

Remark 1 (projected gradient). It can be easily verified that the updates of Xr and Y r can be written down in closed form using the following alternating projected gradient descent/ascent steps:

Xr+1 = projX Y r+1 = projY

Xr - 1 

1 n

n

(X hi(Xr) + X Tr(gi(Xr)Y r))

i=1

Yr +

1 n

n

Y li(Y r) + gi(Xr+1)

i=1

.

,

Remark 2 (stochastic vs deterministic algorithm). This work will be focused on the deterministic algorithm given in equation 14, because such an algorithm is representative of the primal-dual

5

Under review as a conference paper at ICLR 2019

first-order dynamics used in training GANs and optimizing robust ML problems, and it is already challenging to analyze. However, we do want to remark that, it is relatively straightforward to build upon our proof, by incorporating the standard technique in stochastic constrained optimization (Ghadimi et al., 2016) (such as using decreasing stepsizes, and certain randomization rule in picking the final solutions), to analyze the stochastic version of the algorithm, in which mini-batches of the component functions are randomly selected to update at each iteration. However, in order to keep the discussion of the paper simple, we choose not to present such results.

3.2 CONVERGENCE ANALYSIS

In this section, we present our main convergence results for the primal-dual first-order algorithm given in equation 14. We first present a few necessary lemmas.
Lemma 1. (Descent Lemma) Let (Xr, Y r) be a sequence generated by equation 14. The descent of the objective function can be quantified by

f (Xr+1, Y r) - f (Xr, Y r)  -  - LX 22

11

+ 2

LY + Lg,1 + 

Xr+1 - Xr 2

Y r+1 - Y r

2

+

LX

+

1 

Y r - Y r-1 2.

(15)

2

From Lemma 1, it is not clear whether the objective function is decreased or not, since the primal step will consistently decrease the objective value while the dual step will increase the objective value.

The key in our analysis is to identify a proper "potential function", which can capture the essential dynamics of the algorithm, and will be able to reduce in all iterations.
Lemma 2. When the following conditions are satisfied,

  < L2Y ,

  max

Lg,1 +

(LY

+ Lg,1 +

1 

)((6LX

+

2)LX

 - LY2

+ 2L2g,1) , LX

+ Y Lg,2

(16)

then there exist c1, c2, c3, d > 0 such that potential function will monotonically decrease, i.e.,

Pr+1 - Pr  -c1 Xr+1 - Xr 2 - c2 Y r+1 - Y r 2 - c3 Y r - Y r-1 2

(17)

where Pr+1 f (Xr+1, Y r+1) + dQr+1 and

Qr+1

LX (Y + 1) + 1 Xr+1 - Xr 2 + 1 -  - LY2

2 2

2 2

Y r+1 - Y r 2.

To state our main result, let us define the proximal gradient of the objective function as

L(X, Y )

X - projX [X - X f (X, Y )] Y - projY [Y + Y f (X, Y )]

(18)

where proj denotes the convex projection operator. Clearly, when L(X, Y ) = 0, then a first-order stationary solution of the problem equation 4 is obtained.
Theorem 1. Suppose that the sequence (Xr, Y r) is generated by equation 14 and ,  satisfy the conditions equation 16. For a given small constant , let T ( ) denote the iteration index satisfying the following inequality

T ( ) min{t | L(Xr, Y r) 2  , r  1}.

(19)

Then there exists some constant C > 0 such that



CP1 .

T( )

(20)

The above result shows that our proposed algorithm converges to the first-order stationary point of the original problem in a sublinear rate.

6

Under review as a conference paper at ICLR 2019

4 EXPERIMENTS
We conduct two experiments to illustrate our results. The first example is on GANs with synthetic data, and the second one is on multi-task learning with real data. Our main goal here is to show that our first-order primal-dual algorithm would converge at least to a local solution. All experiments are implemented on a NVIDIA TITAN Xp.
4.1 GAN IN LQG SETTING
In the LQG setting, as discussed in Section 2.2.1, the generator is modeled by a linear map G(x) = x with parameter  and the discriminator is parametrized by a positive definite matrix A. The seed x is a zero-mean random variable with unit covariance. We randomly generate a positive definite matrix y as our covariance of the data samples. The solution to this GAN problem satisfies A = I, T = y.
We implement our algorithm with different stepsizes. The results are shown in Figure 1 for 20 dimensional data, from which we see that the algorithm converge when stepsize is sufficently small, while diverge for large stepsize. We also compare our algorithm with the one proposed in Sanjabi et al. (2018), which requires solving the inner maximization problem each iteration. As can be seen from the last plot of Figure 1, while these two algorithms take similar number of iterations to converge, the total time consumption is more for Sanjabi et al. (2018) as it takes more time to solve the maximization problem than to update the parameter one step along the gradient direction.

Loss Loss Loss

40000 20000

 A

0

-20000

-40000 0

5000 10000 15000 20000 Number of iterations

(a)  = 0.0001,  = 10000

2000000

1000000 0
-1000000

 A

-2000000 0

5000 10000 15000 20000 Number of iterations

(b)  = 0.005,  = 1000

4000 2000
0 -2000

 A

0 5000 10000 15000 20000 Number of iterations
(c) Sanjabi et al. (2018)

Figure 1: (a) and (b) show the effect of stepsizes; (c) is based on the algorithm in Sanjabi et al. (2018).

To visualize the effectiveness of LQG GAN, we consider the problem in 2 and 3 dimensional spaces. We plot the samples corresponding to both the learned and the real covariance matrices in Figure 2. Clearly the learned models match the underlying truth.

6

4

2

0

-2

-4

-6

-7.5

-5.0

-2.5

0.0

2.5

5.0

7.5

10.0

(a) d = 2

-2 0 2 4 6 8

5 0 -5 -10 4 2 0 -2 -4

(b) d = 3

Figure 2: Real (green) vs generated (red) samples

7

Under review as a conference paper at ICLR 2019

Table 1: Multi-task Learning: Comparison of Accuracies

Method
MNIST CIFAR10 Even Mixture OPT Mixture

Training
99.98 10.25 99.96 99.98

MNIST Test
99.33 10.10 99.40 99.35

CIFAR10

Training

Test

9.24 99.75 99.42 99.85

9.16 76.59 74.97 76.29

4.2 MULTI-TASK LEARNING
We consider two supervised learning tasks with MNIST (LeCun et al., 1998) data set and CIFAR10 (Krizhevsky & Hinton, 2009) data set. We seek a single neural network that works for these two completely unrelated problems (see Section 2.3). First we convert the MNIST data from 28 × 28 gray images to 32 × 32 color images so that it is in the same format as CIFAR10. We use a standard AlexNet (Krizhevsky et al., 2012) as our model. We train the model with the robust multi-task learning framework and compare it to the results from three other methods: train with MNIST only, train with CIFAR10 only and train with both data sets but with even weight [0.5, 0.5]. The batch size we use is 128.
The results are presented in Table 1. The last row is the results for the robust multi-task learning, and the "even mixture" in the second last row standards for uniform weight [0.5, 0.5] between the two tasks. We see that the result of using robust multi-task learning framework is better than the one with even weight. Moreover, its performance on each task is comparable to that trained from a single data set alone. The optimal p is [0.205, 0.795], where the first value is for the MNIST data set. We also implement our algorithm with different levels  of regularizations. The results are shown in Figure 3, where we display the loss functions in the first two plots and the weight for MNIST in the last plot. Even though changing  doesn't affect the convergence rate that much, it changes the optimal p significantly.

Loss Loss
p (MNIST)

2

p W

1

0

-1

-2
0 100 200 300 400 Number of iterations (x 100)
(a)  = 0.1

p2 0.5

= 0.1

W  = 0.005

1 0.4

0
0.3
-1
0.2
-2

0 100 200 300 400 Number of iterations (x 100)

0

100

200

300

400

Number of iterations (x 100)

(b)  = 0.005

(c) The change of p across iterations

Figure 3: Effect of regularization. Here p is the weight, W is the parameter and  is the regularization constant. The regularizer D is taken to be the Kullback-Leibler divergence.

5 CONCLUSION
In this work, we presented a convergence result for a first-order algorithm on a class of non-convex max-min optimization problems that arises in many machine learning applications such as generative adversarial networks and multi-task learning. To the best of our knowledge, this is the first convergence result for this type of primal-dual algorithms. Our results allow us to analyze GANs with neural network generator as well as general multi-task non-convex supervised learning problems. A critical assumption we made is that the inner maximization loop is a strictly convex problem. For applications in GANs, our assumptions require the discriminator to be a linear combination of a predefined basis functions. Extending this to the most general cases where the discriminator is a neural network requires further investigations and will be a future research topic.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky and Le´on Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875, 2017.
Y. Chen, G. Lan, and Y. Ouyang. Optimal primal-dual methods for a class of saddle point problems. SIAM J. Optim, 24(4):1779­1814, 2013.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. arXiv preprint arXiv:1711.00141, 2017.
Soheil Feizi, Changho Suh, Fei Xia, and David Tse. Understanding GANs: the LQG setting. arXiv preprint arXiv:1710.10793, 2017.
S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):267305, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 2672­2680, 2014.
D. Hajinezhad and M. Hong. Perturbed proximal primal dual algorithm for nonconvex nonsmooth optimization. 2017. Submitted for publication.
E. Y. Hamedani and N. S. Aybat. A primal-dual algorithm for general convex-concave saddle point problems. 2018. Submitted for publication, available at: arXiv:1803.01401.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gu¨nter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 4565­4573, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 1097­1105, 2012.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Christian Ledig, Lucas Theis, Ferenc Husza´r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 4, 2017.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. Towards understanding the dynamics of generative adversarial networks. arXiv preprint arXiv:1706.09884, 2017.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. On the limitations of first order approximation in GAN dynamics. 2018.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International Conference on Machine Learning, pp. 1718­1727, 2015.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proc. of International Conference on Computer Vision (ICCV), pp. 2813­2821, 2017.
9

Under review as a conference paper at ICLR 2019
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 1825­1835, 2017.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.
Alfred Mu¨ller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429­443, 1997.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. In Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 5585­5595, 2017.
Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 2208­2216, 2016.
Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play generative networks: Conditional iterative generation of images in latent space. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 7, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 271­279, 2016.
Qi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, and Hao Li. Robust optimization over multiple domains. arXiv preprint arXiv:1805.07588, 2018.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Proc. of Advances in Neural Information Processing Systems (NIPS), pp. 2234­2242, 2016.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D. Lee. On the convergence and robustness of training GANs with regularized optimal transport. In Proc. of Advances in Neural Information Processing Systems (NIPS). 2018.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.
10

Under review as a conference paper at ICLR 2019

A CONVERGENCE ANALYSIS OF THE PRIMAL DUAL ALGORITHM

In our convergence analysis of the primal-dual algorithm, we use the optimality conditions of Xrand Y r-subproblems repeatedly so that the quantities of measuring the size of the difference of the iterates, e.g., Xr+1 - Xr and Y r+1 - Y r , can be obtained. Also, for the simplicity of the
notation, we have the following definitions:

h(X )

1n n hi(X), g(X)
i=1

1n n gi(X), l(Y )
i=1

1n n li(Y ).
i=1

Then, we give the optimality condition of Xr-subproblem and Y r-subproblem as follows,

(21)

h(Xr) + X gj,k(Xr)Yjr,k + (Xr+1 - Xr), Xr+1 - X
j,k
Y r+1 = Y r +  r+1 +  g(Xr+1) - Y l(Y r) .

 0, X  X ,

(22) (23)

where - r+1 denotes the subgradient of the convex indicator function 1(yr+1  Y), Yjr,k denotes the entry at the j row and kth column of Y r, and gj,k(Xr) denotes the matrix value function mapping from Xr to the value at the jth row and kth column of g(Xr). Note that equation 23 is also equivalent

to

-Y

l(Y

r)

+

g(X r+1 )

-

1 

(Y

r+1

-

Y

r ),

Y

r+1

-

Y

 0, Y  Y.

(24)

Before going to the details, we will first introduce the following lemma that characterizes the upper bound of Y r .
Lemma 3. Let (Xr, Y r) be a sequence generated by equation 14. The size of Y r is upper bounded by some constant number denoted by Y .

Proof. Let (X, Y ) l(Y ) - g(X), Y . Define:

Y1

arg

min
Y Y



(X1,

Y

)

Y2

arg

min
Y Y



(X2,

Y

)

(25)

From the optimality conditions, we know that

Y (X1, Y1), Y2 - Y1  0 Y (X2, Y2), Y1 - Y2  0.

(26)

Adding these two inequalities, we have

Y (X1, Y1) - Y (X2, Y2), Y2 - Y1  0.

(27)

which implies that

Lg,2 X1 - X2 Y1 - Y2  g(X2) - g(X1), Y1 - Y2 = Y (X2, Y2) - Y (X1, Y2), Y1 - Y2  Y (X1, Y1) - Y (X1, Y2), Y1 - Y2 = l(Y1) - l(Y2), Y1 - Y2
  Y1 - Y2 2

where in the first inequality we used the Lipschitz continuity; in the last inequality we used the strong convexity of function l(Y ). Therefore, we have

Y1 - Y2

 Lg,1 

X1 - X2

.

(28)

Since X  X , with X bounded by X , we have the claim that the distance between any two Y1 and Y2 are bounded by 2Lg,1X /. In other words, YXr , Xr  X are within a compact sets, where YXr arg minY Y (Xr, Y ) and the radius of the set is upper bounded by 2Lg,1X /.

11

Under review as a conference paper at ICLR 2019

Let Y r+1 denote the r + 1th iterate of the projected gradient descent method of solving the dual

problem which is parameterized by Xr+1. Because the dual problem is strongly convex, it is standard

to show that

Y

r+1

-

Y
X r+1



Y

r

-

Y
X r+1

.

(29)

This implies that the distance between the iterate and the set is not increasing. The proof is complete.

Throughout our analysis, we used Y  Y r 1, r where Y 1

i,j |Yi,j |. Note that Y 1 

Y F . Based on this definition and the previous lemma, it is easy to check that the following holds

f (X, Y ) - f (Z, Y )  (LX + Y Lg,2) X - Z

(30)

where LX and Lg,2 are two constants defined after equation 4.

A.1 PROOF OF LEMMA 1

A.1.1 PRIMAL PROBLEM

Define

f (X, Y ) h(X) - l(Y ) + g(X), Y .

(31)

First, suppose that we choose  large enough such that   LX + Y Lg,2. Then we have the following estimate of the descent of the objective value:
f (Xr+1, Y r) - f (Xr, Y r)

=h(Xr+1) + g(Xr+1), Y r - h(Xr) - g(Xr), Y r

(32)

(a)
 X h(Xr) +

X gi,j (Xr)Yjr,k, Xr+1 - Xr

+ LX + Y Lg,2 2

Xr+1 - Xr

2

i,j

(b)  -

Xr+1 - Xr

2

2

when in (a) we used the equation 30; In (b) wej used the optimality condition equation 22, and we choose   LX + Y Lg,2.

A.1.2 DUAL PROBLEM

For the dual problem, we have f (Xr+1, Y r+1) - f (Xr+1, Y r) = -l(Y r+1) + g(Xr+1), Y r+1 - Y r + l(Y r) - (1(Y r+1  Y) - 1(Y r  Y))

(a)
 - Y l(Y r) - r, Y r+1 - Y r + g(Xr+1), Y r+1 - Y r = - Y l(Y r) - r+1, Y r+1 - Y r - r+1 - r, Y r+1 - Y r + g(Xr+1), Y r+1 - Y r (=b) 1 Y r+1 - Y r, Y r+1 - Y r - r+1 - r, Y r+1 - Y r


(c)


1

Y r+1 - Y r, Y r+1 - Y r

1 +

2

LY

+ Lg,1

-

1 

Y r+1 - Y r 2

+ Lg,1 Xr+1 - Xr 2 + 1 22

1 LY + 

Y r - Y r-1 2

1 =
2

1 LY + Lg,1 + 

Y r+1 - Y r 2 + Lg,1 Xr+1 - Xr 2 2

11

+ 2

LY + 

Y r - Y r-1 2

where in (a) we used the convexity of both function l(X) and indicator function, i.e., l(Y r+1) + 1(Y r+1  Y) - (l(Y r) + 1(Y r  Y)  Y l(Y r) - r, Y r+1 - Y r , in (b) we used equation 23
and (c) is true because the following relations:

12

Under review as a conference paper at ICLR 2019

1. First, from equation 23 we have
Y r+1 =Y r + (g(Xr+1) - l(Y r) + r+1), Y r =Y r-1 + (g(Xr) - l(Y r-1) + r),

(33) (34)

which imply

r+1 - r = 1 (Y r+1 -Y r -(Y r -Y r-1))- g(Xr+1) - g(Xr) +l(Y r)-l(Y r-1).  (35)

2. Second, we have

r+1 - r, Y r+1 - Y r

1 =

V r+1, Y r+1 - Y r

-

g(Xr+1) - g(Xr), Y r+1 - Y r



- Y l(Y r) - Y l(Y r-1), Y r+1 - Y r

(a)
-

1 (

Y r - Y r-1

2-

2

Y r+1 - Y r

2)

-

1 2 (LY

+ Lg,1)

Y r+1 - Y r

2

-

1 2 Lg,1

Xr+1 - Xr

2

-

1 2 LY

Y r - Y r-1 2

- 1 2

LY

+ Lg,1 -

1 

Y r+1 - Y r 2 - Lg,1 Xr+1 - Xr 2 2

-1 2

1 LY + 

Y r - Y r-1 2

where in (a) we used Cauchy­Schwarz inequality.

Finally, combing the above results, we have

f (Xr+1, Y r) - f (Xr, Y r)

 -  Xr+1 - Xr 2 + 1 22

1 LY + Lg,1 + 

primal descent
+ Lg,1 Xr+1 - Xr 2 + 1 22

1 LY + 

Y r+1 - Y r 2 Y r - Y r-1 2.

(36)

A.2 PROOF OF LEMMA 2

Let W r+1 (Xr+1 - Xr) - (Xr - Xr-1). Subtracting equation 22 at iteration r - 1 from itself at the rth iteration, we have the successive difference of equation 23, i.e.,

X h(Xr) - X h(Xr-1) 



+  gj,k(Xr)Yjr,k - gj,k(Xr-1)Yjr,k-1 +W r+1, Xr+1 - Xr  0.
j,k j,k

(37)

Ar

Similarly, from equation 23, we also have

Y r+1 - Y r - (Y r - Y r-1) =  r+1 - r

V r+1

+  g(Xr+1) - g(Xr) -  l(Y r) - l(Y r-1) . (38)

13

Under review as a conference paper at ICLR 2019

A.2.1 INDUCTION OF THE SIZE OF THE SUCCESSIVE ITERATES

First, we note the following key inequality

W r+1, Xr+1 - Xr

1 =

Xr+1 - Xr 2 - 1

Xr - Xr-1 2 + 1

W r+1 2.

222

Second, we can get the lower bound of Ar, Xr+1 - Xr as follows:

(39)

X gj,k(Xr)Yjr,k -

gj,k(Xr-1)Yjr,k-1, Xr+1 - Xr

j,k j,k

(40)

= gj,k(Xr) Yjr,k - Yjr,k-1 , Xr+1 - Xr
j,k

+ gj,k(Xr) - gj,k(Xr-1) Yjr,k-1, Xr+1 - Xr
j,k
= Xm,n gj,k(Xr) Yjr,k - Yjr,k-1 (Xmr+,n1 - Xmr ,n)
m,n j,k
+ Xm,n gj,k(X r) - Xm,n gj,k(X r-1) Yjr,k-1(Xmr+,n1 - Xmr ,n)
m,n j,k
= X gj,k(Xr), Xr+1 - Xr Yjr,k - Yjr,k-1
j,k

(41) (42)

+
j,k

Ar1
X gj,k(Xr) - X gj,k(Xr-1), Xr+1 - Xr Yjr,k-1 .

(43)

A2r
In the above derivation, we basically changed the order of summations.

From the mean value theorem, we know that there exits X(rm+,1n)  [Xr, Xr+1] such that

gm,n(Xr+1) - gm,n(Xr) = X gm,n(X(rm+,1n)), Xr+1 - Xr) .

(44)

Hence, Ar1 becomes
X gj,k(Xr), Xr+1 - Xr (Yjr,k - Yjr,k-1)
j,k

= X gj,k(Xr) - X gj,k(Xr+1), Xr+1 - Xr (Yjr,k - Yjr,k-1)
j,k

(45)

+ X gj,k(Xr+1), Xr+1 - Xr (Yjr,k - Yjr,k-1)
j,k

= X gj,k(Xr) - X gj,k(Xr+1), Xr+1 - Xr (Yjr,k - Yjr,k-1)
j,k

+ (gj,k(Xr+1) - gj,k(Xr))(Yjr,k - Yjr,k-1)
j,k

 - Y r - Y r-1 1 Xr+1 - Xr Xr+1 - Xr LX

+

V

r+1


+

Y l(Y r) - Y

l(Y

r-1), Y r

- Y r-1

-

r+1 - r, Y r - Y r-1

(a)
 - Y r - Y r-1 1 Xr+1 - Xr 2LX +

V r+1 

+

Y l(Y

r)

- Y

l(Y r-1), Y

r

- Y r-1

- r+1 - r, Y r - Y r-1

14

Under review as a conference paper at ICLR 2019

where in (a) we used Cauchy­Schwarz inequality.

Combining Ar1 and A2r, we can get the lower bound of Ar, Xr+1 - Xr , i.e., Ar, Xr+1 - Xr

 - Y r - Y r-1 1 Xr+1 - Xr 2LX +

V r+1 

+

Y l(Y

r)

- Y

l(Y r-1), Y

r

- Y r-1

- r+1 - r, Y r - Y r-1 +

X gj,k(Xr)) - X gj,k(Xr-1)), Xr+1 - Xr Yjr,k-1

j,k

 - Y r - Y r-1 1 Xr+1 - Xr 2LX +

V r+1 - r+1 - r, Y r - Y r-1 

+ Y l(Y r) - Y l(Y r-1), Y r - Y r-1

-1 2

Y r-1

1LX

Xr - Xr-1 2 + Xr+1 - Xr 2 .

To further bound the above terms, we will give the upper bound of Y r - Y r-1, V r+1/ - ( r+1 - r) and Y l(Y r) - Y l(Y r-1), Y r - Y r-1 separately in the following two steps.

Step 1). First, we have

Y r - Y r-1, V r+1 - ( r+1 - r) 

= Y r - Y r-1 - (Y r+1 - Y r) + Y r+1 - Y r, V r+1 - ( r+1 - r) 

=

V r+1 ,

V r+1

-(

r+1

-

r)

-

Y r+1 - Y r, V r+1 - ( r+1 - r)





(=a)  2

V r+1 2 +

V r+1 - ( r+1 -

r)

2
-

r+1 - r 2



(46) (47)

- 1 Y r+1 - Y r 2 - Y r - Y r-1 2 + V r+1 2 + Y r+1 - Y r, r+1 - r 2

0



V r+1 - ( r+1 -

r)

2
-

1

Y r+1 - Y r 2 + 1

Y r - Y r-1 2

2

2 2

(48) (49)

where in (a) we used

- r+1 - (- r), Y r+1 - Y r  0

(50)

due to the fact that - r+1 is the subgradient of the convex indicator function 1(yr+1  Y), and also

the equality

Y r - Y r-1, V r+1

1 =

Y r+1 - Y r

2- 1

Y r - Y r-1

2- 1

V r+1

2.

222

(51)

Further, from the optimality condition of the Y r subprolbem shown in equation 23, we know

V r+1 - ( r+1 - 

r) = g(Xr+1) - g(Xr) - (Y l(Y r) - Y l(Y r-1)),

(52)

which gives

 2

V r+1 - ( r+1 - 

r)

2
 Lg2,1

Xr+1 - Xr

2 + LY2

Y r - Y r-1

2

(53)

where we used the Lipschitz continuity.

Step 2). Next, we need to quantify the lower bound of Y l(Y r) - Y l(Y r-1), Y r - Y r-1 . Since l(Y ) is strongly convex, we have

Y l(Y r) - Y l(Y r-1), Y r - Y r-1   Y r - Y r-1 2.

(54)

15

Under review as a conference paper at ICLR 2019

Combining step 1 and step 2 and equation 46, we have the lower bound of Ar, Xr+1 - Xr

Ar, Xr+1 - Xr

 - Y r - Y r-1 1LX

Xr+1 - Xr 2 - 1 2

Y r-1 1LX

Xr+1 - Xr 2

-1 2

Y r-1

LX

Xr - Xr-1

2+ 1 2

Y r+1 - Y r

2- 1 2

Y r - Y r-1

2

- Lg2,1 Xr+1 - Xr 2 - L2Y Y r+1 - Y r 2 +  Y r - Y r-1 2. (55)

Therefore, dividing equation 37 by  and combining equation 39, equation 55, we have

1 Xr+1 - Xr 2 + 1 Y r+1 - Y r 2 2 2

 1 Xr - Xr-1 2 + 1 Y r - Y r-1 2 - 1 W r+1 2 2 2 2

+ 2LX Y r - Y r-1 1 + LX Y r-1 1 + LX + 2L2g,1 Xr+1 - Xr 2 2

+ LX Y r-1 1 + LX Xr - Xr-1 2 -  - LY2 Y r - Y r-1 2 2 

(a)


1

Xr - Xr-1

2+

1

Y r - Y r-1 2 -  - L2Y Y r - Y r-1 2

2 2



dual descent
+ (5Y + 1)LX + 2L2g,1 Xr+1 - Xr 2 + LX (Y + 1) Xr - Xr-1 2 2 2

(56) (57)

where in (a) we used Y  Y r 1, r.

A.2.2 POTENTIAL FUNCTION Rearranging equation 57, we have

Qr+1

LX (Y + 1) + 1 2 2

Xr+1 - Xr 2 + 1 -  - L2Y 2 2

Y r+1 - Y r 2

 LX (Y + 1) + 1 Xr - Xr-1 2 + 1 -  - L2Y

2 2

2 2

Y r - Y r-1 2

-  - L2Y Y r+1 - Y r 2 + (6Y + 2)LX + 2L2g,1 Xr+1 - Xr 2 2 2

=Qr + (6Y + 2)LX + 2Lg2,1 Xr+1 - Xr 2 2

-  - L2Y Y r+1 - Y r 2 -  - L2Y Y r - Y r-1 2. 2 2

(58)

16

Under review as a conference paper at ICLR 2019

Define a potential function Pr+1 (Xr+1, Y r+1) + dQr+1. We can obtain



P r+1

- Pr



-

 



-

Lg,1

-

d((6Y

+ 2)LX

+

2Lg2,1)

 

Xr+1 - Xr

2

2 2

2 



c1


-

 

d(

- LY2 )

-

LY

+ Lg,1

+

1



 

Y r+1 - Y r

2

 2

2



c2


-

 

d(

- L2Y )

-

LY

+

1



 

Y r - Y r-1

2.

 2

2



c3

(59)

A.2.3 CONVERGENCE CONDITIONS

If the following conditions hold, i.e., c1, c2, c3 > 0, then the potential function has a sufficient decrease at each iteration.

 - Lg,1 - d(6Y + 2)LX + 2L2g,1) >0,

22

2

(60)

d (


- L2Y ) - (LY

+ Lg,1

+

1 )


>0.

(61)

To show that d  0 such that

 - Lg,1 0

0

LY

+ Lg,1

+

1 

+d

-

1 

(6Y + 2)LX + 2L2g,1

0

0

1 

(

-

LY2

)

0 (62)

by

S -procedure

it

is

sufficiently

to

show

the

set

X

=

{x| (

- Lg,1) x2

<

LY

+ Lg,1

+

1 

,

((6Y

+

2)LX + 2L2g,1)x2 > ( - LY2 )} is empty.

Hence, we require that

1. which gives

 - L2Y > 0, 
 < LY2 ;

2.   Lg,1;

3. and we also need





Lg,1 +

(LY

+ Lg,1

+

1 

)((6LX

+

2)LX

 - Ly2

+ 2L2g,1) .

(63) (64)
(65)

Note

that

we

need

1 

-

-LY2 2

 0, since the potential function should be greater than 0, meaning

that

L2Y 2 -  + 2  0,

(66)

which is actually satisfied automatically by checking the two roots of the equation L2Y 2 -  + 2 (note that LY  ).

Combing the descent lemma shown in Lemma 1, we have the conditions of ,  shown in Lemma 2 so that the potential function can have a sufficient descent at each iteration.

17

Under review as a conference paper at ICLR 2019

A.3 PROOF OF THEOREM 1

Proof. First, we can get the upper bound of L(Xr, Y r) , which is

L(Xr, Y r)

 Xr+1 - Xr + Xr+1 - projX (Xr - X f (Xr, Y r))

+ Y r+1 - Y r + Y r+1 - projY (Y r + Y f (Xr, Y r))

(a)
 Xr+1 - Xr + projX (Xr+1 - X f (Xr+1, Y r)) - projX (Xr - X f (Xr, Y r))

+ Y r+1 - Y r + projY (Y r+1 + Y f (Xr+1, Y r+1)) - projY (Y r + Y f (Xr, Y r))

(b)
 Xr+1 - Xr + X f (Xr+1, Y r)) - X f (Xr, Y r))

+ Y r+1 - Y r + Y f (Xr+1, Y r+1)) - Y f (Xr, Y r))

(67)

(c)
 Xr+1 - Xr + X h(Xr+1) - X h(Xr) + Y X g(Xr+1) - g(Xr) + Y l(Y r+1) - Y l(Y r)

d)
(1 + LX + Y Lg,2) Xr+1 - Xr + (1 + LY ) Y r+1 - Y r

where in (a) we used the optimality condition of Xr-subproblem; in (b) we used nonexpansiveness of the projection operator; in (c) we take the gradient of f (X, Y ); in (d) we used the Lipschitz continuous of function h(X) and l(Y ).

Since Xr, Y r are within compact sets, the sizes of Xr, Y r are bounded, which implies that there exits 1 such that

L(Xr, Y r) 2  1( Xr+1 - Xr 2 + Y r+1 - Y r 2).

(68)

Cr

From equation 59, we also know that there exits a constant 2 min{c1, c2} such that Pr - Pr+1  2Cr.

(69)

Combining equation 68, we have L(Xr, Y r) 2  1 Pr - Pr+1 . 2

(70)

Summing both sides of the above inequality over r = 1, . . . , T , we have

T L(Xr, Y r) 2  1 (P1 - PT +1)  1 (P1 - P) r=1 2 2

(71)

where in the last inequality we have used the fact that Pr is decreasing and lower bounded by P since Xr, Y r are within the compact sets. By utilizing the definition T ( ), the above inequality becomes

T ( )  1 (P1 - P)  1 P1. 2 2

(72)

Dividing both sides by T ( ), and by setting C 1/2, the desired result is obtained

18

