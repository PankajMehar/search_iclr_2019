Under review as a conference paper at ICLR 2019

RECOVERING THE LOWEST LAYER OF DEEP NETWORKS WITH HIGH THRESHOLD ACTIVATIONS
Anonymous authors Paper under double-blind review

ABSTRACT
Giving provable guarantees for learning neural networks is a core challenge of machine learning theory. Most prior work gives parameter recovery guarantees for one hidden layer networks, however, the networks used in practice have multiple non-linear layers. In this work, we show how we can strengthen such results to deeper networks ­ we address the problem of uncovering the lowest layer in a deep neural network under the assumption that the lowest layer uses a high threshold before applying the activation, the upper network can be modeled as a well-behaved polynomial and the input distribution is gaussian.

1 INTRODUCTION

Understanding the landscape of learning neural networks has been a major challege in machine learning. Various works gives parameter recovery guarantees for simple one-hidden-layer networks where the hidden layer applies a non-linear activation u after transforming the input x by a matrix W, and the upper layer is the weighted sum operator: thus f (x) = aiu(wiT x). However, the networks used in practice have multiple non-linear layers and it is not clear how to extend these known techniques to deeper networks.
We consider a multilayer neural network with the first layer activation u and the layers above represented by an unknown polynomial P such that it has non-zero non-linear components. More precisely, the function f computed by the neural network is as follows:

fW(x) = P (u(w1T x), u(w2T x), . . . , u(wdT x)) for P (X1, . . . , Xd) =

cr · Xjrj .

rZ+d

j

We assume that the input x is generated from the standard Gaussian distribution and there is an underlying true network (parameterized by some unknown W)1 from which the labels are generated.
In this work we strengthen previous results for one hidden layer networks to a larger class of functions representing the transform made by the upper layer functions if the lowest layer uses a high threshold (high bias term) before applying the activation: u(a - t) instead of u(a). Intuitively, a high threshold is looking for a high correlation of the input a with a direction wi. Thus even if the function f is applying a complex transform after the first layer, the identity of these high threshold directions may be preserved in the training data generated using f .

Learning with linear terms in P . Suppose P has a linear component then we show that increasing the threshold t in the lowest layer is equivalent to amplifying the coefficients of the linear part. Instead of dealing with the polynomial P it turns out that we can roughly think of it as P (µX1, ..., µXd) where µ decreases exponentially in t (µ  e-t2 ). As µ decreases it has the effect of diminishing the non-linear terms more strongly so that relatively the linear terms stand out. Taking advantage of this effect we manage to show that if t exceeds a certain threshold the non linear terms drop in value enough so that the directions wi can be learned by relatively simple methods. We show that we can get close to the wi applying a simple variant of PCA. While an application of PCA can be thought of as finding principal directions as the local maxima of max||z||=1 E[f (x)(zT x)2],
1We suppress W when it is clear from context.

1

Under review as a conference paper at ICLR 2019
we instead perform maxE[f(x)H2(zT x)2]=1 E[f (x)H4(zT x)4]]2. If W has a constant condition number then the local maxima can be used to recover directions that are transforms of wi. Theorem 1 (informal version of Claim 2, Theorem 11). If t > c log d for large enough constant c > 0 and P has linear terms with absolute value of coefficients at least 1/poly(d) and all coefficients at most O(1), we can recover the weight vector wi within error 1/poly(d) in time poly(d).
These approximations of wi obtained collectively can be further refined by looking at directions along which there is a high gradient in f ; for monotone functions we show how in this way we can recover wi exactly (or within any desired precision. Theorem 2. (informal version of Theorem 5) Under the conditions of the previous theorem, for monotone P , there exists a procedure to refine the angle to precision in time poly(1/ , d) starting from an estimate that is 1/poly(d) close.
The above mentioned theorems hold for u being sign and ReLU.3
When P is monotone and u is the sign function, learning W is equivalent to learning a union of half spaces. We learn W by learning sign of P which is exactly the union of halfspaces wiT x = t. Thus our algorithm can also be viewed as a polynomial time algorithm for learning a union of large number of half spaces that are far from the origin ­ to our knowledge this is the first polynomial time algorithm for this problem but with this extra requirement (see earlier work Vempala (2010) for an exponential time algorithm). Refer to Appendix B.6 for more details.
Such linear components in P may easily be present: consider for example the case where P (X) = u(vT X - b) where u is say the sigmoid or the logloss function. The taylor series of such functions has a linear component ­ note that since the linear term in the taylor expansion of u(x) has coefficient u (0), for expansion of u(x-b) it will be u (-b) which is (e-b) in the case of sigmoid. In fact one may even have a tower (deep network) or such sigmoid/logloss layers and the linear components will still be present ­ unless they are made to cancel out precisely; however, the coefficients will drop exponentially in the depth of the networks and the threshold b.
Sample complexity with low thresholds and no explicit linear terms. Even if the threshold is not large or P is not monotone, we show that W can be learned with a polynomial sample complexity (although possibly exponential time complexity) by finding directions that maximize the gradient of f . Theorem 3 (informal version of Corollary 1). If u is the sign function and wi's are orthogonal then in poly(1/ , d) samples one can determine W within precision if the coefficient of the linear terms in P (µ(X1 + 1), µ(X2 + 1), µ(X3 + 1), . . .) is least 1/poly(d)
Learning without explicit linear terms. We further provide evidence that P may not even need to have the linear terms ­ under some restricted cases (section 4), we show how such linear terms may implicitly arise even though they may be entirely apparently absent. For instance consider the case when P = XiXj that does not have any linear terms. Under certain additional assumptions we show that one can recover wi as long as the polynomial P (µ(X1 + 1), µ(X2 + 1), µ(X3 + 1), ..) (where µ is e-t has linear terms components larger than the coefficients of the other terms). Note that this transform when applied to P automatically introduces linear terms. Note that as the threshold increases applying this transform on P has the effect of gathering linear components from all the different monomials in P and penalizing the higher degree monomials. We show that if W is a sparse binary matrix then we can recover W when activation u(a) = ea under certain assumptions about the structure of P . When we assume the coefficients are positive then these results extend for binary low l1- norm vectors without any threshold. Lastly, we show that for even activations (a, u(a) = u(-a)) under orthogonal weights, we can recover the weights with no threshold.
Learning with high thresholds at deeper layers. We also point out how such high threshold layers could potentially facilitate learning at any depth, not just at the lowest layer. If there is any cut in the network that takes inputs X1, . . . , Xd and if the upper layers operations can be modelled by a polynomial P , then assuming the inputs Xi have some degree of independence we could use this to modularly learn the lower and upper parts of the network separately (Appendix E)
2Here H4 and H2 are the fourth and second order hermite polynomials respectively. 3Theorem 1 holds for sigmoid with t  c log d.
2

Under review as a conference paper at ICLR 2019

Related Work. Various works have attempted to understand the learnability of simple neural networks. Despite known hardness results Goel et al. (2016); Brutzkus & Globerson (2017), there has been an array of positive results under various distributional assumptions on the input and the underlying noise in the label. Most of these works have focused on analyzing one hidden layer neural networks. A line of research has focused on understanding the dynamics of gradient descent on these networks for recovering the underlying parameters under gaussian input distribution Du et al. (2017b;a); Li & Yuan (2017); Zhong et al. (2017a); Zhang et al. (2017); Zhong et al. (2017b). Another line of research borrows ideas from kernel methods and polynomial approximations to approximate the neural network by a linear function in a high dimensional space and subsequently learning the same Zhang et al. (2015); Goel et al. (2016); Goel & Klivans (2017b;a). Tensor decomposition methods Anandkumar & Ge (2016); Janzamin et al. (2015) have also been applied to learning these simple architectures.
The complexity of recovering arises from the highly non-convex nature of the loss function to be optimized. The main result we extend in this work is by Ge et al. (2017). They learn the neural network by designing a loss function that allows a "well-behaved" landscape for optimization avoiding the complexity. However, much like most other results, it is unclear how to extend to deeper networks. The only known result for networks with more than one hidden layer is by Goel & Klivans (2017b). Combining kernel methods with isotonic regression, they show that they can provably learn networks with sigmoids in the first hidden layer and a single unit in the second hidden layer in polynomial time. We however model the above layer as a multivariate polynomial allowing for larger representation. Another work Arora et al. (2014) deals with learning a deep generative network when several random examples are generated in an unsupervised setting. By looking at correlations between input coordinates they are able to recover the network layer by layer. We use some of their ideas in section 4 when W is a sparse binary matrix.
Notation. We denote vectors and matrices in bold face. || · ||p denotes the lp-norm of a vector. || · || without subscript implies the l2-norm. For matrices || · || denotes the spectral norm and || · ||F denotes the forbenius norm. N (0, ) denotes the multivariate gausssian distribution with mean 0 and covariance . For a scalar x we will use (x) to denote the p.d.f. of the univariate standard normal distribution with mean zero and variance 1 .For a vector x we will use (x) to denote the p.d.f. of the multivariate standard normal distribution with mean zero and variance 1 in each direction.  denotes the c.d.f. of the standard gausssian distribution. Also define c = 1 - . Let hi denote the ith normalized Hermite polynomial Wikipedia contributors (2018). For a function f , let f^i denote the ith coefficient in the hermite expansion of f , that is, f^i = EgN (0,1)[f (g)hi(g)]. For a given function f computed by the neural network, we assume that the training samples (x, y) are such that x  Rn is distributed according to N (0, 1) and label has no noise, that is, y = f (x).
Note: Most proofs are deferred to the Appendix due to lack of space.

2 APPROXIMATE RECOVERY WITH LINEAR TERM

In this section we consider the case when P has a positive linear component and we wish to recover the parameters of true parameters W. For simplicity in this section we will work with P where the highest degree in any Xi is 1. See Appendix B.8 for a version of the theorem with more general P . More formally,
Assumption 1 (Structure of network). We assume that P has the following structure P (X1, . . . , Xk) = c0 + i[d] ciXi + S[d]:|S|>1 cS jS Xj such that ci = (1)4 for all i  [d] and for all S  [d] such that |S| > 1, |cS|  O(1). W has constant condition number.

We denote flin(x) = c0 + i[d] ciu((wi)T x) to be the linear part of f .

Next we will upper bound expected value of u(x): for "high-threshold" ReLU, that is, ut(a) =

max(0, a - t), EgN(0,2)[ut(g)] is bounded by a function (t, )



e-

t2 22

(see Lemma 10).

We

also get a lower bound on |u^4| in terms of (t, ) 5 This enables us to make the following assumption.

4We can handle  [d-C , dC ] for some constant C by changing the scaling on t. 5For similar bounds for sigmoid and sign refer to Appendix B.7.

3

Under review as a conference paper at ICLR 2019

Assumption 2. Activation function u is a positive high threshold activation with threshold t, that is, the bias term is t. EgN(0,2)[ut(g)]  (t, ) where  is a positive decreasing function of t. Also, |u^k| = t(1)(t, 1) for k = 2, 4.
Assumption 3 (Value of t). t is large enough such that (t, ||W||)  d- and (t, 1)  d-p with for large enough constant  > 0 and p  (0, 1].

For t=

example, for high threshold 2 log d for large enough

ReLU, (t, 1) = e-t2/2 and µ = (t, ||W||) d suffices to get the above assumption ((W

= e-t2/2||W||2 ) is a constant).

,

thus

These high-threshold activation are useful for learning as in expectation, they ensure that f is close to flin since the product terms have low expected value.
Lemma 1. For |S| > 1, under Assumption 2 we have,


E  ut((wj)T x)  (t, 1) ((W)(t, ||W||))|S|-1 .
jS

So if µ := (W)(t, ||W||), then E[ jS Xj[x]]  (t, 1)µ|S|-1 Lemma 2. Let (x) = f (x) - flin(x). Under Assumptions 1, 2 and 3, if t is such that d(t, ||W||)  c for some small enough constant c > 0 we have,

E[|(x)|]  O d3(t, 1)(t, ||W||) = O d-(1+p)+3 .

Note: We should point out that f (x) and flin(x) are very different point wise; they are just close in expectation under the distribution of x. In fact, if d is some constant then even the difference in expectation is some small constant.
This closeness suggests that algorithms for recovering under the labels from flin can be used to recover with labels from f approximately.

Learning One Layer Neural Networks using Landscape Design. Ge et al. (2017) proposed an algorithm for learning one-hidden-layer networks. Intuitively, the approach of Ge et al. (2017) is to design a well behaved loss function based on correlations to recover the underlying weight vectors. They show that the local minima of the following optimization corresponds to some transform of each of the wi ­ thus it can be used to recover a transform of wi, one at a time.

max

sgn(u^4)E[flin(x)H4(zT x)]

z:E[flin(x)H2(zT x)]=u^2

which they optimize using the Lagrangian formulation (viewed as a minimization):

min Glin(z) := -sgn(u^4)E[flin(x)H4(zT x)] + (E[flin(x)H2(zT x)] - u^2)2
z

where H2(zT x) = ||z||2h2

 6

(z

T x)4 12

-

||z||2(zT x)2 2

+

zT x ||z||
||z||4 4

= (see

(zT x)2 2

-

||z||2 2

and H4(zT x)

=

Appendix A.1 for more details).

||z||4h4 Using

zT x ||z||

=

properties

of Hermite polynomials, we have E[flin(x)H2(zT x)] = u^2 i ci(zT wi)2 and similarly

E[flin(x)H4(zT x)] = u^4 i(zT wi)4. Thus

2

Glin(z) = -|u^4| ci(zT wi)4 + u^22

ci(zT wi)2 - 1 .

ii

Using results from Ge et al. problem are close to columns

(2017), it can be shown of (TW)-1 where T is

that the approximate local minima a diagonal matrix with Tii = ci.

of

this

Definition 1 (( ,  )-local minimum/maximum). z is an ( ,  )-local minimum of F if ||F (z)||  and min(2F (z))   .

Claim 1 (Ge et al. (2017)). An ( ,  )-local minima of the Lagrangian formulation z with 

O  3/|u^4| is such that for an index i |zT wi| = 1 ± O( /u^22) ± O(d /|u^4|) and j = i, |vT wj| = O(  /|u^4|) where wi are columns of (TW)-1.

4

Under review as a conference paper at ICLR 2019
Ge et al. (2017) do not mention u^2 but it is necessary in the non-orthogonal weight vectors case for the correct reduction. Since for us, this value can be small, we mention the dependence.Note that these are not exactly the directions wi that we need, one way to think about is that we can get the correct directions by estimating all columns and then inverting.
One-hidden-layer to Deep Neural Network. Consider the loss with f instead of flin: min z : G(z) = -sgn(u^4)E[f (x)H4(zT x)] + (E[f (x)H2(zT x)] - u^2)2
We previously showed that f is close to flin in expectation due to the high threshold property. This also implies that Glin and G are close and so are the gradients and (eignevalues of) hessians of the same. This closeness implies that the landscape properties of one approximately transfers to the other function. More formally, Theorem 4. Let Z be an ( ,  )-local minimum of function A. If ||(B -A)(Z)||   and ||2(B - A)(Z)||   then Z is an ( + ,  + )-local minimum of function B and vice-versa.
We will now apply above lemma on our Glin(z) and G(z). Claim 2. For  = (|u^4|/u^22)  d, an ( ,  )-approximate local minima of G (for small enough ,   d-2) is an (O(log d)d-(1+p)+3, O(log d)d-(1+p)+3)-approximate local minima of Glin. This implies z is such that for an index i, |zT wi| = 1 ± O(1)d-2/3p+3 and j = i, |zT wj| = O(1)d-1/3p+3/2 where wi are columns of (TW)-1 (ignoring log d factors).
 Note: For ReLU, setting t = C log d for large enough C > 0 we can get closeness 1/poly(d) to the columns of (TW)-1. Refer Appendix B.7 for details for sigmoid.
The paper Ge et al. (2017) also provides an alternate optimization that when minimized simultaneously recovers the entire matrix W instead of having to learn columns of (TW)-1 separately. We show how applying our methods can also be applied to that optimization in Appendix B.4 to recover W by optimizing a single objective.
2.1 APPROXIMATE TO ARBITRARILY CLOSE FOR MONOTONE P
Assuming P is monotone, we can show that the approximate solution from the previous analysis can be refined to arbitrarily closeness using a random search method followed by approximately finding the angle of our current estimate to the true direction.
The idea at a high level is to correlate with  (zT x - t) where  is the Dirac delta function. It turns out that the correlation is maximized when z is equal to one of the wi. Correlation with  (zT x-t) is checking how fast the correlation of f with (zT x-t) is changing as you change t. To understand this look at the case when our activation u is the sign function then note that correlation of ut(wT x - t) with  (wT x - t) is very high as its correlation with (wT x - t ) is 0 when t < t and significant when t > t. So as we change t' slightly from t - to t + there is a sudden increase. If z and w differ then it can be shown that correlation of ut(wT x - t) with  (zT x - t) essentially depends on cot() where  is the angle between w and z (for a quick intuition note that one can prove that E[ut(wT x) (zT x)] = c cot(). See Lemma 16 in Appendix). In the next section we will show how the same ideas work for non-monotone P even if it may not have any linear terms but we only manage to prove polynomial sample complexity for finding w instead of polynomial time complexity.
In this section we will not correlate exactly with  (zT x - t) but instead we will use this high level idea to estimate how fast the correlation with (zT x - t ) changes between two specific values as one changes t , to get an estimate for cot(). Secondly since we can't to a smooth optimization over z, we will do a local search by using a random perturbation and iteratively check if the correlation has increased. We can assume that the polynomial P doesn't have a constant term c0 as otherwise it can easily be determined and cancelled out6. We will refine the weights one by one. WLOG, let us assume that w1 = e1 and we have z such that zT w1 = z1 = cos-1(1). Let l(z, t, ) denote {x : zT x  [t - , t]} for z  Sn-1.
6for example with RELU activation, f will be c0 most of the time as other terms in P will never activate. So c0 can be set to say the median value of f .
5

Under review as a conference paper at ICLR 2019

Algorithm 1 RefineEstimate
1: Run EstimateT anAlpha on z to get s = tan() where  is the angle between z and w1. 2: Perturb current estimate z by a vector along the d - 1 dimensional hyperplane normal to z with
the distribution n(0, (/d))d-1 to get z . 3: Run EstimateT anAlpha on z to get s = tan( ) where  is the angle between z and w1. 4: if   (/d) then 5: z  z 6: Repeat till   .

Algorithm 2 EstimateTanAlpha

1: Find t1 and t2 such that P r[sgn(f (x))|x  l(z, t , )] at t1 is 0.4 and at t2 is 0.6.

2:

Return

.t2 -t1
-1 (0.6)--1 (0.4)

The algorithm (Algorithm 1) estimates the angle of the current estimate with the true vector and then subsequently perturbs the vector to get closer after each successful iteration.

Theorem 5. Given a vector z  Sd-1 such that it is 1/poly(d)-close to the underlying true vector

w1, that is cos-1(zT w1)  1/poly(d), running Ref ineEstimate for O(T ) iterations outputs a

vector z  Sd-1 such that cos-1((z)T w1) 

1

-

c d

T  for some constant c > 0.

Thus after

O(d log(1/ )) iterations cos-1((z)T w1)  .

We prove the correctness of the algorithm by first showing that EstimateT anAlpha gives a multiplicative approximation to tan(). The following lemma captures this property.
Lemma 3. EstimateT anAlpha(z) outputs y such that y = (1 ± O()) tan() where  is the angle between z and w1.

Proof. We first show that the given probability when computed with sgn(xT w1 -t) is a well defined function of the angle between the current estimate and the true parameter up to multiplicative error.
Subsequently we show that the computed probability is close to the one we can estimate using f (x) since the current estimate is close to one direction. The following two lemmas capture these
properties.

Lemma 4. For t, t and  1/t , we have

P r[xT w1  t and x  l(z, t , )|x  l(z, t, )] = c

t - t cos(1) | sin(1)|

± O( )t

Lemma 5. For t  [0, t/ cos(1)], we have

P r[sgn(f (x))|x  l(z, t , )] = P r[sgn((w1)T x - t)|x  l(z, t, )] + de-(t2).

Using the above, we can show that,

t2 - t1 = -1(0.6 - 1 ± O( )t1) - -1(0.4 - 2 ± O( )t2) tan() = -1(0.6) - -1(0.4) - (1 ± O( )t1)(-1) (p1) + (2 ± O( )t2)(-1) (p2) tan()

where 1, 2 > 0 are the noise due to estimating using f and p1  [0.6 - 1 ± O( )t1, 0.6] and p2  [0.4 - 2 ± O( )t2, 0.4] as long as t1, t2  [0, t/ cos(1)]. The following lemma bounds the
range of t1 and t2.

Lemma 6.

We have 0  t1

 t2



t cos(1

)

.

Thus, we have,

t2 -1(0.6)

- -

t1 -1(0.4)

=

(1

±

O

(1

+

2

+

t2)) tan()

as long as 2 +O( )t2  c for some constant c > 0. Thus, we can get a multiplicative approximation to tan() up to error  ( can be chosen to make its contribution smaller than ).

6

Under review as a conference paper at ICLR 2019

Finally we show (proof in Appendix ??) that with constant probability, a random perturbation reduces the angle by a factor of (1 - 1/d) of the current estimate hence the algorithm will halt after O(d log(1/)) iterations.
Lemma 7. By applying a random Gaussian perturbation along the d - 1 dimensional hyperplane normal to z with the distribution n(0, (/d))d-1 and scaling back to the unit sphere, with constant probability, the angle  (< /2) with the fixed vector decreases by at least (/d).

3 SAMPLE COMPLEXITY

We extend the methods of the previous section to a broader class of polynomials but only to obtain

results in terms of sample complexity. The main idea as in the previous section is to correlate with

 (zT x-t) (the derivative of the dirac delta function) and find arg max||z||2=1 E[f (x) (zT x-t)]. We will show that the correlation goes to infinity when z is one of wi and bounded if it is far from

all of them. From a practical standpoint we calculate  (zT x - s) by measuring correlation with

1 2

((zT

x

-

s

+

) - (zT x - s -

). In the limit as

 0 this becomes  (zT x - s). (zT x - s)

in turn is estimated using 1 (sgn(zT x - s + ) - sgn(zT x - s)), as in the previous section, for an

even smaller ; however, for ease of exposition, in this section, we will assume that correlations with

(zT x - s) can be measured exactly.

Let us recall that f (x) = P (u((w1)T x), u((w2)T x), . . . , u((wd)T x)). Let C1(f, z, s) denote E[f (x)(zT x - s)] and let C2(f, z, s) denote E[f (x)((zT x - s - ) - (zT x - s + )].

If

u

=

sgn

then

P

has

degree at

most

1

in

each

Xi.

Let

P Xi

denote

the

symbolic

partial

derivative

of P with respect to Xi; so, it drops monomials without Xi and factors off Xi from the remaining

ones. Let us separate dependence on Xi in P as follows:

P (X1, , .., Xd) = XiQi(X1, ..Xi-1, Xi+1, .., Xd) + R1(X1, .Xi-1, Xi+1, .., Xd)

then

P Xi

=

Qi.

We will overload the polynomial P such that P [x] to denote the polynomial computed by substituting Xi = u((w1)T x) and similarly for Q and R. Under this notation f (x) = P [x]. We will also assume that |P (X)|  ||X||O(1) = ||X||c1 (say). By using simple correlations we will show:

Theorem 6. If u is the sgn function, P (X)  ||X||c1 and for all i, E[Qi[x]|(wi)T x = t]  3

then using poly(

d
3

2

)

samples

one

can

determine

the

wi's

within

error

2.7

Note that if all the wi's are orthogonal then Xi are independent and E Qi[x] (wi)T x = t is just value of Qi evaluated by setting Xi = 1 and setting all the the remaining Xj = µ where µ = E[Xj]. This is same as 1/µ times the coefficient of Xi in P (µ(X1 + 1), . . . , µ(Xd + 1)).

Corollary 1. If u is one can determine

the sgn function and W within error 2

wi in

s are orthogonal then in sample complexity poly( d 3 each entry, if the coefficient of the linear terms

)
2
in

P (µ(X1 + 1), µ(X2 + 1), µ(X3 + 1), ..) is larger than 3µ, where µ = E[Xi].

The main point behind the proof of Theorem 6 is that the correlation is high when z is along one of wi and negligible if it is not close to any of them.

Lemma 8. Assuming P (X) < ||X||c1 . If z = wi then C2(f, z, t) = (t)E

P Xi

zT x = t

+

dO(1). Otherwise if all angles i between z and wi are at least 2 it is at most dO(1)/ 2.

We will use the notation g(x)x=s to denote g(x) evaluated at x = s. Thus Cauchy's mean value theorem can be stated as g(x + ) - g(x) = [g (s)](s = s  [x, x + ]). We will over load the notation a bit: (zT x = s) will denote the probability density that vzT x = s; so if z is a unit vector this is just (s); (z1T x = s1, z2T x = s2) denotes the probability density that both z1T x = s1, z2T x = s2; so again if z1, z2 are orthonormal then this is just (s1)(s2).
7The theorem can be extended to ReLU by correlating with the second derivative  (see Appendix C.1).

7

Under review as a conference paper at ICLR 2019

The following claim interprets correlation with (zT x - s) as the expected value along the corresponding plane zT x = s.
Claim 3. E[f (x)(zT x - s)] = E[f (x)|zT x = s](zT x = s).

The following claim computes the correlation of P with  (zT x - s).

Claim 4. E[P [x] (zT x = s)] is equal to i | cot(i)|(zT x = s, (wi)T x = t)

E

P Xi

[x]|zT

x

=

s,

(wi)T

x

=

t

+  (s)E[P [x]|zT x = s].

We use this to show that the correlation is bounded if all the angles are lower bounded.
Claim 5. If P (X)  ||X||c1 and if z has an angle of at least 2 with all the wi's then C2(f, z, s)  dO(1)/ 2.

Above claims can be used to prove main Lemma 8. Refer to the Appendix C for proofs.

Proof O( 3

of Theorem 2(t)d-c).

6. If From

we wish Lemma

to 8,

determine wi for some large

within an enough c,

angle of this will

accuracy 2 let us set ensure that if all i >

to be 2 the

correlation is o((t) 3). Otherwise it is (t) 3(1±o(1)). Since (t) = poly(1/d), given poly( d )

samples, we can test if a given direction is within accuracy 2 of a wi or not.

23

4 STRONGER RESULTS UNDER STRUCTURAL ASSUMPTIONS

Under additional structural assumptions on W such as the weights being binary, that is, in {0, 1}, sparsity or certain restrictions on activation functions, we can give stronger recovery guarantees. Proofs have been deferred to Appendix D.
Theorem 7. For activation ut(a) = e(a-t). Let the weight vectors wi be 0, 1 vectors that select the coordinates of x. For each i, there are exactly d indices j such that wij = 1 and the coefficient of the linear terms in P (µ(X1 + 1), µ(X2 + 1), µ(X3 + 1), ..) for µ = e-t is larger than the coefficient of all the product terms (constant factor gap) then we can learn the W.

In order to prove the above, we will construct a correlation graph over x1, . . . , xn and subsequently identify cliques in the graph to recover wi's.
With no threshold, recovery is still possible for disjoint, low l1-norm vector. The proof uses simple correlations and shows that the optimization landscape for maximizing these correlations has local maximas being wi's. Theorem 8. For activation u(a) = ea. If all wi  {0, 1}n are disjoint, then we can learn wi as long as P has all positive coefficients and product terms have degree at most 1 in each variable.

For even activations, it is possible to recover the weight vectors even when the threshold is 0. The technique used is the PCA like optimization using hermite polynomials as in Section 2. Denote C(S, µ) = SS [n] cS µ|S |.

Theorem 9. If the activation is even and for every i, j: C({i}, u^0) + C({j}, u^0) >

6u^22 u^0 u^4

C

({i,

j},

u^0)

then

there

exists

an

algorithm

that

can

recover

the

underlying

weight

vectors.

5 CONCLUSION

In this work we show how activations in a deep network that have a high threshold make it easier to learn the lowest layer of the network. We show that for a large class of functions that represent the upper layers, the lowest layer can be learned with high precision. Even if the threshold is low we show that the sample complexity is polynomially bounded. An interesting open direction is to apply these methods to learn all layers recursively. It would also be interesting to obtain stronger results if the high thresholds are only present at a higher layer based on the intuition we discussed.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Anima Anandkumar and Rong Ge. Efficient approaches for escaping higher order saddle points in non-convex optimization. arXiv preprint arXiv:1602.05908, 2016.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 584­592, Bejing, China, 22­24 Jun 2014. PMLR. URL http://proceedings.mlr. press/v32/arora14.html.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017b.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017.
Surbhi Goel and Adam Klivans. Eigenvalue decay implies polynomial-time learnability for neural networks. arXiv preprint arXiv:1708.03708, 2017a.
Surbhi Goel and Adam Klivans. Learning depth-three neural networks in polynomial time. arXiv preprint arXiv:1709.06010, 2017b.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the ReLU in polynomial time. arXiv preprint arXiv:1611.10258, 2016.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation. arXiv preprint arXiv:1705.09886, 2017.
Jorge Nocedal and Stephen J Wright. Numerical optimization 2nd, 2006.
Santosh S. Vempala. A random-sampling-based algorithm for learning intersections of halfspaces. J. ACM, 57(6):32:1­32:14, November 2010. ISSN 0004-5411. doi: 10.1145/1857914.1857916. URL http://doi.acm.org/10.1145/1857914.1857916.
Wikipedia contributors. Hermite polynomials -- Wikipedia, the free encyclopedia, 2018. URL https://en.wikipedia.org/w/index.php?title=Hermite_polynomials& oldid=851173493. [Online; accessed 26-September-2018].
Qiuyi Zhang, Rina Panigrahy, Sushant Sachdeva, and Ali Rahimi. Electron-proton dynamics in deep learning. arXiv preprint arXiv:1702.00458, 2017.
Yuchen Zhang, Jason D Lee, Martin J Wainwright, and Michael I Jordan. Learning halfspaces and neural networks with random initialization. arXiv preprint arXiv:1511.07948, 2015.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b.
9

Under review as a conference paper at ICLR 2019

A PREREQUISITES
A.1 HERMITE POLYNOMIALS
Hermite polynomials form a complete orthogonal basis for the gaussian distribution with unit variance. For more details refer to Wikipedia contributors (2018). Let hi be the normalized hermite polynomials. They satisfy the following,
Fact 0. E[hn(x)] = 0 for n > 0 and E[h0(x)] = 1.
Fact 1. EaN(0,1)[hi(a)hj(a)] = ij where ij = 1 iff i = j. This can be extended to the following:
Fact 2. For a, b with marginal distribution N (0, 1) and correlation , E[hi(a)hj(b)] = ijj. Consider the following expansion of u into the hermite basis (hi),

u(a) = u^ihi(a).
i=0
Lemma 9. For unit norm vectors u, v, E[u(vT x)hj(wT x)] = u^j(vT w)j.
Proof. Observe that vT x and wT x have marginal distribution N (0, 1) and correlation vT w. Thus using Fact 2,

E[u(vT x)hj(wT x)] = u^iE[hi(vT x)hj(wT x)] = u^iij(vT w)j = u^j(vT w)j.
i=1 i=1

For gaussians with mean 0 and variance 2 define weighted hermite polynomials Hl(a) = ||lhl(a/). Given input vT x for x  N (0, I), we suppress the superscript  = ||v||. Corollary 2. For a non-zero vector v (not necessarily unit norm) and a unit norm vector w, E[Hi(vT x)hj(wT x)] = ij(vT w)j.
Proof. It follows as the proof of the previous lemma,

E[u(vT x)hj(wT x)] = u^iE[hi(vT x)hj(wT x)] = u^iij(vT w)j = u^j(vT w)j.
i=1 i=1

Fact 3.

hn(x

+

y)

=

2-

n 2

n k=0

n k

 hn-k(x 2)hk(y 2).

Fact 4. hn(x) =

n 2
k=0

 n-2k ( 2

-

1)k

n 2k

(2k)! k!

2-k

hn-2k

(x).

Fact 5.

(n, m, ) = E[hm(x)hn(x)] = n-2k(2 - 1)k

n 2k

(2k)! k!

2-k

for

k

=

n-m 2

if

k



Z+

else 0.

A.2 PROPERTIES OF MATRICES

Consider matrix A  Rm×m. Let i(A) to be the ith singular value of A such that 1(A)  2(A)  . . .  m(A) and set (A) = 1(A)/m(A).

Fact 6. |det(A)| =

m i=1

i

(A).

Fact 7. Let B be a (mk) × (mk) principal submatrix of A, then (B)  (A).

10

Under review as a conference paper at ICLR 2019

A.3 ACTIVATION FUNCTIONS

Lemma 10. For u being a high threshold ReLU, that is, ut(a) = max(0, a - t) we have for t  C

for

large

enough

constant

C

>

0,

EgN (0,2 ) [ut (g)]



e-

t2 22

.

Also,

u^4, u^2

=

t(1)

e-

t2 2

.

Proof. We have Also,

EgN (0,2 ) [ut (g)]

=

1 2



max(0, g

-

t)e-

g2 22

dg

-

= 1


(g

-

t)e-

g2 22

dg

2 t

 1



ge-

g2 22

dg

2 t

= 


e-hdh

2 t2 22

= 

e .-

t2 22

2

u^4 = EgN(0,1)[ut(g)h4(g)]

= 1



max(0, g

-

t)(g4

-

6g2

+

3)e-

g2 2

dg

2 -

= 1


(g

-

t)(g4

-

6g2

+

3)e-

g2 2

dg

2 t

 1

(t4

-

6t2)

1

e-

t2 2

-1-

1 2t2

2 t



t3

e-

t2 2

.

To upper bound,

u^4

=

1 2



max(0, g

-

t)(g4

-

6g2

+

3)e-

g2 2

dg

-

= 1


(g

-

t)(g4

-

6g2

+

3)e-

g2 2

dg

2 t

 1



2g5e-

g2 2

dg

2 t

= 1


h2e-hdh

2 t2

2

=O

t4

e-

t2 2

.

Similar analysis holds for u^2.

Observe that sgn can be bounded very similarly replacing g - t by 1 which can affect the bounds up to only a polynomial in t factor.

Lemma 11. For u being a high threshold sgn, that is, ut(a) = sgn(a - t) we have for t  C for

large

enough

constant

C

>

0,

EgN (0,2 ) [ut (g)]



e-

t2 22

.

Also,

u^4, u^2

=

t(1)

e-

t2 2

.

For sigmoid, the dependence varies as follows:

Lemma 12.

For u being a high threshold sigmoid, that is, ut(a)

=

1 1+e-(a-t)

we have for t



C

for

large

enough

constant

C

>

0,

EgN (0,2 ) [ut (g)]



e-t+

2 2

.

Also,

u^4, u^2

=

(e-t).

11

Under review as a conference paper at ICLR 2019

Proof. We have Also,

EgN (0,2 ) [ut (g)]

=

1 2

 -

1

+

1 e-(g-t)

e-

g2 22

dg

= e-t 2

 -

e-t

1 +

e-g

e-

g2 22

dg

 e-t



eg e-

g2 22

dg

2 -

=

e-t

e

2 2

2

e dg

-

(g-2 22

)2

-

=

e-t

e

2 2

u^4 = EgN(0,1)[ut(g)h4(g)]

= 1 2

 -

1

+

1 e-(g-t)

e-

g2 2

dg

= e-t 2

 -

e-t

1 +

e-g

(g4

-

6g2

+

3)e-

g2 2

dg

 e-t 2

 0

e-t

1 +

e-g

(g4

-

6g2

+

3)e-

g2 2

dg

 e-t



1 (g4

-

6g2

+

3)e-

g2 2

dg

2 0 2

= (e-t).

We can upper bound similarly and bound u^2.

B APPROXIMATE RECOVERY WITH LINEAR TERMS

B.1 CONSTRAINED OPTIMIZATION VIEW OF LANDSCAPE DESIGN

Let us consider the linear case with wi's are orthonormal. Consider the following maximization problem for even l  4,
max sgn(u^l) · E f (x) · Hl zT x
zSn-1
where hl is the lth hermite polynomial. Then we have,

sgn(u^l) · E f (x) · hl zT x

= sgn(u^l) · E

k
ciut((wi)T x) · hl zT x

i=1

k
= sgn(u^l) · ciE ut((wi)T x) · hl zT x

i=1

k
= |u^l| ci((wi)T z)l.

i=1

It is easy to see that for z  Sn-1, the above is maximized at exactly one of the wi's (up to sign flip for even l) for l  3 as long as ul = 0. Thus, each wi is a local minima of the above problem.

Let L(z) = -

k i=1

ci

zil

.

For

constraint

||z||2

=

1,

we

have the

following optimality

conditions

(see Nocedal & Wright (2006) for more details).

12

Under review as a conference paper at ICLR 2019

First order:

L(z)

-

zT L(z) ||z||2 z

=

0

and

||z||2

=

1.

This applied to our function gives us that for  = -

i cizil ||z||2

( < 0),

-lcizil-1 - 2zi = 0

The

above

implies

that

either

zi

=

0

or

zil-2

=

-

 lci

with

||z||2

=

1.

For

this

to

hold

z

is

such

that

for some set S  [n], |S| > 1, only i  S have zi = 0 and iS zi2 = 1. This implies that for all

i



S,

zil-2

=

-

2 lci

.

Second order:

For all w = 0 such that wT z = 0, wT (2L(z) - 2I)w  0.

For our function, we have: 2L(z) = -l(l - 1)diag(c · z)l-2

= (2L(z))ij =

2(l - 1) 0

if i = j and i  S otherwise.

The last follows from using the first order condition. For the second order condition to be satisfied
we will show that |S| = 1. Suppose |S| > 2, then choosing w such that wi = 0 for i  S and such that wT z = 0 (it is possible to choose such a value since |S| > 2), we get wT (2L(z) - 2I)w = 2(l - 2)||w||2 which is negative since  < 0, thus these cannot be global minima. However, for |S| = 1, we cannot have such a w, since to satisfy wT z = 0, we need wi = 0 for all i  S, this gives us wT (2L(z) - 2I)w = -2||w||2 which is always positive. Thus z = ±ei are the only
local minimas of this problem.

B.2 IMPORTANT RESULTS FROM GE ET AL. (2017)

Lemma 13 (Ge et al. (2017)). If z is an ( ,  )-local minima of F (z) = - i izi4 + ( i zi2 - 1)2 for   3/min where min = mini i, then

· (Lemma 5.2) |z|2nd 

 min

where

|z|2nd

denotes

the

magnitude

of

the

second

largest

entry in terms of magnitude of z.

· (Derived from Proposition 5.7) zmax = ±1 ± O(d /min) ± O( /) where |z|max is the value of the largest entry in terms of magnitude of z.

B.3 OMITTED PROOFS FOR ONE-BY-ONE RECOVERY

Proof of Lemma 1. Let O  Rd×d be the orthonormal basis (row-wise) of the subspace spanned by wi for all i  [d] generated using Gram-schmidt (with the procedure done in order with elements of |S| first). Now let OS  R|S|×d be the matrix corresponding to the first S rows and let OS  R(d-|S|)×n be that corresponding to the remaining rows. Note that OW (W also has the same ordering) is an upper triangular matrix under this construction.



E  ut((wj)T x)
jS

1 =
(2)n/2 1
= (2)n/2

ut(xT

wi)e-

||x||2 2

dx

x iS

ut((OS wi)T

OS

x)e-

||OS x||2 +||OSx||2 2

dx

x iS

1 = |S|

ut((OS wi)T

x

)e-

||x ||2 2

dx

(2) 2 x R|S| iS

1
d-|S|
(2) 2

e dx-

||x ||2 2

x Rd-|S|

13

Under review as a conference paper at ICLR 2019

1 = |S|
(2) 2

ut((OS wi)T

x

)e-

||x ||2 2

dx

x R|S| iS

=

|det(OSWS )|-1
|S|

(2) 2

u (b )e dbt

i

-

||(OS

WS 2

)-T

b||2

bR|S| iS

Now observe OW. Thus

that OSWS using Fact 6

is also an upper and 7, we get the

triangular matrix since last equality. Also, the

it is a single

principal non-zero

sub-matrix of entry row has

non-zero entry being 1 (||wi|| = 1 for all i). This gives us that the inverse will also have the single

non-zero entry row has non-zero entry being 1. WLOG assume index 1 corresponds to this row.

Thus we can split this as following


E  ut((wj)T x)
jS

 |det(OSWS )|-1

1

ut

(b1

)e-

b12 2

db1

2 b1

 |det(OSWS )|-1

1

ut

(b1

)e-

b21 2

db1

2 b1

 (t, 1) ((W)(t, ||W||))|S|-1

  1
iS\{1} 2
  1
iS\{1} 2



-
ut(bi)e

bi2 2||OS WS ||2

dbi

bi


ut(bi)e- ||Wbi2||2 dbi
bi

PrcoioWf oTf

Claim wi for

1. Consider the SVD of all i. It is easy to see that

matrix M = UDUT . yi are orthogonal. Let F

Let W = UD-1/2 (z) = G(Wz):

and yi

=

2

F (z) = |u^4| ci(zT WT wi)4 - u^22

ci(zT WT wi)2 - 1

ii

= |u^4|

i

1 ci

(zT

yi)4

-

u^22

2
(zT yi)2 - 1 .
i

Since yi are orthogonal, for means of analysis, we can assume that yi = ei, thus the formulation

reduces to maxz |u^4|

i

1 ci

(zi

)4

-



||z||2 - 1 2 up to scaling of  = u^22. Note that this is of

the form in close to yi

Lemma 13 hence using that and thus the local maximas

we can of G(z

)shaorewctlhoastetthoeWappyrio=ximacteiWlocWal Tmwiniim=asocfiFM(z-)1warie

due to the linear transformation. This can alternately be viewed as the columns of (TW)-1 since

TWM-1(TW)T = I.

Proof of Theorem 4. Let Z be an ( ,  )-local minimum of A, then we have ||A(Z)||  min(2A(Z))  - . Observe that
||B(Z)|| = ||(A + (B - A)(Z)||  ||A(Z)|| + ||(B - A)(Z)||  + .

and

Also observe that
min(2B(Z)) = min(2(A + (B - A))(Z))  min(2A(Z)) + min(2(B - A)(Z))  - - ||2(B - A)(Z)||  - - 

Here we use |min(M)|  ||M|| for any symmetric matrix. To prove this, we have ||M|| =

maxxSn-1 ||Mx||. We have x = i xivi where vi are the eigenvectors. Thus we have Mx =

i xii(M)vi and x2i = 1. Which gives us that ||M|| =

i x2i i2(M)  |min(M)|.

14

Under review as a conference paper at ICLR 2019

Proof of Lemma 2. Expanding f , we have



E[|(x)|] = E 

cS ut((wj)T x) 

S[d]:|S|>1 jS



using Lemma 1
using d  di i
using assumption on t

 |cS|E  ut((wj)T x)

S[d]:|S|>1

jS

 C (t, 1)
S[d]:|S|>1

1 min(W

)

(t,

||W

||)

|S|-1

d
=C
i=1

d (t, 1)
i

1 min(W

)

(t,

||W

||)

i-1

d
 C d(t, 1)
i=1

d min(W

)

(t,

||W

||)

i-1

 Cd2(t, 1)

d min(W

)

(t,

||W

||)

Lemma 14. For any function L such that ||L(z, x)||  C(z)||x||O(1) where C is a function that is not dependent on x, we have ||E[(x)L(x)]||  C(z)d-(1+p)+3O(log d).

Proof. We have

||E[(x)L(x)]||  E[|(x)||L(x)||]  E[|(x)C(z)||x||O(1)]
= C(z) E[|(x)| ||x||O(1)| ||x||  c]P r[||x||  c]

+ E[|(x)| ||x||O(1)| ||x|| < c]P r[||x|| < c]

 C(z)(E[||x||O(1)|||x||  c]P r[||x||  c] + cE[|(x)|])

=

C (z )(cO(1) e-

c2 2

+ cO(1)E[|(x)|]).

 Now using Lemma 2 to bound E[|(x)|], for c = (  log d we get the required result.

Lemma 15. For ||z|| = (1) and  = (|u^4|/u^22)  d, ||G(z)||  (1)d-.

Proof. Let K = (W) which by assumption is (1). We will argue that local minima of G cannot have z with large norm. First lets argue this for Glin(z). We know that Glin(z) = - (zT wi)4 + 2(( (zT wi)2) - 1)2 where  = |u^4| and  = u^2. We will argue that zT Glin(z) is large if z is large.

zT Glin(z) = -4 (zT wi)3(zT wi) + 22

(zT wi)2 - 1

2(zT wi)(zT wi)

= -4 (zT wi)4 + 42

(zT wi)2 - 1

(zT wi)2

Let y = Wz then K||z||  ||y||  ||z||/K since K is the condition number of W. Then this implies

zT Glin(z) = -4 yi4 + 42(||y||2 - 1)||y||2 = 4||y||2((- + 2)||y||2 + 2)  ||y||4(- + 2)  (1)d-||y||4

15

Under review as a conference paper at ICLR 2019

Since ||y||  ||z||/K = (1) by assumptions on , z we have zT Glin(z)  (2||y||4) = (1)d-||z||4. This implies ||Glin(z)|| = (1)d-||z||3.
Now we need to argue for G. G(z) - Glin(z) = -sgn(u^4)E[(flin(x) + (x))H4(zT x)] + (E[(flin(x) + (x))H2(zT x)] - )2
+ sgn(u^4)E[(flin(x))H4(zT x)] - E[(flin(x))H2(zT x)] - ]2 = -sgn(u^4)E[(x)H4(zT x)] + E[(x)H2(zT x)]2 + 2E[(x)H2(zT x)]E[flin(x)H2(zT x) - ] = -sgn(u^4)||z||4E[(x)h4(zT x/||z||)] + ||z||4E[(x)h2(zT x/||z||)]2
+ 2||z||4E[(x)h2(zT x/||z||)]E[flin(x)h2(zT x/||z||)] - 2||z||2E[(x)h2(zT x/||z||)] Now h4(zT x/||z||) doesn't have a gradient in the direction of z so zT h4(zT x/||z||) = 0. Similarly zT h2(zT x/||z||) = 0. So zT (G(z) - Glin(z))
= -4sgn(u^4)||z||4E[(x)h4(zT x/||z||)] + 4||z||4(E[(x)h2(zT x/||z||)])2 + 8||z||4E[(x)h2(zT x/||z||)]E[flin(x)h2(zT x/||z||)] - 4||z||2E[(x)h2(zT x/||z||)]
We know that E[flin(x)h2(zT x/||z||)] has a factor of  giving us using Lemma 14: |zT (G(z) - Glin(z))|  O(log d)d-(1+p)+3||z||4.
So zT G(z) is also (||z||4). so ||G(z)||  (1)d-

Proof of Claim 2. We have G - Glin as follows, G(z) - Glin(z)
= -sgn(u^4)E[(flin(x) + (x))H4(zT x)] + (E[(flin(x) + (x))H2(zT x)] - u^2)2
+ sgn(u^4)E[(flin(x))H4(zT x)] - (E[(flin(x))H2(zT x)] - u^2)2
= -sgn(u^4)E[(x)H4(zT x)] + (E[(x)H2(zT x)])2
+ 2E[(x)H2(zT x)]E[flin(x)H2(zT x) - u^2] Thus we have,
(G(z) - Glin(z))
= -sgn(u^4)E[(x)H4(zT x)] + 2E[(x)H2(zT x)]E[(x)H2(zT x)]
+ 2E[flin(x)H2(zT x) - u^2]E[(x)H2(zT x)]
+ 2E[(x)H2(zT x)]E[flin(x)H2(zT x)] Observe that H2 and H4 are degree 2 and 4 (respectively) polynomials thus norm of gradient and hessian of the same can be bounded by at most O(||z||||x||4). Using Lemma 14 we can bound each term by roughly O(log d)d-(1+p)+3||z||4. Note that  being large does not hurt as it is scaled appropriately in each term. Subsequently, using Lemma 15, we can show that ||z|| is bounded by a constant since ||G(z)||  d-2. Similar analysis holds for the hessian too.
Now applying Theorem 4 gives us that z is an (O(log d)d-(1+p)+3, O(log d)d-(1+p)+3)approximate local minima of Glin. This implies that it is also an ( := C log(d)d-(1+2p)+3,  := C log(d)d-(1+2p/3)+3)-approximate local minima of Glin for large enough C > 0 by increasing  . Observe that  3/|u^4| = C3/2 log3/2(d)d-(3/2+p)+9/2/d-/2 = C3/2 log3/2(d)d-(1+p)+9/2  . Now using Claim 1, we get the required result.

B.4 SIMULTANEOUS RECOVERY

Ge et al. (2017) also showed simultaneous recovery by minimizing the following loss function Glin defined below has a well-behaved landscape.







Glin(W) = E flin(x)

(wj, wk, x) - E flin(x) H4(wjT x)

j,k[d],j=k

j[d]

(1)

16

Under review as a conference paper at ICLR 2019

+  E flin(x)H2(wiT x) - u^2 2
i

(2)

where (v, w, x) = H2(vT x)H2(wT x) + 2(vT w)2 + 4(vT x)(wT x)vT w.

They gave the following result.

Theorem 10 (Ge et al. (2017)). Let c be a sufficiently small universal constant (e.g. c = 0.01

suffices), and suppose the activation function u satisfies u^4 = 0. Assume  and W be the true weight matrix. The function Glin satisfies the following:



c, 



(|u^4|/u^22),

1. Any saddle point W has a strictly negative curvature in the sense that min(2Glin(W))  -0 where 0 = c min{|u^4|/d, u^22}.
2. Suppose W is an ( , 0)-approximate local minimum, then W can be written as W-T = PDW + E where D is a diagonal matrix with Dii  {±1 ± O(|u^4|/u^22) ± O( /)}, P is a permutation matrix, and the error term ||E||  O( d/u^4).

We show that this minimization is robust. Let us consider the corresponding function G to Glin with the additional non-linear terms as follows:







G(W) = E f (x)

(wj, wd, x) - E f (x) H4(wj, x)

j,k[d],j=k

j[d]

+  (E [f (x)H2(wi, x)] - u^2)2
i

Now we can show that G and Glin are close as in the one-by-one case.
R(W) := G(W) - Glin(W)
= E [(x)A(W, x)] - E [(x)B(W, x)] +  E [f (x)C(W, x)]2 - E [flin(x)C(W, x)]2
= E [(x)A(W, x)] - E [(x)B(W, x)] + E [((x)C(W, x)(f (x ) + flin(x ))C(W, x )] = E [(x)A(W, x)] - E [(x)B(W, x)] + E [((x)D(W, x)] = E [(x)(A(W, x) - B(W, x) + D(W, x))] = E [(x)L(W, x)]
where A(W, x) = j,k[d],j=k (wj , wd, x), B(W, x) = j[d] H4(wj , x), C(W, x) = i H2(wi, x), D(W, x) = C(W, x)E[(f (x ) + flin(x ))C(W, x )] and L(W, x) = A(W, x) -
B(W, x) + D(W, x).
Using similar analysis as the one-by-one case, we can show the required closeness. It is easy to see that ||L|| and ||2L|| will be bounded above by a constant degree polynomial in O(log d)d-(1+p)+3 max ||wi||4. No row can have large weight as if any row is large, then looking at the gradient for that row, it reduces to the one-by-one case, and there it can not be larger than a constant. Thus we have the same closeness as in the one-by-one case. Combining this with Theorem 10 and 4, we have the following theorem:
Theorem 11. Let c be a sufficiently small universal constant (e.g. c = 0.01 suffices), and under Assumptions 1, 2 and 3. Assume   c,  = (d), and W be the true weight matrix. The function G satisfies the following
1. Any saddle point W has a strictly negative curvature in the sense that min(2Glin(W))  - where 0 = O(log d)d-(1).
2. Suppose W is a (d-(1), d-(1))-approximate local minimum, then W can be written as W-T = PDW + E where D is a diagonal matrix with Dii  {±1 ± O() ± d-(1))}, P is a permutation matrix, and the error term ||E||  O(log d)d-(1).

Using standard optimization techniques we can find a local minima.

17

Under review as a conference paper at ICLR 2019

B.5 APPROXIMATE TO ARBITRARY CLOSE
Lemma 16. If u is the sign function then E[u(wT x) (zT x)] = c| cot()| where w, z are unit vectors and  is the angle between them and c is some constant.
Proof. WLOG we can work the in the plane spanned by z and w and assume that z is the vector i along and w = i cos  + j sin . Thus we can replace the vector x by ix + jy where x, y are normally distributed scalars. Also note that u =  (Dirac delta function).
E[u(wT x) (zT x)] = E[u(x cos  + y sin ) (x)]
= u(x cos  + y sin ) (x)(x)(y)dxdy
yx
Using the fact that x  (x)h(x)dx = h (0) this becomes
= (y)[(/x)u(x cos  + y sin )(x)]x=0dy
y
= (y)[n(x)u (x cos  + y sin ) cos  +  (x)u(x cos  + y sin )]x=0dy
y 
= (y)(0)(y sin ) cos dy
y=-
Substituting s = y sin  this becomes
/ sin 
= (s/ sin )(0)(s) cos (1/ sin )ds
s=-/ sin 
=sgn(sin ) cot()(0) (s/ sin )(s)ds
s
=| cot()|(0)(0)

Proof of Lemma 4. Let us compute the following probabilities first:

P r[x  l(z, t, 0)] = Pr[zT x = t] =

Pr

[g = t] =  1

e-

t2 2||z||2

gN (0,||z||2)

2||z||

Also,

P r[xT w1  t and x  l(z, t , 0)]

1

=

(2)

n 2

sgn(x1

-

t)1[zT

x

=

t

]e-

||x||2 2

dx

x

1

=

(2)

1 2

e

-

x21 2

x1 =t

11

(2)

n-1 2

x-1

[z-T 1x-1 = t

-

z1x1]e-

||x-1 ||2 2

dx-1

1

=

(2)

1 2



e-

x21 2

P r[x-1



l(z-1, t

-

z1x1,

0)]dx-1

x1 =t

1 = 2||z-1||

e e dx

-

x12 2

-

(t -z1x1)2 2||z-1 ||2

1

x1 =t

= 1 e-

t2 2||z||2

2||z-1||

-
e
x1 =t

x1

-

t z1 ||z||2

2

||z-1 ||2 ||z||2

2
dx1

= 1

e -

t2 2||z||2

c

t||z||2 - t z1)

2||z||

||z-1| |||z||

dx1

18

Under review as a conference paper at ICLR 2019

= 1

e-

t2 2

c

t - t cos(1)

2 | sin(1)|

Bound depends on t , if too large, then it can be negative.

Observe that

t

P r[xT w1  t and x  l(z, t , )] =

P r[xT w1  t and x  l(z, a, )]da

t-

= P r[xT w1  t and x  l(z, t, )]

=

e-

t 2 2

c

t - t cos(1)

2 | sin(1)|

where the second equality follows by mean value theorem for some t  [t - , t ].

Similarly,

P r[x  l(z, t , )] = 

e-

t¯2 2||z||2

2||z||

for some t¯  [t - , t ].

P r[xT w1  t and x  l(z, t , )|x  l(z, t, )]

= e -

t

2 -t¯2 2

c

t - t cos(1)

= c

t - t cos(1)

| sin(1)|

| sin(1)|

± O( )t

for  1/t .

Proof of Lemma 5. Recall that P is monotone with positive linear term, thus for high threshold u (0 unless input exceeds t and positive after) we have sgn(f (x)) = sgn(xT wi - t). This is because, for any i, P applied to Xi > 0 and j = i, Xj = 0 gives us ci which is positive. Also, P (0) = 0. Thus, sgn(P ) is 1 if any of the inputs are positive. Using this, we have,

P r[sgn(f (x))|x  l(z, t , )]  P r[sgn((w1)T x - t)|x  l(z, t , )]

Also,

P r[sgn(f (x))|x  l(z, t , )]
 P r[sgn(xT wi - t)|x  l(z, t , )]
= P r[sgn((w1)T x - t)|x  l(z, t , )] + P r[sgn(xT wi - t)|x  l(z, t , )]
i=1
 P r[sgn((w1)T x - t)|x  l(z, t, )] + 
where i=1 P r[sgn(xT wi - t)|x  l(z, t , )]  . We will show that  is not large since a z is close to one of the vectors, it can not be close to the others thus i will be large for all i = j. Let us bound ,

P r[sgn(xT wi - t)|x  l(z, t , )] 
i=1 i=1

c

t - ti cos(i) | sin(i)|

+ O( )ti


i=1

c

t - ti cos(i) | sin(i)|

+ O( )t


i=1

c

t - t cos(i) | sin(i)|

+ O( )t



1

e-

i2 2

+ O(

)kt

i=1 2i

19

Under review as a conference paper at ICLR 2019

where i

=

t-t cos(i | sin(i)|

)

.

The above

follows since i



0 by assumption

on t

.

Under

the assumption,

let  = maxi=1 cos(i) we have

t i 

1

-

 cos(1 )

1 - 2

= (t)

under our setting. Thus we have,

P r[sgn(xT wi - t))|x  l(z, t , )]  de-(t2) + O( )dt = de-(t2)
i=1

for small enough .

Proof of Lemma 6. Let us assume that < c/t for sufficiently small constant c, then we have that

0.6 = P r[sgn(f (x)) and x  l(z, t2, )|x  l(z, t2, )]

 P r[xT w1  t and x  l(z, t2, )|x  l(z, t2, )]

 c

t - t cos(1) | sin(1)|

- 0.1

=

0.7

 c

t - t cos(1) | sin(1)|

= (c)-1(0.7)  t - t cos(1) | sin(1)|

=

t2

 t - (c)-1(0.7) sin(1) + O(1)  t + O(1)

cos(1)

cos()

Similarly for t1. Now we need to argue that t1, t2  0. Observe that
P r[sgn(f (x)) and x  l(z, 0, )|x  l(z, 0, )]
 P r[xT wi  t and x  l(z, 0, )|x  l(z, 0, )]
= c t - cos(1) + O( 2)d  de-(t2) < 0.4 | sin(1)| 
Thus for sufficiently large t = ( log d), this will be less than 0.4. Hence there will be some t1, t2  0 with probability evaluating to 0.4 since the probability is an almost increasing function of t up to small noise in the given range (see proof of Lemma 5).

Proof of Lemma 7. Let V be the plane spanned by w1 and z and let v1 = w1 and v2 be the basis of this space. Thus, we can write z = cos()v1 + sin()v2.

Let us apply a Gaussian perturbation  along the tangential hyperplane normal to z. Say it has
distribution N (0, 1) along any direction tangential to the vector z. Let 1 be the component of  on to V and let 2 be the component perpendicular to it. We can write the perturbation as  = 1(sin()v1 - cos()v2) + 2v3 where v3 is orthogonal to both v1 and v2.

So the new angle  of z after the perturbation is given by

cos( ) = v1T (z + ) ||z + ||

cos() + =

1 sin()

1 + ||||2

Note that with constant probability1  as  is a Gaussian variable with standard deviation . And with high probability |||| < O( d - 1). We will set = (sin()/d) = (/d). Thus with constant probability:

cos( )  cos() + sin() 1 + O( 2d)

20

Under review as a conference paper at ICLR 2019

 (cos() + sin())(1 - O( 2d))  cos() + ( sin()) - O( 2d)  cos() + ( sin()).
Thus change in cos() is given by  cos()  ( sin()). Now change in the angle  satisfies by the Mean Value Theorem:
d  cos() =  cos(x)
dx x[, ] 1
= - =  cos() sin(x) x[, ]
 ( sin()) = ( ) = (/d). sin()

B.6 LEARNING UNION OF HALFSPACES FAR FROM THE ORIGIN 
Theorem 12. Given non-noisy labels from a union of halfspaces that are at a distance ( log d) and are each a constant angle apart, there is an algorithm to recover the underlying weights to closeness in polynomial time.
Proof. Observe that Xi is equivalent to P (X1, ·, Xd) = 1 - (1 - Xi). Thus
f (x) = sgn(xT wi - t) = 1 - (1 - sgn(xT wi - t)). 
Since P and sgn here satisfies our assumptions 1, 2, for t = ( log d) (see Lemma 11) we can apply Theorem 11 to recover the vectors wi approximately. Subsequently, refining to arbitrarily close using Theorem 5 is possible due to the monotonicity. Thus we can recover the vectors to arbitrary closeness in polynomial time.

B.7 SIGMOID ACTIVATIONS
Observe that for sigmoid activation, Assumption 2 is satisfied for (t, ) = e-t+2/2. Thus to satisfy Assumption 3, we need t = ( log d).
Note that for such value of t, the probability of the threshold being crossed is small. To avoid this we further assume that f is non-negative and we have access to an oracle that biases the samples towards larger values of f ; that after x is drawn from the Gaussian distribution, it retains the sample (x, f (x)) with probability proportional to f (x) ­ so P r[x] in the new distribution. This enables us to compute correlations even if ExN~(0,I[f (x)] is small. In particular by computing E[h(x)] from this distribution, we are obtaining E[f (x)h(x)]/E[f (x)] in the original distribution. Thus we can compute correlations that are scaled.
We get our approximate theorem: Theorem 13. For t = (log d), columns of (TW)-1 can be recovered within error 1/poly(d) using the algorithm in polynomial time.

B.8 POLYNOMIALS P WITH HIGHER DEGREE IN ONE VARIABLE

In the main section we assumed that the polynomial has degree at most 1 in each variable. Let us give a high level overview of how to extend this to the case where each variable is allowed a constant degree (say at most D). Assume,

d

P (X1, . . . , Xd) =

cr Xiri

rZd+ i=1

If P has a higher degree in Xi then Assumption 2 changes to a more complex (stronger) condition. Let qi(x) = rZd+|j=i,rj=0 crxri , that is P obtained by setting all Xj for j = i to 0.

21

Under review as a conference paper at ICLR 2019

Assumption 4. EgN(0,2)[(ut(g))r]  (t, ) for r  D. E[qi(ut(g))hk(g))] = t(1)(t, 1) for k = 2, 4. Also |cr| = O(1).

Let us collect the univariate terms Puni(X) =

d i=1

qi(Xi).

Corresponding to the same we get

funi. This will correspond to the flin we had before. Note that the difference now is that instead of

being the same activation for each weight vector, now we have different ones qi for each. Using H4

correlation as before, now we get that:

dd
E[funi(x)H4(zT x)] = qi  ut4(zT wi)4 and E[funi(x)H2(zT x)] = qi  ut2(zT wi)2
i=1 i=1
where qi  ut are hermite coefficients for qi  ut. Now the assumption guarantees that these are positive which is what we had in the degree 1 case.

Second we need to show that even with higher degree, E[|f (x) - funi(x)|] is small. Observe that Lemma 17. For r such that ||r||0 > 1, under Assumption 4 we have,
d
E ut((wj)T x) ri  (t, 1)O ((t, ||W||))||r||0-1 .
i=1

The proof essentially uses the same idea, except that now the dependence is not on ||r||1 but only the number of non-zero entries (number of different weight vectors). With this bound, we can now
bound the deviation in expectation.
Lemma 18. Let (x) = f (x) - funi(x). Under Assumptions 4, if t is such that d(t, ||W||)  c for some small enough constant c > 0 we have, E[|(x)|]  O dD+3(t, 1)(t, ||W||) .

Proof. We have,





E[|(x)|]

=

E

 

d

cr

ut((wj)T x)

ri

  

 rZd+
ri D,||r||0 >1

i=1



 |cr|E
rZd+ ri D,||r||0 >1

d
ut((wj)T x) ri
i=1

= |cr|(t, 1) ((t, ||W||))||r||0-1

rZ+d ri D,||r||0 >1

d
 C diiD(t, 1) ((t, ||W||))i-1

i=1
 CdD+3(t, 1)(t, ||W||).

Thus as before, if we choose t appropriately, we get the required results. Similar ideas can be used to extend to non-constant degree under stronger conditions on the coefficients.

C SAMPLE COMPLEXITY

Proof of Lemma 3. C1(f, z, s) = E[f (x)(zT x - s)] = x f (x)(zT x - s)(x)dx Let x0 be the component of x along z and y be the component along z. So x = x0z^ + yz. Interpreting x as a function of x0 and y:

C1(f, z, s) =

f (x)(x0 - s)(x0)(y)dx0dy

y x0

22

Under review as a conference paper at ICLR 2019

= [f (x)]x0=s(y)dy
y
= (s)E[f (x)|x0 = s] = (zT x = s)E[f (x)|x0 = s]
where the second equality follows from x (x - a)f (x) = [f (x)]x=a.

Proof of Claim 4. Let x0 be the component of x along z and y be the component of x in the space

orthogonal to z.

Let z^ denote a unit vector along z.

We have x

=

x0z^ + y and

x x0

= z^.

So,

correlation can be computed as follows:

E[P [x]. (zT x - s)] = (y)  (x0 - s)P [x](x0)dx0dy
y x0

Since

x



(x

-

a)f (x)dx

=

[

df dx

](x

=

a)

this

implies:

E[P [x] (zT x - s)]



= P [x](x) dy

y x0

x0 =s

=

y

(x0)

i

P . Xi Xi x0

+

P [x]

(x0))

(y)dy

x0 =s

=
i

y

(x0

)

P Xi

.

Xi x0

(y)dy +  (s)
x0 =s

P [x](y)dy
y

Note that

Xi x0

=

 x0

u(xT

wi

-

t)

=

u (xT wi - t)z^T wi.

If u is the sign function then u (x)

=

(x). So focusing on one summand in the sum we get

y

(x0)

P Xi

.

Xi x0

(y)dy
x0 =s

=

[(x0)u
y

(xT

wi

-

t)(z^T

wi)

P Xi

]x0=s(y)dy

=

y

(z^T

wi)[(x0)(z^T

wix0

+

(wi)T

y

-

t)

P Xi

]x0=s(y)dy

= (z^T wi)

y

(s)(s(w1

)T

z^

+

(w1

)T

y

-

t)

P Xi

(y)dy

Again let y component

= y0 of wi

(wi) to z.

+ z where Interpreting

z x

is =

perpendicular tz^ + y0(w1)

to wi + z as

and z. And a function of

(wi) is perpendicular y0, z we get:

= (z^T wi)

z

y0

(s)(s(w1)T

z^

+

((w1)T

(w1)

)y0

-

t)(y0)(z)

P Xi

dy0dz

Note that by substituting v = ax we get

 x=-

f

(x)(ax

-

b)dx

=

/a x=-/a

f

(x)(ax

-

b)dx

=

sgn(a)

1 a

f

(

b a

)

=

1 |a|

f

(

b a

).

So

this

becomes:

=

z^T wi |(wi)T (wi)

|

P

z

(s)[(y0)

Xi

]
y0 =

t-sz^T wi (wi )T (wi )

(z)dz

23

Under review as a conference paper at ICLR 2019

=

z^T wi |(wi)T (wi)

|

P

z

[(y0

)(x0

)(z)



Xi

]
x0

=s,y0

=

t-sz^T wi (wi )T (wi )

dz

=

z^T wi |(wi)T (wi)

|

y0

=

t - sz^T wi (wi)T (wi)

P

(x0 = t)

[(z) X ] dzz

i

x0

=t,y0

=

t-sz^T wi (wi )T (wi )

=

z^T wi |(wi)T (wi)

|

P

[(x) X ] dzz

i

x0

=t,y0

=

t-sz^T wi (wi )T (wi )

=

z^T wi |(wi)T (wi)

|

P

(x) dx

x:zT x=s,xT wi=t

Xi

=

z^T wi |(wi)T (wi)

(zT x |

=

s, xT wi

=

t)E[

P Xi

|xT

z

=

s, xT wi

=

t].

Let i be the angle between z and wi. Then this is

=

|

cot(i)|(zT

x

=

t,

(wi)T

x

=

P t)E[ Xi

]|xT

z

=

s,

xT

wi

=

t]

Thus, overall correlation

=

iS

|

cot(i)|(zT x

=

s, xT wi

=

P t)E[ Xi

|xT

z

=

s,

xT

wi

=

t]

+  (s)E[P [x]|xT z = s]

Proof of Claim 5. Note that for small , | cot | = O(1/)  O(1/ 2). Since P (X)  ||X||c1 , we have f (x)  dc1 (as with sgn function each sgn((wi)T x)  1) and all Qi[x], Ri[x] are at most 2dc1 .
By Cauchy's mean value theorem C2(f, z, t) = 2 [E[f (x) (zT x = s)]]st± . Note that cot(i)(zT x = t, (wi)T x = t) = cot(i)(t tan(i/2)) which is a decreasing function of i in the range [0, ] So if all i are upper bounded by 2 then by above corollary,
C2(f, z, s)  2 n cot( 2)(zT x = t, (w1)T x = t)(2dc1 ) + (2dc1 ) = 2 n cot( 2)(t tan( 2/2))(2dc1 ) + (2dc1 )
 dO(1).
2

Observe that the above proof does not really depend on P and holds for for any polynomial of u((wi)T x) as long as the polynomial is bounded and the wi are far off from z.
Proof Of Lemma 8. If z = wi, then C2(f, z, t) = E[f (x)((zT x - t - ) - (zT x - t + ))] = E[u((wi)T x)Qi[x]((zT x - t - ) - (zT x - t + ))] + E[Ri[x]((zT x - t - ) - (zT x - t + ))]
Since u((wi)T x) = 0 for zT x = t - and 1 for zT x = t + , and using the Cauchy mean value theorem for the second term this is
= E[Qi[x](zT x - t - )] + 2 [E[Ri[x] (zT x - s1)]s1t± = E[Qi[x](zT x - t)] + E[Ri[x]((zT x - t - ) - (zT x - t))] = (t)E[Qi[x]|zT x = t + ] + [C2(Qi, z, s2)]s2[t,t+ ] + 2 [C2(Ri, z, s)]st±
24

Under review as a conference paper at ICLR 2019

= (t)E[Qi[x]|zT x = t] + dO(1)

The last from z

step follows = wi and

from Claim 5 wi is absent

applied on from both

Qi and Ri as Qi and Ri.

all the Also

directions of wj are well separated the corresponding Qi and Ri are

bounded.

C.1 RELU ACTIVATION

If u is the RELU activation, the high level idea is to use correlation with the second derivative  of the Dirac delta function instead of  . More precisely we will compute C3(f, z, s) = E[f.( (zT x- s - ) -  (zT x - s + )]. Although we show the analysis only for the RELU activation, the same
idea works for any activation that has non-zero derivative at 0.

Note that now u = sgn and u = .

For ReLU activation, Lemma 8 gets replaced by the following Lemma. The rest of the argument is as for the sgn activation. We will need to assume that P has constant degree and sum of absolute value of all coefficients is poly(d)

Lemma 19. Assuming polynomial P has constant degree, and sum of the magnitude of

all coefficients is at most poly(d), if z

=

wi then C3(f, z, t)

=

E[(t)



 Xi

P

+

(t)

j=i

cos(j )sgn(xT

wi

-

t)



 Xj

P

+



(t)P |xT

wi

=

t]

+

dO(1). Otherwise if all angles i

between z and wi are at least 2 it is at most dO(1)/ 2.

We will prove the above lemma in the rest of this section. First we will show that z is far from any of the wi's then E[P. (zT x - s)] is bounded.
Lemma 20. If the sum of the absolute value of the coefficients of P is bounded by poly(d), its degree is at most constant, i > 2 then E[P. (zT x - s)] is dO(1)/ 2.

Proof. Let x0 be the component of x along z and y be the component of x in the space orthogonal

to z

as before.

We have x

=

x0z^ + y

and

x x0

=

z^.

We will look at monomials Ml

in P

=

l Ml.

As before since

x  (x - a)f (x)dx =

d2 f dx2

we get
x=a

E[f (x). (zT x - s)] = = =

f (x)(x) (zT x - s)dy

y

2

y

x02 (P [x](x))

dy
x0 =s

2

l

y

x20 (Ml[x](x0))

(y)dy
x0 =s

Now consider a monomial M = X1i1 ..Xkik .

Take the symbolic second derivative

2  x02

of M [x](x0) w.r.t x0.

This will produce a polynomial

involving Xi's,

Xi x0

,

 2 Xi  x20

,

,



,



.

Let us examine each of these terms.

 x0 Xi[x]

=

 x0

u(xT

wi

-

t)

=

sgn(xT

wi

-

t)(wi)T

 x0

x

= sgn(xT wi - t)((wi)T z)

= cos(i)sgn(xT wi - t)

Thus

 x0

Xi

[x]

is

a

bounded

function

of

x.

We

have

2 x20 Xi(x)

=

 x0

sgn(xT

wi

- t)((wi)T z)

=

cos2(i)(xT wi

- t).

25

Under review as a conference paper at ICLR 2019

Again as before

[(xT wi - t)g(x)(x0)(y)]x0=sdy
y
= (1/| sin(i)|)E[g(x)|x0 = s, xT wi = t](x0 = s, xT wi = t)

Note that if the degree is bounded, since | sin(i)| is at least 2 expected value of each monomial obtained is bounded. So the total correlation is poly(d)/ 2.

Proof of Lemma 20. As in the case of sgn activation, if z = wi, E[P [x] (xT wi - s)]

= P [x](x) (zT x - s)dx

x



= (P [x](x)) dy

y x0

x0 =s



=
yj

P Xj

[x]



Xj [x] x0

(x)

+

P

[x]

(x) x0



dy

x0 =s



= (y) sgn(x0 - t)(s)
y

P Xi

[x] + sgn(xT wj - t) cos(j)(s)
j=i

P Xj

[x] + P  (x0) dy
x0 =s



= (y) sgn(s - t)(s)
y

P Xi

[x] + sgn(xT wj - t) cos(j)(s)
j=i

P Xj

[x] + P  (x0) dy
x0 =s

For s = t + this is



= (y) (s)
y

P Xi

[x] + cos(j)sgn(xT wj - t)(s)
j=i

P Xj

[x] + P  (s) dy
s=t+





=  (y) (t)
y

P Xi

[x] + cos(j)sgn(xT wj - t)(t)
j=i

P Xj

[x] + P  (t) dy + dO(1)



=E (t)

P Xi

[x] + (t) cos(j)sgn(xT wi - t)
j=i

P Xj

[x] +  (t)P xT wi = t + dO(1)

If z is away from every wi by at least 2 then again E[P C3(f, z, t)] = E[P  (xT wi - s)](s  [t - , t + ]) = dO(1)/ 2.

D STRUCTURAL RESTRICTIONS HELPS LEARNING

D.1 PROOF OF THEOREM 7

To construct this correlation graph, we will run the following Algorithm 3 Denote Ti := {j : wij = 1}. Let us compute E[f (x)xixj]:

  

E[f (x)xixj] =

cSE  u(xT wp) xixj

S[d]

pS

= cS e-t|S|E e
S[d]

x xpS xT wp i j

26

Under review as a conference paper at ICLR 2019

Algorithm 3 ConstructCorrelationGraph
1: Let G be an undirected graph on n vertices each corresponding to the xi's. 2: for every pair i, j do 3: Compute ij = E[f (x)xixj]. 4: if ij   then 5: Add edge (i, j) to the graph.

= cS e-t|S|E e
S[d]

x xqpS Tp xq i j

= cS e-t|S|1[i, j  pS Tp]E [xiexi ] E [xj exj ]

E [exq ]

S[d]

qpS Tp\{i,j}

= cS e-t|S|1[i, j  pS Tp]2e2 E [xj exj ]

e2/2

S[d]

qpS Tp\{i,j}

1=

cS e-t|S|

[i, j



pS

Tp

]2

e

2

|pS
2

Tp

|

S[d]

By assumption, for all p, Tp are disjoint. Now, if i, j  Tr for some r, we have

E[f (x)xixj] = 2

-|S|(t-d)
cS e 2

S[d]:rS

Similarly, if i  Tr1 and j  Tr2 with r1 = r2, we have

E[f (x)xixj] = 2

-|S|(t-d)
cS e 2

S [d]:r1 ,r2 S

It is easy to see that these correspond to coefficients of Xr and Xr1 Xr2 (respectively) in the following polynomial:
Q(X1, . . . , Xn) = 2P (µ(X1 + 1), . . . , µ(Xn + 1))
-(t-d)
for µ = e 2 . For completeness we show that this is true. We have,

Q(X1, . . . , Xn) = 2

cS µ(Xj + 1)

S[d] jS

= 2

cS µ|S| (Xj + 1)

S[d]

jS

The coefficient of Xr in the above form is clearly 2 S[d]:rS cSµ|S| (corresponds to picking the 1 in each product term). Similarly coefficient of Xr1 Xr2 is 2 S[d]:r1,r2S cSµ|S|.
Now as in the assumptions, if we have a gap between these coefficients, then we can separate the large values from the small ones and form the graph of cliques. Each clique will correspond to the corresponding weight vector.

D.2 PROOF OF THEOREM 8

Consider f (x) = cS iS exT wi where wi = jSi ej for Si  [n] such that for all i = j,

Si  Sj

= n and cS

 0.

Let

us

compute

g(z)

=

e E-

||z||2 2

f (x)ezT x

where z =

ziei for

some i.

E f (x)ezT x = E

c eS jSi xj
iS

ezT x

27

Under review as a conference paper at ICLR 2019

= = = = = = g(z) =

cSE e
iS


pSi xp

e q[n] zq xq 



cSE 

e(1+zp)xp  

ezqxq 

piS Si

q[n]\iS Si

  

cS 

E e(1+zp)xp ]  

E [ezqxq ]

piS Si

q[n]\iS Si

 cS 


(1+zp )2
e 2 


zq2
e2

piS Si

q[n]\iS Si

||z||2
cS e 2

e1 2

+zp

piS Si

cS

e1 2

+zp

piS Si

Consider the following optimization problem:

max
z

g(z) - ||z||1 - ||z||22

h(z)

for ,  > 0 to be fixed later.

We can assume that zi  0 for all i at a local maxima else we can move in the direction of ei and this will not decrease g(z) since cS  0 for all S and will decrease ||z||1 and ||z||2 making h(z)
larger. From now on, we assume this for any z at local maxima.

We will show that the local maximas of the above problem will have z such that most of the mass is equally divided among j  Si for some i and close to 0 everywhere else.

Lemma 21. There exists at most one i such that there exists j  Si with |zj|   for  satisfying

4 < mini=j e

c e |iS Si|

S:iS,jSiS,jS S

2

.

Proof. Let us prove by contradiction. Assume that there is a local maxima such that there are at
least 2 indices say 1, 2 such that j  S1, k  S2, |zj|, |zk|  . Now we will show that there
exists a perturbation such that g(z) can be improved. Now consider the following perturbation, z + s ej - s ek for s  {±1}. Observe that ||z||1 remains unchanged for <  also ||z||22 changes by 2s2 2 + 2(zj - zk)s . We have

Es[h(z + s ej - s ek) - h(z)]

=

cS

e1 2

+zp

(Es [es

] - 1) +

cS

e1 2

+zp

Es

e-s

S:1S,2S piS Si

S:1S,2S piS Si

- Es 2 2 + 2(zj - zk)s



2

 2 -4 +

cS

e1 2

+zp

+

cS

e

1 2

+zp



S:1S,2S piS Si

S:1S,2S piS Si

-1

The

inequality

follows

since

E [es

]

=

e

+e- 2

1+

2
2 . Observe that

cS

e1 2

+zp



c e e S

|iS Si| 2

zj

c e e .S

|iS Si| 2



S:1S,2S piS Si

S:1S,2S

S:1S,2S

For chosen value of , there will always be an improvement, hence can not be a local maxima.

28

Under review as a conference paper at ICLR 2019

Lemma 22. At the local maxima, for all i  [n], z is such that for all j, k  Si, zj = zk at local maxima.

Proof. We prove by contradiction. Suppose there exists j, k such that zj < zk. Consider the following perturbation: z + (zk - zj)(ej - ek) for 1  > 0. Observe that g(z) depends on
only rSi zr and since that remains constant by this update g(z) does not change. Also note that ||z||1 does not change. However ||z||22 decreases by 2 (1 - )(zk - zj)2 implying that overall h(z) increases. Thus there is a direction of improvement and thus it can not be a local maxima.

Lemma 23. At the local maxima, ||z||1   for  <

c eS

|iS Si| Sn

|iS Si| 2

- (2 + 1).

Proof. We prove by contradiction. Suppose ||z||1 < , consider the following perturbation, z + 1.
Then we have

h(z + 1) - h(z) =

c eS

|iS 2

Si

|

+

piSSi zp (e |iSSi| - 1) - n - n (2||z||1 + )

>

c e ||iS Si|
S2

iS

Si|

- n

- n (2 + 1)

For given  there is a direction of improvement giving a contradiction that this is the local maxima.
Combining the above, we have that we can choose ,  = poly(n, 1/ , s) where s is a paramater that depends on structure of f such that at any local maxima there exists i such that for all j  Si, zj  1 and for all k  jSi , zk  .

D.3 PROOF OF THEOREM 9

Let function f = S[n] cS iS u((wi)T x) for orthonormal wi. WLOG, assume wi = ei.

Let u(x)

 i=1

u^2i

= h2i(x).

 i=0

u^2i

h2i(x)

where

hi

This implies E[u (x)] =

are hermite polynomials 0. Observe that,

and

u

(x)

=

u(x) - u^0

=

|S|

u(xi) = (u (xi) + u^0) = u^|0S|-k

u (xi).

iS iS

k=0

S S:|S |=k iS

Let us consider correlation with h4(zT x). This above can be further simplified by observing that when we correlate with h4, iS u (xi)h4(zT x) = 0 for |S |  2. Observe that h4(zT x) =
d1,...,dn[4]: di4 c(d1, . . . , dn) hdi (xi) for some coefficients c which are functions of z. Thus when we correlate iS u (xi)h4(zT x) for |S |  3 then we can only get a non-zero term if we have at least h2k(xi) with k  1 for all i  S . This is not possible for |S |  3, hence, these terms
are 0. Thus,

2
E u(xi)h4(zT x) = u^|0S|-k

E u (xi)h4(zT x) .

iS

k=0

S S:|S |=k iS

Lets compute these correlations. E u (xi)h4(zT x)

=E

u^2ph2p(xi)h4(zixi + z-T ix-i)

p>0

1 =
4

4
u^2pE h2p(xi)

4 k

 h4-k(zixi 2)hk(z-T ij x-ij 2)

p>0

k=0

1 =
4

 u^2pE h2p(xi) h4(zixi 2) + 6h2(zixi 2)(2||z-i||2 - 1) + 3(2||z-i||2 - 1)2

p>0

29

Under review as a conference paper at ICLR 2019

1 =
4





u^4(4, 4, zi 2) + u^2(4, 2, zi 2) + 6u^2(2, 2, zi 2)(2||z-i||2 - 1)

1 =
4

4u^4zi4 + 12u^2zi2(2zi2 - 1) + 6u^2(2zi2)(2||z-i||2 - 1)

= u^4zi4 + 3u^2zi2(2||z||2 - 1)

E u (xi)u (xj)h4(zT x)

=E

u^2pu^2qh2p(xi)h2q(xj )h4(zixi + zj xj + z-T ij x-ij )

p,q>0

1 =
4

4
u^2pu^2qE h2p(xi)h2q(xj )

4 k

 h4-k((zixi + zj xj ) 2)hk(z-T ij x-ij 2)

p,q>0

k=0

1 =
4

 u^2pu^2qE h2p(xi)h2q(xj ) h4((zixi + zj xj ) 2) + 6h2((zixi + zj xj ) 2)(2||z-ij ||2 - 1)

p,q>0

+3(2||z-ij||2 - 1)2

1 44

= 16

u^pu^q

k E [h2p(xi)h2q(xj )h4-k(2zixi)hk(2zj xj )]

p,q k=0

1

+

3 4

(2||z-ij

||2

-

1)

2
u^2pu^2q

2 k E [h2p(xi)h2q(xj )h2-k(2zixi)hk(2zj xj )]

p,q<0

k=0

2

We will compute 1 and 2 :

14 4

1= 16

k

u^2pu^2qE [h2p(xi)h4-k(2zixi)] E [h2q(xj )hk(2zj xj )]

k=0

p,q>0

14 4 =
16 k

u^2pu^2q(4 - k, 2p, 2zi)(k, 2q, 2zj)

k=0

p,q>0

=

3 8

u^22(2,

2,

2zi

)(2,

2,

2zj

)

= 6u^22zi2zj2

Similarly,

2

=

3 4

(2||z-ij

||2

-

1)

2
u^2pu^2q

2 k E [h2p(xi)h2q(xj )h2-k(2zixi)hk(2zj xj )]

p,q>0

k=0

=

3 4

(2||z-ij

||2

-

1)

2

2 k

u^2u^2(2 - k, 2, 2zi)(k, 2, 2zj) = 0.

k=0

Combining, we get

E u (xi)u (xj )h4(zT x) = 6u^22zi2zj2.

Further, taking correlation with f , we get:

E f (x)h4(zT x)

2
= cS u^0|S|-k

E

S[n] k=0

S S:|S |=k

u (xi)h4(zT x)
iS

30

Under review as a conference paper at ICLR 2019



= cSu^0|S|-2 u^0 (u^4zi4 + 3u^2zi2(2||z||2 - 1)) + 6u^22

zj2zk2 + constant

S[n]

iS

j=kS

nn
= izi4 + (2||z||2 - 1) izi2 +

ij zj2zk2 + constant

i=1 i=1 1i=jn

nn
= izi4 + izi2 +

ij zj2zk2 + constant

i=1 i=1 1i=jn

where i = u^4 S [n]|iS cS u^|0S |-1, i = 3u^2 6u^22 S [n]|i,jS cS u^|0S |-2.

S [n]|iS cS u^|0S |-1 and ij

=

If ij < i + j for all i, j then the local maximas of the above are exactly ei. To show that this holds, we prove by contradiction. Suppose there is a maxima where zi, zj = 0. Then consider the following second order change zi2  zi2 + s and zj2  zj2 - s where  min zi2, zj2 and s is 1 with probability 0.5 and -1 otherwise. Observe that the following change does not violate the constraint
and in expectation affects the objective as follows:

 = Es i(2s zi2 + 2) + j (-2s zi2 + 2) + is - j s + ij (s zj2 - s zi2 - 2) = (i + j - ij ) 2 > 0

Thus there is a direction in which we can improve and hence it can not be a maxima.

E MODULAR LEARNING BY DIVIDE AND CONQUER
Finally we point out how such high threshold layers could potentially facilitate the learning of deep functions f at any depth, not just at the lowest layer. Note that essentially for Lemma 2 to hold, outputs X1, ., Xd needn't be present after first layer but they could be at any layer. If there is any cut in the network that outputs X1, ...Xd, and if the upper layer functions can be modelled by a polynomial P , then again assuming the inputs Xi have some degree of independence one can get something similar to Lemma 2 that bounds the non-linear part of P . The main property we need is EiS[Xi]/E[Xj] < µ|S|-1 for a small enough µ = 1/poly(d) which is essentially replaces Lemma 1. Thus high threshold layers can essentially reduce the complexity of learning deep networks by making them roughly similar to a network with lower depth. Thus such a cut essentially divides the network into two simpler parts that can be learned separately making it amenable to a divide and conquer approach. If there a robust algorithm to learn the lower part of the network that output Xi, then by training the function f on that algorithm would recover the lower part of the network, having learned which one would be left with learning the remaining part P separately.
Remark 1. If there is a layer of high threshold nodes at an intermediate depth l, u is sign function, if outputs Xi at depth l satisfy the following type of independence property: EiS[Xi]/E[Xj] < µ|S|-1 for a small enough µ = 1/poly(d), if there is a robust algorithm to learn Xi from ciXi that can tolerate noise, then one can learn the nodes Xi, from a function P (X1, .., Xd)

31

