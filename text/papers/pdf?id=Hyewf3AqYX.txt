Under review as a conference paper at ICLR 2019
A FRANK-WOLFE FRAMEWORK FOR EFFICIENT AND EFFECTIVE ADVERSARIAL ATTACKS
Anonymous authors Paper under double-blind review
ABSTRACT
Depending on how much information an adversary can access to, adversarial attacks can be classified as white-box attack and black-box attack. In both cases, optimization-based attack algorithms can achieve relatively low distortions and high attack success rates. However, they usually suffer from poor time and query complexities, thereby limiting their practical usefulness. In this work, we focus on the problem of developing efficient and effective optimization-based adversarial attack algorithms. In particular, we propose a novel adversarial attack framework for both white-box and black-box settings based on the non-convex Frank-Wolfe algorithm. We show in theory that the proposed attack algorithms are efficient with an O(1/ T ) convergence rate, which, to our knowledge, is the first convergence rate analysis for the zeroth-order non-convex Frank-Wolfe type algorithm. The empirical results on attacking Inception V3 model with the ImageNet dataset also verify the efficiency and effectiveness of the proposed algorithms. They attain a 100% attack success rate in both white-box and black-box attacks, and are more time and query efficient than the state-of-the-art baseline algorithms.
1 INTRODUCTION
Deep Neural Networks (DNNs) have made many breakthroughs in different areas of artificial intelligence such as image classification (Krizhevsky et al., 2012; He et al., 2016), object detection (Ren et al., 2015; Girshick, 2015), and speech recognition (Mohamed et al., 2012; Bahdanau et al., 2016). However, recent studies show that deep neural networks can be vulnerable to adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2015) ­ a tiny perturbation on an image that is almost invisible to human eyes could mislead a well-trained image classifier towards misclassification. Soon later this is proved to be not a coincidence: similar phenomena have been observed in other problems such as speech recognition (Carlini et al., 2016), visual QA (Xu et al., 2017), image captioning (Chen et al., 2017a), machine translation (Cheng et al., 2018b), reinforcement learning (Pattanaik et al., 2018), and even on systems that operate in the physical world (Kurakin et al., 2016).
Depending on how much information an adversary can access to, adversarial attacks can be classified into two classes: white-box attack (Szegedy et al., 2013; Goodfellow et al., 2015) and black-box attack (Papernot et al., 2016a; Chen et al., 2017c). In the white-box setting, the adversary has full access to the target model, while in the black-box setting, the adversary can only access the input and output of the target model but not its internal configurations. Among the approaches proposed for white-box and black-box attacks, optimization-based methods (Carlini & Wagner, 2017; Chen et al., 2017b;c; Ilyas et al., 2018; Cheng et al., 2018a) are the most effective: they usually achieve relatively low distortions and high attack success rates. However, these methods are not that efficient. In the white-box setting, they need to solve constrained optimization problems (Carlini & Wagner, 2017), and are usually significantly slower than the gradient-based methods such as Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) or Iterative FGSM (I-FGM) (Kurakin et al., 2016). In the black-box setting, it becomes even more severe since they need to make coordinate-wise gradient estimations (Chen et al., 2017c). Therefore, a large number of queries are needed for them to perform a successful attack, especially when the data dimension is large. For example, attacking a 299 × 299 × 3 Imagenet image may take them hundreds of thousands of queries. This significantly limits their practical usefulness since they can be easily defeated by limiting the number of queries that an adversary can make to the target model.
In this study, we aim to examine the following questions in this study:
1

Under review as a conference paper at ICLR 2019

Can we improve the efficiency of the optimization-based attack algorithms? In other words, can we use less time and queries to conduct adversarial attacks?

In this work, we provide an affirmative answer to this question by proposing an efficient FrankWolfe optimization framework for both white-box and black-box attacks. In summary, we make the following main contributions:

· We propose a novel Frank-Wolfe based adversarial attack framework. The white-box at-

tack algorithm is an iterative first-order method which admits the fast gradient sign method

(FGSM) as the one-step special case. And the corresponding black-box attack algorithm

adopts zeroth-order optimization with two sensing vector options (either from the Eu-

clidean unit sphere or from the standard Gaussian distribution) provided.



· We show that the proposed white-box and black-box attack algorithms enjoy an O(1/ T )

convergence rate. To the best of our knowledge, we provide the first convergence rate

analysis for the zeroth-order non-convex Frank-Wolfe type algorithm.

· We show that the query complexity of the proposed black-box attack algorithm is linear in

data dimension d.

· Our empirical results on attacking Inception V3 model with the ImageNet dataset show that

(i) the proposed white-box attack algorithm is more efficient than all the baseline white-

box algorithms evaluated here, and (ii) the proposed black-box attack algorithm is highly

efficient and is also the only one algorithm that achieves a 100% attack success rate.

2 RELATED WORK
There is a large body of work on adversarial attacks. In this section, we review the most relevant work in both white-box and black-box attack settings, as well as the non-convex Frank-Wolfe optimization.
White-box Attacks: Szegedy et al. (2013) proposed to use box-constrained L-BFGS algorithm for conducting white-box attacks. Goodfellow et al. (2015) proposed the Fast Gradient Sign Method (FGSM) based on linearization of the network as a simple alternative to L-BFGS. Kurakin et al. (2016) proposed to iteratively perform one-step FGSM (Goodfellow et al., 2015) algorithm and clips the adversarial point back to the distortion limit after every iteration. It is called Basic Iterative Method (BIM) or I-FGM in the literature. Madry et al. (2018) showed that for the L norm case, BIM/I-FGM is equivalent to Projected Gradient Descent (PGD), which is a standard tool for constraint optimization. Papernot et al. (2016b) proposed JSMA to greedily attack the most significant pixel based on the Jacobian-based saliency map. Moosavi-Dezfooli et al. (2016) proposed attack methods by projecting the data to the closest separating hyperplane. Carlini & Wagner (2017) introduced the so-called CW attack by proposing multiple new loss functions for generating adversarial examples. Chen et al. (2017b) followed CW's framework and use an Elastic Net term as the distortion penalty.
Black-box Attacks: One popular family of black-box attacks (Hu & Tan, 2017; Papernot et al., 2016a; 2017) is based on the transferability of adversarial examples (Liu et al., 2018; Bhagoji et al., 2017), where an adversarial example generated for one DNN may be reused to attack other neural networks. This allows the adversary to construct a substitute model that mimics the targeted DNN, and then attack the constructed substitute model using white-box attack methods. However, this type of attack algorithms usually suffer from large distortions and relatively low success rates (Chen et al., 2017c). To address this issue, Chen et al. (2017c) proposed the Zeroth-Order Optimization (ZOO) algorithm that extends the CW attack to the black-box setting and uses a zeroth-order optimization approach to conduct the attack. Although ZOO achieves much higher attack success rates than the substitute model-based black-box attacks, it suffers from a poor query complexity since its naive implementation requires to estimate the gradients of all the coordinates (pixels) of the image. To improve its query complexity, several approaches have been proposed. For example, Tu et al. (2018) introduces an adaptive random gradient estimation algorithm and a well-trained Autoencoder to speed up the attack process. Ilyas et al. (2018) and Liu et al. (2018) improved ZOO's query complexity by using Natural Evolutionary Strategies (NES) (Wierstra et al., 2014; Salimans et al., 2017) and active learning, respectively.
Non-convex Frank-Wolfe Algorithms: The Frank-Wolfe algorithm (Frank & Wolfe, 1956), also known as the conditional gradient method, is an iterative optimization method for constrained op-

2

Under review as a conference paper at ICLR 2019

timization problem. Jaggi (2013) revisited Frank-Wolfe algorithm in 2013 and provided a stronger and more general convergence analysis in the convex setting. Yu et al. (2017) proved the first convergence rate for Frank-Wolfe type algorithm in the non-convex setting. Lacoste-Julien (2016) provided the convergence guarantee for Frank-Wolfe algorithm in the non-convex setting with adaptive step sizes. Reddi et al. (2016) further studied the convergence rate of non-convex stochastic Frank-Wolfe algorithm in the finite-sun optimization setting. Very recently, Staib & Jegelka (2017) proposed to use Frank-Wolfe for distributionally robust training (Sinha et al., 2018).

3 METHODOLOGY

3.1 NOTATIONS

Throughout the paper, scalars are denoted by lower case letters, vectors by lower case bold face

letters and sets by calligraphy upper cae letters. For a vector x  Rd, we denote the Lp norm of x

by x p = (

d i=1

ip)1/p.

Specially,

for

p

=

,

the

L

norm

of

x

by

x  = maxid=1 |i|. We

denote PX (x) as the projection operation of projecting vector x into the set X .

3.2 PROBLEM FORMULATION

According to the attack purposes, attacks can be divided into two categories: untargeted attack and targeted attack. In particular, untargeted attack aims to turn the prediction into any incorrect label, while the targeted attack, which is considerably harder, requires to mislead the classifier to a specific target class. In this work, we follow the literature (Carlini & Wagner, 2017; Ilyas et al., 2018) and focus on the strictly harder targeted attack setting. It is worth noting that our proposed algorithm can be extended to untargeted attack straightforwardly.

Let us define f (·) as the classification loss function of the targeted DNN. For targeted attacks, we aim to learn an adversarial example x that is close enough to the original input xori and can be misclassified to the target class ytar. The corresponding optimization problem 1 is defined as:

minx f (x, ytar) subject to x - xori p  .

(3.1)

Evidently, the constraint set X := {x | x - xori p  } is a bounded convex set when p  1. Normally, p = 2 and p =  are used to measure the distortions x - xori p, resulting in L2 attack
model and L attack model respectively. In this work, we study both models.

3.3 FRANK-WOLFE WHITE-BOX ATTACKS

Frank-Wolfe algorithm (Frank & Wolfe, 1956), also known as the conditional gradient descent, is a popular optimization tool for constraint optimization. Different from PGD that first performs gradient descent followed by a projection step at each iteration, Frank-Wolfe algorithm calls a Linear Minimization Oracle (LMO) over the the constraint set X at each iteration, i.e.,

LMO = argmin v, f (xt, ytar) .
vX

The LMO can be seen as the minimization of the first-order Taylor expansion of f (·) at point xt:

min f (xt) +
vX

v - xt, f (xt, ytar) .

By calling LMO, Frank Wolfe solves the linear problem in X and then perform weighted average with previous iterate to obtain the final update formula.

We present our proposed Frank-Wolfe white-box attack algorithm in Algorithm 1, which is built
upon the original Frank-Wolfe algorithm. The key difference between Algorithm 1 and the standard
Frank-Wolfe algorithm is in Line 4, where the LMO is called over a slightly relaxed constraint set X := {x | x - xori p   } with   1, instead of the original constraint set X . When  = 1, set X reduces to X , and Algorithm 1 reduces to standard Frank Wolfe. We argue that this modification makes our algorithm more general, and gives rise to better attack results.

The LMO solution itself can be expensive to obtain in general. Fortunately, applying Frank-Wolfe to solve (3.1) actually gives us a close-form LMO solution. We provide the solutions of LMO (Line

1Note that there is usually an additional constraint on the input variable x, e.g., x  [0, 1]n for normalized image inputs.

3

Under review as a conference paper at ICLR 2019

Algorithm 1 Frank-Wolfe White-box Attack Algorithm
1: input: number of iterations T , step sizes {t},  > 0, original image xori, target label ytar; 2: x0 = xori 3: for t = 0, . . . , T - 1 do 4: vt = argminvX v, f (xt, ytar) // LMO 5: dt = vt - xt 6: xt+1 = xt + tdt 7: if  > 1 then 8: xt+1 = PX (xt+1) 9: end if
10: end for 11: output: xT

4 in Algorithm 1) for L2 norm and L norm cases respectively:

v

=

-  · f (xt, ytar) f (xt, ytar) 2

+

xori,

v = - · sign(f (xt, ytar)) + xori.

(L2 norm) (L norm)

The derivation can be found in the supplemental materials.
Note that when T = 1,  = 1, substituting the above LMO solutions into Algorithm 1 yields the final update of x1 = x0 - t · f (xt, ytar), which reduces to FGSM 2 when t = 1. Similar derivation also applies to L2 norm case. Therefore, just like PGD, our proposed Frank-Wolfe white-box attack also includes FGSM (FGM) as a one-step special instance.

3.4 FRANK-WOLFE BLACK-BOX ATTACKS

Next we consider the black-box setting, where we cannot perform back-propagation to calculate the gradient of the loss function anymore. Instead, we can only query the DNN system's outputs with specific inputs. To clarify, here the output refers to the logit layer's output (confidence scores for classification), not the final prediction label. The label-only setting is doable under our framework, but will incur extra difficulty such as designing new loss functions. For simplicity, here we consider the confidence score output.

We propose a zeroth-order Frank-Wolfe based algorithm to solve this problem. Algorithm 2 show our proposed Frank-Wolfe black-box attack algorithm. The key difference between our proposed black-box attack and white-box attack is one extra gradient estimation step, which is presented in Line 4 in Algorithm 2. Also note that for the final output, we provide two options. While option II is the common choice in practice, option I is also provided for the ease of theoretical analysis.
Algorithm 2 Frank-Wolfe Black-box Attack Algorithm
1: input: number of iterations T , step sizes {t},  > 0, original image xori, target label ytar; 2: x0 = xori 3: for t = 0, . . . , T - 1 do 4: qt = ZERO ORD GRAD EST(xt, ytar) // Algorithm 3 5: vt = argminvX v, qt 6: dt = vt - xt 7: xt+1 = xt + tdt 8: if  > 1 then 9: xt+1 = PX (xt+1) 10: end if 11: end for 12: Option I: xa is uniformly random chosen from {xt}tT=1 13: Option II: xa = xT 14: output: xa

2The extra clipping operation in FGSM is to project to the additional box constraint for image classification task. We will also need this clipping operation at the end of each iteration for specific tasks such as image classification.

4

Under review as a conference paper at ICLR 2019

As many other zeroth-order optimization algorithms (Shamir, 2017; Flaxman et al., 2005), Algorithm 3 uses symmetric finite differences to estimate the gradient and therefore, gets rid of the dependence on back-propagation in white-box setting. Different from Chen et al. (2017c), here we do not utilize natural basis as our sensing vectors, instead, we provide two options: one is to use vectors uniformly sampled from Euclidean unit sphere and the other is to use vectors uniformly sampled from standard multivarite Gaussian distribution. This will greatly improve the gradient estimation efficiency comparing to sensing with natural basis as such option will only be able to estimate one coordinate of the gradient vector per query. In practice, both options here provide us competitive experimental results. It is worth noting that NES method (Wierstra et al., 2014) with antithetic sampling (Salimans et al., 2017) used in Ilyas et al. (2018) yields similar formula as our Option II in Algorithm 3.

Algorithm 3 Zeroth-Order Gradient Estimation (ZERO ORD GRAD EST)

1: parameters: number of gradient estimation samples b, sampling parameter t;

2: q = 0

3: for i = 1, . . . , b do

4: Option I: Sample ui uniformly from the Euclidean unit sphere with ui 2 = 1,  = d

5: Option II: Sample ui uniformly from the standard Gaussian distribution N (0, I),  = 1

6:

q

=

q

+

 2t b

f (xt + tui, ytar) - f (xt - tui, ytar)

ui

7: end for

8: output: q

4 MAIN THEORY

In this section, we establish the convergence guarantees for our proposed Frank-Wolfe adversarial attack algorithms described in Section 3. First, we introduce the convergence criterion for our FrankWolfe adversarial attack framework.

4.1 CONVERGENCE CRITERION

The loss function for common DNN models are generally nonconvex. In addition, (3.1) is a constrained optimization. For such general nonconvex constrained optimization, we typically adopt the Frank-Wolfe gap as the convergence criterion (since gradient norm of f is no longer a proper criterion for constrained optimization problems):

g(xt)

=

max
xX

x

-

xt, -f (xt, ytar)

.

Note that for the Frank-Wolfe gap, we always have g(xt)  0 and xt is a stationary point for the constrained optimization problem if and only if g(xt) = 0. Also the Frank-Wolfe gap is affine invariant and do not tie to any specific choice of norm, which makes itself a perfect convergence
criterion for Frank-Wolfe based algorithms.

4.2 CONVERGENCE GUARANTEE FOR FRANK-WOLFE WHITE-BOX ATTACK

Before we are going to provide the convergence guarantee of Frank-Wolfe white-box attack (Algorithm 1), we introduce the following assumptions that are essential to the convergence analysis.
Assumption 4.1. Function f (·) is L-smooth with respect to x, i.e., for any x, x , it holds that

f (x , y)  f (x, y) + f (x, y)

(x - x) + L 2

x

-x

2 2

.

Assumption 4.1 is a standard assumption in nonconvex optimization, and is also adopted in other Frank-Wolfe literature such as Lacoste-Julien (2016); Reddi et al. (2016). Note that even though the smoothness assumption does not hold for general DNN models, a recent study (Santurkar et al., 2018) shows that batch normalization that is used in many modern DNNs such as Inception V3 model, actually makes the optimization landscape significantly smoother 3. This justifies the validity of Assumption 4.1.
3The original argument in Santurkar et al. (2018) refers to the smoothness with respect to each layer's parameters. Note that the first layer's parameters are in the mirror position (in terms of backpropagation) as the network inputs. Therefore, the argument in Santurkar et al. (2018) can also be applied here with respect to the network inputs.

5

Under review as a conference paper at ICLR 2019

Assumption 4.2. Set X is bounded with diameter D, i.e., x - x 2  D for all x, x  X .

Assumption 4.2 implies that the input space is bounded. For common tasks such as image classification, given the fact that images have bounded pixel range and is a small constant, this assumption trivially holds.

Now we present the theorem, which characterizes the convergence rate of our proposed Frank-Wolfe white-box adversarial attack algorithm presented in Algorithm 1.

Theorem 4.3. Under Assumptions 4.1 and 4.2, let  = 2(f (x0, y) - f (x, y))/(LD2T ), denote gT = min1kT g(xk) where {xk}kT=1 are iterates in Algorithm 1 with  = 1, we have:

gT 

LD2(f (x0, y) - f (x, y)) , 2T

where x is the optimal solution to (3.1).

Remark 4.4. Theorem 4.3 suggests that our proposed Frank-Wolfe white-box attack algorithm achieves a O(1/ T ) rate of convergence.

4.3 CONVERGENCE GUARANTEE FOR FRANK-WOLFE BLACK-BOX ATTACK
Next we analyze the convergence of our proposed Frank-Wolfe black-box adversarial attack algorithm presented in Algorithm 2.
In order to prove the convergence of our proposed Frank-Wolfe black-box attack algorithm, we need the following additional assumption that f (0, y) 2 is bounded. Assumption 4.5. Gradient of f (·) at zero point f (0, y) satisfies maxy f (0, y) 2  Cg.

Following the analysis in Shamir (2017), let f(x, y) = Eu[f (x + u, y)], which is the smoothed version of f (x, y). This smoothed function value plays a central role in our theoretical analysis, since it bridges the finite difference gradient approximation with the actual gradient. The following lemma shows this relationship.

Lemma 4.6. For the gradient estimator qt in Algorithm 3, its expectation and variance satisfy

E[qt] = f(xt, ytar),

E

qt - E[qt]

2 2



1 b

2d(Cg

+

LD)2

+

1 2

t2

L2d2

.

Now we are going to present the theorem, which characterizes the convergence rate of Algorithm 2.

Theorem 4.7. Under Assumptions 4.1, 4.2 and 4.5, let  = 2(f (x0, y) - f (x, y))/(LD2T ),
b = T d and t = 2/T d2, suppose we use Option I in Algorithm 2 and option II for Algorithm 3, then the output xa from Algorithm 2 with  = 1 satisfies:

g(xa)



D 2T

L(f (x0, y) - f (x, y)) + 2(L + Cg + LD) ,

where x is the optimal solution to (3.1). 
Remark 4.8. Theorem 4.7 suggests that Algorithm 2 also enjoys a O(1/ T ) rate of convergence. In terms of query complexity, the total number of queries needed is T b = T 2d, which is linear in the data dimension d. In fact, in the experiment part, we observed that this number can be substantially smaller than d, e.g., b = 25, which is much lower than the theorem suggests. Note that although we
only prove for option II in Algorithm 3, our result can be readily extended to Option I (the Gaussian
sensing vector case).

5 EXPERIMENTS
In this section, we present the experimental results for our proposed Frank-Wolfe attack framework against other state-of-the-art adversarial attack algorithms in both white-box and black-box settings. All of our experiments are conducted on Amazon AWS p3.2xlarge servers which come with Intel Xeon E5 CPU and one NVIDIA Tesla V100 GPU (16G RAM). All experiments are implemented in Tensorflow platform version 1.10.0 within Python 3.6.4.

6

Under review as a conference paper at ICLR 2019

Table 1: Comparison of L2 norm based white-box attacks on Inception V3 model with = 5. We report attack success rate, average time and average distortion.

METHODS FGM BIM/ I-FGM CW EAD FW-White

SUCCESS RATE (%) 0 100
100 100 100

AVERAGE TIME (s) -
197.67 167.07 175.20 28.39

AVERAGE DISTORTION -
0.79 0.55 1.07 0.87

5.1 EVALUATION SETUP AND METRICS
We test the attack effectiveness of all algorithms by evaluating on a pre-trained Inception V3 model (Szegedy et al., 2016) that is trained on ImageNet dataset (Deng et al., 2009). The pre-trained model is reported to have a 78% top-1 accuracy. We randomly choose 200 images from the ImageNet validation set that are verified to be correctly classified by the pre-trained model and also randomly choose a target class for each image. Each image has a dimension of 299 × 299 × 3 and we test all attack algorithms through the same randomly chosen data samples and target labels.
We test for both L2 norm based and L norm based attacks. For L2 norm based attack, we set = 5 and for L based attack, we set = 0.05. For white-box attack, we restrict a maximum of 1, 000 iterations per attack for each method. And for black-box attack, we set a maximum query limit of 500, 000 per attack per image for each method.
For all algorithms, we stop the algorithm when a successful attack is found. For our proposed blackbox attack, we use option II in Algorithm 2 and test both options in Algorithm 3. We set the number of gradient estimation samples b = 25 for Algorithm 2. More detailed description on parameter settings can be found in the supplemental materials.
We evaluate the final performance through attack success rate where the success is defined as making the classifier output the exact target class label (not any incorrect labels). We also measure average attack time per image, average distortion (only on successful attacked samples) and average number of queries needed (only for black-box attack) per image. For a fair time comparison, even though some of the algorithms including ours can be written in batch form (attack multiple images at one time), all algorithms are set to attack one image at a time.
5.2 BASELINE METHODS
We compare the proposed algorithms with several state-of-the-art baseline algorithms. Specifically, we compare the proposed white-box attack algorithm with (i) FGM (FGSM) (Goodfellow et al., 2015) (ii) BIM / I-FGM (Kurakin et al., 2016) (iii) CW attack (Carlini & Wagner, 2017) and (iv) EAD attack (Chen et al., 2017b). We compare the proposed black-box attack algorithm with (i) ZOO attack (Chen et al., 2017c) and (ii) NES-PGD attack (Ilyas et al., 2018).
5.3 WHITE-BOX ATTACK EXPERIMENTS
In this subsection, we present the white-box attack experiments on Inception V3 model. Tables 1 and 2 present our experimental results for L2 norm and L norm based white-box attacks respectively. As we can observe from the tables, the attack success rate is basically 100% for every method except for the simplest FGM (FGSM) attack. For the other baselines in the L2 norm case, CW method achieves the smallest average distortion, yet it comes with an expansive time cost. EAD method does not have either time advantage or distortion advantage in this experiment, probably due to its different motivation in attacking. BIM/ I-FGM has relatively small average distortion, yet it also costs the most attack time among all methods. On the other hand, our proposed algorithm achieves the shortest attack time with moderate distortion. It significantly reduces the time complexity needed for attacking data with large dimensionality. For the L norm case, CW method takes significantly longer time and does not perform very well on average distortion either. This is largely due to the special design of CW method to make it possible for L norm attack. Again, our proposed whitebox attack algorithm achieves the shortest average attack time and a moderate average distortion.

7

Under review as a conference paper at ICLR 2019

Table 2: Comparison of L norm based white-box attacks on Inception V3 model with = 0.05. We report attack success rate, average time and average distortion.

METHODS FGM BIM/ I-FGM CW FW-White

SUCCESS RATE (%) 0 100 100 100

AVERAGE TIME (s) -
99.44 722.90 4.41

AVERAGE DISTORTION -
0.0026 0.0067 0.0045

5.4 BLACK-BOX ATTACK EXPERIMENTS
In this subsection, we present the black-box experiments on Inception V3 model. For black-box attacks, attack success rate, time needed and number of queries needed are more meaningful evaluation metrics than distortion distances. Therefore, we omit all the grid search / binary search steps that are used in the white-box setting since extra time / queries are needed for finding parameters that can obtain better distortion distances.
Tables 3 and 4 present our experimental results for L2 norm and L norm based black-box attacks respectively. For ZOO method, note that it only has the L2 norm version and it follows CW's framework and thus uses different loss function and problem formulation (cannot exactly control the adversarial example to fit the distortion limit, we manage to keep the average distortion around for ZOO while other methods have average distortions very close to ). Furthermore, we can observe that ZOO is quite slow in this task. Attack on a single image can take up to 2 hours for ZOO and it is only able to achieve a 74.5% success rate (comparing with the 88.9% success rate in the original paper, we think main cause is the query limit here is only half of the query limit in the origin paper). NES-PGD method, while greatly improving ZOO's performance, still cannot achieve 100% success rate in both settings and takes relatively more time and queries. In sharp contrast, our proposed Frank-Wolfe black-box attacks (both option I and option II) achieve 100% success rate in both L2 norm and L norm based attacks and further improve the attack efficiency to another level (around 100s per image for L2 norm case and less than 50s per image for L norm case).
Table 3: Comparison of L2 norm based black-box attacks on Inception V3 model with = 5. We report attack success rate, average time and average number of queries needed per image. Opt I and Opt II refer to the two options in Algorithm 2.

METHODS ZOO NES-PGD FW-Black (Opt I) FW-Black (Opt II)

SUCCESS RATE (%) 74.5 96.5 100 100

AVERAGE TIME (s) 5825.89 130.45 100.75 100.91

AVERAGE QUERIES 294735.87 58957.55 45410.50 44999.75

Table 4: Comparison of L norm based black-box attacks on Inception V3 model with = 0.05. We report attack success rate, average time and average number of queries needed per image. Opt I
and Opt II refer to the two options in Algorithm 2.

METHODS NES-PGD FW-Black (Opt I) FW-Black (Opt II)

SUCCESS RATE (%) 98 100 100

AVERAGE TIME (s) 73.58 47.56 46.48

AVERAGE QUERIES 32741.32 21129.25 20694.75

6 CONCLUSIONS
In this work, we propose a Frank-Wolfe framework for efficient and effective adversarial attacks. Our proposed white-box and black-box attack algorithms enjoy an O(1/ T ) rate of convergence, and the query complexity of the proposed black-box attack algorithm is linear in data dimension d. Finally, our empirical study on attacking Inception V3 model with ImageNet dataset yields a 100% attack success rate for our proposed algorithms, even in the setting of black-box attack.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. Endto-end attention-based large vocabulary speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pp. 4945­4949. IEEE, 2016.
Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Exploring the space of black-box attacks on deep neural networks. arXiv preprint arXiv:1712.09491, 2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39­57. IEEE, 2017.
Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. Hidden voice commands. In USENIX Security Symposium, pp. 513­ 530, 2016.
Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Show-and-fool: Crafting adversarial examples for neural image captioning. arXiv preprint arXiv:1712.02051, 2017a.
Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks to deep neural networks via adversarial examples. arXiv preprint arXiv:1709.04114, 2017b.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15­26. ACM, 2017c.
Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Queryefficient hard-label black-box attack: An optimization-based approach. arXiv preprint arXiv:1807.04457, 2018a.
Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. arXiv preprint arXiv:1803.01128, 2018b.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. Ieee, 2009.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms, pp. 385­394. Society for Industrial and Applied Mathematics, 2005.
Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95­110, 1956.
Xiang Gao, Bo Jiang, and Shuzhong Zhang. On the information-adaptive variants of the admm: an iteration complexity perspective. Journal of Scientific Computing, 76(1):327­363, 2018.
Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 1440­1448, 2015.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ICLR, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770­778, 2016.
Weiwei Hu and Ying Tan. Generating adversarial malware examples for black-box attacks based on gan. arXiv preprint arXiv:1702.05983, 2017.
Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin, Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Black-box adversarial attacks with limited queries and information. In Proceedings of the 35th International Conference on Machine Learning,{ICML} 2018, 2018.
9

Under review as a conference paper at ICLR 2019
Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1), pp. 427­435, 2013.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.
Simon Lacoste-Julien. Convergence rate of frank-wolfe for non-convex objectives. arXiv preprint arXiv:1607.00345, 2016.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. International Conference on Data Mining (ICDM), 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. International Conference on Learning Representations, 2018.
Abdel-rahman Mohamed, George E Dahl, Geoffrey Hinton, et al. Acoustic modeling using deep belief networks. IEEE Trans. Audio, Speech & Language Processing, 20(1):14­22, 2012.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574­2582, 2016.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016a.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372­387. IEEE, 2016b.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506­519. ACM, 2017.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep reinforcement learning with adversarial attacks. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2040­2042. International Foundation for Autonomous Agents and Multiagent Systems, 2018.
Sashank J Reddi, Suvrit Sra, Barnaba´s Po´czos, and Alex Smola. Stochastic frank-wolfe methods for nonconvex optimization. In Communication, Control, and Computing (Allerton), 2016 54th Annual Allerton Conference on, pp. 1244­1251. IEEE, 2016.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91­99, 2015.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization?(no, it is not about internal covariate shift). arXiv preprint arXiv:1805.11604, 2018.
Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. Journal of Machine Learning Research, 18(52):1­11, 2017.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. International Conference on Learning Representations, 2018.
10

Under review as a conference paper at ICLR 2019
Matthew Staib and Stefanie Jegelka. Distributionally robust deep learning as a generalization of adversarial training. Machine Learning and Computer Security Workshop, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818­2826, 2016.
Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. CoRR, abs/1805.11770, 2018.
Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Ju¨rgen Schmidhuber. Natural evolution strategies. The Journal of Machine Learning Research, 15(1):949­980, 2014.
Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darell, and Dawn Song. Can you fool ai with adversarial examples on a visual turing test? arXiv preprint arXiv:1709.08693, 2017.
Yaoliang Yu, Xinhua Zhang, and Dale Schuurmans. Generalized conditional gradient for sparse estimation. The Journal of Machine Learning Research, 18(1):5279­5324, 2017.
11

Under review as a conference paper at ICLR 2019

A LINEAR MINIMIZATION ORACLE (LMO) SOLUTIONS

Denote u = (v - xori)/( ), the linear minimization problem can be written as

min
v-xori p

v, f (xt, ytar)

= min  · u, f (xt, ytar)
u p1
= max  · u, -f (xt, ytar)
u p1
=  · f (xt, ytar) p,

where · p denotes the dual norm of · p. For p = 2 case, we have

(v - xori)/( ), -f (xt, ytar) = f (xt, ytar) 2.

It immediately implies that

For p =  case, we have

v

=

-  · f (xt, ytar) f (xt, ytar) 2

+

xori.

(v - xori)/( ), -f (xt, ytar) = f (xt, ytar) 1. It immediately implies that

v = - · sign(f (xt, ytar)) + xori.

B PROOF OF THE MAIN THEORY IN SECTION 4

B.1 PROOF OF THEOREM 4.3

Proof. For simplicity, we denote f (xt, ytar) by f (xt, y) for the rest of the proof. First by Assumption 4.1, we have

f (xt+1, y)  f (xt, y) + f (xt, y)

(xt+1

-

xt)

+

L 2

xt+1 - xt

2 2

= f (xt, y) + f (xt, y)

L2 (vt - xt) + 2

vt - xt

2 2

 f (xt, y) + f (xt, y)

(vt

-

xt)

+

LD22 2

,

where the last inequality uses the bounded domain condition in Assumption 4.2. Note that by definition of the Frank-Wolfe gap, we have

LD22 f (xt+1, y)  f (xt, y) - g(xt) + 2 . Do telescope sum over t of the above inequality, we obtain

f (xT

,

y)



f (x0, y)

-

T -1

 g (xk )

+

T LD22 2

k=0

T LD22

 f (x0, y) - T gT +

, 2

where the second inequality follows from the definition of gt. Note that by optimality we easily have f (xt+1, y)  f (x, y). Rearrange the above inequality we have

gT



f (x0, y) - f (x, y) T

+

LD2 2

 LD2(f (x0, y) - f (x, y)) , 2T

where the second inequality is achieved when  = 2(f (x0, y) - f (x, y))/(LD2T ).

12

Under review as a conference paper at ICLR 2019

B.2 PROOF OF LEMMA 4.6

Proof. For simplicity we denote f (·, ytar) by f (·, y) for the rest of the proof. Let us denote i =

d 2t b

f (xt + tui, y) - f (xt - tui, y)

ui. For the first part, we have

E[i] = Eu

d 2tb

f (xt + tui, y) - f (xt - tui, y) ui

= Eu

d 2tb f (xt + tui, y)ui

+ Eu

d 2t

b

f

(xt

-

t

ui

,

y)(-ui)

d = Eu tb f (xt + tui, y)ui

=

1 b

f

(xt

,

y),

where the third equality holds due to symmetricity of ui and the last eqaulity follows from Lemma 4.1(a) in Gao et al. (2018). Therefore, we have

b
E[qt] = E i = f(xt, y).
i=1
For second part, note that i's are independent from each other due to the independence of ui, we have

b

E

qt - E[qt]

2 2

=E

i - Ei

i=1

2b

b

= E i - Ei 2  E i 2.

2 i=1

i=1

Now take a look at E i 2:

E

i

2 = Eu

d 2tb

f (xt + tui, y) - f (xt, y) + f (xt, y) - f (xt - tui, y) ui

2 2



1 2b2 Eu

d t

f (xt + tui, y) - f (xt, y) ui

21 2 + 2b2 Eu

d t

f (xt, y) - f (xt - tui, y) ui

2 2

1 = b2 Eu

d t

f (xt + tui, y) - f (xt, y) ui

2 2

1 
b2

2d

f (xt, y)

2 2

+

1 2

t2

L2

d2

,

where the first inequality is due to the fact that (a + b)2  2a2 + 2b2, the second equality follows from the symmetricity of ui and the last inequality is by Lemma 4.1(b) in Gao et al. (2018). Also note that by Assumption 4.1 and 4.5 we have

f (xt, y)

2 2



(

f (0, y))

2+L

xt

2)2  (Cg + LD)2.

Combine all above results, we obtain

E

qt - E[qt]

2 2



1 b

2d(Cg

+

LD)2

+

1 2

t2L2d2

.

13

Under review as a conference paper at ICLR 2019

B.3 PROOF OF THEOREM 4.7
Proof. For simplicity we denote f (xt, ytar) by f (xt, y) for the rest of the proof. First by Assumption 4.1, we have

f (xt+1, y)  f (xt, y) + f (xt, y)

(xt+1

-

xt)

+

L 2

xt+1 - xt

2 2

= f (xt, y) + f (xt, y)

(vt

-

xt)

+

L2 2

vt - xt

2 2

LD22  f (xt, y) + f (xt, y) (vt - xt) + 2

= f (xt, y) + qt (vt - xt) + (f (xt, y) - qt)

(vt

-

xt)

+

LD22 ,
2

where the second inequality uses the bounded domain condition in Assumption 4.2. Now define an auxialiary quantity:

vt = argmin v, f (xt, y) .
vX

According to the definition of g(xt), this immediately implies

g(xt) = vt, f (xt, y) .

Then we further have

f (xt+1, y)  f (xt, y) + qt (vt - xt) + (f (xt, y) - qt)

(vt

-

xt)

+

LD22 2

= f (xt, y) + f (xt, y)

(vt - xt) + (f (xt, y) - qt)

(vt

-

vt)

+

LD22 2

= f (xt, y) - g(xt) + (f (xt, y) - qt)

(vt

-

vt)

+

LD22 2

 f (xt, y) - g(xt) + D ·

f (xt, y) - qt

LD22 2+ 2 ,

where the first inequality follows from the optimally of vt in Algorithm 2 and the last inequality holds due to Cauchy-Schwarz inequality. Take expectations for both sides of the above inequality,
we have

E[f (xt+1, y)]

 E[f (xt, y)] - E[g(xt)] + D · E

f (xt, y) - qt

LD22 2+ 2

LD22

 E[f (xt, y)] - E[g(xt)] + D · f (xt, y) - E[qt] 2 + E qt - E[qt] 2 +

, 2

 E[f (xt, y)] - E[g(xt)] + D ·

f (xt, y) - E[qt] 2 +

E

qt - E[qt]

2 2

LD22 +
2

 E[f (xt, y)] - E[g(xt)] + D ·

f (xt, y) - f(xt, y) 2 +

4d(Cg + LD)2 + t2L2d2 2b

LD22 +,
2  E[f (xt, y)] - E[g(xt)] + D ·

 tLd + 2 d(Cg +LD) + tLd
2 2b

LD22 +,
2

where the second inequality follows from triangle inequality, the third inequality is due to Jenson's inequality and the last inequality holds due to Lemma 4.6.

14

Under review as a conference paper at ICLR 2019

Do telescope sum over t of the above inequality, we obtain

E[f (xT , y)]

T -1
 f (x0, y) - E[g(xt)] + DT
t=0

 tLd + 2 d(Cg +LD) + tLd
2 2b

T LD22 +
2



 f (x0, y) - T ga + DT

tLd + 2 d(Cg +LD) + tLd 2 2b

T LD22 +,
2

where the second inequality follows from the definition of ga. Note that by the zeroth-order optimality, we have f (xt+1, y)  f (x, y). Rearrange the above inequality we obtain



ga



f (x0, y) - f (x, y) T

+

LD2 2

+D

tLd + 2 2

d(Cg +LD) + tLd 2b

 D 2T

L(f (x0, y) - f (x, y)) + 2(L + Cg + LD) ,

where the second inequality is achieved by setting  = 2(f (x0, y) - f (x, y))/(LD2T ), b = T d and t = 2/T d2.

C PARAMETERS SETTINGS FOR SECTION 5
For Frank-Wolfe white-box attack algorithm, we list the parameters we use in Section 5 at Table 5.

Table 5: Parameters used in Frank-Wolfe white-box attack. PARAMETER L2 CASE Linf CASE

T

1000

1000

{t} 0.03 0.005

 20 5

Similarly, for Frank-Wolfe black-box attack algorithm, we also list the parameters we use in Section 5 at Table 6.

Table 6: Parameters used in Frank-Wolfe black-box attack. PARAMETER L2 CASE Linf CASE

T

10000

10000

{t} 0.05/ t 0.03/ t

 30 30

b 25 25

t

0.001

0.01

D ADDITIONAL EXPERIMENTS
For the completeness, we also provide some visual illustrations on the adversarial examples generated by various algorithms. Figure 1 shows some examples adversarial examples generated through different L2 norm based white-box attacks. Figure 2 shows some adversarial examples generated through different L norm based black-box attacks.

15

Under review as a conference paper at ICLR 2019
Figure 1: Sample adversarial examples generated through different L2 norm based white-box attacks. Left side labels denote the original class label and right side labels denote the target class label.
16

Under review as a conference paper at ICLR 2019
Figure 2: Sample adversarial examples generated through different L norm based black-box attacks. Left side labels denote the original class label and right side labels denote the target class label.
17

