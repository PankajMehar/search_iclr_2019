Under review as a conference paper at ICLR 2019
COMPOSING ENTROPIC POLICIES USING DIVERGENCE CORRECTION
Anonymous authors Paper under double-blind review
ABSTRACT
Deep reinforcement learning (RL) algorithms have made great strides in recent years. An important remaining challenge is the ability to quickly transfer existing skills to novel tasks, and to combine existing skills with newly acquired ones. In domains where tasks are solved by composing skills this capacity holds the promise of dramatically reducing the data requirements of deep RL algorithms, and hence of greatly increasing their applicability. Recent work has studied ways of composing behaviors represented in the form of action-value functions. We analyze these methods to highlight their strengths and weaknesses, and point out situations where each of them is susceptible to poor performance. To perform this analysis we extend generalized policy improvement to the max-entropy framework and introduce a method for the practical implementation of successor features in continuous action spaces. Then we propose a novel approach which achieves an approximately optimal result. This method works by explicitly learning the (discounted, future) divergence between policies. We study this approach in the tabular case and propose a scalable variant that is applicable in multi-dimensional continuous action spaces. We compare our approach with existing ones on a range of non-trivial continuous control problems with compositional structure, and demonstrate near-optimal performance despite requiring less information than competing approaches.
1 INTRODUCTION
Reinforcement learning algorithms coupled with powerful function approximators have recently achieved a series of successes playing video games (Mnih et al., 2015) or Go (Silver et al., 2016), as well as in simulated and real robotic control (Lillicrap et al., 2015; Kalashnikov et al., 2018). Unfortunately, while being extremely powerful deep reinforcement learning (DRL) algorithms often require a large number of interactions with the environment to achieve good results, partially because they are often applied "from scratch" rather than in settings where they can leverage existing experience. This reduces their applicability in domains where generating experience is expensive, or learning from scratch is challenging.
The data efficiency of DRL algorithms is affected by various factors and significant research effort has been directed at achieving improvements (e.g. Popov et al., 2017). At the same time the development of basic locomotor behavior in humans can, in fact, require large amounts of experience and practice (Adolph et al., 2012), and it can take significant effort and training to master complex, high-speed skills (Haith & Krakauer, 2013). Once such skills have been acquired, however, humans rapidly put them to work in new contexts and to solve new tasks, suggesting transfer learning as an important mechanism.
Transfer learning has been explored extensively in multiple fields of the machine learning community (see e.g. Weiss et al., 2016, for a recent review). For instance, in the computer vision literature it is common practice to transfer experience in the form of visual feature vectors(Huh et al., 2016; Kornblith et al., 2018).
In RL and robotics the transfer of knowledge from one task to another has been studied from a variety of angles. For the purpose of this paper we are interested in methods that are suitable for transfer in the context of high-dimensional motor control problems. We further focus on model-free approaches, which are evident in human motor control (Haith & Krakauer, 2013), and have recently
1

Under review as a conference paper at ICLR 2019

been used by a variety of scalable deep RL methods (e.g. Lillicrap et al., 2015; Mnih et al., 2015; Schulman et al., 2017; Kalashnikov et al., 2018).
Transfer may be especially valuable in domains where a small set of skills can be composed, in different combinations, to solve a variety of tasks. Different notions of compositionality have been considered in the RL and robotics literature. For instance, `options' are associated with discrete units of behavior that can be sequenced, thus emphasizing composition in time (Precup et al., 1998). In this paper we are concerned with a rather distinct notion of compositionality, namely how to combine and blend potentially concurrent behaviors. This form of composition is particularly relevant in high-dimensional continuous action spaces, where it is possible to achieve more than one task simultaneously (e.g. walking somewhere while juggling).
One approach to this challenge is via the composition of task rewards. Specifically, we are interested in the following question: If we have previously solved a set of tasks with similar transition dynamics but different reward functions, how can we leverage this knowledge to solve new tasks which can be expressed as a convex combination of those rewards functions?
This question has recently been studied in two independent lines of work: by Barreto et al. (2017; 2018) in the context of successor feature (SF) representations used for Generalized Policy Improvement (GPI) with deterministic policies, and by Haarnoja et al. (2018a); van Niekerk et al. (2018) in the context of maximum entropy policies. These approaches operate in distinct frameworks but both achieve skill composition by combining the Q-functions associated with previously learned skills.
We clarify the relationship between the two approaches and show that both can perform well in some situations but achieve poor results in others, often in complementary ways. We introduce a novel method of behavior composition that that can consistently achieve good performance.
Our contributions are as follows:
1. We provide an analysis of when GPI, and "optimistic" compositions of entropy-regularized policies (Haarnoja et al., 2018a) transfer. We construct both tabular and continuous action tasks where both fail to transfer well.
2. We introduce succcessor features (SF) in the context of maximum entropy and extend GPI to this case (max-ent GPI).
3. We propose a correction term ­ which we call Divergence Correction (DC)­ based on the Re´nyi divergence between policies which allows us, in principle, to recover the optimal policy for transfer for any convex combination of rewards.
4. We demonstrate a practical implementation of these methods in continuous action spaces using adaptive importance sampling and compare the approaches introduced here: max-ent GPI and DC with optimism(Haarnoja et al., 2018a) and Conditional Q functions (Schaul et al., 2015) in a variety of non-trivial continuous action transfer tasks.

2 BACKGROUND

2.1 MULTI-TASK RL

We consider Markov Decision Processes defined by the tuple M containing: a state space S, action

space A, a start state distribution p(s1), a transition function p(st+1|st, at), a discount   [0, 1) and

a reward function r(st, at, maximises the discounted

st+1). The objective of RL is to find expected return from any state J()

a policy (a|s) :

= E,M [

  =t

S 

 P(A) which -tr ] where the

expected reward is dependent on the policy  and the MDP M.

We formalize transfer as in Barreto et al. (2017); Haarnoja et al. (2018a), as the desire to perform well across all tasks in a set M  T after having learned policies for tasks M  T , without additional experience. We assume that T and T are related in two ways: all tasks share the same state transition function, and tasks in T can be expressed as convex combinations of rewards associated with tasks in set T . So if we write the reward functions for tasks in T as the vector  = (r1, r2, . . . ), tasks in T can be expressed as rw =  · w. We focus on combinations of two policies rb = bri + (1 - b)rj but the methods can be extended to more than two tasks.

2

Under review as a conference paper at ICLR 2019

2.2 SUCCESSOR FEATURES

Successor Features (SF) (Dayan, 1993) and Generalised Policy Improvement (GPI) (Barreto et al., 2017; 2018) provide a principled solution to the transfer in the setting defined above. SF make the additional assumption that the reward feature  is fully observable, that is, the agent has access to the rewards of all tasks in T but not T during training on each individual task.

The key observation of SF representations is that linearity of the reward rw with respect to the features  implies the following decomposition of the value policy of  as



Qw(st, at) = E

-t · w|at = E

-t |at · w  (st, at) · w,

 =t

i=t

(1)

where  is the expected discounted sum of features  induced by policy . This decomposition allows us to compute the action-value for  on any task w by learning .

If we have a set of policies 1, 2, ..., n indexed by i, GPI provides a principled approach to transfer on task w. Namely, GPI guarantees that the GPI policy wGP I (at|st)  (at - arg maxat QGwP I (st, at)) where

QGwP I (st, at)  maxi Qwi (st, at)

(2)

has

a

return

at

least

as

good

as

any

component

policy,

that

is,

V GP I
w

(s)



maxi

Vwi

(s)

s



S.

2.3 MAXIMUM ENTROPY RL

The maximum entropy (max-ent) RL objective augments the reward with an entropy term to favor entropic solutions

J () = E,M [

 i=

  -t (r

+

H[(·|s ))]]

(3)

where  is a parameter that determines the relative importance of the entropy term.

This objective has been considered in a number of works including Kappen (2005); Todorov (2009); Haarnoja et al. (2017; 2018a); Ziebart et al. (2008); Fox et al. (2015).

We define the action-value Q associated with eq. 3 as

Q(st, at)  rt + E

  =t+1

  -t (r

+

H[(·|s )])

(4)

(notice Q(st, at) does not include any entropy terms for the state st). Soft Q iteration

Q(st, at)  r(st, at, st+1) + Ep(st+1|st,at) [V (st+1)]

V (st)  E [Q(st, at)] + H[(·|st)] =  log

A

exp(

1 

Q(st,

at))da





log

Z (st )

(5) (6)

where

(at|st)



exp(

1 

Q(st

,

at))

converges

to

the

optimal

policy

with

standard

assumptions

(Haarnoja et al., 2017). The general form of the Boltzmann policy  allows it to capture multi-

modal solutions. However, it also makes sampling from  challenging.

3 COMPOSING POLICIES IN MAX-ENT REINFORCEMENT LEARNING
In this section we present two novel approaches for max-ent transfer learning. In section 4 we then outline a practical method for making use of these results.
3.1 MAX-ENT SUCCESSOR FEATURES AND GENERALIZED POLICY IMPROVEMENT
We introduce max-ent SF, which provide a practical method for computing the value of a maximum entropy policy under any convex combination of rewards. We then formulate GPI (Barreto et al., 2017) for maximum entropy policies.

3

Under review as a conference paper at ICLR 2019

We define the action-dependent SF to include the entropy of the policy, excluding the current state, analogous to the max-entropy definition of Q in (4):

(st, at)  t + E

 i= +1

  -t (

+

1

·

H [(·|s)])

= t + Ep(st+1|st,at) [(st+1)]

(7)

where 1 is a vector of ones of the same dimensionality as  and we define the state-dependent successor features as the expected  in analogy with V (s):

(s)  Ea(·|s) [(s, a)] + 1 · H[(·|s)].

(8)

The max-entropy action-value of  for any convex combination of rewards w is then given by Qw(s, a) = (s, a) · w. Max-ent SF allow us to estimate the action-value of previous policies on a new task. We show that, as in the deterministic case, there is a principled way to combine these
estimates.

Theorem 3.1 (Max-Ent Generalized Policy Improvement) Let 1, 2, ..., n be n policies with -max-ent action-value functions Q1, Q2, ..., Qn and value functions V 1, V 2, ..., V n. Define

(a|s)  exp

1 

maxi

Qi(s,

a)

.

Then,

Q(s, a)  max Qi(s, a) for all s  S and all a  A,
i
V (s)  max V i(s) for all s  S,
i

(9) (10)

where Q(s, a) is the -max-ent action-value function of , and V (s) is the max-ent state-value function of .

Proof: See appendix A.1. Once the agent has learned i (s, a), the SFs of policies i, we define the

max-ent

GPI

policy

for

task

w

as

wGP I (a|s)



exp(

1 

maxi

Qwi (s,

a))

=

exp(

1 

maxi

i (s,

a)

·

w).

3.2 DIVERGENCE CORRECTION (DC)

Haarnoja et al. (2018a) introduced a simple approach to policy combination by estimating the action-
value for the joint task rb = bri + (1 - b)rj from the optimal action-values of the component tasks Qi and Qj

QOb pt(s, a)  bQi(s, a) + (1 - b)Qj(s, a).

(11)

When

using

Boltzmann

policies

defined

by

Q,

the

resulting

policy,

bOpt(a|s)



exp(

1 

QbOpt(s,

a)),

is the product distribution of the two component policies. We refer to bOpt as the "optimistic"

policy, as it acts according to the optimistic assumption that the optimal returns of Qi and Qj will

be,

simultaneously,

achievable.

Haarnoja

et

al.

(2018a)

focus

on

the

case

when

b

=

1 2

and

prove

that

the regret of this estimator can be bounded by the discounted sum of Re´nyi divergence of the two

component policies. Intuitively, if the two policies have large areas of agreement in the actions, the

composed policy will perform well. As we analyse later, both max-ent GPI we presented above, and

the optimistic policy can, in different ways, fail to transfer well in some situations.

Here we show, at the cost of learning a function conditional on the task combination, it is in principle
possible to recover the optimal policy for the convex transfer tasks, without direct experience on those tasks, by correcting for the optimistic bias in QbOpt. For simplicity, as in Haarnoja et al. (2018a), we restrict this to the case with only 2 tasks, but it can be extended to multiple tasks.

Theorem 3.2 (DC Optimality) Let i, j be  max-ent optimal policies for tasks with rewards ri and rj with max-ent action-value functions Qi, Qj. Define Cb(st, at) as the fixed point of

Cb(k+1)(st, at) = -Ep(st+1|st,at)

log

A

i(at+1|st+1)bj

(at+1|st+1)(1-b)

exp(-

1 

Cb(k)(st+1,

at+1))dat+1

Given the conditions for Soft Q convergence, the max-ent optimal Qb (s, a) for rb = bri + (1 - b)rj is

Qb (s, a) = bQi(s, a) + (1 - b)Qj(s, a) - Cb(s, a) s  S, a  A, b  [0, 1].

4

Under review as a conference paper at ICLR 2019

Method Optimistic CondQ GPI DC

Optimal

Bounded loss na na

Requires  Requires f (s, a|b)

Table 1: Theoretical properties of different approaches to max-ent transfer. The methods compared are: Optimistic, CondQ, max-ent GPI (over a fixed, finite set of policies), and DC. The columns indicate whether the transfer policy is optimal, the regret of the transfer policy is bounded, whether rewards for all tasks  need to be observed simultaneously during training and whether the method requires learning a function conditional on the transfer task b. DC is the only method that both recovers (in principle) the optimal policy and does not require observing  during training.

Proof: See appendix A.2. We the Re´nyi divergence between

call this Divergence Correction (DC) as the quantity policies (see appendix A.2 for details). Learning Cb

Cb is related to does not require

any additional information (in principle) than that required to learn policies 1 and 2. Unlike with

SF, it is not necessary to observe other task features while training the policies. On the other hand,

unlike with GPI, which can be used to naturally combine any number of tasks with arbitrary weight

vectors w, in order to apply DC one must estimate of learning C increases significantly if more than

Cb(s, a) for all values 2 tasks are combined.

of

b.

so

the

complexity

Table 1 provides a comparison on the properties of the methods we consider here. We also compare with simply learning a conditional Q function Q(s, a|b) (CondQ) (e.g. Schaul et al., 2015; Andrychowicz et al., 2017). As with GPI, this requires observing the full set of task features , in order to compute rb for arbitrary b.

4 IMPLEMENTATION USING ADAPTIVE IMPORTANCE SAMPLING
Compositional transfer appears to be the most useful in high-dimensional, continuous action spaces, since in such action spaces it is often possible to simultaneously work towards achieving multiple objectives. Sampling the Boltzmann policies defined by Q in continuous action spaces is challenging, particularly during transfer when we need to sample from joint distributions of existing policies. We now present an approach using adaptive importance sampling with mixture proposal distributions to learn and transfer with max-ent policies.
Our approach is motivated by two observations: (i) batch computation in modern GPUs and CPUs makes importance sampling with a relatively large number of samples efficient (since the samples can be treated as a batch and computed efficiently), (ii) the product distribution for a mixture of normal distributions can be tractably sampled from. This motivates us to use adaptive importance sampling with a mixture of (truncated) normal distributions for the proposal distribution.

4.1 LEARNING MAX-ENT BOLTZMANN POLICIES WITH ADAPTIVE IMPORTANCE SAMPLING

The basic method we use is to learn proposal distributions for each policy and use this to importance
sample the policy for estimating the other terms. Specifically, for each policy we learn an actionvalue QQ (s, a), and value VV (s) network, and a proposal distribution qq (a|s).

The proposal distribution is learned by minimizing the forward KL divergence with the Boltzmann

policy distribution (a|s)



exp

1 

QQ

(s,

a).

We use the forward divergence because it is "zero

avoiding" and will typically over-estimate the support of  (Murphy, 2012) which is desirable when

for the use as proposal distribution. The loss on the proposal distribution is then

L(q) = E Ea(·|s)[log (a|st) - log qq (a|st)]

(12)

where the expectation is over some off-policy state density  generated by exploration policy .

However, this proposal loss itself requires we sample from , so we importance sample. We use
self-normalized importance weighting so that we do not need to compute the partition function for . An obvious choice for the proposal distribution would be qq (a|s), however, we found this to

5

Under review as a conference paper at ICLR 2019

be unstable so we use a target proposal distribution qq and use a mixture distribution of the target proposal distributions for all policies. The estimator for all losses are given in Appendix B.

With the learned proposal distribution we can then estimate the losses for Soft Q updates of the

action-value and value.

L(Q) =E

1 2

(QQ

(st,

at)

-

(r(st,

at,

st+1)

+

 VV

(st+1)))2

(13)

L(V ) =E

1 2 (VV (st) -  log

A

exp(

1 

QQ

(st

,

a))da)2

(14)

The target value  log

A

exp(

1 

Q(s,

a))da

is

estimated

using

self-normalized

importance

sam-

pling with the proposal distribution qq (a|s). This introduces two sources of bias, due to the self-

normalization and because we apply a concave function log to the expectation. In practise, we found

this estimator sufficient provided the proposal distribution was close enough to . For acting we use

the the learned proposal distribution to importance sample the policy. We use two common tricks for

DeepRL to make our algorithm more stable: target networks for VV and qq (Mnih et al., 2015; Lill-
icrap et al., 2015) and parameterizing Q with an advantage function QQ (s, a) = VV (s)+AA (s, a) (Baird, 1994; Wang et al., 2015; Harmon et al., 1995) which is more stable when the advantage is

small compared with the value.

All of these updates are off-policy, and we learned using a replay buffer of experience. This algorithm can be straightforwardly extended to learn more than one policy by using estimators for each policy i. Because our estimators are off-policy, we can store all experience in a single replay buffer, and make use of all experience when training our policies. We implemented this in a distributed framework with actors and learner working in parallel, see supplementary algorithm 1.

4.2 IMPORTANCE SAMPLED MAX-ENT GPI

We extended our importance sampled approach to estimate max-ent SF. Max-ent GPI requires us
to learn, for each policy i, the expected (maximum entropy) features i, in order to estimate the (entropic) value of each policy under a new convex combination task w.

This requires that our experience tuple contain the full feature vector , rather than just the reward
for the policy which generated the experience ri. We learn the base policies for each reward ri as in algorithm 1. For learning the max-ent successor features, as with the action-value estimate, we learn a state-dependent target i V along with iQ .

L() =E

1 2

(

(st)

-

Eat  (at |st ) [

(st,

at)

+

1(-QQ

(st,

at)

+



log

Z (st ))])2

To learn these quantities we use an analogous approach to the action-values (using the relationship
in eq (8) and the proposal distribution qq (a|s), full details in appendix B). We use a target network for  and found that, because these updates when using experience shared between tasks is far off-policy, it is necessary to have a longer target update period than for V . We parametrized  as a "psi-vantage" (s, a) = (s) + A(s, a).

4.3 DIVERGENCE CORRECTION

All that is required for transfer using optimistic policy combination (eq. 11, Haarnoja et al. (2018a))

is the max-ent action values of each task. We showed in section 3.2 that if we can learn the fixed point of Cb(s, a) we can correct this optimism and recover the optimal action-value Qb (s, a).

We use the recursive relationship in Cb(s, a) to estimate it with a TD-0 estimator. However, this

requires learning a conditional estimator for any value of b, so as to support arbitrary task combina-

tebisoatnitmse.aactFehoCrutpbudnaafttoeer.lydW,ifsefienlreceaenrtnCvCbalbudeeaspsoeCfnbdCsw(oistnh,laoy,uobtn)d,itfrhoeecrteplyaoclgihceipneaesirraatnoidnf gptroraolnilcsli-ieotisuotnsi

function, it is possible to of this value by sampling , j with loss

L(C )

=

Es ,bU (0,1)[

1 2

(CC

(s,

a,

b)

+

Ep(s

|s,a)[log

A exp(b log i(a |s )+

(15)

(1 - b)j(a

|s

)-

1  CC

(s

,a

, b))da

])2].

6

Under review as a conference paper at ICLR 2019

We approximate this loss with an importance sample using a mixture distribution p(a |s ) of qi, qi and the product distribution qprod  qi(a |s )qj(a |s ) and sample b  U (0, 1) at each update (the
full estimator is given in appendix B). As before with functions we use target networks and an advantage parametrization C(s, a, b) = CA(s, a, b) + CB(s, b). Note that, unlike GPI and CondQ (next section), learning Cb does not require observer .

We also considered a heuristic approach where we learned C only for b

=

1 2

(this is typically

approximately the largest divergence). This avoids the complexity of a conditional estimator and

we estimate Cb as C^b(s, a)  4b(1 - b)C1/2(s, a). This heuristic can be justified by considering

Gaussian policies with similar variance (see appendix C) and often worked well in practise. We call

this heuristic DC-Cheap, and we can also make use of the max-ent GPI information to correct for

over-estimates of Cb, QDC-Cheap+GP I (s, a) = max(QOP T (s, a) - C^b(s, a), QGP I (s, a)).

4.4 COND Q
As a baseline, we consider instead of learning C(s, a, b), just directly learning a conditional Q function using a similar approach of sampling b each update Q(s, a, b) (Schaul et al., 2015). This, like GPI but unlike DC, requires observing  during training so the reward on task b can be estimated. We provide the full details of this in appendix B.

4.5 SAMPLING COMPOSITIONAL POLICIES
Above we have defined different approaches to estimating the action-value Qb(s, a) for the transfer task rb from the existing policies. Sampling from the Boltzmann policy defined by Qb is non-trivial. We wish to avoid offline training of a sampler, as required in Haarnoja et al. (2018a) during transfer.
We take advantage of the product distribution of proposals being approximately tractable. So we can sample from qij(a|s)  qi(a|s)qj(a|s). If the proposal distributions fit the policies well, the product distribution will be a good proposal distribution for when the optimistic assumption is valid. However, when Cb(s, a) is large, then the optimal action during transfer may be unlikely under the base policies and this proposal will be poor. To deal with this case, we take advantage of the efficiency of computing a large importance sample in the same state space (since it can be efficiently batched) and we sample from a mixture distribution of all policies, all policy products and the uniform distribution. Empirically, we find this is sufficient to approximately recover the optimal policy in our experiments. We use the same proposal distribution for all transfer approaches. Alternatively, we could iteratively adapt the proposal distribution online (Kalashnikov et al. (2018) did this for deterministic policies).

5 EXPERIMENTS
5.1 DISCRETE, TABULAR ENVIRONMENT
We first consider some illustrative tabular cases of compositional transfer. These highlight where GPI and Optimistic transfer can have high regret (figure 1). As expected, we find that GPI performs well when the optimal policy is close to one of the existing policies; the optimistic composition performs well when both subtask policies are compatible. The "tricky" task is illustrative of the sorts of task we will focus on in the next section: It provides non-overlapping rewards for each task in one corner, while lower value overlapping rewards are provided in the other corner of the grid world (cf. 1). This means that the optimal policy for the joint task does not resemble either existing policy and both GPI and optimism perform poorly. DC performs well in all cases.
5.2 CONTINUOUS ACTION SPACES
We next compare the different approaches in more challenging continuous control tasks. We train max-ent policies to solve individual tasks using the importance sampling approach from section 4 and then assess transfer on convex combinations of the rewards. We train all approaches using the same experience and proposal distribution.

7

Under review as a conference paper at ICLR 2019

(a) (L)eft task (b) (T)ricky 1 (c) (T)ricky 2

Log loss from optimal Log loss from optimal Log loss from optimal

101 101 10-1 10-1 10-3 10-3 10-5 10-5 10-7 10-7
00 0.0 0b.5 1.0 0.0 0b.5 1.0
(d) LR Regret (e) LU Regret

101 10-1 10-3 10-5 10-7
0 0.0

Optimistic GPI LTD 0b.5 1.0

(f) T Regret

Value Value Value Policy Divergence

54 98.4

51 97.6

48 96.8

Value

45 96.0

42 39 36 33

95.2 94.4 93.6 92.8

72 69 66 63 60 57 54 51 48

72 69 66 63 60 57 54 51 48

4 0

(g) Opt LR

(h) GPI LU

(i) GPI T

(j) DC T

(k) D 1 2

Figure 1: Policy combination in the tabular case. All tasks are in an infinite-horizon tabular 8x8 world. The action space is the 4 diagonal movements (actions at the boundary transition back to the same state) (a-c) shows 3 reward functions (color indicates reward, dark blue r = +1, light blue r = 0.75). The arrows indicate the action likelihoods for the max-ent optimal policy for each task. (d-f) The log regret for the max-ent returns for 3 qualitatively distinct compositional tasks rb = bri+(1-b)rj, using different approaches to transfer from the base policies. The compositional tasks we consider are left-right (LR), left-up (LU) and the "tricky" tasks (T). (d) GPI performs well when the subtasks are incompatible, meaning the optimal policy is near one of the component policies. (g) The optimistic policy performs poorly in these situations, resulting in indecision about which subtask to commit to. (e) Conversely, when the subpolicies are similar, such as on the LU task, optimistic transfers well while the GPI policy (h) does not consistently take advantage of the compatibility of the two tasks to simultaneously achieve both subgoals. (i) Neither GPI nor the optimistic policies (j shows GPI, by the Optimistic policy is similar) perform well when the optimal policy is dissimilar to either existing task policy. The two tricky task policies are compatible in many states but have a high-divergence in the bottom-left corner since the rewards are non-overlapping there (k), thus the optimal policy on the composed task is to move to the top right corner where there are overlapping rewards. By learning, and correcting for, this future divergence between policies, DC results in optimal policies for all task combinations including tricky (k).

We focus on the "tricky" tasks with multi-modal solutions. Figure 2 examines the transfer policies in detail in a simple point-mass task and shows how the estimated Cb corrects the optimistic Q and dramatically changes the policy.
We then examine conceptually similar tasks in more difficult domains: a 5 DOF planar manipulator reaching task (figure 3), 3 DOF jumping ball and 8 DOF ant (figure 4). We see that DC recovers a near optimal policy in all case. The performance of GPI depends noticeably on the choice of . DC-Cheap, which is a simpler heuristic, performs almost as well as DC in the tasks we consider except for the point mass task. When bounded by GPI (DC-Cheap+GPI) it performs well for the point mass task as well, suggesting simple approximations of Cb may be sufficient in some cases.1
6 DISCUSSION
We have presented two approaches to transfer learning via convex combinations of rewards in the maximum entropy framework: max-ent GPI and DC. We have shown that, under standard assumptions, the max-ent GPI policy performs at least as well as its component policies, and that DC recovers the optimal policy. Todorov (2009) and (Saxe et al., 2017; van Niekerk et al., 2018) previously considered optimal composition of max-ent policies. However, these approaches require stronger assumptions than max-ent SF or DC, namely that reward states are absorbing and that the
1 We provide videos of the more interesting tasks at https://tinyurl.com/yaplfwaq.
8

Under review as a conference paper at ICLR 2019

Optimistic

GPI

DC

1.0

DC-Cheap DC-Cheap+GPI

CondQ

0.5

0.8 15

11 5

x2 Return
x2 a2 a2 a2

x1
(a) Trajectories

0.00.0 0b.5 1.0
(b) Returns

x1 0.0
(c) D 1 2

14 a1
(d) QOpt

3 a1
(e) C1/2

a1
(f) QLT D

Figure 2: Tricky point mass. The "tricky" tasks in a simple, continous action space (2-D velocity controlled pointed mass). (a) Environment and example trajectories. The rewards are (r1 = 1, r2 = 0), (0, 1) and (0.75, 0.75) for the green, red and yellow squares. Lines show sampled trajectories (starting in the center) for the compositional task r1/2 with Optimistic (red), GPI (blue) and DC (black). Only DC, DC heuristics and CondQ (not shown) find the optimal transfer policy of
navigating to yellow reward area for the joint task which is the optimal solution for the combined task. (b) The returns for each transfer method. DC and CondQ methods recover significantly better
performance than GPI, and the optimistic policy performs poorly due to the non-overlapping rewards. (c) The Re´nyi divergence of the two base policies as a function of position. As expected, the
two policies are compatible until near the bottom left corner where the rewards are non-overlapping. (d) QOpt at the center position for the combined task. As both policies prefer moving left and down, most of the energy is on these actions. (e) However, the future divergence C1/2 under these actions is high, which results in the (f) DC Q policy different significantly from Optimistic.

(a) Planar manipulator tricky

x2
Return

Optimistic GPI DC CondQ
x1 (b) Finger Position

Optimistic

GPI

1.00

DC DC-Cheap

DC-Cheap+GPI

CondQ

0.75

0.500.0 0b.5 1.0
(c) Returns on transfer task

Figure 3: "Tricky" task with planar manipulator. The "tricky" tasks with a 5D torque-controlled planar manipulator. The training tasks consist of (mutually exclusive) rewards of (1, 0), (0, 1) when the finger is at the green and red targets respectively, reward (0.75, 0.75) at the blue target. (b) Finger position at the en (of the trajectoriesstard ting from randomly sampled start states) for the transfer task with circles indicating the rewards. DC and Cond-Q trajectories reach towards the blue target (the optimal solution) while optimistic and GPI trajectories primarily reach towards one of the suboptimal partial solutions. (c) The returns on the transfer tasks (shaded bars show SEM, 5 seeds).

joint reward is restricted to the softmax of the component rewards (soft OR). By contrast, DC does not restrict the class of MDPs and learns how compatible policies are, allowing approximate recovery of optimal transfer policies both when the component rewards are jointly achievable (AND), and when only one sub-goal can be achieved (OR).
We have compared our methods with conditional action-value functions (CondQ) (Schaul et al., 2015, e.g.) and optimistic policy combination (Haarnoja et al., 2018a). Further, we have presented a practical algorithm for training DC and max-ent GPI models in continous action spaces using adaptive importance sampling. We have compared these approaches, along with heuristic approximations of DC, and demonstrated that DC recovers an approximately optimal policy during transfer across a variety of high-dimensional control tasks. Empirically we have found CondQ may be harder to learn than DC, and it requires additional observation of  during training.
9

Under review as a conference paper at ICLR 2019

1.0

Optimistic GPI

0.75

DC

DC-Cheap

DC-Cheap+GPI

CondQ

Return Return
x2

(a) Ant

0.5 0.0 0b.5
(b) JB Returns

0.50

1.0 0.0

0b.5

(c) Ant returns

1.0

x1
(d) Trajectories

Figure 4: "Tricky" task with mobile bodies. "Tricky" task with two bodies: a 3 DOF jumping

ball (supplementary figure 5) and (a) 8 DOF ant (both torque controlled). The task has rewards

(1, 0), (0, 1) in the green and red boxes respectively and (0.75, 0.75) in the blue square. (b-c) Re-

turns for both walkers when started in the center position. The optimistic approach does not recover

the optimal policy for the combinatorial task while the other approaches largely do, although CondQ

does not learn a good policy on the ant (shaded bars show SEM, 3 seeds for jumping ball, 5 seeds

for ant). (e) Sampled trajectories of the ant on the transfer task starting from a neutral position for

b

=

1 2

.

GPI

and

DC

consistently

go

to

the

blue

square

(optimal),

CondQ

and

Optimistic

do

not.

REFERENCES
Karen E Adolph, Whitney G Cole, Meghana Komati, Jessie S Garciaguirre, Daryaneh Badaly, Jesse M Lingeman, Gladys LY Chan, and Rachel B Sotsky. How do you learn to walk? thousands of steps and dozens of falls per day. Psychological science, 23(11):1387­1394, 2012.
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048­5058, 2017.
Leemon C Baird. Reinforcement learning in continuous time: Advantage updating. In Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, volume 4, pp. 2448­2453. IEEE, 1994.
Andre´ Barreto, Will Dabney, Re´mi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. In Advances in neural information processing systems, pp. 4055­4065, 2017.
Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In Proceedings of the International Conference on Machine Learning, pp. 501­510, 2018.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613­624, 1993.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. arXiv preprint arXiv:1512.08562, 2015.
Manuel Gil, Fady Alajaji, and Tamas Linder. Re´nyi divergence measures for commonly used univariate continuous distributions. Information Sciences, 249:124­131, 2013.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine. Composable deep reinforcement learning for robotic manipulation. arXiv preprint arXiv:1803.06773, 2018a.

10

Under review as a conference paper at ICLR 2019
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018b.
Adrian M Haith and John W Krakauer. Model-based and model-free mechanisms of human motor learning. In Progress in motor control, pp. 1­21. Springer, 2013.
Mance E Harmon, Leemon C Baird III, and A Harry Klopf. Advantage updating applied to a differential game. In Advances in neural information processing systems, pp. 353­360, 1995.
Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer learning? arXiv preprint arXiv:1608.08614, 2016.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018.
Hilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of statistical mechanics: theory and experiment, 2005(11):P11011, 2005.
Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? arXiv preprint arXiv:1805.08974, 2018.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012. ISBN 0262018020, 9780262018029.
Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin Riedmiller. Data-efficient deep reinforcement learning for dexterous manipulation. arXiv preprint arXiv:1704.03073, 2017.
Doina Precup, Richard S Sutton, and Satinder Singh. Theoretical results on reinforcement learning with temporally abstract options. In European conference on machine learning, pp. 382­393. Springer, 1998.
Andrew M Saxe, Adam Christopher Earle, and Benjamin S Rosman. Hierarchy through composition with multitask lmdps. 2017.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pp. 1312­1320, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.
Emanuel Todorov. Compositionality of optimal control laws. In Advances in Neural Information Processing Systems, pp. 1856­1864, 2009.
11

Under review as a conference paper at ICLR 2019 Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012. Benjamin van Niekerk, Steven James, Adam Earle, and Benjamin Rosman. Will it blend? composing value functions in reinforcement learning. arXiv preprint arXiv:1807.04439, 2018. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015. Karl R. Weiss, Taghi M. Khoshgoftaar, and Dingding Wang. A survey of transfer learning. Journal of Big Data, 3:1­40, 2016. Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pp. 1433­1438. Chicago, IL, USA, 2008.
12

Under review as a conference paper at ICLR 2019

A PROOFS

A.1 MAX-ENT GENERALIZED POLICY IMPROVEMENT

Theorem 3.1 (Max-Ent Generalized Policy Improvement) Let 1, 2, ..., n be n policies with -max-ent action-value functions Q1, Q2, ..., Qn and value functions V 1, V 2, ..., V n. Define

(a|s)  exp

1 

maxi

Qi(s,

a)

.

Then,

Q(s, a)  max Qi(s, a) for all s  S and all a  A,
i
V (s)  max V i(s) for all s  S,
i

(9) (10)

where Q(s, a) is the -max-ent action-value function of , and V (s) is the max-ent state-value function of .

For brevity we denote Qmax  maxi Qi. Define the soft Bellman operator associated with policy  as

T Q(s, a)  r(s, a, s ) + Ep(s |s,a) H[(·|s )] + Ea (·|s ) [Q(s , a )] .

Haarnoja et al. (2018b) have pointed out that the soft Bellman operator T  corresponds to a con-
ventional, "hard", Bellman operator defined over the same MDP but with reward r(s, a, s ) = r(s, a, s ) + Ep(s |s,a) [H[(·|s )]]. Thus, as long as r(s, a, s ) and H[(·|s )] are bounded, T  is a contraction with Q as its fixed point. Appplying T  to Qmax(s, a) we have:

T Qmax(s, a) = r(s, a, s ) + Es p(·|s,a),a (·|s ) [- log (a |s ) + Qmax(s , a )]

= r(s, a, s ) + Es p(·|s,a),a (·|s )

-

log

exp(-1Qmax(s Z(s )

,

a

))

+

Qmax(s

,

a

)

= r(s, a, s ) + Es p(·|s,a) [ log Z(s )] .

Similarly, if we apply T i , the soft Bellman operator induced by policy i, to Qmax(s, a), we obtain:

T i Qmax(s, a) = r(s, a, s ) + Es p(·|s,a),a i(·|s ) [- log i(a |s ) + Qmax(s , a )] .

We now note that the Kullback-Leibler divergence between i and  can be written as

DKL(i(·|s) (·|s)) = Eai(·|s) [log i(a|s) - log (a|s)]

= Eai(·|s)

log

i(a|s)

-

1 Qmax(s, 

a)

+

log

Z  (s)

.

The quantity above, which is always nonnegative, will be useful in the subsequent derivations. Next we write

T Qmax(s, a) - T i Qmax(s, a) = Es p(·|s,a)  log Z(s ) - Ea i(·|s )[- log i(a |s ) + Qmax(s , a )] = Es p(·|s,a) Ea i(·|s )[ log Z(s ) +  log i(a |s ) - Qmax(s , a )] = Es p(·|s,a) [DKL(i(·|s ) (·|s ))]  0. (16)

From (16) we have that T Qmax(s, a)  T i Qmax(s, a)  T i Qi(s, a) = Qi(s, a) for all i.
Using the contraction and monotonicity of the soft Bellman operator T  we have
Q(s, a) = lim (T )kQmax(s, a)  Qi(s, a) for all i.
k
We have just showed (9). In order to show (10), we note that V (s)  H[(·|s)] + Ea [Q(s, a)]  H[(·|s)] + Ea [Qmax(s, a)] =  log Z(s).

(17)

13

Under review as a conference paper at ICLR 2019

Similarly, we have, for all i,
V i(s) = Eai(·|s) Qi(s, a) -  log i(a|s)  Eai(·|s) [Qmax(s, a) -  log i(a|s)] =  log Z(s) - DKL(i(·|s) (·|s))   log Z(s).
The bound (10) follows from (17) and (18).

(18)

A.2 DC PROOF

Theorem 3.2 (DC Optimality) Let i, j be  max-ent optimal policies for tasks with rewards ri and rj with max-ent action-value functions Qi, Qj. Define Cb(st, at) as the fixed point of

Cb(k+1)(st, at) = -Ep(st+1|st,at)

log

A

i(at+1|st+1)bj

(at+1|st+1)(1-b)

exp(-

1 

Cb(k)(st+1,

at+1))dat+1

Given the conditions for Soft Q convergence, the max-ent optimal Qb(s, a) for rb = bri + (1 - b)rj is

Qb (s, a) = bQi(s, a) + (1 - b)Qj(s, a) - Cb(s, a) s  S, a  A, b  [0, 1].

We follow a similar approach to Haarnoja et al. (2018a) but without making approximations and generalizing to all convex combinations.

First

note

that

since

i

and

j

are

optimal

then

i(a|s)

=

exp(

1 

(Qi

(s,

a)

-

V

i(s))).

For brevity we use s and s notation rather than writing the time index.

Define

Q(b0)(s, a)  bQi(s, a) + (1 - b)Qj(s, a) C(0)(s, a)  0

(19) (20)

and consider soft Q-iteration on rb starting from Q(b0). We prove, inductively, that at each iteration Qb(k) = bQi(s, a) + (1 - b)Qj(s, a) - C(k)(s, a).

This is true by definition for k = 0.

Q(bk+1)(s, a) = rb(s, a) + Ep(s |s,a)

log

A

exp

1 

Q(bk)(s

,

a

)da

(21)

= rb(s, a)+

(22)

Ep(s |s,a)

log

exp( 1 (bQi(s , a ) + (1 - b)Qj(s , a ) - C(k)(s , a )))da A

= rb(s, a)+

(23)

Ep(s |s,a)

bV i(s ) + (1 - b)V j(s ) +  log

A

exp(b

log

i(a

|s

)

+

(1

-

b)

log

j (a

|s

)

-

1 C(k)(s 

,

a

))da

= bQi(s, a) + (1 - b)Qj(s, a)+

(24)

Ep(s |s,a)

log

A

exp(b

log

1(a

|s

)

+

(1

-

b)

log

2(a

|s

)

-

1 C(k)(s 

,

a

))da

= bQi(s, a) + (1 - b)Qi(s, a) - Cb(k+1)(s, a).

(25)

Since soft Q-iteration converges to the  max-ent optimal soft Q then equation 19 holds at the limit.

One can get an intuition for Cb(s, a) by noting that

Cb(1)(s, a) = Ep(s |s,a) [(1 - b) Db (1(·|s) 2(·|s))]

(26)

where Db is the Re´nyi divergence of order b. Cb(s, a) can be seen as the discount sum of divergences, weighted by the unnormalized product distribution 1(a|s)b2(a|s)1-b.

14

Under review as a conference paper at ICLR 2019

B ALGORITHM DETAILS
B.1 IMPORTANCE SAMPLED ESTIMATORS
For clarity we enumerate the various importance-sampled estimators used in our approach. All of these updates are off-policy. We use a replay buffer R and learn by sampling minibatches of SARS tuples of size B, we index over the batch dimension with l and use sl to denote the state following sl, so the tuple consists of (sl, al, rl, sl). For importance sampled estimators we sample N actions for each state sl and use alk to denote sample k for state l.
Proposal loss The proposal loss equation 12 is sampled using a mixture distribution p(a|s) as described in the main text.

p(a|s) = 1 n+1

1 VA +

n

qiq (a|s)

i=1

where V A is the volume of the action space (which is always bounded in our case).

(27)

L(q )



-

1 B

B

N
wkl log qq (a|st),

k=1 l=1

wkl =

1 

(QQ

(sk

,

akl

))

;

p(akl |sk )

wkl =

wkl

N m=1

wkm

.

(28) (29)

Value loss The value estimator for equation 15

L(V )



1 B

B

l=1

VV (sl) -  log

1

N

exp(

1 

QQ

(sl

,

alk

))

N
k=1

qq (alk|sl)

2
.

(30)

Action-value loss The action-value estimator for equation 34

L(Q)



1 2B

B
(QQ (sl, al) - (rl + VV (s )))2.

l=1

(31)

Successor Feature Value Estimator

L(i )



1 2B

B

N
wlk (ii (sl, alk) - Qi Qi (sl, alk) +  log Zi(sl))2 ,

l=1 k=1

wlk



exp(

1 

Qi(sl,

al

qiq (alk|sl)

k

))

.

(32) (33)

We use the importance sampled estimate of  log Z from eq 35, rather than the value network which may be lagging the true partition function. As with the value estimate, we use self-normalized importance sampling to avoid the weights depending on  log Z(sl) (this introduces a bias, but in practise appears to work well).

Successor Features Action Value Loss

L() =E

1 2

(Q

(st

,

at

)

-

((st

,

at

,

st+1

)

+





(st+1

)))2

.

(34)

Successor Feature Action-Value estimator

1 L(i )  2B

B
(i (sl, al) - (l +  (sl)))2.

l=1

(35)

15

Under review as a conference paper at ICLR 2019

CC (s, a, b) estimator

L(C )



1 N

B

l=1

CC (sl, al, bl) -  log

1 N CtaCrget(sl, alk, bl)

N
k=1

p(alk |sl)

2
, (36)

Ctarget(sl, alk, bl)C



exp(

1 

(bl

Qi Q

(sl

,

alk

)

+

(1

-

bl

)QjQ

(sl

,

alk

)

-

CC

(sl

,

alk

,

bl

)).

(37)

CondQ We also consider, as a control, learning the action-value function conditional on b di-
rectly (Schaul et al., 2015). We learn both a conditional value VV (s, b) and QQ (s, a, b), again by sampling b uniformly each update.

L(V ) = E,bU(0,1)

1 ( log
2

exp( 1 Q(s, a, b)))2 , 

(38)

LQ = E,bU(0,1)

1 2

(QQ

(s,

a,

b)

-

(rb

+

V

(s

,

b)))2

,

(39)

where computing rb for arbitrary b requires  to have been observed.

We estimate Cond-Q with the same importance samples as C. We use target networks for VV (s, b) and parametrize QQ (s, a, b) = VV (s, b) + AA (s, a, b).

Algorithm 1 Distributed single task adaptive importance sampled algorithm

Initialize proposal network q, value network parameters V and action-value network parameters Q and replay R

Copy to target nets 

while training do

in parallel on each actor

Obtain parameters  from learner

Roll out episode

Add experience to replay R

end while

while training do

in parallel on the learner

Sample SARS tuple from R

Minimize L(q) Minimize L(V ) Minimize L(Q)

if target update period then

Update target network parameters Q  Q, q  q end if

end while

C JUSTIFICATION FOR THE DC-CHEAP HEURISTIC

We wish to estimate Cb(s, a) (defined in Theorem 3.2) while avoiding learning a conditional function of b. We make two (substantial) assumptions to arrive at this approximation.

Firstly, we assume policies i(a|s), j(a|s) are Gaussian

i(a|s) = exp

- (a - µi(s))2 2(s)2

(40)

and the variance (s) is the same for both policies given a state (it may vary across states).

Secondly, we assume Cb(k)(s, a) = Cb(k)(s) is independent of action. This is approximately correct when nearby states have similar Re´nyi divergences between policies.

We make use of a result by Gil et al. (2013) that states that the Re´nyi divergence of order b for two

Gaussians of the same variance is

Db (N (µ1, )

N (µ2, )) =

1 2

b(µ1

- 2

µ2)2

.

(41)

16

Under review as a conference paper at ICLR 2019

We first define Gb(s)  (1 - b) Db (i(·|s) j(·|s)) = - log i(a|s)bj(a|s)(1-b)da.

(42)

From equation 40

Gb(s) = 4b(1 - b)G 1 (s). 2

(43)

Given these assumptions we show inductively that Cb(k)(s, a) = 4b(1 - b)C1(k/2) (s, a) k, b  [0, 1].

Since Cb(0)(s, a) = 0 b  [0, 1], a  A, s  S this is true for k = 0. We show it holds inductively

Cb(k+1)(s, a) = -Ep(s |s,a)

log

A

i(a

|s

)bj

(a

|s

)(1-b)

exp(-

1 

Cb(k)(s

,

a

))da

= Ep(s |s,a) Gb(s ) + Cb(k)(s )

(44) (45)

=

4(1

-

b)bC

(k+1)
1

(s,

a).

2

(46)

Obviously these assumptions are not justified. However, note that we estimate the true divergence for C1/2, i.e. without any assumptions of Gaussian policies and this heuristic is used to estimate Cb from C1/2. In practise, we find this heuristic works in many situations where the policies have similar variance, particulary when bounded by GPI.

D EXPERIMENT DETAILS
All control tasks were simulated using the MuJoCo physics simulator and constructed using the DM control suite (Tassa et al., 2018) which uses the MuJoCo physics simulator (Todorov et al., 2012).

Figure 5: Jumping ball tricky task
The point mass was velocity controlled, all other tasks were torque controlled. The planar manipulator task was based off the planar manipulator in the DM control suite. The reward in all tasks was sparse as described in the main text.
During training for all tasks we start states from the randomly sampled positions and orientations. For the point mass, jumping ball and ant we evaluated transfer starting from the center (in the walker environments, the starting orientation was randomly sampled during transfer, the point mass does not have an orientation). For the planar manipulator transfer was tested from same random distribution as in training. For all tasks were learned infinite time horizon policies.
Transfer is made challenging by the need for good exploration. That was not the focus on this work. We aided exploration in several ways: during training we acted according to a higher-temperature policy e = 2. We also sampled actions uniformly in an -greedy fashion with = 0.1 and added Gaussian exploration noise during training. This was sufficient to explore the state space for most
17

Under review as a conference paper at ICLR 2019

tasks. For the planar manipulator and the jumping ball, we found it necessary to induce behavior tasks by learning tasks for reaching the blue target. This behavior policy was, of course, only used for experience and not during transfer.
Below we list the hyper-parameters and networks use for all experiment. The discount  and  were the only sensitive parameters that we needed to vary between tasks to adjust for the differing magnitudes of returns and sensitivity of the action space between bodies. If  is too small then the policies often only find one solution and all transfer approaches behave similarly, while for large  the resulting policies are too stochastic and do not perform well.

Proposal learning rate All other learning rates Value target update period Proposal target update period  target update period Number of importance samples for all estimators during learning Number of importance samples for acting during training Number of importance samples for acting during transfer

10-3 10-4 200 200 500 200
50 1000

Table 2: Parameters the same across all experiments

The state vector was preprocessed by a linear projection of 3× its dimension and then a tanh nonlinearity. All action-state networks (Q, , C) consisted of 3 hidden layers with elu non-linearities (Clevert et al., 2015), with both action and preprocessed state projected by linear layers to be of the same dimensionality and used for input the first layer. All value networks and proposal networks consisted of 2 layers with elu non-linearities. The number of neurons in each layer was varied between environments, but was kept the same in all networks and layers (we did not sweep over this parameter, but choose a reasonable number based on our prior on the complexity of the task).
Below we list the per task hyper-parameters

Task

Number of units  

Point mass

22 1 0.99

Planar Manipulator 192 0.05 0.99

Jumping Ball

192 0.2 0.9

Ant 252 0.1 0.95

Table 3: Parameters varied between experiments

18

