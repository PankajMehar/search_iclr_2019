Under review as a conference paper at ICLR 2019
REGULARIZED LEARNING FOR DOMAIN ADAPTATION UNDER LABEL SHIFTS
Anonymous authors Paper under double-blind review
ABSTRACT
We propose Regularized Learning under Label shifts (RLLS), a principled and practical approach to correct for shifts in the label distribution between the source and target domains, and is a special case of domain adaptation. We estimate the importance weights using both labeled source and unlabeled target data and then train a classifier on the weighted source samples. Depending on the number of source and target samples, we regularize the influence of these estimated weights and train a corrected classifier on the source set with the estimated weights. We derive a generalization bound for the loss of the classifier on the target set. This bound is independent of the (ambient) data dimensions and instead depends on the complexity of the function class. To the best of our knowledge, this is the first generalization bound for label-shift problems where the labels in the target domain are unknown. Based on this bound, we design a regularized estimator for importance weights that allows us to adapt to the uncertainty in the estimated weights for the small-sample regime. Experiments show that regularized RLLS significantly improves accuracy in the low (target) sample and large-shift regimes compared to previous methods.
1 INTRODUCTION
In supervised learning, the task to make predictions on a given unlabeled target set is usually solved by training a predictive model on a labeled source set. This is a sensible approach particularly when the source and target sets are expected have i.i.d. samples from the same distribution. However, this assumption does not hold in many real-world applications. For example, often times predictions for medical diagnostics are based on data from a particular population of patients and machines due to the limited variety of data that is available in the training phase.
To further expand on the example above, consider the following scenario. A classifier is trained to predict whether a patient has contracted a severe disease in country A based on the physical measurements and diagnosis (labeled source data). The disease is potentially even more prevalent in country B, where however no doctors are available to make a diagnosis which can only be reliably made using expert knowledge. The following questions are relevant in this scenario:
· If we send a group of volunteers to country B with the same equipment to record measurements of patients, how can the data in country A and the new unlabeled data from country B be used to update the classifier to give good diagnostic predictions for patients in country B?
· If the patients in country B come in slowly over time and the disease is deadly, can the classifier be updated after only few patients have been measured, so as to maximize the accuracy of the diagnosis for the following patients?
Similar problems arise in many other domains besides health care, but we will use this medical example throughout the paper for ease of presentation. If we view the data in the source and target set, each consisting of an input x  X and a label y in a finite domain Y, as i.i.d. samples drawn from joint distributions P and Q, we are dealing with the distributional shift problem when P = Q. There are various options to model the difference between P and Q, depending on the use case at hand. Schölkopf et al. (2012) studies two main perspectives: (i)
1

Under review as a conference paper at ICLR 2019

causal learning where the explanatory input x causes the label y, and (ii) anti-causal setting, where the label y causes the input x.

For the causal setting, one usually makes the covariate shift assumption that the marginal distribution of the covariates x shifts from P to Q, whereas the conditional probability mass function of y given x stays constant, which we denote as p(y|x) = q(y|x) for all y  Y and x  X . Although this is a valid model in some situations, in many data science applications there is an inherent ground truth y that we want to learn given observations x, which can be "explained" by y. For example, the physical symptoms reported by a patient might be approximately similar among all sick patients. This simplified relation can be modeled e.g. by a directed graphical model Y  X. It is now possible that the percentage of ill patients in the two countries A and B are very different, i.e. P(Y = i) = Q(Y = i), or p(y) = q(y) for short. Similarly, within one country, the number of ill patients may suddenly change because of an irregular event such as a natural catastrophe.

In all of the above cases, there is a shift in the

distribution of the label y (sick or healthy), while given a fixed label, the conditional distribution of x given y is constant. By the Bayes rule, label shift causes the covariate shift condition to be violated, i.e.

Covariate Shift Label Shift
p(x) = q(x) p(y) = q(y) p(y|x) = q(y|x) p(x|y) = q(x|y)

p(y|x) = q(y|x). Despite being an intuitively natural scenario in some cases, even this simplified

model has not yet been sufficiently studied in the literature. This paper aims to fill the gap in terms of

both theoretical understanding and practical methods for the label shift setting and thereby move a

step closer towards more complicated and realistic distributional shift models.

Lipton et al. (2018) introduced a provably good importance-weight estimator called BBSE, which has access to the labeled source data set and a large batch of unlabeled target data, and demonstrated promising performance on real data. There are a two natural follow-up questions which remain unanswered by their work: What are the generalization guarantees for the final predictor which uses the weighted samples? How do we deal with the uncertainty of the weight estimation when only few samples are available and how can we update the classifier?

In this work, we propose an efficient method to estimate the importance weights and relax the problem-dependent warm-up sample complexity requirement for BBSE (which is unknown and can be arbitrary large). We further improve the estimation error in the finite-sample setting by a factor of k log(k) over the prior work, where k is the number of classes. We deploy this estimator and propose a label-shift correcting estimator and provide generalization guarantees. Similar to Lipton et al. (2018), we first use a black-box classifier, which can be biased, uncalibrated and inaccurate, to estimate importance weights for the source samples. Depending on the number of source and target samples, we regularize the influence of these estimated weights and train a corrected classifier on the source set with the estimated weights. We then derive a dimension-independent generalization bound for the final Regularized Learning under Label Shift (RLLS) classifier.

We empirically study the performance of RLLS and show weight estimation as well as prediction accuracy for a variety of shifts, including shifting source data and shifting target data with various parameters, different sample sizes and regularization parameters. We achieve an order of magnitude smaller weight estimation error than baseline methods in large shift cases and enjoy 20% higher accuracy or F-1 score in corresponding predictive tasks. We also see at least 10% accuracy improvement using our regularized estimator when faced with small target sample sizes.

2 REGULARIZED LEARNING OF LABEL SHIFTS (RLLS)

Let us formally define the short hand for the marginal probability mass functions of Y with respect to

P, Q as p, q : [k]  [0, 1] with p(i) = P(Y = i), and q(i) = Q(Y = i) for all i  [k], representable

by vectors in Rk+ which sum to one. In the label shift setting, we define the importance weight vector

w



Rk

between

these

two

domains

as

w(i)

=

q(i) p(i)

.

We

quantify

the

shift

using

the

exponent

of

the

infinite and second order Renyi divergence as follows

q(i) k q(i)

d(q||p)

:=

sup
i

p(i)

,

and

d(q||p) := E(Y )Q [w(Y )Y ] =

q(i) . p(i)

i

2

Under review as a conference paper at ICLR 2019

Given a hypothesis class H, a loss function : Y × Y  [0, 1] our aim is to find the hypothesis h  H which minimizes

L(h) = E(X,Y )Q [ (Y, h(X))] = E(X,Y )P [w(Y ) (Y, h(X))]
In the usual finite sample setting however, L unknown and we are given samples {(xj, yj)}jn=1 from P instead. If we are given the vector of importance weights w we could then minimize the empirical loss with importance weighted samples defined as

Ln(h)

=

1 n

n

w(yj) (yj, h(xj))

j=1

where n is the number of available observations drawn from P used to learn the classifier h. As w is unknown in practice, we have to find the minimizer of the empirical loss with estimated importance weights

1n Ln(h; w) = n w(yj) (yj, h(xj))
j=1

(1)

where w are estimates of w. Given a set Dp of np samples from the source distribution P, we first divide the data set into two where we use (1 - )np samples in set Dpweight to compute the estimate w and the remaining n = np in the set Dpclass to find the classifier which minimizes the loss (1),
i.e. hw = arg min hH Ln(h; w). In the following, we describe how to estimate the weights w and
provide guarantees for the resulting estimator hw.

Plug-in weight estimation The following simple correlation between the label distributions p, q was noted in Lipton et al. (2018): for a fixed hypothesis, if for all y  Y it holds that q(y)  0 = p(y)  0 (), we have

qh(Y = i) = Q(h(X) = i|Y = j)q(j) = P(h(X) = i|Y = j)q(j)
jj

q(j)

=

P(h(X) = i, Y

=

j) p(j)

=

P(h(X) = i, Y = j)wj

jj

for all i, j  Y, which can equivalently be written in matrix vector notation (by slight abuse of notation) as

qh = Chw

(2)

where Ch is the confusion matrix, that is [Ch]i,j = P(h(X) = i, Y = j) and qh is the vector which represents the probability mass function of h(X) under distribution Q. The assumption () is a reasonable condition since without any prior knowledge, there is no way to properly reason about a
class in the target domain that is not represented in the source domain.

In reality, both qh and Ch can only be estimated by the corresponding finite sample averages qh, Ch.
Lipton et al. (2018) simply compute the inverse of the estimated confusion matrix Ch in order to estimate the importance weight, i.e. w = Ch-1qh. While Ch-1qh is a statistically efficient estimator, w with estimated Ch-1 can be arbitrarily bad especially for small sample sizes and small minimum singular values. Intuitively, when there are very few samples, the weight estimation will have high variance in which case it might be better to avoid importance weighting altogether. Furthermore, even when the sample complexity in Lipton et al. (2018), unknown in practice, is met, the resulting error of this estimator is linear in k which is problematic for large k.

We therefore aim to address these shortcomings by proposing the following two-step procedure to compute importance weights: Given a "decent" black box estimator which we denote by h0, we

1. calculate the measurement error adjusted  as described in Section 2.1 for h0
2. compute the regularized weight w = 1 +  where  depends on the sample size (1 - )np
By "decent" we refer to a classifier h0 which yields a full rank confusion matrix Ch0 An example for such a classifier h0 is one that always outputs a fixed class. As it does not capture any characteristics of the the data, there is no hope to gain any statistical information.

3

Under review as a conference paper at ICLR 2019

2.1 ESTIMATOR CORRECTING FOR FINITE SAMPLE ERRORS

Both the confusion matrix Ch0 and the label distribution qh0 on the target for the black box hypothesis h0 are unknown and we are instead only gave access to finite sample estimates Ch0 , qh0 . In what follows all empirical and population confusion matrices as well as label distributions are defined with respect to the hypothesis h = h0. For notational simplicity, we thus drop the subscript h0 in what follows.
In the unweighted case we have w = 1 so that define the amount of weight shift as  = w - 1. The reparameterized linear model (2) with respect to  then reads
b := q - C1 = C

with corresponding finite sample quantity b = q - C1. When C is near singular, the estimation of 

becomes unstable. Furthermore, large values in the true shift  result in large variances. We address

this problem by adding a regularizing 2 penalty term to the usual loss and thus push the amount of shift towards 0, a method that has been proposed in (Pires & Szepesvári, 2012). In particular, we

compute

 = arg min

C - b 2 + 3C  2

:{2i+1C b,iN}

(3)

where  is defined by

 = arg min

C

-b

2 2

+





2.



The proof can be found in Section A.1. Here, C, b are parameters which will eventually be high

probability upper bounds for C - C 2 and b - b 2 respectively. Let min be the minimum singular

value of the true confusion matrix C. For the estimate , we then have the following upper bound

Lemma 1 For  as defined in equation (3), we have with probability at least 1 -  that

- 2 O

1 min

2

where  < 0.51.

log(k/) (1 - )np +

log(1/) (1 - )np +

log(1/) nq

A couple of remarks are in order at this point. First of all, the upper bound in Lemma 1 includes min(C) as a parameter, apart from the norm of the true weight shift . Recall that these quantities are all heavily dependent on the black box estimator h0 that we are given a priori. In particular, if h0 is an ideal estimator, and the source set is balanced, C is the unit matrix with min = 1/k. In contrast, when the model h0 is uncertain, the singular value min is close to zero. For least square problems under the Gaussian assumption and with measurement errors in the matrix, it is standard to use regularized total least squares approaches which requires a single value decomposition. Our choice for the alternative estimator in Eq. 3 with norm instead of norm squared regularization is
motivated by the label shift setting, where true shifts  with a large norm may shrink the estimate  too much, away from the true .

2.2 REGULARIZED ESTIMATOR AND GENERALIZATION BOUND

When the number of samples in the target set is very small or the label shift is mild, the estimated

weights might be too uncertain to be applied. We therefore propose a regularized estimator defined as

follows

w = 1 + .

(4)

Note that w implicitly depends on , . By rewriting w = (1 - )1 + (1 + ), we see that intuitively  should be closer to 1 the more reason there is to believe that 1 +  is in fact the true weight.

Let us define the set G( , H) = {gh(x, y) = w(y) (h(x), y) : h  H} for a loss function with   1 and hypothesis class H and the commonly used Rademacher complexity measure

1Throughout the paper, O hides universal constant factors. Furthermore, we use O (· + ·) for short to denote O (·) + O (·).

4

Under review as a conference paper at ICLR 2019

Algorithm 1 RLLS
1: Input: source set Dp, Dq, max, black box estimator h0, model class H 2: Determine optimal split ratio  and regularizer  by minimizing the RHS of Eq. (6) 3: Randomly partition source set Dp into Dpclass, Dpweight such that |Dpclass| =  np =: n
4: Compute  using Eq. (3) and w := 1 +   5: Minimize the importance weighted empirical loss to obtain the weighted estimator

hw

=

arg

min
hH

Ln(h; w),

where

1

Ln(h; w) = n

w(y) (y, h(x))

(x,y)Dpclass

6: Deploy hw if the risk is acceptable

Rn(G)

:=

EX(i),Y (i)P:i[n]

Ei :i[n]

1 n

[suphH

n i=1

iw(yi

)

(Xi, h(Yi))]

(see e.g. Bartlett

& Mendelson (2002)). We can now state a generalization bound for the classifier hw in a general hypothesis class H, which is trained on source data with the estimated weights defined in equation (4).

Theorem 1 (Generalization bound for hw) Given np samples from the source data set and nq samples from the target set, a hypothesis class H and loss function , the following upper bound

|L(hw) - L(h)|  2Rn(G) + d(q||p)

log(1/) + (1 - ) np



2

+ O min

2

log (k/) +
(1 - )np

log (1/) +
(1 - )np

log (1/) nq

holds with probability at least 1 -  where   0.5.

(5)

The

proof

can

be

found

in

Section

A.2

as

well

a

tighter

bound

of

O(

d (q ||p) nq

+

d(q||p) nq

)

in

terms

of

dependence on the quantities d(q||p) for finite hypothesis classes in Section A.3. The size of Rn(G)

is determined by the structure of the function class H and the loss . For example for the 0/1 loss,

the VC dimension of H can be deployed to upper bound the Rademacher complexity.

In order to use Theorem 1 in practice and exploit the possibility of choosing parameters ,  depending
on the sample sizes, we need to assume some prior knowledge about the shift  and min of the possible shifts we might encounter. In particular, we assume that it is known that the label shift is not too big, i.e.  2  max (on the other hand it is unreasonable to have knowledge about the shift estimator, i.e. min) and we consider the following upper bound instead for practical implementation

|L(hw) - L(h)|  2Rn(G) + d(q||p)

log(2/) np

+

(1

-

) max

+ O min

max

log (k/) (1 - )np +

log (3/) (1 - )np +

log (3/) .
nq

(6)

The bound in equation (6) suggests to use algorithm 1, in particular

when only very few target samples have been observed before

an update of the classifier is necessary. The bound (6) gives us

guidance for a good choice of  as a function of the number of

samples in practice.

Whenever nq



1 m2 ax(min- 1np )2

(hereby

neglecting the log factors), the best choice is  = 1 and 0 else.

In Figure 2.2 we show curves of fixed min in parameter space, under which  should be chosen as 0 and above which it should

be 1. When the confusion matrix has small singular values, we see

that the estimated weights should only be trusted for rather high

nq and high believed shifts max.

nq

5 10
4 10
3 10
2 10

min

0.2

0.4

0.6

0.8

1.0

max

Figure 1: Given a min and max,  switches from 0 to 1 at a particular nq. np is kept fixed.

5

Under review as a conference paper at ICLR 2019

However, since max is just an upper bound on the true amount of shift  2, when
11 m2 ax(min - 1nq )2  nq   2(min - 1nq )2
 should in fact ideally be 0. Thus for nq that are a little bit below the threshold (depending on the certainty of the belief on max), it could be sensible to use an intermediate value   (0, 1) which we explore in the experimental section 3.3.

3 EXPERIMENTS

In this section we illustrate our theoretical bounds by running our RLLS estimation procedure on a variety of artificially generated shifts on the MNIST (LeCun & Cortes, 2010) and CIFAR10

(Krizhevsky & Hinton, 2009) datasets. In order to find  = 1 +  in Eq. (3), performing a grid search over  on the entire set of natural numbers is not feasible. Instead, we call a built-in solver to directly

solve the low dimensional problem min

C - b

2 2

+

3C

 2 where the choice of  = 0.01 is

observed to yield a low loss in the first line of Eq. (3) on various levels of label shift.

We follow three different kinds of sampling strategies to generate a label shift between the source and target set: Tweak-one Shift, Dirichlet shift, and Minority-class shift. For the Tweak-one Shift, we randomly choose one class, e.g. i and set p(y = i) =  while all other classes are distributed evenly. For the Dirichlet shift, we draw a probability vector p from the Dirichlet distribution with concentration parameter set to  for all classes, before including sample points which correspond to the multinomial label variable according to p. In Minority-Class Shift, we set a certain number of classes to have probability p = 0.001, while the distribution over the rest of the classes is uniform. One of these sampling strategies is used to sample either the source or the target set, while the other set is sampled with a uniform distribution.

After artificially shifting the label distribution in one of the source and target sets, we then follow algorithm 1, where we choose the black box predictor h0 to be a two-layer fully connected neural network trained on the (unweighted) source dataset. We then evaluate the mean square estimation error (MSE) E w - w 2, predictive accuracy on the target set as well as F-1 score, 2(precision  recall)/(precision + recall), and compare them with the performance of the black box shift estimation
method (BBSE) in Lipton et al. (2018). Here precision is the the proportion of true predicted labels
in all the predicted labels, while recall is the proportion of true predicted labels in all the true labels.
Theoretically, KMM (Zhang et al., 2013) is also a possible baseline to compare with. However, as
mentioned in Lipton et al. (2018), it is not scalable to large sample sizes for weight estimation.

3.1 WEIGHT ESTIMATION COMPARISON
In this set of experiments on the CIFAR10 dataset, we illustrate our weight estimation performance using a simple two-layer neural network as the black box estimator h0 by comparing it with BBSE from Lipton et al. (2018).
The precise procedure to artificially create a particular shift is as follows: we sample target data uniformly and sample data points from the source set according to a probability vector p(y) that corresponds to different shift parameters  in tweak-one Shift and  in Dirichlet Shift for 20 times. Hereby, we fix the number of source and target data points to 10000 for the Dirichlet shift and tweak-one shift for   0.5. For bigger shifts, i.e.   0.5, we keep the maximum possible number of samples. In Figure 2, we compare the proposed RLLS weight estimation to the BBSE from (Lipton et al., 2018). When the shift is small ( is small), the two methods perform similarly, while RLLS outperforms BBSE dramatically when the shift is large.

3.2 PREDICTIVE TASKS WITH LARGE SAMPLE SIZE
In this section, we compare the predictive performances in terms of accuracy and F-1 score between a classifier trained on unweighted source data and the classifiers trained on weighted loss obtained by using the RLLS and BBSE procedure (which is called BBSC).

6

Under review as a conference paper at ICLR 2019
(a) (b) Figure 2: Comparing MSE of estimated weights using BBSE and RLLS on CIFAR10 with (a) tweak-one shift on source and uniform target, and (b) Dirichlet shift on source and uniform target
When nq is large,  is set to one and thus the intermediate weight estimate  is "fully" used, i.e. corresponds to w = 1 + . In this case, better weight estimation should directly lead to better performance of the estimator. One subtlety arises when we compare shifts on the target vs. shifts on the source domain. When the target is seriously biased for smaller , the predictive task becomes rather easy, such that even the majority class vote would give high accuracy, though it would have zero accuracy on all but one class. Therefore, metrics that account for class imbalance and other types of errors in confusion matrix, like the F-1 score, are in order to allow for a more comprehensive performance evaluation for the prediction. Figure 3 depicts the MSE of the weight estimation (a), the corresponding performance comparison on accuracy (b) and F-1 score (c) when the target data on CIFAR10 is shifted using the Dirichlet Shift. We use the Dirichlet parameters  = [0.01, 0.1, 1, 10] to bias the target set and run 20 different experiments for each parameter. We use the ResNet-18 (He et al., 2016) deep neural net as our hypotheis class H. We can see that in the large shift case when  = 0.01, the F-1 score for BBSC and unweighted classifier can be very bad while the accuracy is still high. This is due to classifiers making wrong predictions on minority classes in the target data.
(a) (b) (c) Figure 3: (a) Mean squared error in estimated weights, (b) accuracy and (c) F-1 score on CIFAR10 for uniform source and Dirichlet shifted target.
A harder problem which does not allow "cheating" is when the shift is in fact on the source set and the target set has uniform label class distribution. Figure 4 shows results using Minority-Class shifted source data and uniform target data from MNIST. For this set of experiments, we first randomly separate the original dataset into source and target before randomly sampling 5000 data points from the target set. The number of minority class ranges from 1 to 4 to bias the source set and run 20 different experiments for each parameter. To better see the difference between methods, we create relatively large shift with one or more class of data being minority. We also use a fixed black-box classifier that is trained on biased source data. We use two-layer fully connected neural networks for both weight estimation and predictive tasks. We can see that the MSE in weight estimation is relatively large. As the error goes up, the performance for all methods goes down, except for BBSC approaching RLLS due to similar weight estimation in the extreme case. Moreover, RLLS always outperforms BBSC whenever the weight estimation error is lower. Notice that generalizing from an imbalanced training set is a common data science task where reweighting the samples in the source set is also standard practice (and it is a priori assumed that the target set is uniform).
7

Under review as a conference paper at ICLR 2019
(a) (b) Figure 4: (a) Mean squared error in estimated weights and (b) accuracy on MNIST for minority-class shifted source and uniform target.
3.3 LABEL SHIFT PREDICTIVE TASKS IN THE LOW SAMPLE REGIME As discussed in Section 2.2,  in equation (4) may be chosen according to the believed upper and lower bounds on max, min. In the following, we present simulation results of our procedure in the low sample regime in Figure 5 where we choose different values of  and vary the number of target samples nq. Here we fix the sample size in source as 10000 and investigate a Dirichlet Shift with fixed  = 0.5 on the source data. Figure 5 suggests that our method with   (0, 1) outperforms the fully weighted case, i.e.  = 1, for small sample sizes. This shows that we are able to adjust to uncertainty on importance weights and generalize well for a varying range of target sample sizes. We also show a comparison of using different black-box predictors h0 for weight estimation and observe that fully weighted methods with  = 1 achieve the best performance faster with a better trained black-box classifier (a), while it takes longer for it to improve with corrupted one (c). Here we train the black-box classifier with full training data (a), tweak-one with  = 0.2 (b) and tweak-one with  = 0.6 (c) to obtain different predictors. This reflects the relation between eigenvalue of confusion matrix min and target sample size nq in Theorem 1. In other words, we need more samples from the target data to compensate a bad predictor in weight estimation. So the generalization error decreases faster with an increasing number of samples for good predictors.
(a) (b) (c) Figure 5: Performance on MNIST for Dirichlet shifted source and uniform target with various target sample size and  using (a) better predictor, (b) neutral predictor and (c) corrupted predictor.
4 RELATED WORK
The task of being robust against distributional shifts has been widely studied in the literature. One line of work focuses on distributional robustness (see e.g. Esfahani & Kuhn; Namkoong & Duchi (2016)) without explicitly modeling the shift by making assumptions on the distribution. However, these methods achieve robustness only against tiny changes in the distribution and are often impractical for large-scale high dimensional data. Another line of work has addressed the causal setting where the conditional distribution of Y given X is assumed to be constant. This assumption is sensible when there is reason to believe that there is a true optimal mapping from X to Y which does not change if the distribution of X changes. In our example, if a given set of symptoms correctly predicts a disease for the "right reasons", the
8

Under review as a conference paper at ICLR 2019
predictive function should be the same, no matter which country you are moving to. Among the various methods to correct for covariate shift, the majority uses the concept of importance weights q(x)/p(x) (Zadrozny, 2004; Cortes et al., 2010; Cortes & Mohri, 2014; Shimodaira, 2000), which are unknown but can be estimated for example via kernel embeddings (Huang et al., 2007; Gretton et al., 2009; 2012; Zhang et al., 2013; Zaremba et al., 2013) or by learning a binary discriminative classifier between source and target (Lopez-Paz & Oquab, 2016; Liu et al., 2017). Common issues shared by these methods is that they may result in a massive computational burden for large scale problems. Moreover, inaccurate estimates make classifiers suffer from high variance.
Despite the wide applicability of label shift, approaches with global guarantees in high dimensional data regimes remain under-explored. The correction of label shift mainly requires to estimate the importance weights q(y)/p(y) over the labels which typically live in a very low-dimensional space. Bayesian approaches are studied when a prior over the marginal label distribution is assumed (Storkey, 2009; Chan & Ng, 2005). However, these methods often require explicitly computing the posterior distribution of y and suffer from the curse of dimensionality. Recently, for the label shift setting, Lipton et al. (2018) proposed Black Box Shift Estimation (BBSE), where a black box classifier is used to estimate the importance weights q(y)/p(y) which are then used to train a classifier on the source data. This approach is related to Buck et al. (1966); Forman (2008); Saerens et al. (2002) in the low dimensional setting but lacks guarantees for the excess risk.
Generalization theory, which is usually used to bound the excess loss, has historically been developed for the case when P = Q (see e.g. (Vapnik, 1999; Bartlett & Mendelson, 2002; Kakade et al., 2009; Wainwright, 2019)). Ben-David et al. (2010) provides theoretical analysis and generalization guarantee for domain adaption when the H-divergecne between join distributions is considered, while Crammer et al. (2008) shows the learning from multiple sources. For the covariate shift setting, Cortes et al. (2010) provides a generalizing bound when q(x)/p(x) is known which however does not apply in practice.
5 DISCUSSION
In this work, we establish the first generalization guarantee for the label shift setting where no prior knowledge of q(y)/p(y) is required. Although RLLS is inspired by BBSE, it carries a more robust importance weight estimator as well as generalization guarantees for both the large and small sample regime. RLLS is also equipped with a sample-size-dependent regularization technique and further improves the classifier in both regimes.
Note that we consider this a necessary first step in the direction of solving shifts of this type, although the label shift assumption might not hold in the real world. In particular, x in practice cannot be solely explained by the wanted label y, but also depends on attributes z which might not be observable. For example, the symptoms might not only depend on the disease but also the country and living conditions, in the example of disease prediction. In such a case, the label shift assumption only holds in a slightly modified sense, i.e. that P(X|Y = y, Z = z) = Q(X|Y = y, Z = z). If the attributes Z are observed, then our framework can readily be used to perform importance weights. In the future work, we plan to study the setting where the label shift assumption is slightly violated.
Furthermore, it is not clear whether the final predictor is in fact "better" or more robust to shifts just because it achieves a better target accuracy than a vanilla unweighted estimator. In fact, there is a reason to believe that under certain shift scenarios, the predictor might learn to use spurious correlations to boost accuracy. Finding a procedure which can both learn a robust model and achieve high accuracies on new target sets remains to be an ongoing challenge. Moreover, the current choice of regularization depends on the number of samples rather than date driven regularization is desired.
An interesting direction toward active learning for the same disease-symptoms problem is when in the country B we also have a practitioner whom we can ask to diagnose some of the patients. Now the question is which patient we would diagnose? Also, consider the case of high risk, we might be able to send some of the patients to the country A for further medical diagnosis or treatment, up to some varying cost. We plan to extend the current framework to active learning setting where we actively query the label of certain x's (Beygelzimer et al., 2009) as well as the cost-sensitive setting where we also consider the cost of querying labels (Krishnamurthy et al., 2017).
9

Under review as a conference paper at ICLR 2019
REFERENCES
Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture models and hidden markov models. In Conference on Learning Theory, pp. 33­1, 2012.
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning of pomdps using spectral methods. arXiv preprint arXiv:1602.07764, 2016.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151­175, 2010.
Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In Proceedings of the 26th annual international conference on machine learning, pp. 49­56. ACM, 2009.
AA Buck, JJ Gart, et al. Comparison of a screening test and a reference test in epidemiologic studies. ii. a probabilistic model for the comparison of diagnostic tests. American Journal of Epidemiology, 83(3):593­602, 1966.
Yee Seng Chan and Hwee Tou Ng. Word sense disambiguation with distribution estimation. In IJCAI, volume 5, pp. 1010­5, 2005.
Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and algorithm for regression. Theoretical Computer Science, 519:103­126, 2014.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In Advances in neural information processing systems, pp. 442­450, 2010.
Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple sources. Journal of Machine Learning Research, 9(Aug):1757­1774, 2008.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming, pp. 1­52.
George Forman. Quantifying counts and costs via classification. Data Mining and Knowledge Discovery, 17(2):164­206, 2008.
Arthur Gretton, Alexander J Smola, Jiayuan Huang, Marcel Schmittfull, Karsten M Borgwardt, and Bernhard Schölkopf. Covariate shift by kernel mean matching. 2009.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723­773, 2012.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Daniel Hsu, Sham M Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):1460­1480, 2012.
Jiayuan Huang, Arthur Gretton, Karsten M Borgwardt, Bernhard Schölkopf, and Alex J Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pp. 601­608, 2007.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pp. 793­800, 2009.
Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daume III, and John Langford. Active learning for cost-sensitive classification. arXiv preprint arXiv:1703.01014, 2017.
10

Under review as a conference paper at ICLR 2019
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann. lecun.com/exdb/mnist/.
Zachary C Lipton, Yu-Xiang Wang, and Alex Smola. Detecting and correcting for label shift with black box predictors. arXiv preprint arXiv:1802.03916, 2018.
Song Liu, Akiko Takeda, Taiji Suzuki, and Kenji Fukumizu. Trimmed density ratio estimation. In Advances in Neural Information Processing Systems, pp. 4518­4528, 2017.
David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. arXiv preprint arXiv:1610.06545, 2016.
Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In Advances in Neural Information Processing Systems, pp. 2208­2216, 2016.
Bernardo Avila Pires and Csaba Szepesvári. Statistical linear estimation with penalized estimators: an application to reinforcement learning. arXiv preprint arXiv:1206.6444, 2012.
Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure. Neural computation, 14(1):21­41, 2002.
Bernhard Schölkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij. On causal and anticausal learning. arXiv preprint arXiv:1206.6471, 2012.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227­244, 2000.
Amos Storkey. When training and test sets are different: characterizing learning transfer. Dataset shift in machine learning, pp. 3­28, 2009.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389­434, 2012.
Vladimir Naumovich Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988­999, 1999.
M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press, 2019.
Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In Proceedings of the twenty-first international conference on Machine learning, pp. 114. ACM, 2004.
Wojciech Zaremba, Arthur Gretton, and Matthew Blaschko. B-test: A non-parametric, low variance kernel two-sample test. In Advances in neural information processing systems, pp. 755­763, 2013.
Kun Zhang, Bernhard Schölkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In International Conference on Machine Learning, pp. 819­827, 2013.
11

Under review as a conference paper at ICLR 2019

A PROOFS

A.1 PROOF OF LEMMA 1

From Thm. 3.7 in (Pires & Szepesvári, 2012) we know that for w as defined in equation (3), if with

probability at least 1 - , C - C 2  C and b - b 2  b hold simultaneously, then

()  inf {( ) + 3C  } + 3b.
 Rk

(7)

where we use the shorthand () = C - b 2.

We can get an upper bound on the right hand side of (7) is the infimum by simply choosing a feasible

 = . We then have C - b 2 = 0 and hence

as a consequence,

inf{( ) + 3C  2}  3C  2


() = C - b 2 = C  -  2  3C  2 + 3b

Since C  - 

2  min(C)  -  2 by definition of the minimum singular value, we thus have 1
 -  2  min (3C  2 + 3b)

Let us first notice that

bh = qh - C1 = qh - ph

The mathematical definition of the finite sample estimates Ch, bh (in matrix and vector representation) with respect to some hypothesis h are as follows

1

[Ch]ij

=

(1 - )np

Ih(x)=i,y=j
(x,y)Dp

11

[bh](i)

=

m

(x,y)Dq

Ih(x)=i

-

(1 - )np

Ih(x)=i
(x,y)Dpweight

where m = |Dq| and I is the indicator function. Ch, bh can equivalently be expressed with the population over P for Ch and over Q for bh respectively. We now use the following concentration

Lemmas to bound the estimation errors of C, b where we drop the subscript h for ease of notation.

Lemma 2 (Concentration of measurement matrix C) For finite sample estimate C we have

C -C

2

2 log (2k/) +
3(1 - )np

with probability at least (1 - ).

2 log (2k/) (1 - )np

Lemma 3 (Concentration of label measurements) For the finite sample estimate b with respect to any hypothesis h it holds that

b - b 2  2 log(1/) with probability at least 1 -  for  < 0.5.

1 nq +

1 (1 - )np

By Lemma. 2 for concentration of C and Lemma. 3 for concentration of b we now have with probability at least 1 - 

-

2

1 min

2  2 log (2k/) + (1 - )np

2

18 log (4k/) (1 - )np

36 log (2/) 36 log (2/) + nq + (1 - )np .

which,

considering

that

O(

1 n

)

dominates

O(

1 n

),

yields

the

statement

of

the

Lemma

1.

12

Under review as a conference paper at ICLR 2019

A.1.1 PROOF OF LEMMA 2

We prove this lemma using the theorem 1.4[Matrix Bernstein] and Dilations technique from Tropp (2012).

We can rewrite Ch = E(x,y)P eh(x)ey where e(i) is the one-hot-encoding of index i.

Consider a finite sequence {(i)} of independent random matrices with dimension k. By dilations,

lets construct another sequence of self-adjoint random matrices of {(i)} of dimension 2k, such that

for all i

(i) =

0 (i)

(i) 0

therefore,

(i)2 =

(i)(i) 0

0 (i) (i)

(8)

which results in (i) 2 = (i) 2. The dilation technique translates the initial sequence of random matrices to the sequence of random self-adjoint matrices where we can apply the Matrix Bernstein theorem which states that, for a finite sequence of i.i.d. self-adjoint matrices (i), such that, almost
surely i, E (i) = 0 and (i)  R, then for all t  0,

1 t R log (2k/) 2 2 log (2k/)

(i) 

+

t 3t t

i=1

with probability at least 1 -  where 2 := E 2(i) 2, i which is also 2 = E 2(i) 2, i due to Eq. 8. Therefore, thanks to dilation trick,

1 t (i)  R log (2k/) + t 3t
i=1
with probability at least 1 - .

2 2 log (2k/) t

Now, by plugging in (i) = eh(x(i))ey(i) - C, we have E (i) = 0. Together with (i)  2 as well as 2 = E 2(i) 2 = 1 and setting t = n, we have

C -C

2

2 log (2k/) 3(1 - )n +

2 log (2k/) (1 - )n

A.1.2 PROOF OF LEMMA 3

The proof of this lemma is mainly based on a special case of and appreared at proposition 6 in Azizzadenesheli et al. (2016), Lemma F.1 in Anandkumar et al. (2012) and Proposition 19 of Hsu et al. (2012).
Analogous to the previous section we can rewrite bh = E(x,y)Q[eh(x)] - E(x,y)P[eh(x)] where e(i) is the one-hot-encoding of index i. Note that (dropping the subscript h) we have

bh - bh 2  qh - qh 2 + ph - ph 2

We now bound both estimates of probability vectors separately.

Consider a fixed multinomial distribution characterized with probability vector of   k-1 where

k-1 is a k - 1 dimensional simplex. Further, consider t realization of this multinomial distribution

{(i)}it=1 where (i) is the one-hot-encoding of the i'th sample. Consider the empirical estimate

mean

of

this

distribution

through

empirical

average

of

the

samples;



=

1 t

(i)t(i), then

 -   1 + log (1/) tt

13

Under review as a conference paper at ICLR 2019

with probability at least 1 - . By plugging in  = qh,  = qh with t = nq and finally {(i)}ni=q1 = {eh(x(i))}(i)nq and equivalently for ph we obtain for  < 0.5 that

bh - bh 2  2 log(1/)

11 (1 - )np + nq

we get the statement in Lemma. 3

A.2 PROOF OF THEOREM 1
We want to ultimately bound |L(hw) - L(h )|. By addition and subtraction we have L(hw) - L(h) = L(hw) - Ln(hw) + Ln(hw) - Ln(hw; w)
(b) (a)
+ Ln(hw; w) - Ln(h ; w) + Ln(h ; w) - Ln(h ) + Ln(h ) - L(h ) (9)
0 (a) (b)
where n = np and we used optimality of hw. Here (a) is the weight estimation error and (b) is the finite sample error.

Uniform law for bounding (b) We bound (b) using standard results for uniform laws for uniformly
bounded functions which holds since w   d(q||p) and   1. Since |w(y) (h(x), y)|  d(q||p), x, y  X × Y, by deploying the McDiarmid's inequality we then obtain that

sup |Ln(h) - L(h)|  2Rn(G( , H)) + d(q||p)
hH

log

2 

n

where G( , H) = {gh(x, y) = w(y) (h(x), y) : h  H} and the Rademacher complexity is defined

as Rn(G) := EX(i),Y (i)P:i[n]

Ei :i[n]

1 n

[suphH

n i=1

i

w(yi)

(xi, h(yi))]

.

of the hypothesis class H (see for example Percy Liang notes on Statistical Learning Theory and chapter 4 in Wainwright (2019))

Bounding term (a) Remember that k = |Y| is the cardinality of the finite domain of Y , or

the number of classes. Let us define  Rk with j =

n
i=1 Iy(i)=j

(y(i), h(x(i))).

Notice

that by definition 1  n and   n from which it follows by Hoelder's inequality that

2  n. Furthermore, we slightly abuse notation and use w to denote the k-dimensional vector

with w(i) = w(i). Therefore, for all h we have via the Cauchy Schwarz inequality that

|Ln(h;

w)

-

Ln(h;

w)|

=

|

1 n

n
(w(y(i)) - w(y(i))) (h(x(i)), y(i))|

i=1

 |1

k
(w(j) - w(j)) (j)|

n

j=1

1 n

w-w

2

2  w-w 2

  -  2  (1 - )  2 +   -  2

It then follows by Lemma 1 that
sup |Ln(h; w) - Ln(h; w)|  (1 - )  2
hH

 log(k/)

O

min (  2)

+ (1 - )np

log(1/) (1 - )np

log(1/) nq

Plugging both bounds back into equation (9) concludes the proof of the theorem.

14

Under review as a conference paper at ICLR 2019

A.3 GENERALIZATION FOR FINITE HYPOTHESIS CLASSES

For finite hypothesis classes, one may bound (b) in (9) using Bernstein's inequality.

We bound (b) by first noting that w(Y ) (Y, h(X)) satisfies the Bernstein conditions so that Bernstein's inequality holds

EP[w(Y )] = 1, EP[w(Y )2] = d(q||p), P2(w(Y )) = d(q||p) - 1 by definition. Because we assume  1, we directly have

(10)

EP w(Y )2l2(Y, h(X))  EPw(Y )2 = d(q||p)

(11)

Since we have a bound on the second moment of weighted loss while its first moment is L(h) we can apply Bernstein's inequality to obtain for any fixed h that

|Ln(h)

-

L(h)|



d

(q||p)

log(

2 

)

3n

+

2

(d(q||p)

-

L(h)2

)

log(

2 

)

n

For the uniform law for finite hypothesis classes we can just do union bound on top of Bernstein and could get a slightly better dependence on d(q||p).
The second moment of the importance weighted loss EP (Y )2 2(h(X), Y ) , given a h  H can be bounded for general   0, potentially leading to smaller values than d(q||p):

EP Y2 2(Y, h(X))

k q2(i) = p(i) p2(i) EXp(X|Y =i)

2(h(X), X)

i

k

1 q(i)

-1

=

q(i)  q(i) p(i)



EXp(X|Y =i)

i

2(h(X), i)

1 -1



k q(i)  q(i) p(i)

k
2
q(i)EXp(X|Y =i) 2(h(X), i) -1



ii

1 -1

=

k q(i)  q(i) p(i)

k
+1
q(i)EXp(X|Y =i) l2(h(X), i) EXp(X|Y =i) 2(h(X), i) -1



ii

1



k q(i) q(i) p(i)

k
q(i)EXp(X|Y =i)

2(h(X), i)

1-

1 

EXp(X|Y =i)

2(h(X), i)

1+

1 

ii

(12)

where the first inequality follows from Hölder's inequality, the second one follows from Jensen's

inequality and the fact that the loss is in [0, 1] as well as the fact that the exponentiation function is

convex

in

this

region.

Moreover,

since

1

+

1 



1

and

upper

bound

for

the

loss

square,

l(·, ·)2



1,

then;

EP w(Y )2 2(h(X), Y ) 

k q(i) q(i) p(i)
i

1
k
q(i)EXP|Y =i
i

2(h(X), i)

1-

1 

which gives bound on the second moment of weighted loss.

15

