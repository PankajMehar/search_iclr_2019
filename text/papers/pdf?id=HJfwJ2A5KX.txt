Under review as a conference paper at ICLR 2019
DATA-DEPENDENT CORESETS FOR COMPRESSING NEURAL NETWORKS WITH APPLICATIONS TO GENERALIZATION BOUNDS
Anonymous authors Paper under double-blind review
ABSTRACT
We present an efficient coresets-based neural network compression algorithm that sparsifies the parameters of a trained fully-connected neural network in a manner that provably approximates the network's output. Our approach is based on an importance sampling scheme that judiciously defines a sampling distribution over the neural network parameters, and as a result, retains parameters of high importance while discarding redundant ones. We leverage a novel, empirical notion of sensitivity and extend traditional coreset constructions to the application of compressing parameters. Our theoretical analysis establishes guarantees on the size and accuracy of the resulting compressed network and gives rise to generalization bounds that may provide new insights into the generalization properties of neural networks. We demonstrate the practical effectiveness of our algorithm on a variety of neural network configurations and real-world data sets.
1 INTRODUCTION
Within the past decade, large-scale neural networks have demonstrated unprecedented empirical success in high-impact applications such as object classification, speech recognition, computer vision, and natural language processing. However, with the ever-increasing size of state-of-the-art neural networks, the resulting storage requirements and performance of these models are becoming increasingly prohibitive in terms of both time and space. Recently proposed architectures for neural networks, such as those in Krizhevsky et al. (2012); Long et al. (2015); Badrinarayanan et al. (2015), contain millions of parameters, rendering them prohibitive to deploy on platforms that are resource-constrained, e.g., embedded devices, mobile phones, or small scale robotic platforms.
In this work, we consider the problem of sparsifying the parameters of a trained fully-connected neural network in a principled way so that the output of the compressed neural network is approximately preserved. We introduce a neural network compression approach based on identifying and removing weighted edges with low relative importance via coresets, small weighted subsets of the original set that approximate the pertinent cost function. Our compression algorithm hinges on extensions of the traditional sensitivity-based coresets framework (Langberg & Schulman, 2010; Braverman et al., 2016), and to the best of our knowledge, is the first to apply coresets to parameter downsizing. In this regard, our work aims to simultaneously introduce a practical algorithm for compressing neural network parameters with provable guarantees and close the research gap in prior coresets work, which has predominantly focused on compressing input data points.
In particular, this paper contributes the following:
1. A coreset approach to compressing problem-specific parameters based on a novel, empirical notion of sensitivity that extends state-of-the-art coreset constructions.
2. An efficient neural network compression algorithm, CoreNet, based on our extended coreset approach that sparsifies the parameters via importance sampling of weighted edges.
3. Extensions of the CoreNet method, CoreNet+ and CoreNet++, that improve upon the edge sampling approach by additionally performing neuron pruning and amplification.
4. Analytical results establishing guarantees on the approximation accuracy, size, and generalization of the compressed neural network.
1

Under review as a conference paper at ICLR 2019
5. Evaluations on real-world data sets that demonstrate the practical effectiveness of our algorithm in compressing neural network parameters and validate our theoretical results.
2 RELATED WORK
Our work builds upon the following prior work in coresets and compression approaches.
Coresets Coreset constructions were originally introduced in the context of computational geometry (Agarwal et al., 2005) and subsequently generalized for applications to other problems via an importance sampling-based, sensitivity framework (Langberg & Schulman, 2010; Braverman et al., 2016). Coresets have been used successfully to accelerate various machine learning algorithms such as k-means clustering (Feldman & Langberg, 2011; Braverman et al., 2016), graphical model training (Molina et al., 2018), and logistic regression (Huggins et al., 2016) (see the surveys of Bachem et al. (2017) and Munteanu & Schwiegelshohn (2018) for a complete list). In contrast to prior work, we generate coresets for reducing the number of parameters ­ rather than data points ­ via a novel construction scheme based on an efficiently-computable notion of sensitivity.
Low-rank Approximations and Weight-sharing Denil et al. (2013) were among the first to empirically demonstrate the existence of significant parameter redundancy in deep neural networks. A predominant class of compression approaches consists of using low-rank matrix decompositions, such as Singular Value Decomposition (SVD) (Denton et al., 2014), to approximate the weight matrices with their low-rank counterparts. Similar works entail the use of low-rank tensor decomposition approaches applicable both during and after training (Jaderberg et al., 2014; Kim et al., 2015; Tai et al., 2015; Ioannou et al., 2015; Alvarez & Salzmann, 2017; Yu et al., 2017). Another class of approaches uses feature hashing and weight sharing (Weinberger et al., 2009; Shi et al., 2009; Chen et al., 2015b;a; Ullrich et al., 2017). Building upon the idea of weight-sharing, quantization (Gong et al., 2014; Wu et al., 2016; Zhou et al., 2017) or regular structure of weight matrices was used to reduce the effective number of parameters (Zhao et al., 2017; Sindhwani et al., 2015; Cheng et al., 2015; Choromanska et al., 2016; Wen et al., 2016). Despite their practical effectiveness in compressing neural networks, these works generally lack performance guarantees on the quality of their approximations and/or the size of the resulting compressed network.
Weight Pruning Similar to our proposed method, weight pruning (LeCun et al., 1990) hinges on the idea that only a few dominant weights within a layer are required to approximately preserve the output. Approaches of this flavor have been investigated by Lebedev & Lempitsky (2016); Dong et al. (2017), e.g., by embedding sparsity as a constraint (Iandola et al., 2016; Aghasi et al., 2017; Lin et al., 2017). Another related approach is that of Han et al. (2015), which considers a combination of weight pruning and weight sharing methods. Nevertheless, prior work in weight pruning lacks rigorous theoretical analysis of the effect that the discarded weights can have on the compressed network. To the best of our knowledge, our work is the first to introduce a practical, sampling-based weight pruning algorithm with provable guarantees.
Generalization The generalization properties of neural networks have been extensively investigated in various contexts (Dziugaite & Roy, 2017; Neyshabur et al., 2017a; Bartlett et al., 2017). However, as was pointed out by Neyshabur et al. (2017b), current approaches to obtaining non-vacuous generalization bounds do not fully or accurately capture the empirical success of state-of-the-art neural network architectures. Recently, Arora et al. (2018) and Zhou et al. (2018) highlighted the close connection between compressibility and generalization of neural networks. Arora et al. (2018) presented a compression method based on the Johnson-Lindenstrauss (JL) Lemma (Johnson & Lindenstrauss, 1984) and proved generalization bounds based on succinct reparameterizations of the original neural network. Building upon the work of Arora et al. (2018), we extend our theoretical compression results to establish novel generalization bounds for fully-connected neural networks. Unlike the method of Arora et al. (2018), which exhibits guarantees of the compressed network's performance only on the set of training points, our method's guarantees hold (probabilistically) for any random point drawn from the distribution. In addition, we establish that our method can -approximate the neural network output neuron-wise, which is stronger than the norm-based guarantee of Arora et al. (2018).
In contrast to prior work, this paper addresses the problem of compressing a fully-connected neural network while provably preserving the network's output. Unlike previous theoretically-grounded compression approaches ­ which provide guarantees in terms of the normed difference ­, our method provides the stronger entry-wise approximation guarantee, even for points outside of the available data set. As our empirical results show, ensuring that the output of the compressed network entry-wise approximates that of the original network is critical to retaining high classification accuracy. Overall,
2

Under review as a conference paper at ICLR 2019

our compression approach remedies the shortcomings of prior approaches in that it (i) exhibits favorable theoretical properties, (ii) is computationally efficient, e.g., does not require retraining of the neural network, (iii) is easy to implement, and (iv) can be used in conjunction with other compression approaches ­ such as quantization or Huffman coding ­ to obtain further improved compression rates.

3 PROBLEM DEFINITION

3.1 FULLY-CONNECTED NEURAL NETWORKS

A feedforward fully-connected neural network with L  N+ layers and parameters  defines a map-

ping f : X  Y for a given input x  X  Rd to an output y  Y  Rk as follows. Let

  N+ denote the number of neurons in layer  [L], where [L] = {1, . . . , L} denotes the in-

dex set, and where 1 = d and L = k. Further, let  =

L =2



and  = max {2,...,L}  .

For layers  {2, . . . , L}, let W  R × -1 be the weight matrix for layer with entries de-

noted by wij, rows denoted by wi



R1×

-1
,

and



=

(W 2, . . . , W L).

For notational simplic-

ity, we assume that the bias is embedded in the weight matrix. Then for an input vector x  Rd,

let a1 = x and z = W a -1  R ,   {2, . . . , L}, where a -1 = (z -1)  R -1 de-

notes the activation. We consider the activation function to be the Rectified Linear Unit (ReLU)

function, i.e., (·) = max{· , 0} (entry-wise, if the input is a vector). The output of the net-

work for an input x is f(x) = zL, and in particular, for classification tasks the prediction is

argmaxi[k] f(x)i = argmaxi[k] ziL.

3.2 NEURAL NETWORK CORESET PROBLEM
Consider the setting where a neural network f(·) has been trained on a training set of independent and identically distributed (i.i.d.) samples from a joint distribution on X × Y, yielding parameters  = (W 2, . . . , W L). We further denote the input points of a validation data set as P = {xi}in=1  X and the marginal distribution over the input space X as D. We define the size of the parameter tuple , nnz(), to be the sum of the number of non-zero entries in the weight matrices W 2, . . . , W L.
For any given ,   (0, 1), our overarching goal is to generate a reparameterization ^, yielding the neural network f^(·), using a randomized algorithm, such that nnz(^) nnz(), and the neural network output f(x), x  D can be approximated up to 1 ±  multiplicative error with probability greater than 1 - . We define the 1 ±  multiplicative error between two k-dimensional vectors a, b  Rk as the following entry-wise bound: a  (1 ± )b  ai  (1 ± )bi i  [k], and formalize the definition of an (, )-coreset as follows.
Definition 1 ((, )-coreset). Given user-specified ,   (0, 1), a set of parameters ^ = (W^ 2, . . . , W^ L) is an (, )-coreset for the network parameterized by  if for x  D, it holds that
^P,x(f^(x)  (1 ± )f(x))  1 - ,
where P^,x denotes a probability measure with respect to a random data point x and the output ^ generated by a randomized compression scheme.

4 METHOD
In this section, we introduce our neural network compression algorithm as depicted in Alg. 1. Our method is based on an important sampling-scheme that extends traditional sensitivity-based coreset constructions to the application of compressing parameters.
4.1 CORENET
Our method (Alg. 1) hinges on the insight that a validation set of data points P i.i.d. Dn can be used to approximate the relative importance, i.e., sensitivity, of each weighted edge with respect to the input data distribution D. For this purpose, we first pick a subsample of the data points S  P of appropriate

3

Under review as a conference paper at ICLR 2019

size (see Sec. 5 for details) and cache each neuron's activation and compute a neuron-specific constant to be used to determine the required edge sampling complexity (Lines 2-6).

Algorithm 1 CORENET

Input: ,   (0, 1): error and failure probability, respectively; P  X : a set of n points from the input

space X such that P i.i.d. Dn;  = (W 2, . . . , W L): parameters of the original uncompressed neural network.

Output: ^ = (W^ 2, . . . , W^ L): sparsified parameter set such that f^(·)  (1 ± )f(·) (see Sec. 5 for details).

1:





2

 (L-1)

;

  max {2,...,L-1}  ;



L =2



;

  log( )/2;

2: S  Uniform sample (without replacement) of log (8  /) log( ) points from P;

3: a1(x)  x x  S;

4: for x  S do

5: for  {2, . . . , L} do

6:

a (x)  (W a -1(x));

i (x) 

;k[ -1] |wik ak-1(x)|
k[ -1] wik ak-1(x)

7: for  {2, . . . , L} do

8:

^ 

1 |S|

maxi[

]

xS i (x)

 + , where  = 2

1

+

 2

log

(8





/)

;

9:

W^



(0, . . . , 0)



R

×

-1
;

^  

L k=

^ k;





 ^



;

10: for all i  [ ] do

11: W+  {j  [ -1] : wij > 0}; W-  {j  [ -1] : wij < 0};

12: w^i+  SPARSIFY(W+, wi ,  , , S, a -1); w^i-  SPARSIFY(W-, -wi ,  , , S, a -1);

13: w^i  w^i+ - w^i-; W^ i·  w^i ;

Consolidate the weights into the ith row of W^ ;

14: return ^ = (W^ 2, . . . , W^ L);

Algorithm 2 SPARSIFY(W, w, , , S, a(·))

Input:

W



[

-1]:

index set;

w



R1×

-1
:

row vector corresponding to the weights incoming to node

i  [ ] in layer  {2, . . . , L}; ,   (0, 1): error and failure probability, respectively; S  P: subsample

of the original point set; a(·): cached activations of previous layer for all x  S.

Output: w^: sparse weight vector.

1: for j  W do

2:

sj  maxxS

;wj aj (x)
kW wk ak (x)

Compute the sensitivity of each edge

3: S  jW sj ;

4: for j  W do

Generate the importance sampling distribution over the incoming edges

5:

qj



sj S

;

6: m 

8 S log( ) log(8 /) 2

;

Compute the number of required samples

7: C  a multiset of m samples from W where each j  W is sampled with probability qj;

8:

w^  (0, . . . , 0)  R1×

-1
;

Initialize the compressed weight vector

9: for j  C do

10:

w^j



w^j

+

wj m qj

;

Update the entries of the sparsified weight matrix according to the samples C

Entries are reweighted by

1 m qj

to ensure unbiasedness of our estimator

11: return w^;

Subsequently, we apply our core sampling scheme to sparsify the set of incoming weighted edges to each neuron in all layers (Lines 7-13). For technical reasons (see Sec. 5), we perform the sparsification on the positive and negative weighted edges separately and then consolidate the results (Lines 11-13). By repeating this procedure for all neurons in every layer, we obtain a set ^ = (W^ 2, . . . , W^ L) of sparse weight matrices such that the output of each layer and the entire network is approximately preserved, i.e., W^ a^ -1(x)  W a -1(x) and f^(x)  f(x), respectively1.
4.2 SPARSIFYING WEIGHTS
The crux of our compression scheme lies in Alg. 2 (invoked twice on Line 12, Alg. 1) and in particular, in the importance sampling scheme used to select a small subset of edges of high importance. The
1a^ -1(x) denotes the approximation from previous layers for an input x  D; see Sec. 5 for details.

4

Under review as a conference paper at ICLR 2019
cached activations are used to compute the sensitivity, i.e., relative importance, of each considered incoming edge j  W to neuron i  [ ],  {2, . . . , L} (Alg. 2, Lines 1-2). The relative importance of each edge j is computed as the maximum (over x  S) ratio of the edge's contribution to the sum of contributions of all edges. In other words, the sensitivity sj of an edge j captures the highest (relative) impact j had on the output of neuron i  [ ] in layer across all x  S.
The sensitivities are then used to compute an importance sampling distribution over the incoming weighted edges (Lines 4-5). The intuition behind the importance sampling distribution is that if sj is high, then edge j is more likely to have a high impact on the output of neuron i, therefore we should keep edge j with a higher probability. m edges are then sampled with replacement (Lines 6-7) and the sampled weights are then reweighed to ensure unbiasedness of our estimator (Lines 9-10).
4.3 EXTENSIONS: NEURON PRUNING AND AMPLIFICATION
In this subsection we outline two improvements to our algorithm that that do not violate any of our theoretical properties and may improve compression rates in practical settings.
Neuron pruning (CoreNet+) Similar to removing redundant edges, we can use the empirical activations to gauge the importance of each neuron. In particular, if the maximum activation (over all evaluations x  S) of a neuron is equal to 0, then the neuron ­ along with all of the incoming and outgoing edges ­ can be pruned without significantly affecting the output with reasonable probability. This intuition can be made rigorous under the assumptions outlined in Sec. 5.
Amplification (CoreNet++) Coresets that provide stronger approximation guarantees can be constructed via amplification ­ the procedure of constructing multiple approximations (coresets) (w^i )1, . . . , (w^i ) over  trials, and picking the best one. To evaluate the quality of each approximation, a different subset T  P \ S can be used to infer performance. In practice, amplification would entail constructing multiple approximations by executing Line 12 of Alg. 1 and picking the one that achieves the lowest relative error on T .
5 ANALYSIS
In this section, we establish the theoretical guarantees of our neural network compression algorithm (Alg. 1). The full proofs of all the claims presented in this section can be found in the Appendix.
5.1 PRELIMINARIES
Let x  D be a randomly drawn input point. We explicitly refer to the pre-activation and activation values at layer  {2, . . . , } with respect to the input x  supp(D) as z (x) and a (x), respectively. The values of z (x) and a (x) at each layer will depend on whether or not we compressed the previous layers  {2, . . . , }. To formalize this interdependency, we let z^ (x) and a^ (x) denote the respective quantities of layer when we replace the weight matrices W 2, . . . , W in layers 2, . . . , by W^ 2, . . . , W^ , respectively.
For the remainder of this section (Sec. 5) we let  {2, . . . , L} be an arbitrary layer and let i  [ ] be an arbitrary neuron in layer . For purposes of clarity and readability, we will omit the the variable denoting the layer  {2, . . . , L}, the neuron i  [ ], and the incoming edge index j  [ -1], whenever they are clear from the context. For example, when referring to the intermediate value of a neuron i  [ ] in layer  {2, . . . , L}, zi (x) = wi , a^ -1(x)  R with respect to a point x, we will simply write z(x) = w, a(x)  R, where w := wi  R1× -1 and a(x) := a -1(x)  R -1×1. Under this notation, the weight of an incoming edge j is denoted by wj  R.
5.2 IMPORTANCE SAMPLING BOUNDS FOR POSITIVE WEIGHTS
In this subsection, we establish approximation guarantees under the assumption that the weights are positive. Moreover, we will also assume that the input, i.e., the activation from the previous layer, is non-negative (entry-wise). The subsequent subsection will then relax these assumptions to conclude that a neuron's value can be approximated well even when the weights and activations are not all positive and non-negative, respectively. Let W = {j  [ -1] : wj > 0}  [ -1] be the set of
5

Under review as a conference paper at ICLR 2019

indices of incoming edges with strictly positive weights. To sample the incoming edges to a neuron, we quantify the relative importance of each edge as follows.

Definition 2 (Relative Importance). The importance of an incoming edge j  W with respect to an

input x  supp(D) is given by the function gj(x), where gj(x) =

wj aj (x) kW wk ak(x)

j  W.

Note that gj(x) is a function of the random variable x  D. We now present our first assumption that pertains to the Cumulative Distribution Function (CDF) of the relative importance random variable. Assumption 1. For all j  W, the CDF of the random variable gj(x), denoted by Fj (·), satisfies
Fj (M/K)  exp(-1/K), where M = min{x  [0, 1] : Fj (x) = 1},
and K  [2, log( )] is a universal constant.

Assumption 1 is a technical assumption on the ratio of the weighted activations that will enable us to rule out pathological problem instances where the relative importance of each edge cannot be well-approximated using a small number of data points S  P. Henceforth, we consider a uniformly drawn (without replacement) subsample S  P as in Line 2 of Alg. 1, where |S| = log (8  /) log( ) , and define the sensitivity of an edge as follows.
Definition 3 (Empirical Sensitivity). Let S  P be a subset of distinct points from P i.i.d. Dn.Then, the sensitivity over positive edges j  W directed to a neuron is defined as sj = maxxS gj(x).

Our first lemma establishes a core result that relates the weighted sum with respect to the sparse row vector w^, kW w^k a^k(x), to the value of the of the weighted sum with respect to the ground-truth row vector w, kW wk a^k(x). We remark that there is randomness with respect to the randomly generated row vector w^i , a randomly drawn input x  D, and the function a^(·) = a^ -1(·) defined by the randomly generated matrices W^ 2, . . . , W^ -1 in the previous layers. Unless otherwise stated, we will henceforth use the shorthand notation P(·) to denote Pw^ , x, a^ -1 (·). Moreover, for ease of presentation, we will first condition on the event E1/2 that a^(x)  (1 ± 1/2)a(x) holds. This conditioning will simplify the preliminary analysis and will be removed in our subsequent results.
Lemma 1 (Positive-Weights Sparsification). Let ,   (0, 1), and x  D. SPARSIFY(W, w, , , S, a(·)) generates a row vector w^ such that

P w^k a^k(x) / (1 ± ) wk a^k(x) | E1/2

kW

kW

where nnz(w^) 

8 S log( ) log(8 /) 2

, and S =

jW sj .

 3 8

5.3 IMPORTANCE SAMPLING BOUNDS

We now relax the requirement that the weights are strictly positive and instead consider the following

index sets that partition the weighted edges: W+ = {j  [ -1] : wj > 0} and W- = {j  [ -1] : wj < 0}. We still assume that the incoming activations from the previous layers are positive (this

assumption can be relaxed as discussed in Appendix A.2.4). We define i (x) for a point x  D

and neuron i  [

] as i (x) =

|

k[ -1] |wik k[ -1] wik

ak-1 (x)|
| .ak-1(x)

The

following

assumption

serves

a

similar

purpose as does Assumption 1 in that it enables us to approximate the random variable i (x) via an empirical estimate over a small-sized sample of data points S  P.

Assumption 2 (Subexponentiality of i (x)). For any layer  {2, . . . , L} and neuron i  [ ],

the centered random variable  = i (x) - E xD[i (x)] is subexponential (Vershynin, 2016) with

parameter   log( )/2, i.e., E [exp (s)]  exp(s22)

|s|



1 

.

For   (0, 1) and

 {2, . . . , L}, we let 

=

 2 (L-1)

and define 

=

 ^ 

=



2 (L-1)

L k=

^ k ,

where ^ =

1 |S |

maxi[

]

x S i (x ) + . To formalize the interlayer dependencies, for each

i  [ ] we let Ei denote the (desirable) event that z^i (x)  (1 ± 2 ( - 1)  +1) zi (x) holds, and let E = i[ ] Ei be the intersection over the events corresponding to each neuron in layer .

6

Under review as a conference paper at ICLR 2019

Lemma 2 (Conditional Neuron Value Approximation). Let ,   (0, 1),  {2, . . . , L}, i  [ ], and x  D. CORENET generates a row vector w^i = w^i+ - w^i-  R1× -1 such that

P Ei | E -1 = P z^i (x)  (1 ± 2 ( - 1)  +1) zi (x) | E -1  1 - /,

(1)

where 

=

 ^ 

and

nnz(w^i )



8 S log( ) log(8 /) 2

+ 1, where S =

jW+ sj +

jW- sj .

The following core result establishes unconditional layer-wise approximation guarantees and culminates in our main compression theorem.
Lemma 3 (Layer-wise Approximation). Let ,   (0, 1),  {2, . . . , L}, and x  D. CORENET generates a sparse weight matrix W^  R × -1 such that, for z^ (x) = W^ a^ (x),



P (E ) = P

z^ (x)  (1 ± 2 ( - 1)  +1) z (x)  1 -

(W^ 2,...,W^ ), x

(W^ 2,...,W^ ), x

=2  . 

Theorem 4 (Network Compression). For ,   (0, 1), Algorithm 1 generates a set of parameters ^ = (W^ 2, . . . , W^ L) of size

L
nnz(^) 
=2 i=1

32 (L - 1)2 (^ )2 Si log( ) log(8 /) 2

+1

in O   log  / time such that P^, xD f^(x)  (1 ± )f(x)  1 - .
We note that we can obtain a guarantee for a set of n randomly drawn points by invoking Theorem 4 with  = /n and union-bounding over the failure probabilities, while only increasing the sampling complexity logarithmically, as formalized in Corollary 12, Appendix A.2.

5.4 GENERALIZATION BOUNDS

As a corollary to our main results, we obtain novel generalization bounds for neural networks in terms of empirical sensitivity. Following the terminology of Arora et al. (2018), the expected margin loss of a classifier f : Rd  Rk parameterized by  with respect to a desired margin  > 0 and distribution D is defined by L (f) = P(x,y)DX,Y (f(x)y   + maxi=y f(x)i). We let L^ denote the empirical estimate of the margin loss. The following corollary follows directly from the argument presented in Arora et al. (2018) and Theorem 4.
Corollary 5 (Generalization Bounds). For any   (0, 1) and margin  > 0, Alg. 1 generates weights ^ such that with probability at least 1 - , the expected error L0(f^) with respect to the points in P  X , |P| = n, is bounded by



L0(f^)  L^ (f) + O 

maxxP

f (x)

2 2

L2

L =2

(^

)2

2 n



 i=1

Si



.

6 RESULTS
In this section, we evaluate the practical effectiveness of our compression algorithm on popular benchmark data sets (MNIST (LeCun et al., 1998), FashionMNIST (Xiao et al., 2017), and CIFAR10 (Krizhevsky & Hinton, 2009)) and varying fully-connected trained neural network configurations: 2 to 5 hidden layers, 100 to 1000 hidden units, either fixed hidden sizes or decreasing hidden size denoted by pyramid in the figures. We further compare the effectiveness of our sampling scheme in reducing the number of non-zero parameters of a network, i.e., in sparsifying the weight matrices, to that of uniform sampling and Singular Value Decomposition (SVD). The details of the experimental setup and results of additional evaluations may be found in Appendix B.
Experiment Setup We compare against three variations of our compression algorithm: (i) sole edge sampling (CoreNet), (ii) edge sampling with neuron pruning (CoreNet+), and (iii) edge sampling with

7

Under review as a conference paper at ICLR 2019

Average Drop in Accuracy [%]

Average Relative Error

neuron pruning and amplification (CoreNet++). For comparison, we evaluated the average relative error in output ( 1-norm) and average drop in classification accuracy relative to the accuracy of the uncompressed network. Both metrics were evaluated on a test set that was not used for training and/or compression.

Results Results for varying architectures and datasets are depicted in Figures 1 and 2 for the average drop in classification accuracy and relative error ( 1-norm), respectively. As apparent from Figure 1, we are able to compress networks to about 15% of their original size without significant loss of accuracy for networks trained on MNIST and FashionMNIST, and to about 50% of their original size for CIFAR.

MNIST: L = 3, * = 1000, Pyramid Uniform Sampling
80 SVD CoreNet
60 CoreNet+ CoreNet++
40
20
0Perc1e0ntage of N2o0n-zero Pa3r0ameters R4e0tained

Average Drop in Accuracy [%]

CIFAR: L = 3, * = 1000, Pyramid 40 Uniform Sampling
SVD 30 CoreNet
CoreNet+ 20 CoreNet++ 10 0Per1ce0ntage of2N0on-zero3P0aramete4rs0Retained

Average Drop in Accuracy [%]

FashionMNIST: L = 3, * = 1000, Pyramid Uniform Sampling
80 SVD CoreNet
60 CoreNet+ CoreNet++
40
20
0Perc1e0ntage of N20on-zero Pa3r0ameters R4e0tained

Figure 1: Evaluation of drop in classification accuracy after compression against the MNIST, CIFAR, and FashionMNIST datasets with varying number of hidden layers (L) and number of neurons per hidden layer (). Shaded region corresponds to values within one standard deviation of the mean.

MNIST: L = 3, * = 1000, Pyramid 24

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of N2o0n-zero Pa3r0ameters R4e0tained

Average Relative Error

CIFAR: L = 3, * = 1000, Pyramid

27 Uniform Sampling

25

SVD CoreNet

23

CoreNet+ CoreNet++

21

21

Per1ce0ntage of2N0on-zero3P0aramete4rs0Retained

Average Relative Error

FashionMNIST: L = 3, * = 1000, Pyramid 24

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of N20on-zero Pa3r0ameters R4e0tained

Figure 2: Evaluation of relative error after compression against the MNIST, CIFAR, and FashionMNIST datasets with varying number of hidden layers (L) and number of neurons per hidden layer ().

Discussion The simulation results presented in this section validate our theoretical results established in Sec. 5. In particular, our empirical results indicate that we are able to outperform networks compressed via uniform sampling or SVD across the considered experiments and trials. The results presented in this section further suggest that empirical sensitivity can effectively capture the relative importance of neural network parameters, leading to a more informed importance sampling scheme. Moreover, the relative performance of our algorithm tends to increase as we consider deeper architectures. These findings suggest that our algorithm may also be effective in compressing modern convolutional architectures, which tend to be very deep.

7 CONCLUSION
We presented a coresets-based neural network compression algorithm for compressing the parameters of a trained fully-connected neural network in a manner that approximately preserves the network's output. Our method and analysis extend traditional coreset constructions to the application of compressing parameters, which may be of independent interest. Our work distinguishes itself from prior approaches in that it establishes theoretical guarantees on the approximation accuracy and size of the generated compressed network. As a corollary to our analysis, we obtain generalization bounds for neural networks, which may provide novel insights on the generalization properties of neural networks. We empirically demonstrated the practical effectiveness our compression algorithm on a variety of neural network configurations and real-world data sets. In future work, we plan to extend our algorithm and analysis to compress Convolutional Neural Networks (CNNs) and other network architectures. We conjecture that our compression algorithm can be used to reduce storage requirements of neural network models and enable fast inference in practical settings.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Pankaj K Agarwal, Sariel Har-Peled, and Kasturi R Varadarajan. Geometric approximation via coresets. Combinatorial and computational geometry, 52:1­30, 2005. 2
Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep neural networks with performance guarantee. In Advances in Neural Information Processing Systems, pp. 3180­3189, 2017. 2
Jose M Alvarez and Mathieu Salzmann. Compression-aware training of deep networks. In Advances in Neural Information Processing Systems, pp. 856­867, 2017. 2
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018. 2, 7
Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine learning. arXiv preprint arXiv:1703.06476, 2017. 2
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. CoRR, abs/1511.00561, 2015. URL http: //arxiv.org/abs/1511.00561. 1
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241­6250, 2017. 2
Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming coreset constructions. arXiv preprint arXiv:1612.00889, 2016. 1, 2
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing convolutional neural networks. CoRR, abs/1506.04449, 2015a. URL http://arxiv.org/ abs/1506.04449. 2
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. CoRR, abs/1504.04788, 2015b. URL http://arxiv. org/abs/1504.04788. 2
Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2857­2865, 2015. 2
Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski, Tony Jebara, Sanjiv Kumar, and Yann LeCun. Binary embeddings with structured hashed projections. In International Conference on Machine Learning, pp. 344­353, 2016. 2
Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, and Nando de Freitas. Predicting parameters in deep learning. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 2148­2156. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/ 5025-predicting-parameters-in-deep-learning.pdf. 2
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. CoRR, abs/1404.0736, 2014. URL http://arxiv.org/abs/1404.0736. 2
Benjamin Doerr. Probabilistic tools for the analysis of randomized optimization heuristics. arXiv preprint arXiv:1801.06733, 2018. 23
Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. In Advances in Neural Information Processing Systems, pp. 4860­4874, 2017. 2
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017. 2
9

Under review as a conference paper at ICLR 2019
Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 569­578. ACM, 2011. 2
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing Deep Convolutional Networks using Vector Quantization. arXiv preprint arXiv:1412.6115, 2014. 2
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015. URL http: //arxiv.org/abs/1510.00149. 2
Jonathan H Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian logistic regression. arXiv preprint arXiv:1605.06423, 2016. 2
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016. 2
Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio Criminisi. Training cnns with low-rank filters for efficient image classification. arXiv preprint arXiv:1511.06744, 2015. 2
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. 2
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984. 2
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. arXiv preprint arXiv:1511.06530, 2015. 2
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. 7, 26
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097­ 1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks. pdf. 1
Michael Langberg and Leonard J Schulman. Universal -approximators for integrals. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pp. 598­607. SIAM, 2010. 1, 2
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pp. 2554­2564. IEEE, 2016. 2
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pp. 598­605, 1990. 2
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. 7, 26
Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In Advances in Neural Information Processing Systems, pp. 2178­2188, 2017. 2
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 1
Alejandro Molina, Alexander Munteanu, and Kristian Kersting. Core dependency networks. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI). AAAI Press Google Scholar, 2018. 2
10

Under review as a conference paper at ICLR 2019
Alexander Munteanu and Chris Schwiegelshohn. Coresets-methods and history: A theoreticians design pattern for approximation and streaming algorithms. KI-Ku¨nstliche Intelligenz, 32(1):37­53, 2018. 2
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a. 2
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949­5958, 2017b. 2
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017. 26
Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and SVN Vishwanathan. Hash kernels for structured data. Journal of Machine Learning Research, 10(Nov):2615­2637, 2009. 2
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. In Advances in Neural Information Processing Systems, pp. 3088­3096, 2015. 2
Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015. 2
Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. arXiv preprint arXiv:1702.04008, 2017. 2
Roman Vershynin. High-dimensional probability. An Introduction with Applications, 2016. 6, 17
Kilian Q. Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford, and Alexander J. Smola. Feature hashing for large scale multitask learning. CoRR, abs/0902.2206, 2009. URL http: //arxiv.org/abs/0902.2206. 2
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074­2082, 2016. 2
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized Convolutional Neural Networks for Mobile Devices. In Proceedings of the Inernational Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. 7, 26
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7370­7379, 2017. 2
Liang Zhao, Siyu Liao, Yanzhi Wang, Jian Tang, and Bo Yuan. Theoretical properties for neural networks with weight matrices of low displacement rank. CoRR, abs/1703.00144, 2017. URL http://arxiv.org/abs/1703.00144. 2
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights. In Proceedings of the International Conference on Learning Representations (ICLR) 2017, feb 2017. 2
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Compressibility and generalization in large-scale deep learning. arXiv preprint arXiv:1804.05862, 2018. 2
11

Under review as a conference paper at ICLR 2019

A PROOFS OF THE ANALYTICAL RESULTS IN SECTION 5
This section includes the full proofs of the technical results given in Sec. 5.

A.1 ANALYTICAL RESULTS FOR SECTION 5.2 (IMPORTANCE SAMPLING BOUNDS FOR POSITIVE WEIGHTS)

A.1.1 ORDER STATISTIC SAMPLING
We now establish a couple of technical results that will quantify the accuracy of our approximations of edge importance (i.e., sensitivity).
Lemma 6. Let K > 0 be a universal constant and let D be a distribution with CDF F (·) satisfying F (M/K)  exp(-1/K), where M = min{x  [0, 1] : F (x) = 1}. Let P = {X1, . . . , Xn} be a set of n = |P| i.i.d. samples each drawn from the distribution D. Let Xn+1  D be an i.i.d. sample. Then,

P

K

max X
X P

<

Xn+1

 exp(-n/K)

Proof. Let Xmax = maxXP ; then,

M

P(K Xmax < Xn+1) =

P(Xmax < x/K|Xn+1 = x) d P(x)

0

M
= P (X < x/K)n d P(x)

0

M
 F (x/K)n d P(x)

0

M

 F (M/K)n

d P(x)

0

= F (M/K)n

 exp(-n/K)

since X1, . . . , Xn are i.i.d. where F (·) is the CDF of X  D
by monotonicity of F
CDF Assumption,

and this completes the proof.

We now proceed to establish that the notion of empirical sensitivity is a good approximation for the relative importance. For this purpose, let the relative importance g^j(x) of an edge j after the previous layers have already been compressed be

g^j(x) =

wj a^j(x) . kW wk a^k(x)

Lemma 7 (Empirical Sensitivity Approximation). Let   (0, 1/2),   (0, 1),  {2, . . . , L}, Consider a set S = {x1, . . . , xn}  P of size |S|  log (8  /) log( ) . Then, conditioned on the event E1/2 occurring, i.e., a^(x)  (1 ± 1/2)a(x),



P
xD

j  W : C sj < g^j(x) | E1/2

, 8

where C = 3 log( ) and W  [ -1].

Proof. Consider an arbitrary j  W and x  S corresponding to gj(x ) with CDF Fj (·) and recall that M = min{x  [0, 1] : Fj (x) = 1} as in Assumption 1. Note that by Assumption 1, we have
F (M/K)  exp(-1/K),
12

Under review as a conference paper at ICLR 2019

and so the random variables gj(x ) for x  S satisfy the CDF condition required by Lemma 6. Now let E be the event that K sj < gj(x) holds. Applying Lemma 6, we obtain

P(E) = P(K sj < gj(x)) = P K max gj(x ) < gj(x)  exp(-|S|/K).
x S

Now let E^ denote the event that the inequality Csj < g^j(x) =

wj a^j (x) kW wk a^k(x)

holds

and

note

that

the right side of the inequality is defined with respect to g^j(x) and not gj(x). Observe that since we

conditioned on the event E1/2, we have that a^(x)  (1 ± 1/2)a(x).

Now assume that event E^ holds and note that by the implication above, we have

C sj < g^j(x) =

wj a^j(x)  (1 + 1/2)wj aj(x) kW wk a^k(x) (1 - 1/2) kW wk ak(x)

3·

wj aj(x) kW wk ak(x)

=

3 gj(x).

where the second inequality follows from the fact that 1+1/2/1-1/2  3. Moreover, since we know that C  3K, we conclude that if event E^ occurs, we obtain the inequality

3 K sj  3 gj(x)  K sj  gj(x),

which is precisely the definition of event E. Thus, we have shown the conditional implication E^ | E1/2  E, which implies that

P(E^ | E1/2) = P(C sj < g^j (x) | E1/2)  P(E)  exp(-|S|/K).

Since our choice of j  W was arbitrary, the bound applies for any j  W. Thus, we have by the union bound

P(j  W : C sj < g^j(x) | E1/2)  P(C sj < g^j(x) | E1/2)  |W| exp(-|S|/K)

jW

|W|  

=



. 8 8

In practice, the set S referenced above is chosen to be a subset of the original data points, i.e., S  P (see Alg. 1, Line 2). Thus, we henceforth assume that the size of the input points |P| is large enough (or the specified parameter   (0, 1) is sufficiently large) so that |P|  |S|.

A.1.2 PROOF OF LEMMA 1

We now state the proof of Lemma 1. In this subsection, we establish approximation guarantees under the assumption that the weights are strictly positive. The next subsection will then relax this assumption to conclude that a neuron's value can be approximated well even when the weights are not all positive.
Lemma 1 (Positive-Weights Sparsification). Let ,   (0, 1), and x  D. SPARSIFY(W, w, , , S, a(·)) generates a row vector w^ such that

P w^k a^k(x) / (1 ± ) wk a^k(x) | E1/2

kW

kW

where nnz(w^) 

8 S log( ) log(8 /) 2

, and S =

jW sj .

 3 8

Proof. Let ,   (0, 1) be arbitrary. Moreover, let C be the coreset with respect to the weight indices W  [ -1] used to construct w^. Note that as in SPARSIFY, C is a multiset sampled from W of size

13

Under review as a conference paper at ICLR 2019

m=

8 S log( ) log(8 /) 2

, where S =

jW sj and C is sampled according to the probability

distribution q defined by

qj

=

sj S

j  W.

Let ^a(·) be an arbitrary realization of the random variable a^ -1(·), let x be a realization of x  D, and let
z^ = w^k ^ak(x)

kW

be the approximate intermediate value corresponding to the sparsified matrix w^ and let

z~ = wk ^ak(x).
kW

Now define E to be the (favorable) event that z^ -approximates z~, i.e., z^  (1±)z~, We will now show that the complement of this event, Ec, occurs with sufficiently small probability. Let Z  supp(D) be the set of well-behaved points (defined implicitly with respect to neuron i  [ ] and realization ^a) and defined as follows:

Z = {x  supp(D) : g^j(x )  Csj j  W} ,

where C = 3 log( ). Let EZ denote the event that x  Z where x is a realization of x  D.

Conditioned

on

EZ ,

event Ec

occurs

with

probability



 4

:

Let x be a realization of x  D such

that x  Z and let C = {c1, . . . , cm} be m samples from W with respect to distribution q as before.

Define m random variables Tc1 , . . . , Tcm such that for all j  C

Tj

=

wj ^aj(x) m qj

=

S

wj ^aj(x) . m sj

(2)

For any j  C, we have for the conditional expectation of Tj:

E [Tj

|

^a(·), x, EZ , E1/2] =
kW

wk ^ak(x) m qk

·

qk

= wk ^ak(x) m
kW

z~ =,
m

where we use the expectation notation E [·] with the understanding that it denotes the conditional expectation E C | a^l-1(·), x [·]. Moreover, we also note that conditioning on the event EZ (i.e., the event
that x  Z) does not affect the expectation of Tj. Let T = jC Tj = z^ denote our approximation and note that by linearity of expectation,

E [T | ^a(·), x, EZ , E1/2] = E [Tj | ^a(·), x, EZ , E1/2] = z~
jC
Thus, z^ = T is an unbiased estimator of z~ for any realization ^a(·) and x; thus, we will henceforth refer to E [T | ^a(·), x] as simply z~ for brevity.
For the remainder of the proof we will assume that z~ > 0, since otherwise, z~ = 0 if and only if Tj = 0 for all j  C almost surely, which follows by the fact that Tj  0 for all j  C by definition of W and the non-negativity of the ReLU activation. Therefore, in the case that z~ = 0, it follows that
P(|z^ - z~| > z~ | ^a(·), x) = P(z^ > 0 | ^a(·), x) = P(0 > 0) = 0,
which trivially yields the statement of the lemma, where in the above expression, P(·) is short-hand for the conditional probability Pw^ | a^l-1(·), x(·).

14

Under review as a conference paper at ICLR 2019

We now proceed with the case where z~ > 0 and leverage the fact that x  Z2 to establish that for all j  W:

Csj  g^j(x) =

wj ^aj(x) = wj ^aj(x)

kW wk ^ak(x)

z~

 wj ^aj(x)  C z~. sj

(3)

Utilizing the inequality established above, we bound the conditional variance of each Tj, j  C as follows

Var(Tj | ^a(·), x, EZ , E1/2)  E [(Tj)2 | ^a(·), x, EZ , E1/2]

=

kW

(wk ^ak(x))2 (m qk)2

·

qk

=

S m2

(wk ^ak(x))2

kW

sk

S

 m2

wk ^ak(x)

kW

S C z~2 = m2 ,

C z~

where Var(·) is short-hand for VarC | a^l-1(·), x (·). Since T is a sum of (conditionally) independent random variables, we obtain

Var(T | ^a(·), x, EZ , E1/2) = m Var(Tj | ^a(·), x, EZ , E1/2)



S

C

z~2 .

m

(4)

Now, for each j  C let

Tj = Tj - E [Tj | ^a(·), x, EZ , E1/2] = Tj - z~,

and let T = jC Tj. Note that by the fact that we conditioned on the realization x of x such that x  Z (event EZ ), we obtain by definition of Tj in (2) and the inequality (3):

Tj

=

S

wj ^aj(x) m sj



S C z~ .
m

(5)

We also have that S  1 by definition. More specifically, using the fact that the maximum over a set is greater than the average and rearranging sums, we obtain

S=

sj =

max
x S

gj(x )

jW

jW

11

 |S |

gj(x ) = |S|

gj(x )

jW x S

x S jW

1 = 1 = 1.
|S |
x S

Thus, the inequality established in (5) with the fact that S  1 we obtain an upper bound on the absolute value of the centered random variables:

z~ S C z~ |Tj| = Tj - m  m = M,

(6)

2Since we conditioned on the event EZ .

15

Under review as a conference paper at ICLR 2019

which

follows

from

the

fact

that:

if

Tj



z~ m

:

Then,

by

our

bound

in

(5)

and

the

fact

that

z~ m



0,

it

follows that

Tj

=

Tj

-

z~ m



S C z~ m

-

z~ m



S C z~ .
m

if Tj

<

z~ m

:

Then, using the fact that Tj  0 and S  1, we obtain

Tj

=

z~ m

-

Tj



z~ m



S C z~ .
m

Applying Bernstein's inequality to both T and -T we have by symmetry and the union bound,

P(Ec | ^a(·), x, EZ , E1/2) = P |T - z~|  z~ | ^a(·), x, EZ , E1/2

2z~2

 2 exp

- 2 Var(T

|

^a(·), x) +

2  z~M 3

 2 exp = 2 exp

2z~2

- 2SC z~2
m

+

2S C z~2 3m

- 3 2 m 8S C



 ,

4

where the second inequality follows by our upper bounds on Var(T | ^a(·), x) and Tj and the fact

that   (0, 1), and the last inequality follows by our choice of m =

8 S log( ) log(8 /) 2

. This

establishes that for any realization ^a(·) of a^l-1(·) and a realization x of x satisfying x  Z, the event

Ec

occurs

with

probability

at

most

 4

.

Removing the conditioning on EZ : We have by law of total probability

P(E | ^a(·), E1/2)   =

P(E
xZ

|

^a(·),

x,

EZ

,

E1/2)

P (x
xD

=

x

|

^a(·), E1/2) dx

1-  4

xZ

P (x
xD

=

x

|

^a(·),

E1/2) dx



1- 4

xPD(EZ | ^a(·), E1/2)

 1-  1-  4 8

 1 - 3 8

where the second-to-last inequality follows from the fact that P(Ec

|

^a(·), x, EZ , E1/2) 

 4

as was

established above and the last inequality follows by Lemma 7.

Putting it all together Finally, we marginalize out the random variable a^ -1(·) to establish

P(E | E1/2) =

P(E | ^a(·), E1/2) P(^a(·) | E1/2) d^a(·)

^a(·)

 1 - 3 8

P(^a(·) | E1/2) d^a(·)
^a(·)

= 1 - 3 . 8

16

Under review as a conference paper at ICLR 2019

Consequently,

P(E c | E1/2)  1 -

1 - 3 8

3 =,
8

and this concludes the proof.

A.2 ANALYTICAL RESULTS FOR SECTION 5.3 (IMPORTANCE SAMPLING BOUNDS) We begin by establishing an auxiliary result that we will need for the subsequent lemmas.

A.2.1 EMPIRICAL i APPROXIMATION

Lemma 8 (Empirical i Approximation). Let   (0, 1),  = log( )/2, and define

^ =

1

|S |

max
i[ ]

x

S

i (x

)

+ ,

 where  = 2

1

+

 2

log

(8





/)

and S  P is as in Alg. 1. Then,

P
xD

max
i[ ]

i

(x)



^

1-

 .

4

Proof. Define the random variables Yx = E [i (x )] - i (x ) for each x  S and consider the sum

Y = Yx =

E [i (x)] - i (x ) .

x S

x S

We know that each random variable Yx satisfies E [Yx ] = 0 and by Assumption 2, is subexponential with parameter   . Thus, Y is a sum of |S| independent, zero-mean -subexponential random variables, which implies that E [Y] = 0 and that we can readily apply Bernstein's inequality for
subexponential random variables (Vershynin, 2016) to obtain for t  0

P

1Y |S |



t

 exp

-|S| min

t2 t 4 2 , 2 

.

Since S =

log (8  /) log( )



log

(8



/)

2,

we

have

for

t

=

 2,

P

E

[i (x)]

-

1 |S |

i (x )  t

=P

1Y |S |



t

x S

 exp

-|S

|

t2 42

 exp (- log (8  /))

 = 8   .

Moreover, for a single Yx, we have by the equivalent definition of a subexponential random variable (Vershynin, 2016) that for u  0

P(i (x) - E [i (x)]  u)  exp Thus, for u = 2 log (8  /) we obtain

- min

-

u2 4 2

,

u 2 

.

P(i (x)

-

E [i (x)]



u)



exp (- log (8 

/))

=

 8   .

17

Under review as a conference paper at ICLR 2019

Therefore,

by

the

union

bound,

we

have

with

probability

at

least

1

-

 4 

:

i (x)  E [i (x)] + u

1 |S |

i (x ) + t + u

x S

1

= |S|

i (x ) +

x S

2 + 2 log (8  /)

1

= |S |

i (x ) + 

x S

 ^ ,

where the last inequality follows by definition of ^ .

Thus, by the union bound, we have

P
xD

max
i[ ]

i (x)

>

^

= P i  [ ] : i (x) > ^

 P i (x) > ^
i[ ]



 4 



 ,

4

where the last line follows by definition of    .

A.2.2 NOTATION FOR THE SUBSEQUENT ANALYSIS

Let w^i+ and w^i- denote the sparsified row vectors generated when SPARSIFY is invoked with first two arguments corresponding to (W+, wi ) and (W-, -wi ), respectively (Alg. 1, Line 12). We will at times omit including the variables for the neuron i and layer in the proofs for clarity of exposition, and for example, refer to w^i+ and w^i- as simply w^+ and w^-, respectively.
Let x  D and define

z^+(x) =

w^k+ a^k(x)  0 and z^-(x) =

(-w^k-) a^k(x)  0

kW+

kW-

be the approximate intermediate values corresponding to the sparsified matrices w^+ and w^-; let

z~+(x) =

wk a^k(x)  0 and z~-(x) =

(-wk) a^k(x)  0

kW+

kW-

be the corresponding intermediate values with respect to the the original row vector w; and finally, let

z+(x) =

wk ak(x)  0 and z-(x) =

(-wk) ak(x)  0

kW+

kW-

be the true intermediate values corresponding to the positive and negative valued weights.

Note that in this context, we have by definition

z^i (x) = w^, a^(x) = z^+(x) - z^-(x), z~i (x) = w, a^(x) = z~+(x) - z~-(x), zi (x) = w, a(x) = z+(x) - z-(x),

where

we

used

the

fact

that

w^

=

w^+

-

w^-



R1×

-1
.

and

18

Under review as a conference paper at ICLR 2019

A.2.3 PROOF OF LEMMA 2

Lemma 2 (Conditional Neuron Value Approximation). Let ,   (0, 1),  {2, . . . , L}, i  [ ], and x  D. CORENET generates a row vector w^i = w^i+ - w^i-  R1× -1 such that

P Ei | E -1 = P z^i (x)  (1 ± 2 ( - 1)  +1) zi (x) | E -1  1 - /,

(1)

where 

=

 ^ 

and

nnz(w^i )



8 S log( ) log(8 /) 2

+ 1, where S =

jW+ sj +

jW- sj .

Proof. Let ,   (0, 1) be arbitrary and let W+ = {j  [ -1] : wj > 0} and W- = {j 

[ -1] : wj < 0} as in Alg. 1. Let 

be defined as before, 

=

 ^



,

where

^



=

L k=

^ k and

^ =

1 |S |

maxi[

]

x S i (x ) + .

Observe that wj > 0 j  W+ and similarly, for all (-wj) > 0 j  W-. That is, each of index sets W+ and W- corresponds to strictly positive entries in the arguments wi and -wi , respectively passed into SPARSIFY. Observe that since we conditioned on the event E -1, we have

2(

- 2) 

 2(

- 2)  2 (L - 1)

L k=

^ k





L k=

^ k



 2L-

+1



 ,

2

Since ^ k  2 k  { , . . . , L}

where the inequality ^ k  2 follows from the fact that

^ k =

1

|S |

max
i[ ]

x

S

i (x

)

1+

 2.

+

Since i (x )  1 x  supp(D) by definition

we obtain that a^(x)  (1 ± /2)a(x), where, as before, a^ and a are shorthand notations for

a^ -1  R -1×1 and a -1  R -1×1, respectively. This implies that E -1  E1/2 and since

m=

8 S log( ) log(8 /) 2

in Alg. 2 we can invoke Lemma 1 with  =  on each of the SPARSIFY

invocations to conclude that

P

z^+(x) / (1 ±  )z~+(x) | E -1

P

z^+(x) / (1 ±  )z~+(x) | E1/2



3 ,

8

and

P

z^-(x) / (1 ±  )z~-(x) | E -1



3 .

8

Therefore, by the union bound, we have

P

z^+(x) / (1 ±  )z~+(x) or z^-(x) / (1 ±  )z~-(x) | E -1



3

+

3

=

3 .

8 8 4

Moreover,

by

Lemma

8,

we

have

with

probability

at

most

 4

that

i (x) > ^ .
Thus, by the union bound over the failure events, we have that with probability at least 1 - (3/4 + /4) = 1 - / that both of the following events occur

1. z^+(x)  (1 ±  )z~+(x) and z^-(x)  (1 ±  )z~-(x)

(7)

19

Under review as a conference paper at ICLR 2019

2. i (x)  ^

(8)

Recall that 

=

2

 (L-1)

,



=

 ^



,

and

that event

Ei

denotes

the

(desirable)

event

that

z^i (x) (1 ± 2 ( - 1)  +1) zi (x)

holds, and similarly, E = i[ ] Ei denotes the vector-wise analogue where

z^ (x) (1 ± 2 ( - 1)  +1) z (x). Let k = 2 ( - 1) and note that by conditioning on the event E -1, i.e., we have by definition

a^ -1(x)  (1 ± 2 ( - 2) )a -1(x) = (1 ± k  )a -1(x),

which follows by definition of the ReLU function. Recall that our overarching goal is to establish that

z^i (x)  (1 ± 2 ( - 1) +1) zi (x), which would immediately imply by definition of the ReLU function that

a^i (x)  (1 ± 2 ( - 1) +1) ai (x).

Having clarified the conditioning and our objective, we will once again drop the index i from the expressions moving forward.

Proceeding from above, we have with probability at least 1 - /

z^(x) = z^+(x) - z^-(x)  (1 +  ) z~+(x) - (1 -  ) z~-(x)  (1 +  )(1 + k  ) z+(x) - (1 -  )(1 - k  ) z-(x) = 1 +  (k + 1) + k2 z+(x) + -1 + (k + 1) - k2 z-(x) = 1 + k 2 z(x) + (k + 1)  z+(x) + z-(x)

By Event (7) above Conditioning on event E -1

=

1 + k 2

(k + 1) 

z(x) +

L k=

^ k

z+(x) + z-(x)



1 + k 2

(k + 1) 

z(x) + i (x)

L k=

+1

^ k

z+(x) + z-(x)

=

1 + k 2

z(x) +

(k + 1) 

L k=

+1

^ k

|z(x)|

= 1 + k 2 z(x) + (k + 1)  +1 |z(x)|.

By Event (8) above

z+(x) + z-(x)

By i (x) =

|z(x)|

To upper bound the last expression above, we begin by observing that k2   , which follows from

the fact that 



1 2 (L-1)



1 k

by definition.

Moreover,

we

also note

that 

  +1 by definition of

^  1.

Now, we consider two cases. Case of z(x)  0: In this case, we have

z^(x)  1 + k 2 z(x) + (k + 1)  +1 |z(x)|  (1 +  )z(x) + (k + 1) +1z(x)  (1 +  +1)z(x) + (k + 1) +1z(x) = (1 + (k + 2)  +1) z(x) = (1 + 2 ( - 1) +1) z(x),
where the last line follows by definition of k = 2 ( - 2), which implies that k + 2 = 2( - 1). Thus, this establishes the desired upper bound in the case that z(x)  0.

20

Under review as a conference paper at ICLR 2019

Case of z(x) < 0: Since z(x) is negative, we have that 1 + k 2 z(x)  z(x) and |z(x)| = -z(x) and thus

z^(x)  1 + k 2 z(x) + (k + 1)  +1 |z(x)|  z(x) - (k + 1) +1z(x)  (1 - (k + 1) +1) z(x)  (1 - (k + 2) +1) z(x) = (1 - 2 ( - 1) +1) z(x),

and this establishes the upper bound for the case of z(x) being negative.

Putting the results of the case by case analysis together, we have the upper bound of z^(x)  z(x) +

2 ( - 1) +1|z(x)|. The proof for establishing the lower bound for z(x) is analogous to that given

above, and yields z^(x)  z(x)-2 ( -1) +1|z(x)|. Putting both the upper and lower bound together,

we

have

that

with

probability

at

least

1

-

 

:

z^(x)  (1 ± 2 ( - 1) +1) z(x),

and this completes the proof.

A.2.4 REMARKS ON NEGATIVE ACTIVATIONS

We note that up to now we assumed that the input a(x), i.e., the activations from the previous layer,

are strictly nonnegative. For layers  {3, . . . , L}, this is indeed true due to the nonnegativity of

the ReLU activation function. For layer 2, the input is a(x) = x, which can be decomposed into

a(x)

=

apos(x)

-

aneg(x),

where

apos(x)



0



R

-1

and

aneg(x)



0



R

-1
.

Furthermore,

we can define the sensitivity over the set of points {apos(x), aneg(x) | x  S} (instead of {a(x) |

x  S}), and thus maintain the required nonnegativity of the sensitivities. Then, in the terminology

of Lemma 2, we let

zp+os(x) =

wk apos,k(x)  0 and zn-eg(x) =

(-wk) aneg,k(x)  0

kW+

kW-

be the corresponding positive parts, and

zn+eg(x) =

wk aneg,k(x)  0 and zp-os(x) =

(-wk) apos,k(x)  0

kW+

kW-

be the corresponding negative parts of the preactivation of the considered layer, such that

z+(x) = zp+os(x) + zn-eg(x) and z-(x) = zn+eg(x) + zp-os(x).

We also let

z+(x) + z-(x)

i (x) =

|z(x)|

be as before, with z+(x) and z-(x) defined as above. Equipped with above definitions, we can rederive Lemma 2 analogously in the more general setting, i.e., with potentially negative activations. We also note that we require a slightly larger sample size now since we have to take a union bound over the failure probabilities of all four approximations (i.e. z^p+os(x), z^n-eg(x), z^n+eg(x), and z^p-os(x)) to obtain the desired overall failure probability of /.

A.2.5 PROOF OF THEOREM 4
The following corollary immediately follows from Lemma 2 and establishes a layer-wise approximation guarantee.

21

Under review as a conference paper at ICLR 2019

Corollary 9 (Conditional Layer-wise Approximation). Let ,   (0, 1),  {2, . . . , L}, and x  D. CORENET generates a sparse weight matrix W^ = w^1, . . . , w^  R × -1 such that

P(E

| E -1) = P z^ (x)  (1 ± 2 ( - 1)  +1) z (x) | E -1

1-  , 

(9)

where 

=

 ^



,

z^

(x)

=

W^

a^ (x), and z (x) = W

a (x).

Proof. Since (1) established by Lemma 2 holds for any neuron i  [ ] in layer and since (E )c =

i[ ](Ei )c, it follows by the union bound over the failure events (Ei )c for all i  [ ] that with

probability

at

least

1

-

 

z^ (x) = W^ a^ -1(x)  (1 ± 2 ( - 1)  +1) W a -1(x) = (1 ± 2 ( - 1)  +1) z (x).

The following lemma removes the conditioning on E -1 and explicitly considers the (compounding) error incurred by generating coresets W^ 2, . . . , W^ for multiple layers.
Lemma 3 (Layer-wise Approximation). Let ,   (0, 1),  {2, . . . , L}, and x  D. CORENET generates a sparse weight matrix W^  R × -1 such that, for z^ (x) = W^ a^ (x),

P (E ) = P

z^ (x)  (1 ± 2 ( - 1)  +1) z (x)  1 - 

(W^ 2,...,W^ ), x

(W^ 2,...,W^ ), x

=2  . 

Proof. Invoking Corollary 9, we know that for any layer  {2, . . . , L},

P (E
W^ , x, a^ -1(·)

| E -1)  1 -   . 

(10)

We also have by the law of total probability that

P(E ) = P(E | E -1) P(E -1) + P(E | (E -1)c) P((E -1)c)  P(E | E -1) P(E -1)

(11)

Repeated applications of (10) and (11) in conjunction with the observation that P(E1) = 13 yield

P(E )  P(E ...

| E -1) P(E -1)

Repeated applications of (11)

 P(E | E -1)
=2
 1-  
=2
1-   
=2

By (10) By the Weierstrass Product Inequality,

3Since we do not compress the input layer.

22

Under review as a conference paper at ICLR 2019

where the last inequality follows by the Weierstrass Product Inequality4 and this establishes the lemma.

Appropriately invoking Lemma 3, we can now establish the approximation guarantee for the entire neural network. This is stated in Theorem 4 and the proof can be found below.
Theorem 4 (Network Compression). For ,   (0, 1), Algorithm 1 generates a set of parameters ^ = (W^ 2, . . . , W^ L) of size

L
nnz(^) 
=2 i=1

32 (L - 1)2 (^ )2 Si log( ) log(8 /) 2

+1

in O   log  / time such that P^, xD f^(x)  (1 ± )f(x)  1 - .

Proof. Invoking Lemma 3 with = L, we have that for ^ = (W^ 2, . . . , W^ L),

P
^, x

f^(x)  2 (L - 1) L+1f(x)

= P (z^L(x)  2 (L - 1) L+1zL(x))
^, x

= P(EL)

1- 

L =2





= 1 - ,

where the last equality follows by definition of  =

L =2



.

Note

that

by

definition,



L+1 = 2 (L - 1)

L k=L+1

^ k

 =,
2 (L - 1)

where the last equality follows by the fact that the empty product

L k=L+1

^ k

is

equal

to

1.

Thus, we have

2 (L - 1)L+1 = ,

and so we conclude

P
^, x

f^(x)  (1 ± )f(x)

 1 - ,

which, along with the sampling complexity of Alg. 2 (Line 6), establishes the approximation guarantee provided by the theorem.

For the computational time complexity, we observe that the most time consuming operation per itera-

tion of the loop on Lines 7-13 is the weight sparsification procedure. The asymptotic time complexity

of each SPARSIFY invocation for each neuron i  [ ] in layers  {2, . . . , L} (Alg. 1, Line 12) is

dominated by the relative importance computation for incoming edges (Alg. 2, Lines 1-2). This can be

done by evaluating wikak-1(x) for all k  W and x  S, for a total computation time that is bounded above by O |S|  -1 since |W|   -1 for each i  [ ]. Thus, SPARSIFY takes O |S|  -1

time. Summing the computation time over all layers and neurons in each layer, we obtain an asymp-

totic time complexity of O |S|

L =2



-1

 O (|S|  ). Since |S|  O(log( /)), we

conclude that the computational complexity our neural network compression algorithm is

O   log  / .

(12)

4The Weierstrass Product Inequality (Doerr, 2018) states that for p1, . . . , pn  [0, 1],
nn
(1 - pi)  1 - pi.
i=1 i=1

23

Under review as a conference paper at ICLR 2019

A.2.6 PROOF OF THEOREM 11

In order to ensure that the established sampling bounds are non-vacuous in terms of the sensitivity, i.e., not linear in the number of incoming edges, we show that the sum of sensitivities per neuron S is small. The following lemma establishes that the sum of sensitivities can be bounded instance-independent by a term that is logarithmic in roughly the total number of edges ( · ).

Lemma 10 (Sensitivity Bound). For any  {2, . . . , L} and i  [ ], the sum of sensitivities S =

S+ + S- is bounded by

S  2 |S| = 2 log (8  /) log( ) .

Proof. Consider S+ for an arbitrary  {2, . . . , L} and i  [ ]. For all j  W we have the following bound on the sensitivity of a single j  W,

sj = max gj(x)  gj(x) =

xS

xS

xS

wj aj(x) , kW wk ak(x)

where the inequality follows from the fact that we can upper bound the max by a summation over x  S since gj(x)  0, j  W. Thus,

S+ =

sj 

gj (x)

jW

jW xS

=
xS

jW wj aj (x) = |S|, kW wk ak(x)

where we used the fact that the sum of sensitivities is finite to swap the order of summation.

Using the same argument as above, we obtain S- = jW- sj  |S|, which establishes the lemma.

Note that the sampling complexities established above have a linear dependence on the sum of sen-

sitivities,

L =2

 i=1

Si

,

which

is

instance-dependent,

i.e.,

depends

on

the

sampled

S



P

and

the

actual weights of the trained neural network. By applying Lemma 10, we obtain a bound on the size

of the compressed network that is independent of the sensitivity.

Theorem 11 (Sensitivity-Independent Network Compression). For any given ,   (0, 1) our sampling scheme (Alg. 1) generates a set of parameters ^ of size

nnz(^)  O

log(/) log( /) log2( )  L2 2

L
(^ )2

,

=2

in O   log  / time, such that P^, xD f^(x)  (1 ± )f(x)  1 - .

Proof. Combining Lemma 10 and Theorem 4 establishes the theorem.

A.2.7 GENERALIZED NETWORK COMPRESSION
Theorem 4 gives us an approximation guarantee with respect to one randomly drawn point x  D. The following corollary extends this approximation guarantee to any set of n randomly drawn points using a union bound argument, which enables approximation guarantees for, e.g., a test data set composed of n i.i.d. points drawn from the distribution. We note that the sampling complexity only increases by roughly a logarithmic term in n.
Corollary 12 (Generalized Network Compression). For any ,   (0, 1) and a set of i.i.d. input points P of cardinality |P |  N+, i.e., P i.i.d. D|P |, consider the reparameterized version of Alg. 1 with
1. S  P of size |S|  log (16 |P |  /) log( ) ,

24

Under review as a conference paper at ICLR 2019

2. ^ =

1 |S |

maxi[

]

x S i (x ) +  as before, but  is instead defined as

 = 2 1 + 2 log (16 |P |  /) , and

3. m 

8 S log( ) log(16|P | /) 2

in the sample complexity in SPARSIFYWEIGHTS.

Then, Alg. 1 generates a set of neural network parameters ^ of size at most

L
nnz(^) 
=2 i=1

32 (L - 1)2 (^ )2 Si log( ) log(16 |P | /) 2





O

log( 

)

log( 2

|P

|/)

L2

L
(^ )2

Si  ,

=2 i=1

+1

in O   log   |P |/ time such that



P
^, x

x  P

: f^(x)  (1 ± )f(x)

1- . 2

Proof. The reparameterization enables us to invoke Theorem 4 with  = /2 |P |; applying the union bound over all |P | i.i.d. samples in P establishes the corollary.

25

Under review as a conference paper at ICLR 2019

B ADDITIONAL RESULTS
In this section, we give more details on the evaluation of our compression algorithm on popular benchmark data sets and varying fully-connected neural network configurations. In the experiments, we compare the effectiveness of our sampling scheme in reducing the number of non-zero parameters of a network to that of uniform sampling and the singular value decomposition (SVD). All algorithms were implemented in Python using the PyTorch library (Paszke et al., 2017) and simulations were conducted on a computer with a 2.60 GHz Intel i9-7980XE processor (18 cores total) and 128 GB RAM.
For training and evaluating the algorithms considered in this section, we used the following off-theshelf data sets:
· MNIST (LeCun et al., 1998) -- 70, 000 images of handwritten digits between 0 and 9 in the form of 28 × 28 pixels per image.
· CIFAR-10 (Krizhevsky & Hinton, 2009) -- 60, 000 32 × 32 color images, a subset of the larger CIFAR-100 dataset, each depicting an object from one of 10 classes, e.g., airplanes.
· FashionMNIST (Xiao et al., 2017) -- A recently proposed drop-in replacement for the MNIST data set that, like MNIST, contains 60, 000, 28 × 28 grayscale images, each associated with a label from 10 different categories.
We considered a diverse set of network configurations for each of the data sets. We varied the number of hidden layers between 2 and 5 and used either a constant width across all hidden layers between 200 and 1000 or a linearly decreasing width (denoted by "Pyramid" in the figures). Training was performed for 30 epochs on the normalized data sets using an Adam optimizer with a learning rate of 0.001 and a batch size of 300. The test accuracies were roughly 98% (MNIST), 45% (CIFAR10), and 96% (FashionMNIST), depending on the network architecture. To account for the randomness in the training procedure, for each data set and neural network configuration, we averaged our results across 4 trained neural networks.

B.1 DETAILS ON THE COMPRESSION ALGORITHMS

We evaluated and compared the performance of the following algorithms on the aforementioned data sets.

1. Uniform (Edge) Sampling -- A uniform distribution is used, rather than our sensitivity-based importance sampling distribution, to sample the incoming edges to each neuron in the network. Note that like our sampling scheme, uniform sampling edges generates an unbiased estimator of the neuron value. However, unlike our approach which explicitly seeks to minimize estimator variance using the bounds provided by empirical sensitivity, uniform sampling is prone to exhibiting large estimator variance.

2. Singular Value Decomposition (SVD) -- The (truncated) SVD decomposition is used to generate a low-rank (rank-r) approximation for each of the weight matrices (W^ 2, . . . , W^ L)
to obtain the corresponding parameters ^ = (W^ r2, . . . , W^ rL) for various values of r  N+. Unlike the compared sampling-based methods, SVD does not sparsify the weight matrices.
Thus, to achieve fair comparisons of compression rates, we compute the size of the rank-r matrices constituting ^ as,

Lr

nnz(^) =

nnz(ui ) + nnz(vi ) ,

=2 i=1

where W = U  (V ) for each  {2, . . . , L}, with 1  2 . . .   -1 and ui and vi denote the ith columns of U and V respectively.
3. CoreNet (Edge Sampling) -- Our core algorithm for edge sampling shown in Alg. 2, but without the neuron pruning procedure.

4. CoreNet+ (CoreNet & Neuron Pruning) -- Our algorithm shown in Alg. 1 that includes the neuron pruning step.

26

Under review as a conference paper at ICLR 2019

5. CoreNet++ (CoreNet+ & Amplification) -- In addition to the features of Corenet+, multi-
ple coresets C1, . . . , C are constructed over   N+ trials, and the best one is picked by evaluating the empirical error on a subset T  P \ S (see Sec. 4 for details).

B.2 PRESERVING THE OUTPUT OF A NEURAL NETWORK
We evaluated the accuracy of our approximation by comparing the output of the compressed network with that of the original one and compute the 1-norm of the relative error vector. We computed the error metric for both the uniform sampling scheme as well as our compression algorithm (Alg. 1). Our results were averaged over 50 trials, where for each trial, the relative approximation error was averaged over the entire test set. In particular, for a test set Ptest  Rd consisting of d dimensional points, the average relative error of with respect to the f^ generated by each compression algorithm was computed as
1 errorPtest (f^) = |Ptest| xPtest f^(x) - f(x) 1 .
Figures 3, 4, and 5 depict the average performance of the compared algorithms for various network architectures trained on MNIST, CIFAR-10, and FashionMNIST, respectively. Our algorithm is able to compress networks trained on MNIST and FashionMNIST to about 10% of their original size without significant loss of accuracy. On CIFAR-10, a compression rate of 50% yields classification results comparable to that of uncompressed networks. The shaded region corresponding to each curve represents the values within one standard deviation of the mean.

B.3 PRESERVING THE CLASSIFICATION PERFORMANCE

We also evaluated the accuracy of our approximation by computing the loss of prediction accuracy on
a test data set, Ptest. In particular, let accPtest (f) be the average accuracy of the neural network f, i.e,.

11
accPtest (f) = |Ptest| xPtest

argmax f(x) = y(x)
i[ L ]

,

where y(x) denotes the (true) label associated with x. Then the drop in accuracy is computed as

accPtest (f) - accPtest (f^).
Figures 6, 7, and 8 depict the average performance of the compared algorithms for various network architectures trained on MNIST, CIFAR-10, and FashionMNIST respectively. The shaded region corresponding to each curve represents the values within one standard deviation of the mean.

B.4 DISCUSSION
As indicated in Sec. 6, the simulation results presented here validate our theoretical results and suggest that empirical sensitivity can lead to effective, more informed sampling compared to other methods. Moreover, we are able to outperform networks that are compressed via uniform sampling or SVD. We also note that there is a notable difference in the performance of our algorithm between different datasets. In particular, the difference in performance of our algorithm compared to the other method for networks trained on FashionMNIST and MNIST is much more significant than for networks trained on CIFAR. We conjecture that this is partially due to considering only fully-connected networks as these network perform fairly poorly on CIFAR (around 45% classification accuracy) and thus edges have more uniformly distributed sensitivity as the information content in the network is limited. We envision that extending our guarantees to convolutional neural networks may enable us to further reason about the performance on data sets such as CIFAR.

27

Under review as a conference paper at ICLR 2019

Average Relative Error

Average Relative Error

Average Relative Error

MNIST: L = 2, * = 200, Pyramid

23

21

Uniform Sampling

21

SVD CoreNet

23

CoreNet+ CoreNet++

Per1ce0ntage of2N0on-zero P3a0rameters4R0etained

MNIST: L = 3, * = 200, Pyramid

25 Uniform Sampling

24 23

SVD CoreNet CoreNet+

22 CoreNet++

21

20

2

1
Per1ce0ntage

of2N0on-zero P3a0rameters40Retained

MNIST: L = 5, * = 200, Pyramid

26

Uniform Sampling SVD

24

CoreNet CoreNet+

22 CoreNet++

20

Per1c0entage of2N0on-zero 3P0arameter4s0Retained

MNIST: L = 3, * = 200

25 Uniform Sampling

23

SVD CoreNet

21

CoreNet+ CoreNet++

21

23

Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

27

MNIST: L = 5, * = 200 Uniform Sampling

25 23

SVD CoreNet CoreNet+

21 CoreNet++

21

23

Per1c0entage of20Non-zero3P0aramete4rs0Retained

Average Relative Error

Average Relative Error

Average Relative Error

Average Relative Error

Average Relative Error

MNIST: L = 2, * = 500, Pyramid 24

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of N20on-zero Pa3r0ameters R4e0tained 25 MNIST: L = 3, * = 500, Pyramid

23

21 Uniform Sampling

21

SVD CoreNet

23

CoreNet+ CoreNet++

Perc1e0ntage of N20on-zero Pa3r0ameters R4e0tained

MNIST: L = 5, * = 500, Pyramid

25

Uniform Sampling SVD

23

CoreNet CoreNet+

21 CoreNet++

21

23

Per1ce0ntage of2N0on-zero P3a0rameters4R0etained 25 MNIST: L = 3, * = 500

23

21 Uniform Sampling

21

SVD CoreNet

23

CoreNet+ CoreNet++

Perc1e0ntage of2N0on-zero P3a0rameters40Retained

MNIST: L = 5, * = 500

25

Uniform Sampling SVD

23 CoreNet

21

CoreNet+ CoreNet++

21

23

Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

Average Relative Error

Average Relative Error

Average Relative Error

Average Relative Error

Average Relative Error

24 MNIST: L = 2, * = 1000, Pyramid

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of 2N0on-zero P3a0rameters 4R0etained MNIST: L = 3, * = 1000, Pyramid
24

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of N2o0n-zero Pa3r0ameters R4e0tained

MNIST: L = 5, * = 1000, Pyramid

25 Uniform Sampling

SVD

23 CoreNet

21

CoreNet+ CoreNet++

21

23

Perc1e0ntage of 2N0on-zero Pa3r0ameters R4e0tained MNIST: L = 3, * = 700
24

22

20 Uniform Sampling

SVD

2 2 CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of2N0on-zero P3a0rameters40Retained

MNIST: L = 5, * = 700

25

Uniform Sampling SVD

23 CoreNet

21

CoreNet+ CoreNet++

21

23

Perc1e0ntage of N2o0n-zero Para3m0 eters Ret4a0ined

Average Relative Error

Average Relative Error

Figure 3: Evaluations against the MNIST dataset with varying number of hidden layers (L) and number of neurons per hidden layer (). Shaded region corresponds to values within one standard deviation of the mean. The figures show that our algorithm's relative performance increases as the number of layers (and hence the number of redundant parameters) increases.

28

Under review as a conference paper at ICLR 2019

Average Relative Error

Average Relative Error

Average Relative Error

CIFAR: L = 2, * = 200, Pyramid

26 Uniform Sampling

SVD

24

CoreNet CoreNet+

CoreNet++

22

20

Per1c0entage2o0f Non-ze3r0o Param4e0ters Re5ta0ined

CIFAR: L = 3, * = 200, Pyramid

27 Uniform Sampling

SVD

25

CoreNet CoreNet+

CoreNet++

23

21

Average Relative Error

Average Relative Error

CIFAR: L = 2, * = 500, Pyramid

25

Uniform Sampling SVD

CoreNet

23 CoreNet+

CoreNet++

21

21

Per1c0entage 2o0f Non-ze3r0o Param4e0ters Ret5a0ined

CIFAR: L = 3, * = 500, Pyramid

26

Uniform Sampling SVD

24

CoreNet CoreNet+

CoreNet++

22

20

Average Relative Error

Average Relative Error

26 CIFAR: L = 2, * = 1000, Pyramid

24

22 Uniform Sampling

20

SVD CoreNet

22

CoreNet+ CoreNet++

Per1ce0ntage of2N0on-zero3P0aramete4rs0Retained

CIFAR: L = 3, * = 1000, Pyramid

27 Uniform Sampling

25

SVD CoreNet

23

CoreNet+ CoreNet++

21

21

Per1c0entage2o0f Non-ze3r0o Param4e0ters Re5t0ained

CIFAR: L = 5, * = 200, Pyramid

29

Uniform Sampling SVD

27 CoreNet

25

CoreNet+ CoreNet++

23

21

Per1c0entage2o0f Non-ze3r0o Param4e0ters Re5t0ained

CIFAR: L = 3, * = 200

27

Uniform Sampling SVD

25

CoreNet CoreNet+

23 CoreNet++

21

Per1c0entage2o0f Non-ze3r0o Param40eters Re5t0ained

CIFAR: L = 5, * = 200

29

Uniform Sampling SVD

27

CoreNet CoreNet+

25 CoreNet++

23

21

Per1c0entage2o0f Non-ze3r0o Param4e0ters Re5ta0ined

Average Relative Error

Average Relative Error

Average Relative Error

Per1c0entage 2of0Non-ze3ro0 Param4e0ters Ret5a0ined

210 28

CIFAR: L = 5, * = 500, Pyramid Uniform Sampling SVD

26

CoreNet CoreNet+

24 CoreNet++

22

20

Per1c0entage 2of0Non-ze3ro0 Param4e0ters Reta5i0ned
CIFAR: L = 3, * = 500 27 Uniform Sampling
SVD 25 CoreNet
CoreNet+ 23 CoreNet++

21

21

Per1c0entage 2o0f Non-ze3r0o Param4e0ters Ret5a0ined

CIFAR: L = 5, * = 500

210 Uniform Sampling

28

SVD CoreNet

26 CoreNet+

24 CoreNet++

22

20

Per1c0entage o2f0Non-zero30Paramete4r0s Retain5e0d

Average Relative Error

Average Relative Error

Average Relative Error

Per1ce0ntage of2N0on-zero3P0aramete4rs0Retained

210

CIFAR: L = 5, * = 1000, Pyramid Uniform Sampling

28 SVD

26

CoreNet CoreNet+

24 CoreNet++

22

20

Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

CIFAR: L = 3, * = 700

27 Uniform Sampling

25

SVD CoreNet

23

CoreNet+ CoreNet++

21

21

Per1c0entage o2f0Non-zer3o0Parame4te0rs Retai5n0ed

CIFAR: L = 5, * = 700

210 Uniform Sampling

28

SVD CoreNet

26 CoreNet+

24 CoreNet++

22

20

Perc1e0ntage of2N0on-zero P3a0rameters4R0etained

Average Relative Error

Average Relative Error

Figure 4: Evaluations against the CIFAR-10 dataset with varying number of hidden layers (L) and number of neurons per hidden layer (). The trend of our algorithm's improved relative performance as the number of parameters increases (previously depicted in Fig. 3) also holds for the CIFAR-10 data set.

29

Under review as a conference paper at ICLR 2019

Average Relative Error

Average Relative Error

Average Relative Error

FashionMNIST: L = 2, * = 200, Pyramid

23

21

Uniform Sampling

21

SVD CoreNet

23

CoreNet+ CoreNet++

Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

FashionMNIST: L = 3, * = 200, Pyramid

25 Uniform Sampling

24 23

SVD CoreNet CoreNet+

22 CoreNet++

21

20

2

1
Per1c0entage

of2N0on-zero 3P0arameter4s0Retained

FashionMNIST: L = 5, * = 200, Pyramid

27 Uniform Sampling

25

SVD CoreNet

23

CoreNet+ CoreNet++

21

21

Per1c0entage of2N0on-zero 3P0arameter4s0Retained

FashionMNIST: L = 3, * = 200

25 Uniform Sampling

23

SVD CoreNet

21

CoreNet+ CoreNet++

21

23

Per1c0entage of20Non-zero3P0aramete4rs0Retained

27

FashionMNIST: L = 5, * = 200 Uniform Sampling

25 SVD

23

CoreNet CoreNet+

21 CoreNet++

21

23

Per1c0entage of2N0on-zero3P0arameter4s0Retained

Average Relative Error

Average Relative Error

Average Relative Error

Average Relative Error

Average Relative Error

24 FashionMNIST: L = 2, * = 500, Pyramid

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of 2N0on-zero P3a0rameters 4R0etained 25 FashionMNIST: L = 3, * = 500, Pyramid

23

21 Uniform Sampling

21

SVD CoreNet

23

CoreNet+ CoreNet++

Perc1e0ntage of 2N0on-zero Pa3r0ameters R4e0tained

FashionMNIST: L = 5, * = 500, Pyramid

25

Uniform Sampling SVD

23

CoreNet CoreNet+

21 CoreNet++

21

23

Per1ce0ntage of2N0on-zero P3a0rameters4R0etained 25 FashionMNIST: L = 3, * = 500

23

21 Uniform Sampling

21

SVD CoreNet

23

CoreNet+ CoreNet++

Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

FashionMNIST: L = 5, * = 500

25

Uniform Sampling SVD

23 CoreNet

21

CoreNet+ CoreNet++

21

23

Per1ce0ntage of2N0on-zero3P0arameter4s0Retained

Average Relative Error

Average Relative Error

Average Relative Error

Average Relative Error

Average Relative Error

24 FashionMNIST: L = 2, * = 1000, Pyramid

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of2N0on-zero P3a0rameters4R0etained FashionMNIST: L = 3, * = 1000, Pyramid 24

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of N20on-zero Pa3r0ameters R4e0tained

FashionMNIST: L = 5, * = 1000, Pyramid

25

Uniform Sampling SVD

23 CoreNet

CoreNet+

21 CoreNet++

21

23

Perc1e0ntage of 2N0on-zero Pa3r0ameters R4e0tained FashionMNIST: L = 3, * = 700
24

22

20 Uniform Sampling

22

SVD CoreNet

24

CoreNet+ CoreNet++

Perc1e0ntage of2N0on-zero P3a0rameters4R0etained

FashionMNIST: L = 5, * = 700

25

Uniform Sampling SVD

23 CoreNet

21

CoreNet+ CoreNet++

21

23

Perc1e0ntage of N2o0n-zero Par3a0meters Re4ta0ined

Average Relative Error

Average Relative Error

Figure 5: Evaluations against the FashionMNIST dataset with varying number of hidden layers (L) and number of neurons per hidden layer ().

30

Under review as a conference paper at ICLR 2019

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

100

MNIST: L = 2, * = 200, Pyramid Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1ce0ntage of2N0on-zero P3a0rameters4R0etained

MNIST: L = 3, * = 200, Pyramid

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

Per1ce0ntage of2N0on-zero P3a0rameters40Retained

MNIST: L = 5, * = 200, Pyramid

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1c0entage of2N0on-zero 3P0arameter4s0Retained

MNIST: L = 3, * = 200 100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

MNIST: L = 5, * = 200

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1c0entage of20Non-zero3P0aramete4rs0Retained

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

MNIST: L = 2, * = 500, Pyramid Uniform Sampling
80 SVD CoreNet
60 CoreNet+ CoreNet++
40

20

0Perc1e0ntage of N20on-zero Pa3r0ameters R4e0tained

100

MNIST: L = 3, * = 500, Pyramid Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Perc1e0ntage of N20on-zero Pa3r0ameters R4e0tained

MNIST: L = 5, * = 500, Pyramid

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1ce0ntage of2N0on-zero P3a0rameters4R0etained

100

MNIST: L = 3, * = 500 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Perc1e0ntage of2N0on-zero P3a0rameters40Retained

100

MNIST: L = 5, * = 500 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

MNIST: L = 2, * = 1000, Pyramid

80

Uniform Sampling SVD

60

CoreNet CoreNet+

40 CoreNet++

20

0Perc1e0ntage of 2N0on-zero P3a0rameters 4R0etained MNIST: L = 3, * = 1000, Pyramid Uniform Sampling
80 SVD CoreNet
60 CoreNet+ CoreNet++
40

20

0Perc1e0ntage of N2o0n-zero Pa3r0ameters R4e0tained

100

MNIST: L = 5, * = 1000, Pyramid Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Perc1e0ntage of 2N0on-zero Pa3r0ameters R4e0tained MNIST: L = 3, * = 700 Uniform Sampling
80 SVD CoreNet
60 CoreNet+ CoreNet++
40

20

0Perc1e0ntage of2N0on-zero P3a0rameters40Retained

MNIST: L = 5, * = 700

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Perc1e0ntage of N2o0n-zero Para3m0 eters Ret4a0ined

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Figure 6: Evaluations against the MNIST dataset with varying number of hidden layers (L) and number of neurons per hidden layer (). Shaded region corresponds to values within one standard deviation of the mean. The figures show that our algorithm's relative performance increases as the number of layers (and hence the number of redundant parameters) increases.

31

Under review as a conference paper at ICLR 2019

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

CIFAR: L = 2, * = 200, Pyramid

30

20

Uniform Sampling SVD

10

CoreNet CoreNet+

CoreNet++

Per1c0entage2o0f Non-ze3r0o Param4e0ters Re5ta0ined CIFAR: L = 3, * = 200, Pyramid

30

20

Uniform Sampling SVD

10

CoreNet CoreNet+

CoreNet++

Per1c0entage2o0f Non-ze3r0o Param4e0ters Re5t0ained CIFAR: L = 5, * = 200, Pyramid

30

20

Uniform Sampling SVD

10

CoreNet CoreNet+

CoreNet++

Per1c0entage2o0f Non-ze3r0o Param4e0ters Re5t0ained CIFAR: L = 3, * = 200

30

20

Uniform Sampling SVD

10

CoreNet CoreNet+

CoreNet++

Per1c0entage2o0f Non-ze3r0o Param40eters Re5t0ained CIFAR: L = 5, * = 200

30

20 Uniform Sampling

SVD

10

CoreNet CoreNet+

CoreNet++

Per1c0entage2o0f Non-ze3r0o Param4e0ters Re5ta0ined

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

40 CIFAR: L = 2, * = 500, Pyramid

30

20

Uniform Sampling SVD

10

CoreNet CoreNet+

CoreNet++

Per1c0entage 2o0f Non-ze3r0o Param4e0ters Ret5a0ined 40 CIFAR: L = 3, * = 500, Pyramid

30

20

Uniform Sampling SVD

10

CoreNet CoreNet+

CoreNet++

0Per1c0entage 2of0Non-ze3ro0 Param4e0ters Ret5a0ined

CIFAR: L = 5, * = 500, Pyramid 40 Uniform Sampling
SVD 30 CoreNet
CoreNet+ 20 CoreNet++

10

Per1c0entage 2of0Non-ze3ro0 Param4e0ters Reta5i0ned

40

CIFAR: L = 3, * = 500 Uniform Sampling

30

SVD CoreNet

CoreNet+

20 CoreNet++

10

Per1c0entage 2o0f Non-ze3r0o Param4e0ters Ret5a0ined

40

CIFAR: L = 5, * = 500 Uniform Sampling

30

SVD CoreNet

CoreNet+

20 CoreNet++

10

0Per1c0entage o2f0Non-zero30Paramete4r0s Retain5e0d

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

40

CIFAR: L = 2, * = 1000, Pyramid Uniform Sampling

30

SVD CoreNet

CoreNet+

20 CoreNet++

10

Per1ce0ntage of2N0on-zero3P0aramete4rs0Retained

CIFAR: L = 3, * = 1000, Pyramid

40 Uniform Sampling

30

SVD CoreNet

CoreNet+

20 CoreNet++

10

0Per1ce0ntage of2N0on-zero3P0aramete4rs0Retained CIFAR: L = 5, * = 1000, Pyramid
40 Uniform Sampling SVD
30 CoreNet CoreNet+
20 CoreNet++

10

0Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

CIFAR: L = 3, * = 700

40 Uniform Sampling

30

SVD CoreNet

CoreNet+

20 CoreNet++

10

Per1c0entage o2f0Non-zer3o0Parame4te0rs Retai5n0ed

CIFAR: L = 5, * = 700

40 Uniform Sampling

30

SVD CoreNet

CoreNet+

20 CoreNet++

10

0Perc1e0ntage of2N0on-zero P3a0rameters4R0etained

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Figure 7: Evaluations against the CIFAR-10 dataset with varying number of hidden layers (L) and number of neurons per hidden layer (). The trend of our algorithm's improved relative performance as the number of parameters increases (previously depicted in Fig. 6) also holds for the CIFAR-10 data set.

32

Under review as a conference paper at ICLR 2019

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

100

FashionMNIST: L = 2,

* = 200, Pyramid Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

FashionMNIST: L = 3, * = 200, Pyramid 100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

Per1c0entage of2N0on-zero3P0arameter4s0Retained

FashionMNIST: L = 5, * = 200, Pyramid

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1c0entage of2N0on-zero 3P0arameter4s0Retained

100

FashionMNIST: L = 3, * = 200 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1c0entage of20Non-zero3P0aramete4rs0Retained

FashionMNIST: L = 5, * = 200

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1c0entage of2N0on-zero3P0arameter4s0Retained

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

FashionMNIST: L = 2, * = 500, Pyramid Uniform Sampling
80 SVD CoreNet
60 CoreNet+ CoreNet++
40

20

0Perc1e0ntage of 2N0on-zero P3a0rameters 4R0etained

100

FashionMNIST: L = 3,

* = 500, Pyramid Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Perc1e0ntage of 2N0on-zero Pa3r0ameters R4e0tained

FashionMNIST: L = 5, * = 500, Pyramid

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1ce0ntage of2N0on-zero P3a0rameters4R0etained

100

FashionMNIST: L = 3, * = 500 Uniform Sampling

80

SVD CoreNet

60 CoreNet+

CoreNet++

40

20

0Per1ce0ntage of2N0on-zero 3P0arameter4s0Retained

FashionMNIST: L = 5, * = 500

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Per1ce0ntage of2N0on-zero3P0arameter4s0Retained

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

FashionMNIST: L = 2, * = 1000, Pyramid

80

Uniform Sampling SVD

60

CoreNet CoreNet+

40 CoreNet++

20

0Perc1e0ntage of2N0on-zero P3a0rameters4R0etained FashionMNIST: L = 3, * = 1000, Pyramid Uniform Sampling
80 SVD CoreNet
60 CoreNet+ CoreNet++
40

20

0Perc1e0ntage of N20on-zero Pa3r0ameters R4e0tained

FashionMNIST: L = 5, * = 1000, Pyramid

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Perc1e0ntage of 2N0on-zero Pa3r0ameters R4e0tained

100

FashionMNIST: L = 3, * = 700 Uniform Sampling

80

SVD CoreNet

60 CoreNet+

CoreNet++

40

20

0Perc1e0ntage of2N0on-zero P3a0rameters4R0etained

FashionMNIST: L = 5, * = 700

100 Uniform Sampling

80

SVD CoreNet

60

CoreNet+ CoreNet++

40

20

0Perc1e0ntage of N2o0n-zero Par3a0meters Re4ta0ined

Average Drop in Accuracy [%]

Average Drop in Accuracy [%]

Figure 8: Evaluations against the FashionMNIST dataset with varying number of hidden layers (L) and number of neurons per hidden layer ().

33

