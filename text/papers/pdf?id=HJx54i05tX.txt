Under review as a conference paper at ICLR 2019
ON RANDOM DEEP AUTOENCODERS: EXACT ASYMPTOTIC ANALYSIS, PHASE TRANSITIONS, AND IMPLICATIONS TO TRAINING
Anonymous authors Paper under double-blind review
ABSTRACT
We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature. Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs "approximate inference" as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies. Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts. Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization. Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.
1 INTRODUCTION
The autoencoder is a cornerstone in machine learning, first as a response to the unsupervised learning problem (Rumelhart & Zipser (1985)), then with applications to dimensionality reduction (Hinton & Salakhutdinov (2006)), unsupervised pre-training (Erhan et al. (2010)), and also as a precursor to many modern generative models (Goodfellow et al. (2016)). Its reconstruction power is well utilized in applications such as anomaly detection (Chandola et al. (2009)) and image recovery (Mousavi et al. (2015)). With the surge of deep learning, thousands of papers have studied multilayer variants of this architecture, but theoretical understanding has been limited, since analyzing the learning dynamics of a highly nonlinear structure is typically a difficult problem even for the shallow autoencoder. To get around this, we tackle the task with a critical assumption: the weights are random and the autoencoder is weight-tied. One enjoys much analytical tractability from the randomness assumption, whereas weight tying enforces the random autoencoder to perform "autoencoding". We also study this in the high-dimensional setting, where all dimensions are comparably large and ideally jointly approaching infinity. We consider the simplest setting: vanilla autoencoders (i.e., ones with fully connected layers only) and their reconstruction capability. This is done for the sake of understanding the effect of depth, while we note our techniques may have broader applicability.
The aforementioned assumptions are not without justifications. There is a growing literature on deep neural networks with random weights, (Li & Saad (2018); Giryes et al. (2016); Poole et al. (2016); Schoenholz et al. (2016); Gabrié et al. (2018); Amari et al. (2018)) to name a few, revealing certain properties of deep feedforward networks1. Several recent works have also studied random multilayer feedforward networks through the lens of statistical inference (Manoel et al. (2017);
1It is worth mentioning that several characteristics of deep random feedforward networks were implicitly investigated a few decades ago, in the form of (continuous- or discrete-time) recurrent dynamics of shallow random neural networks (Sompolinsky et al. (1988); Cessac et al. (1994); Cessac (1995)) under the "local chaos" hypothesis (Amari (1972)).
1

Under review as a conference paper at ICLR 2019
Reeves (2017); Fletcher et al. (2018)). The idea of weight tying is considered in the important paper Vincent et al. (2010) with an empirical finding that autoencoders with and without weight tying perform comparably. Similar features of random connection and symmetry also appear in other neural models (Lillicrap et al. (2016); Scellier et al. (2018)). Finally the high-dimensional setting is common in recent statistical learning advances (Bühlmann & Van De Geer (2011)), and not too far from the actual practice where many large datasets have dimensions of at least a few hundreds and are harnessed by large-scaled models.
We seek quantitative answers to three specific questions that are motivated by previous works:
· In exactly what way does the (vanilla) random weight-tied autoencoder perform "approximate inference"? This term is coined in Scellier et al. (2018) in connection with the theoretical results in Arora et al. (2015), which implicitly studies the said model. In particular, Arora et al. (2015) proves an upper bound on x^ - x 2, where x and x^ are the input and the output of the network, but is limited in the number of layers and specific to the ReLU activation. This direction has been recently extended by Gilbert et al. (2017). In our work, we establish precisely what this approximate inference is by obtaining a general and asymptotically exact characterization2 of x^, for any number of layers and any Lipschitz continuous activations (Theorem 1 and Section 3.3). Theorem 1 is the key theoretical result of our work and lays the foundation for all analyses that follow.
· In what way is the deep autoencoder different from the shallow counterpart? Li & Saad (2018); Poole et al. (2016) reveal this in terms of the candidate function space and expressivity for feedforward networks. It is unclear how these notions are applicable to weighttied autoencoders, which seek replication of the input rather than a generic mapping. In this work, we show that the deep autoencoder exhibits a higher order of sensitivity to perturbations of the parameters (Section 3.4).
· Does it have any implications to the training practice? Many recent works3 Glorot & Bengio (2010); He et al. (2015); Schoenholz et al. (2016); Pennington et al. (2017); Yang & Schoenholz (2017); Xiao et al. (2018); Chen et al. (2018); Hayou et al. (2018); Hanin & Rolnick (2018); Hanin (2018); Burkholz & Dubatovka (2018) demonstrate a connection between the study of random networks, or ones at initialization, and their trainability. Note that these works either do not study weight-tied structures, or assume the analysis of the untying case for weight-tied structures. In our work, we derive and experimentally verify insights on how (not) to initialize deep weight-tied autoencoders, demonstrating that it is possible to train them without resorting to techniques such as greedy layer-wise pretraining, drop-out and batch normalization (Section 3.5). Specifically we experiment with 200-layer autoencoders.
No prior works have attempted all three tasks. The quantitative difference between weight-tied and weight-untied networks is in fact not negligible, yet the analysis is non-trivial due to the weight tying constraint (Arora et al. (2015); Chen et al. (2018)). To address this issue and obtain Theorem 1, we apply the Gaussian conditioning technique, which first appears in the studies of TAP equations in spin glass theory (Bolthausen (2014)) and is extensively used in the approximate message passing algorithm literature (Bayati & Montanari (2011); Javanmard & Montanari (2013); Berthier et al. (2017)). This should be contrasted with untied random networks, whose analysis is typically more straightforwards. More importantly, the difference is not only analytical: the overall picture of deep random weight-tied autoencoders is rich and drastically different from that of feedforward networks. An analysis in the limit of infinite depth reveals three fundamental equations governing the picture (Section 3.1), which displays multiple phase transition phenomena (Section 3.2).
Finally let us quickly mention other recent theoretical works on autoencoders: Arora et al. (2014); Arpit et al. (2015); Rangamani et al. (2017); Nguyen et al. (2018) studying the learned autoencoders in specific settings, Baldi (2012); Alain & Bengio (2014); Bengio et al. (2013) taking a framework where the encoder and the decoder are generic mappings, and Le Roux & Bengio (2008); Sutskever
2Roughly speaking, we prove that x^ = c1x + c2z, for c1 and c2 scalars and z an independent Gaussian vector. It is then straightforwards to deduce x^ - x 2. This quantity is hence meaningful when c1 = 1.
3Many of these works refer to the random network framework as the mean-field analysis. Here we avoid using the term "mean-field" since it is also used to refer to the analysis of the learned infinitely-wide neural networks (Mei et al. (2018)), which is a different setting.
2

Under review as a conference paper at ICLR 2019

& Hinton (2008); Montufar & Ay (2011) exploring representational properties of related architectures. These works head in different directions from ours.

2 SETTING AND MAIN THEOREM

2.1 SETTING AND ASSUMPTIONS

Consider the following 2L-layers autoencoder with weight tying:

x^ = 0 W 1 1 ...L-1 W L L (W LL-1 (...1 (W 10 (x) + b1) + ...) + bL) + vL + ... + v1 .

Here x  Rn0 is the input, W  Rn ×n -1 is the weight, b  Rn is the encoder bias, and v  Rn -1 is the decoder bias, for = 1, ..., L. Also  : R  R and  : R  R are the activations (where for a vector u  Rn and a function  : R  R, we write  (u) to denote the
vector ( (u1) , ...,  (un)) ). It is usually the case in practice that 0 (u) = u the identity function. We introduce some convenient quantities inductively:

x0 = x, x = W  -1 (x -1) + b , = 1, ..., L,
x^L = W L L (xL) + vL, x^ = W  (x^ +1) + v , = L - 1, ..., 1.
Note that x^ = 0 (x^1). We assume weights are random. Specifically we generate the weights and biases according to

(W )ij  N

0, W2 , n -1

i.i.d., (b )i  N 0, b2,

i.i.d., (v )i  N 0, v2,

i.i.d.

independently of each other. The scaling of the variances accords with the literature and actual practice (Glorot & Bengio (2010); Vincent et al. (2010)). We also consider the asymptotic highdimensional regime, indexed by n:

n = n (n) , n   > 0 and n   as n  ,  . n -1

Here W, , b, , v, and  are finite constants independent of n. We enforce W, > 0, but allow b, and v, to be zero. We assume that all activations are Lipschitz continuous, and the

encoder activations  's are non-trivial in the sense that for any  > 0, Ez  ( z)2 > 0 where

z



N

(0, 1).

We also assume that

1 n0

0 (x) 2 tends to a finite and strictly positive constant as

n  . We refer to Appendix A for more clarifications of notations.

2.2 MAIN THEOREM

In order to state the result, we need to define some scalar sequences. First we define { } =1,...,L and {¯ } =0,...,Linductively:

¯02

=

1 n0

0 (x) 2 ,

12 = W2 ,1¯02,

¯2 =  2 + b2, , = 1, ..., L,  2 = W2 , Ez  -1 (¯ -1z)2 ,

= 2, ..., L,

for z  N (0, 1). Next, we define { ,  } =2,...,L+1 inductively:

1 L+1 = ¯L2 Ez1 {¯Lz1L (¯Lz1)} ,

L+1 = Ez1 L (¯Lz1)2 ,

1  = ¯2-1 Ez1,z2

¯ -1z1 -1

 W2 ,  +1 -1 (¯ -1z1) +

 W2 ,  +1 + v2, z2

,

2

 = Ez1,z2  -1  W2 ,  +1 -1 (¯ -1z1) +  W2 ,  +1 + v2, z2

,

= L - 1, ..., 2,

for z1, z2  N (0, 1) independently. With these sequences defined, we have the following theorem.

3

Under review as a conference paper at ICLR 2019

Theorem 1. Consider the settings and assumptions as in Section 2.1, and the sequences { , ¯ } and { ,  } defined as above. Then in the limit n  4:
(a) {¯ } describes the behavior of the encoder output x : x = ¯ z, = 1, ..., L,
for z  N (0, In ).
(b) {¯ ,  ,  } describes the behavior of the decoder output x^ :
x^ =  W2 ,  +1 -1 (¯ -1z1) +  W2 ,  +1 + v2, z2, = 2, ..., L,
for z1, z2  N 0, In -1 independently. One can replace ¯ -1z1 with x -1 in the above, with z2 independent of x -1, in which case the statement also holds for = 1 with x0 = x.
(c) For the autoencoder's output x^,
x^ = 0 1W2 ,120 (x) + 1W2 ,12 + v2,1z2 ,
for z2  N (0, In0 ) independent of x.
The proof of the theorem, as well as an outline of the key ideas, are in Appendix A. The theorem says that x , x^ and x^ admit simple descriptions which are tracked by scalar sequences {¯ ,  ,  }. Hence we can learn about the autoencoder by analyzing {¯ ,  ,  }, which is generally a simpler task than studying x , x^ and x^ directly. Numerical simulations in Appendix B suggest that, although the theorem's statement is in the infinite dimension limit, the agreement is already good for dimensions of a few hundreds. We note that while the theorem assumes Gaussian biases, the same proof technique allows to obtain a similar result with a more relaxed condition on the biases. Remark 2. While the theorem is specific to W following the Gaussian distribution, simulations in Appendix B suggest that the conclusion holds for a much broader class of distributions. We conjecture that it should hold so long as each W has i.i.d. entries and is independent of each other, its distribution has bounded k-th moment for some sufficiently large k, and the activations as well as the input x satisfy certain mild regularity conditions. Remark 3. We comment on the range of  and  . We have   0, which is obvious, and if  -1   C, then   C2. By Stein's lemma (cf. Appendix E.2), L+1 = Ez {L (¯Lz1)} ,
 =  W2 ,  +1Ez1,z2  -1  W2 ,  +1 -1 (¯ -1z1) +  W2 ,  +1 + v2, z2  -1 (¯ -1z1) .
If the activations are non-decreasing, then   0. Furthermore, if the activations are Lipschitz, then | |  Cc for some constants C and c.

3 AN ANALYSIS AT INFINITE DEPTH

In the following, we adopt a semi-rigorous approach, with an emphasis on the overall picture.

3.1 INFINITE DEPTH SIMPLIFICATION

We make several analytical simplifications. First consider  =  > 0, W2 , = W2 > 0, b2, =

b2  shall

0,  = see that

 and  =  all independent of , the specific choice of L is largely

except for L which is chosen immaterial). We also assume

separately (but we that v2, = 0, and

0 and 0 are the identity5. We introduce a parameter ¯  0, whose role will be clear shortly, and

4This is conveyed through the relational operator =; see Appendix A for its definition.

5The assumption v2, = 0 aligns well with several recent theoretical analyses of the autoencoders (Arpit et al. (2015); Rangamani et al. (2017); Gilbert et al. (2017); Nguyen et al. (2018)) which disregard the decoder's

biases. In addition, it is a common practice to initialize an untrained neural network (hence a random one) with

zero biases (Goodfellow et al. (2016)). This assumption also helps making the analysis more tractable. Other

assumptions seem restrictive, but can be relaxed without affecting the analysis that follows, and hence are made

for ease of presentation. For example, the assumption  =  for all may be relaxed to  =  for most

such that 1

L.

4

Under review as a conference paper at ICLR 2019

which satisfies: ¯2 = T ¯2  T ¯2; W2 , b2,  ,

T ¯2 = W2 E  (¯z)2 + b2,

(1)

for z  N (0, 1). Note that this implies W2  W2 ,max = ¯2/E  (¯z)2 . If this cannot be
satisfied, we set ¯2 = +. In addition, let  = W2 > 0. With these, let us consider the following two fixed point equations:

 = G (, )  G , ; , ¯2, ,  ,  = R (, )  R , ; , ¯2, ,  ,

1 G (, ) = ¯2 E ¯z1  (¯z1) + z2
2
R (, ) = E   (¯z1) + z2 ,

, (2) (3)

for z1, z2  N (0, 1) independently. Then Eq. (1), (2) and (3) together form the fundamental equations for deep random weight-tied autoencoders, in either one of the following senses:

Interpretation 1. For 1

L, in the limit L   (and   at a pace sufficiently slow

compared to L), we have ¯  ¯,    and   , where ¯ is a stable fixed point solution to ¯2 = T ¯2 , and (, ) is then jointly a stable fixed point solution to  = G (, ) and  = R (, ).

In light of Theorem 1, (¯, , ) describes the behavior of an intermediate x^ :

x^ = Ssig (x -1) + Svarz, Ssig = , Svar = ,
where z  N 0, In -1 independent of x -1, and x -1 = ¯z for z  N 0, In -1 . If the convergences ¯  ¯,    and    are fast, then the majority of the intermediate layers are well approximately described by the above, in the regime of large L.

Interpretation 2.

Suppose that for ¯0 =

x

 / n0,

we

impose

the

constraint

¯02

=

E

 (¯z)2 .

This should be interpreted as follows: starting with a chosen ¯, we normalize the input data x

according to ¯02 = E  (¯z)2 ; then we choose W2  W2 ,max and b2 according to Eq. (1). Under
this constraint, it is easy to see that ¯ = ¯ for all  1, and hence the norm of the input to each of the encoder's hidden layers is preserved by Claim (a) of Theorem 1. We then have that as L  , at small  2 (i.e., at the layers near the two ends of the autoencoder),    and   , where (, ) is jointly a stable fixed point of  = G (, ) and  = R (, ). The autoencoder's output is

then, in this limit,

x^ = Ssigx + Svarz, Ssig = , Svar = ,

for z  N (0, In0 ) independent of x.

In either cases, we see that x^ or x^ is a composition of a signal component and a Gaussian variation
component. Their respective strengths Ssig and Svar admit simple expressions, thanks to the infiniteL simplification6. We note that G and R do not take b2 as a parameter. We refer to Appendix C.1 for the computation of  and . Fig. 1 shows that our asymptotic simplification is quite accurate
already for L on the order of a few tens.

We also remark that the equation ¯2 = T ¯2 is known in the signal propagation analysis of random
feedforward networks (Poole et al. (2016)), but the equations  = G (, ) and  = R (, ) are new. We also observe that in these equations, there is the presence of  (through ), which is typically missing from such analyses. Hence unlike untied structures, one may expect to see architectural constraints in analyses of weight-tied structures.

3.2 PHASE TRANSITION OF THE FIXED POINT
With the infinite depth simplification, one question is on the existence of the solutions to Eq. (2) and (3), and how these fixed points look like. (Eq. (1) is well-studied, cf. Poole et al. (2016); Hayou et al. (2018), and we will not analyze it here.) We note that  = 0 is always a solution to Eq. (2), in
6Recall we consider the limit of n   before taking L  . Without a proof, we expect that our results hold for L = o (log n), so practically this means 1 L  (log n).

5

Under review as a conference paper at ICLR 2019

100 100 100

10-5

10-5

10-1

10-10

10-10

10-2

10-15

10-15

10-3

10 20 40 60 80 100

10 20 40 60 80 100

10 20 40 60 80 100

Figure 1: The gaps |2 - | and |2 - | versus the depth L, where 2 and 2 (which are dependent on L) are as in Section 2.2, and  and  (the infinite-L limits of 2 and 2) are from Eq. (2) and (3). Here all activations are tanh, ¯2 = 1.2, b2 = 0.211, W2 = 2.312 < W2 ,max  2.806, and ¯02  0.4276, which satisfies ¯02 = E  (¯z)2 . From left to right:  = 0.9,  = 1.0 and  = 1.5.
The gaps decrease exponentially with the depth L.

G(, ) G(, ) G(, ) G(, )

which case Eq. (3) also has a solution, for instance, when  (0) = 0 such as ReLU or tanh (which admits  = 0). However  = 0 is trivial, since it implies Ssig is zero. We will be interested in the existence of non-trivial and stable fixed points. To ease visualization, for the moment, let us consider Eq. (2) only. Fig. 2 shows   G (, ) for different , ,  and  for ¯2 = 1. For a given  and , depending on  and , one may observe one or more fixed points, one of which is at  = 0 and can be stable or unstable. When  = 0 is the only fixed point but is unstable, we have  =  as the "stable solution" to Eq. (2). The solution landscape changes drastically with ; for instance, when  =  = tanh,  = 0 is the only and stable fixed point when  is small, but it becomes unstable and a new fixed point at  > 0 emerges when  is sufficiently large. This hints at certain phase transition behaviors as  varies.
2.5 2 1 1
2 1.5 1.5
1 0.5 0.5 1 0.5 0.5
0000 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 
Figure 2: The mapping   G (, ) for ¯2 = 1 and  = 5 (blue),  = 2.7 (red),  = 0.8 (green). The color intensity varies with   [0.1, 1] with equal spacings, where the darkest curve corresponds to  = 0.1, and the lightest is  = 1. From left to right: ,  are ReLU;  is ReLU,  is tanh; ,  are tanh;  is tanh,  is ReLU. A fixed point is an intersection between this mapping and the identity line (black dashed).
In Appendix C.2, we perform a detailed analysis of Eq. (2) and (3), supported by several rigorously proven properties. In the following, by an initialization for  and , we mean L+1 and L+1 as in Section 2.2, and by convergence to  and , we mean the convergences as in Section 3.1. We highlight some results from the analysis for specific pairs of  and :
ReLU  and . We have two phase transitions at  = 2 and at  = 4. When  < 2, with any initialization, we have convergence to  = 0 and  = 0. When 2 <  < 4, we have, with certain initializations, convergence to  = 0 and divergence to  = +, and with certain other initializations, divergence to  = + and  = +. These include almost all possible initializations. When   4, with any non-zero initialization, we have divergence to  = + and  = +.
ReLU  and tanh . We have two phase transitions at  = 2 and  = 0 (¯)  (2, ). When  < 2 , with any initialization, we have convergence to  = 0 and  = 0. When 2 <  < 0, with any non-zero initialization, we have convergence to  = 0 and divergence to  = +. When  > 0, with any non-zero initialization, we have divergence to  = + and hence  = +.
6

Under review as a conference paper at ICLR 2019

tanh  and . We have two phase transitions at  = 1 and  = 0 (¯) > 1. When   1,
we have convergence to  =  = 0. When 1 <  < 0, with any non-zero initialization, we have convergence to  = 0 and   (0, 1). When  > 0, with any non-zero initialization, we have convergence to  > 0 and   (0, 1). For ¯ > 0,  cannot grow to + as  varies. We note that 0  1 if ¯2  0, and in the case  = 1, this implies W2  1. With respect to Eq. (1), we then have b2  0. An illustration is given in Fig. 3.

4 
3 /
2
1

0.6  
0.4 /
0.2

0.6  
0.4 /
0.2

0 0 0.5 1 1.5 2


0 01234


0 012345


Figure 3:  and  versus , as solved with Eq. (2) and (3). The vertical dotted line is  = W2 ,max.
From left to right: (1)  =  = tanh and ¯2 = 0.0259, (2)  =  = tanh and ¯2 = 1.2, and (3)  = tanh,  is the ReLU, and ¯2 = 0.2.

tanh  and ReLU . We have a picture similar to the case  =  = tanh, with a crucial difference that one cannot have 0 be close to 1. An illustration is given in Fig. 3.
 and  thus exhibit phase transitions, depend crucially on the choice of activations (especially the decoder activation ), and can be trivialized (i.e., being zero or infinity) as in the case of ReLU  and . It is remarkable that the above pictures are general for many other activations, as suggested by our analysis. In the next sections, we explore the implications of these behaviors.
3.3 APPROXIMATE INFERENCE AT INFINITE DEPTH
Theorem 1 gives a quantitatively exact sense of how the random weight-tied autoencoder performs "approximate inference". Here we will be interested in stronger notions. A first question is: does it explain reversibility? Reversibility, as mathematically formalized in previous works Arora et al. (2015); Gilbert et al. (2017), quantitatively concerns with how small the quantity E = x - x^ 2 /n0 is. The smaller it is, the better the decoder "reverses" the encoder. This formalized notion is an attempt to give a theoretical understanding of empirical findings that the input could be reproduced from the values of hidden layers of a trained feedforward network. Let us now consider the infinite depth simplification under Interpretation 2 of Section 3.1. We have E (Ssig - 1)2 ¯02 + Sv2ar. As such, for E  0 with high probability, one must have Ssig  1 and Svar  0, hence   0. Consequently, E  ( (¯z1))2  0. For ¯ > 0, E  ( (¯z1))2 = 0 is impossible for any
non-trivial activations (unless the activation outputs zero almost everywhere). Strikingly, in light of Section 3.2, when  and  are both ReLU, we have that Ssig and Svar are either 0 or +, in which case E  ¯02 and can become unbounded. While this does not contradict the results in Arora et al. (2015) (which also concerns with ReLU activations, but with specific choices of the biases and limited depth, and hence is in a different setting), our discussion suggests that random weight-tied models may be insufficient to explain reversibility.
A second question is: does the model perform signal recovery? In this case, we are interested in whether x^ = cx for some constant c not necessarily 1. Similar to the above, this requires Svar = 0, hence  = 0, and E  ( (¯z1))2 = 0. For non-trivial  and , this requires  (0) = 0 and  = 0. Many activations do not conform with the former, and the latter implies x^ = 0 undesirably. This provides a negative answer to the question.
A critic may argue that in expectation, E {x^}  Ssigx, and as per Section 3.2, there are cases where  > 0 and hence Ssig > 0. Yet in fact, this in-expectation property can already be observed in the simple setting of linear shallow autoencoders (Arora et al. (2015)). What is ignored in such argument is that in many cases, Svar > 0 whenever Ssig > 0, in light Section 3.2. Our analysis hence mitigates
7

Under review as a conference paper at ICLR 2019

the shortcoming of the in-expectation approach, and gives a more precise understanding of what the random weight-tied autoencoder can and cannot achieve when the depth becomes large.

3.4 COMPARISON WITH THE SHALLOW CASE

Our result also allows for the case of L = 1 a shallow autoencoder. In particular, taking a parallel setting with Section 3.1 (in particular, Interpretation 2), by Theorem 1,

x^ = Ssigx + Svarz, Ssig = , Svar = ,

in which, with hid being the activation in the hidden layer,



=

1 ¯2

E

{¯zhid

(¯z)} ,

 = E hid (¯z)2 .

Some observations follow. In the shallow case,  > 0 and  > 0 and both are bounded regardless of the parameters, except for trivial edge cases such as ¯2 = 0 or hid (·) = 0. Furthermore,  and 
are independent of , for a fixed ¯. As such, thereis no phase transition in  and  as  changes. We also have Ssig () =  () and Svar () =   , and hence, the signal component dominates with Ssig/Svar =   . Again this happens regardless of parameter choices.

In comparison with ReLU, as observed

the infinite depth case, from Fig. 3, in certain

for  =  regimes, ,

=antdanh/or

 = tanh and  being the can grow (sublinearly) with

, and hence Ssig () =  (), Svar () =   , and the signal component dominates with Ssig/Svar =   . In particular, near the phase transition of , Ssig/Svar =  1.5 . Recalling

that  = W2 , this implies for the infinite depth case, as compared to the shallow one, firstly a slight perturbation in W2 may result in a larger perturbation in the signal's strength, and secondly an

architecture using larger  may gain more in terms of amplification of the signals. In short, the deep

autoencoder is more sensitive to slight changes in the parameters. As evident in Section 3.2, the case

 being the ReLU also exhibits extreme sensitivity, in that it is possible for a slight perturbation in

 to drastically change  and . As suggested by Fig. 1, it should be the case already for L about

a few tens. It is, however, at the expense of much care in the selection of parameters, since there

are continuous regimes in which the infinite depth diminishes Ssig and Svar to zero or boost them to infinity, a situation that never occurs in the shallow case.

Remark 4. Sensitivity to perturbations is implied by expressivity, a notion put forth in Poole et al.

(2016) in the study of random feedforward networks. Hence we expect that sensitivity is a common

feature of various types of deep neural networks.

3.5 IMPLICATIONS TO TRAINING INITIALIZATION
We examine the implications of Interpretation 1 in Section 3.1 to trainability of the weight-tied autoencoder. Since the majority of intermediate layers can be described approximately by  and  (as well as ¯) and the random weight-tied autoencoder is in fact one at initialization, appropriate values of ,  and ¯ (by a suitable choice of W2 and b2) should lead to better trainability. In particular, if one of them is , we expect numerical errors or too large values resulting in quick saturation, both of which render the autoencoder untrainable. If  =  = 0 in a neighborhood of the chosen W2 and b2, we expect that the progress is slowed down in the beginning. If such situations are avoided, the autoencoder is expected to show a faster progress. As a special remark on the case  is the ReLU, as per Section 3.2, when  = 1, our hypothesis suggests taking W2 = 2 and b2 = 0. This coincides with the celebrated He initialization (He et al. (2015)), which however considers feedforward networks only. Interestingly this does require  to be the ReLU; for instance,  can be tanh, in which case the argument in (He et al. (2015)) is not applicable.
Table 1 lists several initialization schemes, for  = 1. We also mark the schemes that are edge of chaos (EOC) initializations (Schoenholz et al. (2016); Pennington et al. (2017)), which we review in Appendix E.1. The EOC initialization enables better signal propagation in deep feedforward networks, and in our context, is relevant to the encoder part with the activation . We perform simple experiments to verify our expectations on a weight-tied vanilla autoencoder as described in Section 3.1: L = 100, all hidden dimensions of 400, identity input activation 0, and decoder biases initialized to zero. This sets  =  = 1 for  2; here 1 = 1 is irrelevant in light of Interpretation 1. We train the autoencoder on the MNIST dataset with mini-batch gradient descent with a batch

8

Under review as a conference paper at ICLR 2019

No. 

 W2

b2

¯2 EOC Trainable Slowed Inf

1 ReLU ReLU 2.0

0.0

-x

x

2 ReLU ReLU 1.0

0.1

0.2

x

3 ReLU ReLU 2.5

0.0



x

4 ReLU tanh 2.0

0.0 0.618

x

5 ReLU tanh 1.05 2.01 × 10-5 0.0259 xx

x

6 ReLU tanh 2.505

0.3

1.460

x

7 tanh tanh 1.05 2.01 × 10-5 0.0259 xx

x

8 tanh tanh 0.5

0.0136 0.0259

x

9 tanh tanh 2.312

0.211

1.2 x

x

10 tanh tanh 0.5

0.986

1.2

x

11 tanh tanh 1.0

0.771

1.2

x

12 tanh ReLU 2.0

0.0

-x

x

13 tanh ReLU 1.0

0.1

0.2

x

14 tanh ReLU 0.5

0.15

0.2

x

Table 1: List of initialization schemes for each pair of  and , for  = 1. Here "-" indicates a positive finite value that depends on the choice of L (for which we choose the ReLU), but its exact value is irrelevant for our purpose. "EOC" indicates whether the scheme is an EOC initialization with respect to , and "xx" indicates an EOC scheme that is found to be the better one among all EOC initializations with Gaussian weights (Pennington et al. (2017)). "Trainable" indicates better trainability in the beginning as predicted by our theory. "Slowed" indicates  =  = 0 in a neighborhood. "Inf" indicates either    or   . The schemes with  = tanh should be reflected against Fig. 3.

size of 250 and without regularizations, for 5 × 105 iterations (equivalent to 2500 epochs). We perform the experiments in two settings:

· Setting 1: The output activation 0 is tanh, MNIST images are normalized to [-1, +1], and the learning rate is fixed at 5 × 10-3. This is standard for MNIST.
· Setting 2: 0 is the identity, MNIST images are unnormalized (i.e., normalized to [0, +1]), and the learning rate is fixed at 3 × 10-3. This is common for regression.

These learning rates are chosen so that the learning dynamics is typically smooth, in light of recent

works Mei et al. (2018); Smith & Le (2018). We use the normalized

2 2

loss

x^ - x 2 /

x 2, and

are primarily interested in this loss as a quality measure, since we only focus on trainability7. We

also do not apply techniques such as greedy layer-wise pre-training, drop-out or batch normalization.

The results are plotted in Fig. 4. See also Appendix D.1 for visualization of the reconstructions, and Appendix D.2 for the evolution over a broader range of parameters. Note that we plot the evolution in the logarithmic scale of time, since it is typically smooth and revealing on this scale, as found in prior works Baity-Jesi et al. (2018); Mei et al. (2018) and also evident from the plots. The results are in good agreement with our prediction. Note that as predicted, in Setting 2, Scheme 3 and 6 are trapped with numerical errors, and in Setting 1, they saturate quickly at a high loss. As such, we do not include the results of Scheme 3 and 6 in Fig. 4.

We see from the figure that Scheme 2, 5, 8, 10 and 14 show much slower progresses, by a factor of 3 to 10 times in terms of training iterations to reach the same loss. Hence a good amount of training time can be saved by an appropriate initialization. Interestingly Scheme 5 is in fact a special EOC initialization that Pennington et al. (2017) found to be the better one among all EOC schemes with Gaussian weights for tanh activation. This last observation shows that having good signal propagation through the encoder is far from being a sufficient condition for trainability.

7The chosen loss is slightly different from the traditional

2 2

loss

x^ - x 2. On one hand, we found from

our experiments that these two losses perform comparably, with the normalized loss typically yielding slight

improvements, provided that the learning rates are scaled appropriately. On the other hand, the normalized loss

allows ease for interpretation.

9

Under review as a conference paper at ICLR 2019

Test loss Test loss

1.2 1
0.8 0.6 0.4 0.2
0 100

1 2 4 5 7 8 9 10 11 12 13 14

Scheme 11, 13
Scheme 2, 5, 8, 10, 14

102 104 Iteration

106

1.2 1
0.8 0.6 0.4 0.2
0 100

1 2 4 5 7 8 9 10 11 12 13 14

Scheme 11, 13 Scheme 2, 5, 8, 10, 14

102 104 Iteration

106

Figure 4: Test loss x^ - x 2 / x 2 of the schemes from Table 1. Left: the setting with 0 = tanh (Setting 1). Right: the setting where 0 is the identity (Setting 2).

Among the schemes, only Scheme 1 and 4 in Setting 1 and only Scheme 1, 4 and 7 in Setting 2 have their eventual trained networks produce meaningful reconstructions, whereas the rest always output some "average" of the training set regardless of the input, at the end of 5 × 105 iterations (see Appendix D.1). It is unclear whether this is a bad local minimum, or whether these schemes take much longer to show further progresses. An explanation is beyond our current theory, and it is an open question how to create a scheme with meaningful trainability. Remarkably all the schemes that show slower initial progresses (Scheme 2, 5, 8, 10 and 14) are among those that could not yield meaningful reconstructions.
We observe that in Setting 2, the tanh network under Scheme 7 is best performing in terms of the reconstruction loss, and its progress does not seem to reach a plateau after 5 × 105 iterations. In both settings, Scheme 4, which is a hybrid of ReLU and tanh activations, shows slight improvements over Scheme 1, which is a purely ReLU network. This extends the conclusion in Pennington et al. (2017) to the context of weight-tied autoencoders: reasonable training at a large depth is possible even for the notoriously difficult tanh activation, and this necessarily requires careful initializations.
4 DISCUSSION
This paper has shown quantitative answers to the three questions posed in Section 1. This feat is enabled by an exact analysis via Theorem 1. The theorem is stated in a general setting, allowing varying activations, weight variances, etc, but our analyses in Section 3 have made several simplifications. This leaves a question of whether these simplifications can be relaxed, and how the picture changes accordingly, for instance, when the parameters vary across layers, similar to Yang & Schoenholz (2018). Many other questions also remain. For example, what would be the covariance structure between the outputs of two distinct inputs? How does the network's Jacobian matrix look like? These questions have been answered in the feedforward case (Poole et al. (2016); Pennington et al. (2017)), but we believe answering them is more technically involved in our case. We have also seen that an autoencoder that shows initial progress may not necessarily produce meaningful reconstruction eventually after training, and hence much more work is needed to understand the training dynamics far beyond initialization. Recent works Mei et al. (2018); Rotskoff & VandenEijnden (2018); Sirignano & Spiliopoulos (2018); Chizat & Bach (2018) have made progresses in this direction for shallow networks.
REFERENCES
Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating distribution. Journal of Machine Learning Research, 15(1):3563­3593, 2014.
Shun-Ichi Amari. Characteristics of random nets of analog neuron-like elements. IEEE Transactions on systems, man, and cybernetics, (5):643­657, 1972.
10

Under review as a conference paper at ICLR 2019
Shun-ichi Amari, Ryo Karakida, and Masafumi Oizumi. Statistical neurodynamics of deep networks: Geometry of signal spaces. arXiv preprint arXiv:1808.07169, 2018.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pp. 584­592, 2014.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: A simple theory, with implications for training. arXiv preprint arXiv:1511.05653, 2015.
Devansh Arpit, Yingbo Zhou, Hung Ngo, and Venu Govindaraju. Why regularized auto-encoders learn sparse representation? arXiv preprint arXiv:1505.05561, 2015.
Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard Ben Arous, Chiara Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing dynamics: Deep neural networks versus glassy systems. arXiv preprint arXiv:1803.06969, 2018.
Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, pp. 37­49, 2012.
Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764­785, 2011.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems, pp. 899­907, 2013.
Raphael Berthier, Andrea Montanari, and Phan-Minh Nguyen. State evolution for approximate message passing with non-separable functions. arXiv preprint arXiv:1708.03950, 2017.
Erwin Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington­ Kirkpatrick model. Communications in Mathematical Physics, 325(1):333­366, 2014.
Peter Bühlmann and Sara Van De Geer. Statistics for high-dimensional data: methods, theory and applications. Springer Science &amp; Business Media, 2011.
Rebekka Burkholz and Alina Dubatovka. Exact information propagation through fully-connected feed forward neural networks. arXiv preprint arXiv:1806.06362, 2018.
Bruno Cessac. Increase in complexity in random neural networks. Journal de Physique I, 5(3): 409­432, 1995.
Bruno Cessac, Bernard Doyon, Mathias Quoy, and Manuel Samuelides. Mean-field equations, bifurcation map and route to chaos in discrete time neural networks. Physica D: Nonlinear Phenomena, 74(1-2):24­44, 1994.
Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):15, 2009.
Minmin Chen, Jeffrey Pennington, and Samuel S Schoenholz. Dynamical isometry and a mean field theory of rnns: Gating enables signal propagation in recurrent neural networks. arXiv preprint arXiv:1806.05394, 2018.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. Emnist: an extension of mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625­660, 2010.
Alyson K Fletcher, Sundeep Rangan, and Philip Schniter. Inference in deep networks in high dimensions. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 1884­1888. IEEE, 2018.
11

Under review as a conference paper at ICLR 2019
Marylou Gabrié, Andre Manoel, Clément Luneau, Jean Barbier, Nicolas Macris, Florent Krzakala, and Lenka Zdeborová. Entropy and mutual information in models of deep neural networks. arXiv preprint arXiv:1805.09785, 2018.
Anna C Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, and Honglak Lee. Towards understanding the invertibility of convolutional neural networks. arXiv preprint arXiv:1705.08664, 2017.
Raja Giryes, Guillermo Sapiro, and Alexander M Bronstein. Deep neural networks with random gaussian weights: a universal classification strategy? IEEE Trans. Signal Processing, 64(13): 3444­3457, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge, 2016.
Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? arXiv preprint arXiv:1801.03744, 2018.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. arXiv preprint arXiv:1803.01719, 2018.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the selection of initialization and activation function for deep neural networks. arXiv preprint arXiv:1805.08266, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504­507, 2006.
Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115­144, 2013.
Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann machines and deep belief networks. Neural computation, 20(6):1631­1649, 2008.
Bo Li and David Saad. Exploring the function space of deep-learning machines. Physical Review Letters, 120(24):248301, 2018.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature communications, 7: 13276, 2016.
Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural networks. Ann. Appl. Probab., 28(2):1190­1248, 04 2018.
Andre Manoel, Florent Krzakala, Marc Mézard, and Lenka Zdeborová. Multi-layer generalized linear estimation. In 2017 IEEE International Symposium on Information Theory (ISIT), pp. 2098­2102. IEEE, 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 07 2018.
Guido Montufar and Nihat Ay. Refinements of universal approximation results for deep belief networks and restricted boltzmann machines. Neural Computation, 23(5):1306­1319, 2011.
Ali Mousavi, Ankit B Patel, and Richard G Baraniuk. A deep learning approach to structured signal recovery. In 2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 1336­1343. IEEE, 2015.
12

Under review as a conference paper at ICLR 2019
Thanh V Nguyen, Raymond KW Wong, and Chinmay Hegde. Autoencoders learn generative linear models. arXiv preprint arXiv:1806.00572, 2018.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In Advances in Neural Information Processing Systems, pp. 2637­2646, 2017.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pp. 4785­4795, 2017.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances in neural information processing systems, pp. 3360­3368, 2016.
Akshay Rangamani, Anirbit Mukherjee, Ashish Arora, Tejaswini Ganapathy, Amitabh Basu, Sang Chin, and Trac D Tran. Sparse coding and autoencoders. arXiv preprint arXiv:1708.03735, 2017.
Galen Reeves. Additivity of information in multilayer networks via additive gaussian noise transforms. In 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 1064­1070. IEEE, 2017.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. arXiv preprint arXiv:1805.00915, 2018.
David E Rumelhart and David Zipser. Feature discovery by competitive learning. Cognitive science, 9(1):75­112, 1985.
Benjamin Scellier, Anirudh Goyal, Jonathan Binas, Thomas Mesnard, and Yoshua Bengio. Extending the framework of equilibrium propagation to general dynamics. 2018.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv preprint arXiv:1805.01053, 2018.
Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient descent. 2018.
Haim Sompolinsky, Andrea Crisanti, and Hans-Jurgen Sommers. Chaos in random neural networks. Physical review letters, 61(3):259, 1988.
Ilya Sutskever and Geoffrey E Hinton. Deep, narrow sigmoid belief networks are universal approximators. Neural computation, 20(11):2629­2636, 2008.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, pp. 210­268. Cambridge University Press, 2012.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371­3408, 2010.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.
Greg Yang and Sam S Schoenholz. Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion. 2018.
Greg Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances in neural information processing systems, pp. 7103­7114, 2017.
13

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 1

In the following, we give an outline of the proof of Theorem 1, and the complete proof. First,
we start with a few notations and definitions. Recall the setting in Section 2.1. We define {g : Rn -1  Rn }-1 =1,...,L inductively as follows:

gL (u) = W L L (W LL-1 (u) + bL) + vL, g (u) = W  (g +1 (W  -1 (u) + b )) + v ,

= L - 1, ..., 1.

It is easy to see that x^ = g (x -1) for = 1, ..., L. Essentially g 's represent the autoencoding mappings computed by the inner layers.

We use bold-face letters (e.g. x, W ) to denote vectors or matrices. We use C throughout to denote
an arbitrary (and immaterial) constant that is independent of the dimensions. For two vectors x
and y, x, y denotes their inner product. We use I for the identity matrix, and In to emphasize its dimensions n × n. We use · to denote the usual Euclidean norm, ·  the infinity norm, and M 2 the maximum singular value of a matrix M . For a sigma-algebra F and two random variables X and Y , X|F =d Y means that for any integrable function  and any F-measurable
bounded random variable Z, E { (X) Z} = E { (Y ) Z}. We write X =d Y when F is the trivial sigma-algebra.

A sequence of functions n : Rn  R is uniformly pseudo-Lipschitz if there exists a constant C, independent of n, such that for any x, y  Rn,

|n (x) - n (y)|  C

1 + x + y nn

x- y . n

A sequence of functions n : Rn  Rn is uniformly Lipschitz if there exists a constant C,

independent of n, such that for any x, y  Rn, n (x) - n (y)  C x - y . These definitions

are adopted from Berthier et al. (2017). For two sequences of random variables Xn  R and Yn  R

indexed by n, we write Xn Yn to mean that Xn - Yn  0 in probability. The same meaning

hinodldexsewdhbeynnY, nweiswdreitteerXminni=sticY.

For two sequences of random vectors Xn  Rn and Y n  Rn n if for any sequences of uniformly pseudo-Lipschitz test functions

n : Rn  R, n (Xn) E {n (Y n)} (and hence in this context, we do not need Xn and Y n

to be defined on a joint probability space).

A.1 OUTLINE OF THE PROOF OF THEOREM 1

We state several results that are key to prove Theorem 1.

Proposition 5. Consider the asymptotic setting n  , with some sequence m = m (n) such that
m/n   > 0 as n  . Let  : R  R and  : R  R be Lipschitz continuous scalar functions. Consider a sequence of uniformly Lipschitz functions g : Rm  Rm, and sequences of vectors b  Rm, v  Rn, u  Rn such that

b 2  C, 0 < c   (u) 2  C, mn

v 2 C n

for all sufficiently large n. Let us define

f (u) = W  (g (W  (u) + b)) + v,

where W  Rm×n with Wij  N 0, W2 /n i.i.d. for W > 0. Let

 = Ez~

1 m 2  z~,  (g ( z~ + b))

,  = Ez~

1  (g ( z~ + b)) 2 m

,  2 = W2

 (u) 2 ,
n

for z~  N (0, Im). Then:

f (u) = W2  (u) + W2 z + v,

where z  N (0, In) (independent of u, b and v). Here the randomness is solely due to W .

14

Under review as a conference paper at ICLR 2019

A corollary follows from this proposition.
Corollary 6. Consider the same setting as in Proposition 5. Further assume that b  N 0, b2Im , v  N 0, v2In and u  N 0, u2In , independent of each other and of W . Then:

f (u) = W2  (uz) + W2  + v2z ,

for z, z  N (0, In) independently. In particular, for  : R  R Lipschitz continuous,

1  (f (u)) 2 n

Ez1,z2  W2  (uz1) +

2

W2  + v2z2

,

(4)

1 nu2 u,  (f (u)) in which

Ez1 ,z2

1 u z1

W2  (uz1) +

W2  + v2z2

,

(5)

1  = Ez~ m¯2 ¯z~,  (g (¯z~))

,  = Ez~

1  (g (¯z~)) 2 m

, ¯2 = W2 Ez1

 (uz1)2 +b2,

where z~  N (0, Im), and z1, z2 are independently distributed as N (0, 1).

In a nutshell, the proposition and the corollary consider a random weight-tied "autoencoder" with a single hidden layer, with a mapping g in the middle. Since g is not a separable function (i.e., g does not apply entry-wise), this is different from the usual shallow autoencoder, a case that has been investigated in Pennington & Worah (2017); Louart et al. (2018) with techniques and objectives different from ours. By understanding this structure, one can understand the random weight-tied multi-layer autoencoder. Indeed, for each , the proposition applies with u being x -1, f (u) being x^ and g being g +1. In other words, this studies the mapping g . One can start with gL, then progressively move to outer layers gL-1, gL-2, etc, and hence analyze the autoencoder completely by repeating the same procedure L times. We note that in doing so, at each step, one requires certain information about the inner layers to perform calculations for the outer layers, and this is worked out in Corollary 6 via a simple recursive relation. In particular, the left-hand sides of Eq. (4) and Eq. (5) play the role of  and  of the outer layer, whereas their right-hand sides involve  and  from the inner layers. It is also important to note that the assumption that the weights at different layers are independent is crucial in making u, g and (W , b, v) to be independent, allowing the proposition to be applicable at all steps. This is the idea behind the proof of Theorem 1.

We quickly mention the key proof technique for Proposition 5. The main technical challenge in working with the weight-tied structure W h (W u), for some h : Rm  Rm and W  Rm×n Gaussian, is that whereas y = W u is Gaussian with zero mean thanks to independence between
u and W , W h (y) is not since y is correlated with W . It is observed in Bolthausen (2014) that conditioning on a linear constraint y = W u (or the sigma-algebra F generated by y and u), one has that W is distributed as a conditional projection component plus an independent Gaussian component:
W |F =d E {W |F } + W~ Pu, where Pu is an appropriate projection. Here E {W |F} is what propagates the information about u. In addition, W~ is Gaussian and independent of F, and hence exact calculations can be then worked out. We note that the assumption W is Gaussian is crucial for this identity to hold.

A.2 PROOF OF PROPOSITION 5
We state the Gaussian Poincar´e inequality, which will be used multiple times throughout the proof. We remark that the use of the Gaussian Poincar´e inequality is unlikely to lead to a tight nonasymptotic result, but is sufficient for our asymptotic analysis. Theorem 7 (Gaussian Poincar´e inequality). For z  N 0, 2In and  : Rn  R continuous and weakly differentiable, there exists a universal constant C such that Var { (z)}  C2E  (z) 2 .
Now we are ready for the proof.

15

Under review as a conference paper at ICLR 2019

Step 1. We perform Gaussian conditioning. Let y = W  (u). Let F be the sigma-algebra generated by y and u. Conditioning on F is equivalent to conditioning on the linear constraint y = W  (u). Following Bayati & Montanari (2011), we have

W |F =d W P(u) + W~ P(u),

where W~ =d W and is independent of F , P(u) =  (u)  (u) /  (u) 2 the projection onto  (u), and P(u) = I -P(u) the corresponding orthogonal projection. As such, since  (g (y + b)) is F-measurable,

f (u) |F =d

1  (u) 2

W  (u) ,  (g (y + b))  (u) + P(u)W~

 (g (y + b)) + v

=

1  (u) 2

y,  (g (y + b))  (u) + P(u)W~

 (g (y + b)) + v.

For a sequence of uniformly pseudo-Lipschitz functions n : Rn  R:

n (f (u)) |F =d n

y,  (g (y + b))  (u) 2

 (u) + P(u)W~

 (g (y + b)) + v

 n.

(6)

Up to this point, there is no need for the asymptotics n  . The rest of the proof focuses on n.

Step 2. We show that  ,  and  are uniformly bounded as n  . This is trivial for  , and we note  > 0. Consider . We have for any r  Rm,

1  (g (r + b))  1  (g (r + b)) -  (g (0)) + 1  (g (0))

mm

m

 C r + b + 1  (g (0)) mm

 C r + C b + 1  (g (0)) mmm

 C r + C m

(7)

for sufficiently large m. It is then easy to see that  is uniformly bounded. Regarding ,

||   Ez~

1 z~ m

 (g ( z~ + b))



Ez~

1 z~ 2 m

  = ,

by Cauchy-Schwarz inequality. Therefore  is also uniformly bounded.

Step 3. We analyze the first term in n. Notice that y =d  z~ for some z~  N (0, Im). In addition, the mapping y  y,  (g (y + b)) /m is uniformly pseudo-Lipschitz, by noticing that

1 m

y1,  (g (y1 + b))

-1 m

y2,  (g (y2 + b))

1 m

 (g (y1 + b))

y1 - y2

1 +
m

y2

 (g (y1 + b)) -  (g (y2 + b)) ,

along with the fact that y   (g (y + b)) is uniformly Lipschitz, and Eq. (7). As such, by Theorem 7,

1 Var y,  (g (y + b))
m



C2 mE

1 + 1 y + 1 b 2 mm



3C 2 mE

1 1+

y 2+ 1

b2

mm

=O

1 m

,

which tends to 0 as n  . By Chebyshev's inequality, we thus have

1 y,  (g (y + b))
m

1

E

y,  (g (y + b)) m

1

=E

 z~,  (g ( z~ + b)) m

= 2.

16

Under review as a conference paper at ICLR 2019

Step 4. We analyze the second term in n. We have:

P(u)W~

 (g (y + b)) = W~

 (g (y + b)) -

 (u)  (u) 2

 (u) , W~

 (g (y + b))

.

(8)

Note that

the

mapping y



1 m

 (g (y + b)) 2 is uniformly pseudo-Lipschitz by a similar argu-

ment. Hence again by Theorem 7, Var

1 m

 (g (y + b)) 2

= O (1/m), which yields

1  (g (y + b)) 2 m

E

1  (g (y + b)) 2 m

=E

1  (g ( z~ + b)) 2 m

= .

Recall that W~ is independent of F, and as such, there exists a random variable z  N (0, 1) independent of y such that

1  (u) , W~  (g (y + b)) = 1  (u) n nn

 (g (y + b)) W z

1

 (u) n

W z ,

which converges to 0 in probability, where we have used the fact that  is uniformly bounded from Step 2. Furthermore, there also exists a random vector z  N (0, In) independent of F such that

W~

 (g (y + b)) = 1 n

 (g (y + b))

W z.

(9)

Step 5. We finish the proof. From the definition of uniform pseudo-Lipschitz functions and Eq. (8) and (9), we obtain:

n - n W2  (u) + W2 z + v

C 1+

|

y,  (g (y + b))  (u) 2

|

+

W2

||

+

W2



z n

+

2 v n

(u) n

+

P(u) 2

W~

2

 (g (y + b)) n

×

y,  (g (y + b))  (u) 2

- W2 

(u) n

+

1 n

 (g (y + b))

-

 

W

z n

 + n 1  (u) , W~  (g (y + b))
 (u) n

.

Here notice that

P(u) 2 = 1, and as a standard fact from the random matrix theory,

W~ 
2

c , W2

a

constant

with

high

probability

(see

e.g.

Vershynin

(2012)).

Furthermore,

1 n

z

C

with high probability due to the law of large numbers. Combining with the facts from Step 2, 3 and

4, it is then easy to see that

n - n W2  (u) + W2 z + v

0.

(10)

Finally, since  and  are uniformly bounded from Step 2, it is easy to show that the mapping

z  n W2  (u) + W2 z + v

is uniformly pseudo-Lipschitz. Hence by Theorem 7, Var n W2  (u) + W2 z + v
which yields

1 =O ,
n

n W2  (u) + W2 z + v E n W2  (u) + W2 z + v Together with Eq. (6) and (10), this completes the proof.

.

17

Under review as a conference paper at ICLR 2019

A.3 PROOF OF COROLLARY 6 By Proposition 5, for any sequence of uniformly pseudo-Lipschitz n : Rn  R:

n (f (u)) Ez n W2  (u) + W2 z + v ,

for z  N (0, In) independent of u, v and b, in which

1  =  ( , b) = Ez~ m 2  z~,  (g ( z~ + b)) ,

 =  ( , b) = Ez~

1  (g ( z~ + b)) 2 m

 2 =  2 (u) = W2

 (u) 2 ,
n

,

for z~  N (0, Im) independent of u and b. It is easy to see that  2  2 = ¯2 - b2. For any t  (0, ), the mapping b   (t, b) is uniformly pseudo-Lipschitz, and hence by Theorem 7,
Varb { (t, b)} = O (1/m), which yields  (t, b) Eb { (t, b)}. Recall  is independent of b. As such,  ( , b) Eb { ( , b)}. Furthermore, the mapping t  Eb { (t, b)} is continuous and hence Eb { ( , b)} Eb { (, b)}. Performing a similar argument for  ( , b), we thus have:

 , 1
 Ez~,b m 2  z~,  (g ( z~ + b)) .

We note by Stein's lemma,

Ez~,b {  z~,  (g ( z~ + b)) } =  2Ez~ div (  g)  2 + b2z~ ,

Ez~,b { b,  (g ( z~ + b)) } = b2Ez~ div (  g)  2 + b2z~ .

Also notice that

Ez~,b {  z~ + b,  (g ( z~ + b)) } = Ez~

 2 + b2z~,  g  2 + b2z~

.

Direct algebras then yield 1
Ez~,b m 2  z~,  (g ( z~ + b)) and hence  . Therefore,

= ,

n (f (u)) Ez n W2  (u) + W2 z + v  h (u, v) .

The mapping (u, v)  h (u, v) is uniformly pseudo-Lipschitz and hence by Theorem 7, Varu,v {h (u, v)} = O (1/n), which implies

h (u, v) Eu,v,z n W2  (u) + W2 z + v

= Ez,z n W2  (uz) + W2  + v2z

which follows from the fact that u, v and z are independent and normally distributed. The first claim is hence proven.
Eq. (4) follows immediately from the above. The proof of Eq. (5) is similar and hence omitted.

18

Under review as a conference paper at ICLR 2019

A.4 PROOF OF THEOREM 1

The following lemma states that, roughly speaking, g is uniformly Lipschitz (indexed by n) with high probability. First, recall that W = W (n) is indexed by n, and one can easily construct a joint probability space on which the sequence {W (n)}n1 is defined. The exact construction is immaterial, so long as for each n, the marginal distribution satisfies the setting in Section 2.1. We work in this joint space in the following.
Lemma 8. For each = 1, ..., L, there exists a finite constant c > 0 such that P EN  0 as N  , where the event EN is defined as

EN =

sup g (u1) - g (u2) > c

(n,u1 ,u2 )D

u1 - u2

, D = {n > N, u1, u2  Rn ,-1 u1 = u2} .

Proof. Consider = L. Recall that gL (u) = W L L (W LL-1 (u) + bL) + vL. Then for any u1 and u2,

gL (u1) - gL (u2) = W L L (W LL-1 (u1) + bL) - W L L (W LL-1 (u2) + bL)

 W L 2 L (W LL-1 (u1) + bL) - L (W LL-1 (u2) + bL)

(i)
 C W L 2 W LL-1 (u1) - W LL-1 (u2)

C

WL

2 2

L-1 (u1) - L-1 (u2)

(ii)
C

WL

2 2

u1 - u2

,

where steps (i) and (ii) are because L and L-1 are Lipschitz. It is a standard fact in the random

matrix theory that, for each = 1, ..., L, there exist finite constants  =   , W2 , > 0 and

c =c

 , W2 ,

> 0 such that P

W

2 2

>



 e-c n (see e.g. Vershynin (2012)). As such,

using this fact for = L, by the union bound,

P ELN

 e-cLn 
n>N

 e-cLudu = 1 e-cLN ,

u=N

cL

which proves the claim for = L. To see the claim for general , we have from a similar calculation:

g (u1) - g (u2)

 W 2 g +1 (W  -1 (u1) + b ) - g +1 (W  -1 (u2) + b )

 CL- +1

L

Wr

2 2

u1 - u2 .

r=

Therefore,
sup
(n,u1 ,u2 )D
with probability at least

g (u1) - g (u2) u1 - u2

 CL- +1

L
r
r=

1 - L e-crN  1 - L e-c N , c
n>N r=
by the union bound. This proves the claim.

c = min c ,
1 L

Lemma 9. For each  0,  +1 and ¯ +1 are finite and strictly positive. Furthermore,

1 n

 (x ) 2

 2+1/W2 , +1, and x +1 = ¯ +1z for z  N 0, In +1 .

Proof. We prove the lemma by induction. The claim is trivially true for = 0 by assumption. Assume the claim for some  0. Due to independence, conditioning on x ,

x +1|x =d

W2 , +1 n

 (x ) 2 + b2, +1z,

19

Under review as a conference paper at ICLR 2019

where z  N

0, In +1

independent

of

x

.

Since

1 n

 (x ) 2



2 +1

/W2 ,

+1,

we

then

have

x +1 =



2 +1

+

b2,

+1z

=

¯

+1z,

which implies

1 n +1

 +1 (x +1) 2  Ez

 +1 (¯ +1z )2

for z  N (0, 1). By the induction hypothesis, ¯ +1 > 0. Hence by assumption, the right-hand side

is

finite

and

strictly

positive.

One

also

recognizes

that

it

is

equal

to



2 +2

/W2 ,

+2.

Since

W,

+2

is

strictly positive and finite, so is  +2, and hence so is ¯ +2. This completes the proof.

We are now ready for the proof of Theorem 1. Claim (a) of the theorem, which follows directly from Lemma 9, is just a forward pass through the encoder and hence is the same as in the case of random feedforward networks (without weight tying) Poole et al. (2016).

For = 2, ..., L, let H denote the following three claims:

(1) : x^ =  W2 ,  +1 -1 (¯ -1z1) +  W2 ,  +1 + v2, z2,

(2) : (3) :

1 n -1

 -1 (g (¯ -1z~ -1)) 2

,

1 n -1¯2-1 ¯ -1z~ -1,  -1 (g (¯ -1z~ -1))

,

for z1, z2,  N 0, In -1 independently, and z~ -1  N 0, In -1 independent of g , with the

understanding that gL+1 is the identity. We prove them by induction, and Claim (b) then follows

immediately. We first note that with high probability, for any

,

1 n

b

2

and

1 n -1

v

2 are

bounded by the law of large numbers. Consider HL. Note that xL-1 is independent of gL, and that

1 nL-1

L-1 (xL-1) 2 converges to a finite non-zero constant in probability by Lemma 9. Hence

by Corollary 6, with g and  being L-1, and

being the identity, f being the fact xL-1 = ¯L-1z~ -1

gL, u being from Claim

xL-1 (recalling x^L (a), HL is proven.

=

gL (xL-1))

Assuming H +1 for some , {W , b , v , x -1, z~ -1}, and g

we is

prove H . independent

Recall that of x -1 and z~

g +1 is independent -1. Also from Claim

of (a),

 x -1

= =

¯ -1z~

-1.

Consider some N



N

and

let

E

N +1

denote the event as defined in Lemma 8.

On the

event

¬E

N +1

(the

complement

of

E N+1),

by

Corollary

6,

with

respect

to

the

randomness

of



,

x^ =  W2 ,  -1 (¯ -1z1) +  W2 ,  + v2, z2,

1 n -1

 -1 (g (¯ -1z~ -1)) 2

1 n -1 z~ -1,  -1 (g (¯ -1z~ -1))

2

Ez1,z2  -1  W2 ,  -1 (¯ -1z1) +  W2 ,  + v2, z2

,

Ez1,z2 z1  W2 ,  -1 (¯ -1z1) +  W2 ,  + v2, z2 ,

in which

1  = Ez n ¯2 ¯ z ,  (g +1 (¯ z )) ,

 = Ez

1 n

 (g +1 (¯ z )) 2

.

Note that here  and  are functions of g +1 and hence random, and z  N (0, In ) independent of g +1. On the event ¬EN+1, with the fact that ¯ is finite and non-zero by Lemma 9, we have the

mapping

z



1 n ¯2

¯ z , 

(g +1 (¯ z ))

 h (z )

is uniformly pseudo-Lipschitz, and hence applying Theorem 7, one obtains that

Varz {h (z )} = O

1 n

,

20

Under review as a conference paper at ICLR 2019

with the right-hand side independent of g +1 for n > N . Consequently, due to Lemma 8 and Chebyshev's inequality, for any > 0,

P

|h (z ) - | >

,

¬E

N +1

=E

Pz

(|h (z ) - | >

)

I¬E

N +1

1

E

2 Varz

{h

(z

)}

I¬E

N +1

= on (1) (1 - oN (1)) = on (1) ,

where on (1)  0 as n  . On the other hand, since z is independent of g +1, invoking H +1,

P {|h (z ) -  +1| > } = on (1) .

As such,

P {| -  +1| > }  P {|h (z ) -  +1| > } + P {|h (z ) - | > }

 P {|h (z ) -  +1| >

}+P

|h (z ) - | >

,

¬E

N +1

+P

E

N +1

= on (1) + on (1) + oN (1) .

Letting n   then N  , we then have   +1. Similarly, we also have   +1. It is then easy to deduce H , recalling the definitions of  and  .
The claim that ¯ -1z1 can be replaced with x -1 in the expression for x^ in Claim (b) can be recognized easily by doing the same replacement in the proof of Corollary 6 and the above proof. The proof of Claim (c) is similar. We omit these repetitive steps.

B NUMERICAL VERIFICATION OF THEOREM 1

We perform simple simulations to verify Theorem 1 at finite dimensions. In particular, we simulate a random weight-tied autoencoder, as described in Section 2.1, with L = 50, W2 , = 2.312, b2, = 0.211, v2, = 0,  =  = tanh, identity 0 and 0,  = 1, and consequently n = n0, for an input x  Rn0 whose first half of the entries are +1 and the rest are -1. Then we compute the
following:

^ +1 =



x^ ,  -1 (x -1) W2 ,  -1 (x -1)

2,

1 ^ +1 =  W2 ,

1 x^ 2 - 2W4 , ^2+1

n -1

n -1

 -1 (x -1) 2 - v2,

.

We also compute { ,  } =2,...,L+1 as in Section 2.2, and  and  as in Section 3.1. Theorem 1 predicts that ^  and ^  . Section 3.1 also asserts that    and    for L. Fig. 5 shows the results for n0 = 500 and n0 = 2000. We observe a quantitative agreement already for n0 = 500, and this improves with larger n0.
Next we verify the normality of the variation component in x^. We compute

z^ = x^ - 1W2 ,1^2x . 1W2 ,1^2 + v2,1

Its empirical distribution should be close to N (0, 1), in light of Theorem 1. We make this comparison in Fig. 6, and again observe good agreement already for n0 = 500.
Finally we re-simulate the autoencoder with different distributions of the weights W . In particular, we try with the Bernoulli distribution, the uniform distribution, and the Laplace distribution, with their means and variances adjusted to zero and W2 , /n -1 respectively. The results for n0 = 2000 are plotted in Fig. 7 and 8. We observe good quantitative agreements between the simulations for these non-Gaussian distributions and the prediction as in Theorem 1, although the theorem is proven only for Gaussian weights.

21

Under review as a conference paper at ICLR 2019

0.6 0.6

0.4
0.2
0
-0.2 0

20 40 

0.4
^  0.2
 ^ 0 
 -0.2
60 0

20 40 

^   ^  
60

Figure 5: The agreement among ^ ,  and , and among ^ ,  and , for = 2, ..., 51 and Gaussian weights. The setting is described in Appendix B. We take a single run for the simulation of the autoencoder. Here n0 = 500 (left) and n0 = 2000 (right).

Quantiles of z^ Quantiles of z^

4
2
0
-2
-4 -4 -2 0 2 4 Standard Gaussian quantiles

4
2
0
-2
-4 -4 -2 0 2 4 Standard Gaussian quantiles

Figure 6: Quantile-quantile plots for the empirical distribution of z^, described in Appendix B, versus the standard Gaussian distribution. Here n0 = 500 (left) and n0 = 2000 (right).

0.6
0.4
0.2
0
-0.2 0

10 20 30 40 50 

^ (Bernoulli) ^ (Bernoulli) ^ (Uniform) ^ (Uniform) ^ (Laplace) ^ (Laplace)





Figure 7: The agreement among ^ ,  and , and among ^ ,  and , for = 2, ..., 51, for different distributions of the weights. The setting is described in Appendix B. We take a single run for the simulation of the autoencoder. Here n0 = 2000.

Quantiles of z^ Quantiles of z^ Quantiles of z^

Bernoulli 4
2
0
-2
-4 -4 -2 0 2 4 Standard Gaussian quantiles

Uniform 4
2
0
-2
-4 -4 -2 0 2 4 Standard Gaussian quantiles

Laplace 4
2
0
-2
-4 -4 -2 0 2 4 Standard Gaussian quantiles

Figure 8: Quantile-quantile plots for the empirical distribution of z^, described in Appendix B, versus the standard Gaussian distribution, for different distributions of the weights. Here n0 = 2000.

22

Under review as a conference paper at ICLR 2019

C ON THE FIXED POINT EQUATIONS OF  AND 

C.1 COMPUTATION OF  AND 

Computing  and  as from Eq. (2) and (3) amounts to evaluating double integrals. For simple , such as the ReLU, the integrals f (a, b) = E { (a + bz)} and g (a, b) = E  (a + bz)2 ,
for z  N (0, 1), can be calculated in closed forms. In such cases, one can make reduction to one-dimensional integrals:

1  = ¯2 E

¯z1f

W2  (¯z1) ,

W2 

,  = E g W2  (¯z1) , W2 

.

We proceed with computing  and  as follows. From a random initialization (0) > 0 and (0) > 0, one iteratively updates (t+1) = G (t), (t) and (t+1) = R (t), (t) , and stops when the incremental update is negligible or the number of iterations exceeds a threshold. Upon convergence, this procedure finds a stable fixed point.

C.2 PROPERTIES OF  AND 

We prove several properties of  and . We recall G (, ) and R (, ) from Eq. (2) and (3) for ease of reading:

1 G (, ) = ¯2 E ¯z1  (¯z1) + z2
2
R (, ) = E   (¯z1) + z2 ,

(=i) E   (¯z1) + z2  (¯z1) ,

for z1, z2  N (0, 1) independently, where (i) is due to Stein's lemma. Recall that the fixed points equations are  = G (, ) and  = R (, ). We also recall that  = W2 > 0. In light of Remark 3, we will consider Lipschitz continuous, non-decreasing  and , so that   0 (and of course,   0). We will study these equations, first by stating some propositions, then discussing their implications, although we caution that the link between the propositions and the suggested implications is not entirely rigorous. All the proofs are deferred to Section C.3. We note that while the discussions concern with ReLU or tanh activations, the propositions apply to broader classes of functions.
In the following, when we say an initialization for  and , we mean either an initialization in the context of an iterative process to find the fixed points as in Section C.1, or L+1 and L+1 as in Section 2.2 in the context of autoencoders with L  . We also say  is a fixed point, without referencing to , to mean that it is only a fixed point of  = G (, ) for a given , and similarly for . When we mention both  and  as a fixed point, we mean a fixed point to both  = G (, ) and  = R (, ). We will use ukf to denote the kth-order partial derivative of f with respect to u.

C.2.1 THE CASE OF RELU 

The following result is exclusive to ReLU . Proposition 10. Consider that  is the ReLU and  is Lipschitz continuous and non-decreasing:

(a)  = 0 and  = 0 is a fixed point. Furthermore, at  = 0 and  = 2, the mapping   R (0, ) admits  = 0 as the only fixed point, which is stable if  < 2 and unstable if  > 2. Also at  = 0 and  = 2, any  is a stable fixed point.
(b) Assume  is positive on a set of positive Lebesgue measure. If   +, it must be that   +.
(c) Consider ¯  (0, ). Assume  is non-zero on a set of positive Lebesgue measure, and that  (u) = 0 for all u  0. Then no  > 0 is a stable fixed point. Furthermore,
· if E { (¯z1)}  1, there is only one fixed point at  = 0, which is stable; · if E { (¯z1)}  (1, 2), there are two: one at  = 0, which is stable, and the other
at  > 0, which is unstable;

23

Under review as a conference paper at ICLR 2019
· if E { (¯z1)}  2, there is only one fixed point at  = 0, which is unstable.
(d) Assume  is non-zero on a set of positive Lebesgue measure, and that  is an odd function. Then for any , the mapping   G (, ) is a straight line through the point (0, 0). Furthermore, if E { (¯z1)} = 2, there is only one fixed point at  = 0, which is stable for E { (¯z1)} < 2 and unstable for E { (¯z1)} > 2; if E { (¯z1)} = 2, any  is a fixed point.
(e) Assume  is odd, and consider  > 2. Given   0, we have R (, ) >  for all   0.
The proposition suggests the following picture. First consider  is ReLU. Since E { (¯z1)} = P (¯z1  0) = 0.5, we have two phase transitions at  = 2 and at  = 4. In particular, based on Proposition 10:
· When  < 2, with any initialization, we have convergence to  = 0 and  = 0. This is based on Claim (a) and (c).
· When   (2, 4), with certain initializations, we have convergence to  = 0 and divergence to  = +; with certain other initializations, we have divergence to  = + and  = +. This excludes a special initialization at the unstable fixed point, which is a singleton and essentially a rare case. This is based on Claim (a), (b) and (c).
· When   4, with any non-zero initialization, we have divergence to  = + and hence  = +. This is based on Claim (b) and (c).
Now we consider  is tanh. Let 0 = 0 (¯) = 2/E { (¯z1)}, and it is easy to see that 0 > 2 since  = tanh. The following picture is then expected:
· When  < 2, with any initialization, we have convergence to  =  = 0. This is based on Claim (a) and (d).
· When   (2, 0), with any non-zero initialization, we have convergence to  = 0 and divergence to  = +. This is based on Claim (a) and (d).
· When  > 0, with any non-zero initialization, we have divergence to  = + and  = +. This is based on Claim (b) and (d).
· When  = 0, we have that  is unchanged from the initialization. Since 0 > 2, we then have divergence to  = +. This is based on Claim (b) and (e).
One crucial property of the ReLU is that it is unbounded at infinity and its derivative at infinity is bounded away from zero. This allows  and  to grow to infinity. This is a stark contrast to the case  is bounded, for instance,  = tanh as we shall see.
C.2.2 THE CASE OF tanh 
We state a result that is relevant to  = tanh. Proposition 11. Assume that  thrice-differentiable with  (0) = 0,  (0) = , and (k)   C for k = 0, ..., 3, where (k) is the k-th derivative of . Assume  is Lipschitz continuous, nondecreasing. Then:
(a)  = 0 and  = 0 is a fixed point. Furthermore, assuming that  is non-zero on a set of positive Lebesgue measure,  is non-zero almost everywhere and ¯  (0, ), we have if  = 0, it must be that  = 0; in other words, if  > 0, then  > 0.
(b) If  is bounded, then 0    C, and ||  C/¯.
(c) Given  = 0, if  < 1/2,  = 0 is a stable fixed point, and if  > 1/2,  = 0 is unstable.
(d) Consider ¯  (0, ). Assume that
·  is positive on a set of positive Lebesgue measure, and E { (¯z)} > 0 for z  N (0, 1),
· either
24

Under review as a conference paper at ICLR 2019

­ case 1:  (u) = 0 for u  0, and  (u, t) < 0 for u, t,  > 0, or ­ case 2:  is an odd function, and  (u, t) <  (-u, t) for u, t,  > 0, in which  (u, t) =  u + t -  u - t , ·  satisfies E {z (z)}  + for z  N 0, s2 and s  , ·  satisfies E {zI (z)} > 0 for z is any Gaussian with zero mean and non-zero variance, and I (u) =  (u) + u (u).
Then for any given  > 0, there exists  =  (, ¯) > 0 finite such that if   , then  = 0 is the only fixed point of the equation  = G (, ) and is stable; if  > ,  = 0 is unstable, and there is one more fixed point at  > 0 finite, which is stable.
(e) Consider ¯  (0, ). The same conclusion as in Claim (c) holds for  = 0 with  = 1/ (E { (¯z1)}), assuming
·  is positive on a set of positive Lebesgue measure, and E { (¯z)} > 0 for z  N (0, 1),
·  (u) < 0 for u > 0, and either ­ case 1:  (u) = 0 for u  0 , or ­ case 2:  and  are odd functions.

The assumption (k)   C for k = 0, ..., 3 is not critical, only serves to ensure integrability of various terms in the proof and is likely relaxable, but is made for simplicity. The following lemma establishes certain properties of the tanh function.
Lemma 12. Consider  = tanh. Then:

(a) For  (u, t) =  (u + t) -  (u - t), we have  (u, t) < 0 for u, t > 0, and  (u, t) <  (-u, t) for u, t > 0.
(b) lims E {z (z)} = + for z  N 0, s2 .
(c) E {zI (z)} > 0 for z  N 0, s2 , s = 0, and I (u) =  (u) + u (u).

Now let us consider  =  = tanh. Note that tanh (u)  (0, 1) for any u = 0, tanh (0) = 1 and
E tanh (¯z1) < 1 unless ¯ = 0. By Lemma 12, Proposition 11 applies. The following picture is suggested based on Proposition 11:

· When  < 1, we have convergence to  =  = 0. This is based on Claim (c) and (e).
· The phase transition for  locates at  = 1, above which we have convergence to   (0, 1) given a non-zero initialization, and below which  = 0. Here  < 1 since tanh is bounded by 1. This is based on Claim (a), (b), and (c).
· The phase transition of  locates at some  > 1, above which we have convergence to  > 0 given a non-zero initialization, and below which  = 0. For ¯ > 0,  cannot grow to + as  varies. This is based on Claim (a), (b) and (d).

The proposition also suggests that the two phase transitions are close to each other if E { (¯z1)} 

1. This requires that ¯2  0, and W2  1 in the case  = 1. With respect to Eq. (1), we then have

b2  0. Remarkably activations, in which

this is reminiscent (Pennington et al.

of the context of (2017)) finds that

random feedforward networks with an initialization at W2  1 and b2

tanh 0

works better than most edge-of-chaos initialization schemes with Gaussian weights.

We also expect from the proposition a similar picture for  being the ReLU, with a crucial difference. In this case, E { (¯z1)} = 0.5, and therefore one cannot have that the two phase transitions being close to each other.

Interestingly Claim (a) implies that the phase transition of  never occurs before that of , regardless
of the specific  and . One way for the phase transitions to be close to each other is, as above, taking  =  = tanh and ¯2  0. Claim (a), (c) and (e) of the proposition also suggests that if E { (¯z1)} >  (0), then  and  will share the exact same location of the phase transitions, below which they are zero and above which they are positive.

25

Under review as a conference paper at ICLR 2019

C.3 PROOFS FOR SECTION C.2 
Proof of Proposition 10. Let  =  (¯z1) + z2 for brevity.
Claim (a). We have G (0, ) = 0 and R (0, 0) = 0. Simple calculations yield R (0, ) = /2. Claim (a) is then immediate.
Claim (b). To study  in the case   , we calculate:

11

R (, ) 

=

E

+
  (¯z1) +
z=-(¯z1) /

2
z  (z) dz

= E

   (¯z1) 

   (¯z1)

+

1

+

 

2

(¯z1)2



   (¯z1)

where  (u) = 1 exp
2

-u2/2

and  (u) =

u t=-



(t)

dt

the

Gaussian

PDF

and

CDF.

Recall

that

we

must have

1 

R

(,

)

=

1

unless



=

.

Also

notice

that

 ()

=

0

and

 (+)

=

1.

Since



is

positive

on

a

set

of

positive

Lebesgue

measure,

if

2/





as





,

1 

R

(,

)



,

and hence it must be that    when   .

Claim (c). We compute the first partial derivative G (, ), using the fact  (u) =  (u = 0) (in the weak sense) for the ReLU:

G (, ) = E { ()  (¯z1)} + 2E { ()  (¯z1)  (¯z1)}

= E { ()  (¯z1)} + 2E { ( = 0)  (¯z1)  (¯z1)}

= E { ()  (¯z1)} + 2E

1 exp 2

- 2 (¯z1)2 2

 (¯z1)  (¯z1) ,

where in the last step, we use the fact that z1 and z2 are independent. Hence some algebras yield the second partial derivative:

2G (, ) =

2 



2E

 (¯z1)  (¯z1) exp

- 2 (¯z1)2 2

1

-

2 2



(¯z1)2

(=i)

2 

2E

{I

( (¯z1)) 

(¯z1)}

(=ii)

2 

2 ¯

E

{I

(

(¯z1))

z1}

where in step (i), we define

I (u) = 1 u2 exp - 2 u2 , 2 2

in step (ii), we use Stein's lemma. Recall ¯  (0, ). Since  (u) = 0 for all u  0, we have I ( (¯z)) = 0 for z  0. In addition, I (u) > 0 almost everywhere and  is non-zero on a set of positive Lebesgue measure. It follows that 2G (, ) > 0. Therefore for any fixed , the mapping   G (, ) is strictly convex. Notice that G (0, ) = 0. Hence it cannot be that there is a stable
fixed point at  > 0.

Next we have:



G

(+,

)

=

lim
+

E

{

( (¯z1)) 

(¯z1)} = E {

(¯z1)} ,

G (0, ) = E 

1 z2  (¯z1) = 2 E { (¯z1)} .

Recall that   G (, ) is strictly convex. As such, when E { (¯z1)}  1, there is only one fixed point at  = 0, which is stable. When E { (¯z1)}  (1, 2), there are two: one at  = 0, which is stable, and the other at  > 0, which is unstable. When E { (¯z1)}  2, there is only
one fixed point at  = 0, which is unstable.

26

Under review as a conference paper at ICLR 2019

Claim (d). In the case that  is an odd function, u  I (u) is even and hence z  I ( (¯z))

is even. Then from the calculation of Claim (c), it is easy to see that 2G (, ) = 0. Also,

G (0, ) = 0. Therefore for each , the mapping   G (, ) is a straight line which passes

through the point (0, 0).

Since G (0, ) =

1 2

E

{

(¯z1)}, the claim is then immediate.

Claim (e). We recall the formula for R (, ) derived in the proof of Claim (b). Since  is odd,  is even and  - 0.5 is odd, we have

1  R (, ) = E

   (¯z1) 

   (¯z1)

+

1

+

 

2

(¯z1)2



   (¯z1)

-1 2

+E

 2

1

+

 

2

(¯z1)2

=E

 2

1

+

 

2

(¯z1)2

.

Then if  > 2, we always have R (, ) > .

 Proof of Proposition 11. Let  =  (¯z1) + z2 for brevity.
Claim (a). The fact  =  = 0 is a fixed point is obvious by assumption. Note that in order that R (, 0) = 0, one must have E  ( (¯z1))2 = 0. By assumption, this requires  = 0.

Claim (b). The fact that 0    C is obvious. Note that

||

=

1 ¯2

E

¯z1

 (¯z1) +

which yields the claim.

z2

Claim (c). We have:



C ¯

E

{|z1|}

=

C ¯

2 ,


R (, ) =

 

E

{

()



() z2}

(=i)

E

 ()2 + 

()  ()

,

where (i) is due to Stein's lemma. As such, R (0, 0) = 2. The claim is then immediate. Claim (d). Consider  > 0. We have:
G (, ) = E { ()  (¯z1)} + 2E { ()  (¯z1)  (¯z1)} , and therefore,

2G (, ) = 22E { ()  (¯z1)  (¯z1)} + 3E  ()  (¯z1)  (¯z1)2

2 = ¯ E z1 

 (¯z1) +

z2  (¯z1)2

2 = ¯ Ez2 Ez1 

 (¯z1) +

z2 z1 (¯z1)2

2 = ¯ Ez1 Ez2   (¯z1) + z2

z1 (¯z1)2

where we use Stein's lemma and the fact z1 and z2 are independent. Notice that, for  > 0, by Stein's lemma,

Ez2  a + z2

=

1 



Ez2

z2

a+

z2

=1

+


a+

t -  a -

t

t exp - t2

dt.

2 t=0

2

Consider case 1. Since  (u) = 0 for all u  0,

2G (, )

=

2 2¯ 

+ z=0

+
 ( (¯z) , t) tz (¯z)2 exp
t=0

- z2 + t2 2

dtdz.

27

Under review as a conference paper at ICLR 2019

Since  (u, t) < 0 for any u > 0 and t > 0, and by the assumption that  is positive on a set of
positive Lebesgue measure (which cannot intersect with (-; 0] since  (u) = 0 for all u  0), we have 2G (, ) < 0 for  > 0. In case 2, since  is an odd function,

2G (, )

=

2 2¯ 

+ z=0

+
[ ( (¯z) , t) -  (- (¯z) , t)] tz (¯z)2 exp
t=0

z2 + t2 -
2

Since  (u, t) <  (-u, t) for u > 0 and t > 0 and  (u)  0 for u  0 (for  being odd and non-decreasing), we again have 2G (, ) < 0 for  > 0. As such, the mapping   G (, ), for
 > 0, is strictly concave on (0, ).

dtdz.

Next we have

G (0, ) = E  z2  (¯z1) = E  z2 E { (¯z1)} ,

since z1 and z2 are independent. Notice that by Stein's lemma,

 E 

z2

1 = 2E

I

z2



1 = 2 E

z2I

z2 .

By assumption, we thus have that E  z1 and hence G (0, ) are increasing in .

Furthermore, G (0, )  0 when   0, and

lim G (0, ) (=i) E {


(¯z1)}

lim


1 

E

z2

z2 (=ii) +

where (i) is by Stein's lemma, and (ii) is by assumption. Therefore, there must exist  =  (, ¯) > 0 finite such that G (0, ) = 1 at  = , G (0, ) < 1 for  <  and G (0, ) > 1 for  > . Since   G (, ) is strictly concave on (0, ) and  = 0 is a fixed point by Claim (a),  is the threshold as in the claim.

Claim (e). Consider  = 0, in which case:

2G (, 0)

=

2 ¯ 2

+
 ( (¯z)) z (¯z)2 exp
z=-

- z2 2

dz.

Consider case 1. Since  (u) = 0 for all u  0,

2G (, 0)

=

2 ¯ 2

+
 ( (¯z)) z (¯z)2 exp
z=0

- z2 2

dz.

Since  (u) < 0 for u > 0 and  is positive on a set of positive Lebesgue measure (which cannot intersect (-; 0] since  (u) = 0 for all u  0), it is easy to see that 2G (, 0) < 0 for  > 0. In
case 2, since  and  are odd, the mapping z   ( (¯z)) is odd, and hence

2G (, 0)

=

22 ¯ 2

+
 ( (¯z)) z (¯z)2 exp
z=0

- z2 2

dz.

Again we have 2G (, 0) < 0 for  > 0. As such,   G (, ) is strictly concave on (0, ). Notice that G (0, 0) = E { (¯z1)}, linearly increasing in . This proves the claim with  = 1/ (E { (¯z1)}).

Proof of Lemma 12. We prove the first claim:

 (u, t) = tanh2 (u - t) - tanh2 (u + t) .

Since tanh2 is even and increasing on (0, +),  (u, t) < 0 for u, t > 0 and  (u, t) > 0 for u > 0 and t < 0. Simple algebra yields

 (-u, t) -  (u, t) = 2 (u, -t) .

Hence  (u, t) <  (-u, t) for u > 0 and t > 0. To see the second claim, for z  N (0, 1),

lim E {sz (sz)} = lim E {s |z|} = +.

s

s

To see the third claim, since I is odd,

E {zI (z)} = 2E z tanh (z) + z 1 - tanh (z)2 I (z > 0) > 0.

28

Under review as a conference paper at ICLR 2019

Figure 9: The reconstructions by the schemes from Table 1, as described in Appendix D.1, in Setting 1 (i.e., 0 = tanh). From the top row: original images, reconstructions from Scheme 1, 4 and 2. We omit the reconstructions from other schemes, since they are almost identical to those of Scheme 2. For each digit/letter category, the image is selected from the test set by ranking the reconstruction loss, averaged across Scheme 1 and 4, and picking one at the 75% percentile.

Figure 10: Description similar to Fig. 9. The chosen images are the 25% percentile.

D VISUALIZATION FOR SECTION 3.5
D.1 VISUALIZATION OF THE RECONSTRUCTIONS
In Fig. 9, 10, 11 and 12, we show the reconstructions of several images by the trained networks after 5 × 105 training iterations, under the schemes from Table 1, in the experiments of Section 3.5. We draw 10 digit images from the MNIST test set, as well as 3 letter images from the EMNIST Letters test set (Cohen et al. (2017)). Note that the networks are not trained with any letter images from the EMNIST data set. The reconstruction quality is visually imperfect even after intensive training, which is entirely expected for vanilla autoencoders and regression problems.
Observe that for the schemes that yield meaningful reconstructions, they output recognizable digits for digit images, while for letter images, most of their reconstructions are hardly recognizable as letters. As such, the trained networks of these schemes do not simply approximate the identity function, but rather capture some low-dimensional structures of the data. An exception is Scheme 7 under Setting 2, which is not surprising since it is a purely tanh network and tanh is almost identity near zero.

D.2 VISUALIZATION OF THE EVOLUTION

We show the evolution of the test reconstruction loss on the plane W2 , b2 , in conjunction with the experiments of Section 3.5. To make the computation more manageable, we opt for L = 50 with

less iterations, while maintaining other parameters the same as in Section 3.5. The results are shown

in Fig. 13 and 14. Several patterns emerge in good agreement with our hypotheses. Firstly, for

 = ReLU, the evolution starts earliest near W2 = 2, shows almost no progress or is numerically

unstable is much

when W2 slower when

2, and W2

is 1.

much slower Intriguingly

when W2 the evolution

2. Secondly, for  = tanh, is almost insensitive to b2.

the

evolution

29

Under review as a conference paper at ICLR 2019

Figure 11: Description similar to Fig. 9. The setting is Setting 2 (i.e., identity 0). The reconstructions are of Scheme 1, 4, 7 and 2. The chosen images are the 75% percentile. The ranking is by
averaging over Scheme 1, 4, and 7.

Figure 12: Description similar to Fig. 9. The setting is Setting 2 (i.e., identity 0). The reconstructions are of Scheme 1, 4, 7 and 2. The chosen images are the 25% percentile. The ranking is by
averaging over Scheme 1, 4, and 7.

Iter. 300

Iter. 500

Iter. 1000

Iter. 2000

Iter. 3000

33333

22222

11111

0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1

Iter. 200

Iter. 500

Iter. 1000

Iter. 2000

Iter. 3000

33333

22222

11111 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1

Iter. 100

Iter. 400

Iter. 700

Iter. 1000

Iter. 3000

33333

22222

11111

0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1

Iter. 200

Iter. 600

Iter. 1000

Iter. 2000

Iter. 3000

2.0 2.0 2.0 2.0 2.0

1.5 1.5 1.5 1.5 1.5

11111

0.5 0.5 0.5 0.5 0.5 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1

2.2 1.7 1 0.3 2.1 1.5 0.9 0.3 1.2 0.9 0.6 0.3 0.9
0.6
0.3

Figure 13: The test loss for various pairs W2 , b2 at different iterations, as described in Appendix D.2, in Setting 1 (i.e., 0 = tanh). The horizontal axis is b2, and the vertical axis is W2 . First row:  =  = ReLU; second row:  = ReLU and  = tanh; third row:  =  = tanh; fourth row:  = tanh and  = ReLU. Red indicates higher loss, and black indicates lower loss. White indicates a numerical error.

30

Under review as a conference paper at ICLR 2019

Iter. 1000

Iter. 1400

Iter. 2000

Iter. 2400

Iter. 3000

22222

1.5 1.5 1.5 1.5 1.5

11111

0.5 0.5 0.5 0.5 0.5 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1

Iter. 1000

Iter. 1400

Iter. 2000

Iter. 2400

Iter. 3000

2.0 2.0 2.0 2.0 2.0

1.5 1.5 1.5 1.5 1.5

11111

0.5 0.5 0.5 0.5 0.5 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1

Iter. 100

Iter. 200

Iter. 400

Iter. 1000

Iter. 2000

33333

22222

11111

0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1

Iter. 100

Iter. 200

Iter. 400

Iter. 1000

Iter. 2000

22222

1.5 1.5 1.5 1.5 1.5

11111

0.5 0.5 0.5 0.5 0.5 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1

0.9
0.75
0.6 0.9
0.75
0.6 1
0.8
0.6 0.9
0.75
0.6

Figure 14: The test loss for various pairs W2 , b2 at different iterations, as described in Appendix D.2, in Setting 2 (i.e., 0 is the identity). The description is similar to Fig. 13. White indicates either a very large value or a numerical error, which are due to the fact that the chosen learning rate
is not sufficiently small. Note that in this setting with  = ReLU, the training process is trapped in numerical errors for W2 > 2 as we expect, and hence the test loss at W2 > 2 is not plotted.

E MISCELLANIES

E.1 EDGE OF CHAOS INITIALIZATION

We quickly review the edge of chaos (EOC) initialization (Schoenholz et al. (2016)). For an activation , any W2 , b2 such that there exists a finite ¯  0 for which
¯2 = W2 E  (¯z)2 + b2,
W2 = 1/E  (¯z)2

is said to be an EOC initialization scheme, where z  N (0, 1). This is based on the order-to-chaos

phenomenon found in (Poole et al. (2016)). For ReLU , W2 = 2 and b2 = 0 is the only EOC initialization (Hayou et al. (2018)) and coincides with the He initialization (He et al. (2015)). For

 = tanh, there can be multiple pairs that form EOC initialization. (Pennington et al. (2017)) argues

that and

a better EOC scheme b2  2.01 × 10-5 for

should be closer to dynamical isometry and suggests taking W2  1.05  = tanh. The EOC initialization is applicable to feedforward networks.

E.2 USEFUL FACTS
We state several useful facts, which are used in various places throughout. First, for a Gaussian matrix W and an independent vector u, we have W u is also Gaussian. In particular, if entries of W are i.i.d. N 0, s2 , then W u  N 0, s2 u 2 I . The second fact is Stein's lemma:
Lemma 13 (Stein's). E {zf (z)} = E {f (z)} for z  N (0, 1) and f : R  R weakly differentiable whenever the expectations are defined.

31

