Under review as a conference paper at ICLR 2019
STACKED U-NETS: A NO-FRILLS APPROACH TO NATURAL IMAGE SEGMENTATION
Anonymous authors Paper under double-blind review
ABSTRACT
Many imaging tasks require global information about all pixels in an image. Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and down-sampled into a single output. But for semantic segmentation and object detection tasks, a network must provide higherresolution pixel-level outputs. To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost. This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution. SUNets leverage the information globalization power of u-nets in a deeper network architectures that is capable of handling the complexity of natural images. SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.
1 INTRODUCTION
Semantic segmentation methods decompose an image into groups of pixels, each representing a common object class. While the output of a segmentation contains object labels assigned at the local (pixel) level, each label depends on global information about the image, such as textures, colors, and object boundaries that may span large regions. Simple image classification algorithms consolidate global information by successively pooling features until the final output is a single label containing information from the entire image. In contrast, segmentation methods must output a full-resolution map of labels. Thus, a successful segmentation method must address this key question: how can we learn long-distance contextual information while at the same time retaining high spatial resolution at the output for identifying small objects and sharp boundaries?
For natural image processing, most research has answered this question using one of two approaches. One approach is to use very few pooling layers, thus maintaining resolution (although methods may still require a small number of deconvolution layers (Lin et al., 2017a; Fu et al., 2017; Je´gou et al., 2017; Ghiasi & Fowlkes, 2016)). Large fields of view are achieved using dilated convolutions, which span large regions. By maintaining resolution at each layer, this approach preserves substantial amounts of signal about smaller and less salient objects. However, this is achieved at the cost of computationally expensive and memory exhaustive training/inference. The second related approach is to produce auxiliary context aggregation blocks (Kra¨henbu¨hl & Koltun, 2011; Chen et al., 2016a; Chandra & Kokkinos, 2016; Chandra et al., 2017; Zheng et al., 2015; Yu & Koltun, 2015; Zhao et al., 2017) that contain features at different distance scales, and then merge these blocks to produce a final segmentation. This category includes many well-known techniques such as dense CRF (Kra¨henbu¨hl & Koltun, 2011) (conditional random fields) and spatial pyramid pooling (Chen et al., 2016a).
These approaches suffer from the following challenges:
1. Deconvolutional (i.e., encoder-decoder) architectures perform significant nonlinear computation at low resolutions, but do very little processing of high-resolution features. During the convolution/encoding stage, pooling layers can move information over large distances, but information about small objects is often lost. During the deconvolution/decoding stage, high- and lowresolution features are merged to produce upsampled feature maps. However, the high-resolution
1

Under review as a conference paper at ICLR 2019
inputs to deconvolutional layers come from relatively shallow layers that do not effectively encode semantic information. 2. Image classification networks are parameter heavy (44.5M parameters for ResNet-101), and segmentation methods built on top of these classification networks are often even more burdensome. For example, on top of the resnet-101 architecture, PSPNet (Zhao et al., 2017) uses 22M additional parameters for context aggregation, while the ASPP and Cascade versions of the Deeplab network utilize 14.5M (Chen et al., 2016a) and 40M (Chen et al., 2017) additional parameters, respectively.
A popular and simple approach to segmentation is u-nets, which perform a chain of convolutional/downsampling operations, followed by a chain of deconvolutional/upsampling layers that see information from both low- and high-resolution scales. These u-net architectures are state-of-the art for medical image segmentation (Ronneberger et al., 2015), but they do not perform well when confronted with the complex color profiles, lighting effects, and occlusions present in natural images.
We expand the power of u-nets by stacking u-net blocks into deep architectures. This addresses the two challenges discussed above: As data passes through multiple u-net blocks, high-resolution features are mixed with low-resolution context information and processed through many layers to produce informative high-resolution features. Furthermore, stacked U-net models require fewer feature maps per layer than conventional architectures, and thus achieve higher performance with far fewer parameters. Our smallest model exceeds the performance of ResNet-101 on the PASCAL VOC 2012 semantic segmentation task by 4.5% mIoU, while having  7× fewer parameters.
2 RELATED WORK
Many models (Chen et al., 2017; Zhao et al., 2017; Peng et al., 2017; Chen et al., 2016a; Wang et al., 2017; Ghiasi & Fowlkes, 2016; Lin et al., 2017a; Fu et al., 2017) have boosted the performance of semantic segmentation networks. These gains are mainly attributed to the use of pre-trained models, dilated convolutional layers (Chen et al., 2016a; Yu & Koltun, 2015) and fully convolutional architectures (DCNN) (Long et al., 2015). These works employ a range of strategies to tap contextual information, which fall into three major categories.
Context Aggregation Modules: These architectures place a special module on top of a pre-trained network that integrates context information at different distance scales. The development of fast and efficient algorithm for DenseCRF (Kra¨henbu¨hl & Koltun, 2011) led to the development of numerous algorithms (Chen et al., 2016a; Liang-Chieh et al., 2015; Chandra & Kokkinos, 2016; Chandra et al., 2017) incorporating it on top of the output belief map. Moreover, the joint training of CRF and CNN parameters was made possible by Zheng et al. (2015); Schwing & Urtasun (2015). ParseNet (Liu et al., 2015) exploits image-level feature information at each layer to learn global contextual information. In contrast, Zhao et al. (2017); Chen et al. (2017; 2016a) realized substantial performance improvements by employing parallel layers of spatial pyramid pooling.
Image Pyramid: The networks proposed in Lin et al. (2016); Chen et al. (2016b) learn context information by simultaneously processing inputs at different scales and merging the output from all scales. Recently, Dai et al. (2017) developed a deformable network that adaptively determines an object's scale and accordingly adjusts the receptive field size of each activation function. On the other hand, Singh & Davis (2017) proposed a new training paradigm for object detection networks that trains each object instance only using the proposals closest to the ground truth scale.
Encoder-Decoder: These models consist of an encoder network and one or many blocks of decoder layers. The decoder fine-tunes the pixel-level labels by merging the contextual information from feature maps learned at all the intermediate layers. Usually, a popular bottom-up pre-trained classification network such as ResNet (He et al., 2016a), VGG (Simonyan & Zisserman, 2014) or DenseNet (Huang et al., 2017) serves as an encoder model. U-net (Ronneberger et al., 2015) popularly employs skip connections between an encoder and its corresponding decoding layers. Ghiasi & Fowlkes (2016) use a Laplacian pyramid reconstruction network to selectively refine the low resolution maps. Refinenet (Lin et al., 2017a) employs sophisticated decoder modules at each scale on top of the ResNet encoder, while Chen et al. (2018) utilize a simple two-level decoding of feature maps from the Xception network (Chollet, 2017). In short, the structure in Chen et al. (2018); Ghiasi & Fowlkes (2016) is a hybrid of decoding and context aggregation modules.
2

Under review as a conference paper at ICLR 2019
2.1 USE OF PRE-TRAINED NETS
Many of the networks described above make extensive use of image classification networks that were pre-trained for other purposes. Although parameter heavy, much fundamental work on segmentation (FCN (Long et al., 2015), dilated nets (Yu & Koltun, 2015), u-nets (Ronneberger et al., 2015) and CRF (Zheng et al., 2015)) was built on VGG. These architectures share common origins in that they were designed for the ImageNet competition and features are processed bottom-up. This prototype works well when the network has to identify only a single object without exact pixel localization. However, when extended to localization tasks such as segmentation and object detection, it is not clear whether the complete potential of these networks has been properly tapped. Recent work on object detection (Singh & Davis, 2017) also echoes a similar concern.
3 U-NETS REVISITED
The original u-net architecture was introduced by Ronneberger et al. (2015), and produced almost perfect segmentation of cells in biomedical images using very little training data. The structure of u-nets makes it possible to capture context information at multiple scales and propagate them to the higher resolution layers. These higher order features have enabled u-nets to outperform previous deep models on various tasks including semantic segmentation (Milletari et al., 2016), depth-fusion (Riegler et al., 2017), image translation (Isola et al., 2017) and human-pose estimation (Newell et al., 2016). Moreover, driven by the initial success of u-nets, many recent works on semantic segmentation (Lin et al., 2017a; Ghiasi & Fowlkes, 2016; Fu et al., 2017) and object detection (Shrivastava et al., 2016; Lin et al., 2017b) also propose an encoder-decoder deep architecture.
The u-net architecture evenly distributes its capacity among the encoder and decoder modules. Moreover, the complete network can be trained in an end-to-end setting. In contrast, the more recent architectures reviewed in Section 2 do not equally distribute the processing of top-down and bottomup features. Since these architectures are built on top of pre-trained feature extractors (Simonyan & Zisserman, 2014; He et al., 2016a), the decoder modules are trained separately and sometimes in multiple stages. To overcome these drawbacks, Je´gou et al. (2017) proposed an equivalent u-net based on the Densenet (Huang et al., 2017) architecture. However, DenseNet is memory intensive, and adding additional decoder layers leads to a further increase in memory usage. Given these drawbacks, the effectiveness of these architectures on different datasets and applications is unclear.
The goal of this paper is to realize the benefits of u-nets (small size, easy trainability, high performance) for complex natural image segmentation problems. Specifically, we propose a new architecture composed of multiple stacks of u-nets. The network executes repeated processing of both top-down as well as bottom-up features and captures long-distance spatial information at multiple resolutions. The network is trained end-to-end on image classification tasks and can be seamlessly applied to semantic segmentation without any auxiliary modules on top.
Our stacked u-net (SUNet) architecture shares some similarity with other related stacked encoderdecoder structures. Fu et al. (2017) use multiple stacks (upto 3) of de-convolutional networks on top of a powerful encoder (DenseNet) while Newell et al. (2016) apply multiple stacks of u-net modules for human-pose estimation. However, the processing of features inside each u-net module in Newell et al. (2016) differs from ours. Newell et al. (2016) replace each convolutional block with a residual module, utilizes nearest-neighbor upsampling for deconvolution and operate at fix resolution. In contrast, SUNets retain the basic u-net structure from Ronneberger et al. (2015), operates without any intermediate supervision and processes features by progressively downsampling.
3.1 U-NET MODULE IMPLEMENTATION
Figure 1 illustrates the design of the u-net module employed in our stacked architecture. Each module is composed of 10 pre-activated convolutional blocks each preceded by a batch-normalization and a ReLU non-linearity. The pooling/unpooling operation, handled by the strided convolutional/deconvolutional layers, facilitates information exchange between the lower and the higher resolution features. A skip connection branches off at the output of the first encoder block, E1. Following this, the E2 and D2 blocks capture long distance context information using lower resolution feature maps and merge the information back with the high resolution features from E1 at the output of D2. Every layer (except for the bottleneck layers) uses 3 × 3 kernels, and outputs a
3

Under review as a conference paper at ICLR 2019

FoV

1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit>

3 <latexit sha1_base64="t8aF3VUl9YvEkj0lnfv71JJep+U=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIndYqB3RxhIST0jgQvaWOVjZ27vs7pkQwi+wsVBj61+y89+4wBUKvmSSl/dmMjMvTAXXxnW/ncLa+sbmVnG7tLO7t39QPjx60EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nfmtJ1SaJ/LejFMMYjqQPOKMGis1L3rlilt15yCrxMtJBXI0euWvbj9hWYzSMEG17nhuaoIJVYYzgdNSN9OYUjaiA+xYKmmMOpjMD52SM6v0SZQoW9KQufp7YkJjrcdxaDtjaoZ62ZuJ/3mdzERXwYTLNDMo2WJRlAliEjL7mvS5QmbE2BLKFLe3EjakijJjsynZELzll1eJX6teV91mrVK/ydMowgmcwjl4cAl1uIMG+MAA4Rle4c15dF6cd+dj0Vpw8plj+APn8wfrT4yI</latexit>

7 <latexit sha1_base64="1Hwhep53I+BRB19lbR3AUWzH8O0=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxInc0aEe0sYTEExK4kL1lDlb29i67eyaE8AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLU8G1cd1vp7CxubW9U9wt7e0fHB6Vj08edJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5v5377CZXmibw3kxSDmA4ljzijxkqter9ccavuAmSdeDmpQI5mv/zVGyQsi1EaJqjWXc9NTTClynAmcFbqZRpTysZ0iF1LJY1RB9PFoTNyYZUBiRJlSxqyUH9PTGms9SQObWdMzUivenPxP6+bmegqmHKZZgYlWy6KMkFMQuZfkwFXyIyYWEKZ4vZWwkZUUWZsNiUbgrf68jrxa9XrqtuqVRo3eRpFOINzuAQP6tCAO2iCDwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDxW4yM</latexit>

E1

11 <latexit sha1_base64="NQCP9J6Jae+C4HQwtQLO+YOyf3Y=">AAAB6HicbVBNT8JAEJ3iF+IX6tHLRmLiibRc1BvRi0c0VkigIdtlCxu222Z3akIa/oEXD2q8+pO8+W9coAcFXzLJy3szmZkXplIYdN1vp7S2vrG5Vd6u7Ozu7R9UD48eTZJpxn2WyER3Qmq4FIr7KFDyTqo5jUPJ2+H4Zua3n7g2IlEPOEl5ENOhEpFgFK1073n9as2tu3OQVeIVpAYFWv3qV2+QsCzmCpmkxnQ9N8UgpxoFk3xa6WWGp5SN6ZB3LVU05ibI55dOyZlVBiRKtC2FZK7+nshpbMwkDm1nTHFklr2Z+J/XzTC6DHKh0gy5YotFUSYJJmT2NhkIzRnKiSWUaWFvJWxENWVow6nYELzll1eJ36hf1d27Rq15XaRRhhM4hXPw4AKacAst8IFBBM/wCm/O2Hlx3p2PRWvJKWaO4Q+czx9Xf4zB</latexit>

19 <latexit sha1_base64="7QBROOc3s3JaYXciltoOhBgZ3/E=">AAAB6HicbVBNT8JAEJ36ifiFevSykZh4Ii0X5Ub04hGNFRJoyHaZwobtttndmpCGf+DFgxqv/iRv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdZIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZn77CZXmiXwwkxSDmA4ljzijxkr3XqNfqbo1dw6ySryCVKFAq1/56g0SlsUoDRNU667npibIqTKcCZyWe5nGlLIxHWLXUklj1EE+v3RKzq0yIFGibElD5urviZzGWk/i0HbG1Iz0sjcT//O6mYmugpzLNDMo2WJRlAliEjJ7mwy4QmbExBLKFLe3EjaiijJjwynbELzll1eJX681au5dvdq8LtIowSmcwQV4cAlNuIUW+MAggmd4hTdn7Lw4787HonXNKWZO4A+czx9jl4zJ</latexit>

E2

D2

D1

 <latexit sha1_base64="MovFeT8JHV0FBVe650YTrSvqtt4=">AAAB7HicbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMF0xbaUDbbSbt2k427m0Ip/Q9ePKh49Qd589+4bXPQ1gcDj/dmmJkXpoJr47rfzsrq2vrGZmGruL2zu7dfOjisa5kphj6TQqpmSDUKnqBvuBHYTBXSOBTYCAe3U78xRKW5TB7MKMUgpr2ER5xRY6V6G58yPuyUym7FnYEsEy8nZchR65S+2l3JshgTwwTVuuW5qQnGVBnOBE6K7UxjStmA9rBlaUJj1MF4du2EnFqlSyKpbCWGzNTfE2Maaz2KQ9sZU9PXi95U/M9rZSa6CsY8STODCZsvijJBjCTT10mXK2RGjCyhTHF7K2F9qigzNqCiDcFbfHmZ+OeV64p7f1Gu3uRpFOAYTuAMPLiEKtxBDXxg8AjP8ApvjnRenHfnY9664uQzR/AHzucPLjyPDw==</latexit>

RC BeO NLN
UV

M <latexit sha1_base64="4aAPXWxyoZO2L1REZ5uCd/EHIaI=">AAAB8nicbVDLSgNBEOyNrxhfUY9eBoPgKWxC8HELevEiRHBNILuG2clsMmT2wUyvEJb8hhcPKl79Gm/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPHnScKsYdFstYdXyquRQRd1Cg5J1EcRr6krf90fXUbz9xpUUc3eM44V5IB5EIBKNoJPfWRRFyTc4aj/VeuWJX7RnIMqnlpAI5Wr3yl9uPWRryCJmkWndrdoJeRhUKJvmk5KaaJ5SN6IB3DY2o2eRls5sn5MQofRLEylSEZKb+nshoqPU49E1nSHGoF72p+J/XTTG48DIRJSnyiM0XBakkGJNpAKQvFGcox4ZQpoS5lbAhVZShialkQqgtvrxMnHr1smrfNSrNqzyNIhzBMZxCDc6hCTfQAgcYJPAMr/BmpdaL9W59zFsLVj5zCH9gff4AMG+Qqg==</latexit>

 642

1 x

I <latexit sha1_base64="4n98qf+lpcmyo3CE0LctotO/EpI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lFUG9FL3prwdhCG8pmO2nXbjZhdyOU0F/gxYOKV/+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RRWVtfWN4qbpa3tnd298v7Bg45TxdBjsYhVO6AaBZfoGW4EthOFNAoEtoLRzdRvPaHSPJb3ZpygH9GB5CFn1FipedcrV9yqOwNZJrWcVCBHo1f+6vZjlkYoDRNU607NTYyfUWU4EzgpdVONCWUjOsCOpZJGqP1sduiEnFilT8JY2ZKGzNTfExmNtB5Hge2MqBnqRW8q/ud1UhNe+hmXSWpQsvmiMBXExGT6NelzhcyIsSWUKW5vJWxIFWXGZlOyIdQWX14m3ln1quo2zyv16zyNIhzBMZxCDS6gDrfQAA8YIDzDK7w5j86L8+58zFsLTj5zCH/gfP4ADUCMoA==</latexit>

1

33 xx 33

33 xx 33

33 xx 33

2N <latexit sha1_base64="2VNa0cgR/AjUXv7ci89/Du3S9gU=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mLoN6KXjxJFWMLbSib7aZdutmE3YlQQv+BFw8qXv1J3vw3btsctPXBwOO9GWbmBYkUBl332ymsrK6tbxQ3S1vbO7t75f2DRxOnmnGPxTLW7YAaLoXiHgqUvJ1oTqNA8lYwup76rSeujYjVA44T7kd0oEQoGEUr3ddve+WKW3VnIMuklpMK5Gj2yl/dfszSiCtkkhrTqbkJ+hnVKJjkk1I3NTyhbEQHvGOpohE3fja7dEJOrNInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4YWfCZWkyBWbLwpTSTAm07dJX2jOUI4toUwLeythQ6opQxtOyYZQW3x5mXj16mXVvTurNK7yNIpwBMdwCjU4hwbcQBM8YBDCM7zCmzNyXpx352PeWnDymUP4A+fzB4V6jOE=</latexit>

3 x

3

3 x 3

N <latexit sha1_base64="D7mancucQcCJ5r9nhaEAxO5UVIU=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lFUG9FL56kBWMLbSib7aRdu9mE3Y1QQn+BFw8qXv1L3vw3btsctPXBwOO9GWbmBYng2rjut1NYWV1b3yhulra2d3b3yvsHDzpOFUOPxSJW7YBqFFyiZ7gR2E4U0igQ2ApGN1O/9YRK81jem3GCfkQHkoecUWOl5l2vXHGr7gxkmdRyUoEcjV75q9uPWRqhNExQrTs1NzF+RpXhTOCk1E01JpSN6AA7lkoaofaz2aETcmKVPgljZUsaMlN/T2Q00nocBbYzomaoF72p+J/XSU146WdcJqlByeaLwlQQE5Pp16TPFTIjxpZQpri9lbAhVZQZm03JhlBbfHmZeGfVq6rbPK/Ur/M0inAEx3AKNbiAOtxCAzxggPAMr/DmPDovzrvzMW8tOPnMIfyB8/kDFM+MpQ==</latexit>

1 x 1

M <latexit sha1_base64="W/iSpqylAWJ4thXpo3iQxTS+DEw=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lFUG9FL16EFowttKFstpN27WYTdjdCCf0FXjyoePUvefPfuG1z0NYHA4/3ZpiZFySCa+O6305hZXVtfaO4Wdra3tndK+8fPOg4VQw9FotYtQOqUXCJnuFGYDtRSKNAYCsY3Uz91hMqzWN5b8YJ+hEdSB5yRo2Vmne9csWtujOQZVLLSQVyNHrlr24/ZmmE0jBBte7U3MT4GVWGM4GTUjfVmFA2ogPsWCpphNrPZodOyIlV+iSMlS1pyEz9PZHRSOtxFNjOiJqhXvSm4n9eJzXhpZ9xmaQGJZsvClNBTEymX5M+V8iMGFtCmeL2VsKGVFFmbDYlG0Jt8eVl4p1Vr6pu87xSv87TKMIRHMMp1OAC6nALDfCAAcIzvMKb8+i8OO/Ox7y14OQzh/AHzucPE0yMpA==</latexit>

N <latexit sha1_base64="yaZZAQwpC/o/kOA2Y2djCR1+B54=">AAAB83icbVDLTgJBEOzFF+IL9ehlIjHxRBZCfNyIXjwZTFwhgZXMDrMwYfbhTC8J2fAdXjyo8erPePNvHGAPClbSSaWqO91dXiyFRtv+tnIrq2vrG/nNwtb2zu5ecf/gQUeJYtxhkYxUy6OaSxFyBwVK3ooVp4EnedMbXk/95ogrLaLwHscxdwPaD4UvGEUjubekgyLgmpzVHqvdYsku2zOQZVLJSAkyNLrFr04vYknAQ2SSat2u2DG6KVUomOSTQifRPKZsSPu8bWhIzSY3nR09ISdG6RE/UqZCJDP190RKA63HgWc6A4oDvehNxf+8doL+hZuKME6Qh2y+yE8kwYhMEyA9oThDOTaEMiXMrYQNqKIMTU4FE0Jl8eVl4lTLl2X7rlaqX2Vp5OEIjuEUKnAOdbiBBjjA4Ame4RXerJH1Yr1bH/PWnJXNHMIfWJ8/iQSQ1Q==</latexit>

 642

322 <latexit sha1_base64="CiugEef6SQZd1JVgj1iLtfI55Dc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoN6KXjxWMLbQxrLZbtqlu5uwuxFK6F/w4kHFq7/Im//GTZuDtj4YeLw3w8y8MOFMG9f9dkorq2vrG+XNytb2zu5edf/gQcepItQnMY9VJ8Saciapb5jhtJMoikXIaTsc3+R++4kqzWJ5byYJDQQeShYxgk0unTUeG/1qza27M6Bl4hWkBgVa/epXbxCTVFBpCMdadz03MUGGlWGE02mll2qaYDLGQ9q1VGJBdZDNbp2iE6sMUBQrW9Kgmfp7IsNC64kIbafAZqQXvVz8z+umJroMMiaT1FBJ5ouilCMTo/xxNGCKEsMnlmCimL0VkRFWmBgbT8WG4C2+vEz8Rv2q7t6d15rXRRplOIJjOAUPLqAJt9ACHwiM4Ble4c0Rzovz7nzMW0tOMXMIf+B8/gCBXY1q</latexit>

322 <latexit sha1_base64="CiugEef6SQZd1JVgj1iLtfI55Dc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoN6KXjxWMLbQxrLZbtqlu5uwuxFK6F/w4kHFq7/Im//GTZuDtj4YeLw3w8y8MOFMG9f9dkorq2vrG+XNytb2zu5edf/gQcepItQnMY9VJ8Saciapb5jhtJMoikXIaTsc3+R++4kqzWJ5byYJDQQeShYxgk0unTUeG/1qza27M6Bl4hWkBgVa/epXbxCTVFBpCMdadz03MUGGlWGE02mll2qaYDLGQ9q1VGJBdZDNbp2iE6sMUBQrW9Kgmfp7IsNC64kIbafAZqQXvVz8z+umJroMMiaT1FBJ5ouilCMTo/xxNGCKEsMnlmCimL0VkRFWmBgbT8WG4C2+vEz8Rv2q7t6d15rXRRplOIJjOAUPLqAJt9ACHwiM4Ble4c0Rzovz7nzMW0tOMXMIf+B8/gCBXY1q</latexit>

162 <latexit sha1_base64="wH0M7QJjqSp8PlRxtG4UfjZXVFk=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mK+HErevFYwdhCG8tmu2mX7m7C7kYooX/BiwcVr/4ib/4bN20O2vpg4PHeDDPzwoQzbVz32ymtrK6tb5Q3K1vbO7t71f2DBx2nilCfxDxWnRBrypmkvmGG006iKBYhp+1wfJP77SeqNIvlvZkkNBB4KFnECDa55J0/NvrVmlt3Z0DLxCtIDQq0+tWv3iAmqaDSEI617npuYoIMK8MIp9NKL9U0wWSMh7RrqcSC6iCb3TpFJ1YZoChWtqRBM/X3RIaF1hMR2k6BzUgvern4n9dNTXQZZEwmqaGSzBdFKUcmRvnjaMAUJYZPLMFEMXsrIiOsMDE2nooNwVt8eZn4jfpV3b07qzWvizTKcATHcAoeXEATbqEFPhAYwTO8wpsjnBfn3fmYt5acYuYQ/sD5/AGEZY1s</latexit>

162 <latexit sha1_base64="wH0M7QJjqSp8PlRxtG4UfjZXVFk=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mK+HErevFYwdhCG8tmu2mX7m7C7kYooX/BiwcVr/4ib/4bN20O2vpg4PHeDDPzwoQzbVz32ymtrK6tb5Q3K1vbO7t71f2DBx2nilCfxDxWnRBrypmkvmGG006iKBYhp+1wfJP77SeqNIvlvZkkNBB4KFnECDa55J0/NvrVmlt3Z0DLxCtIDQq0+tWv3iAmqaDSEI617npuYoIMK8MIp9NKL9U0wWSMh7RrqcSC6iCb3TpFJ1YZoChWtqRBM/X3RIaF1hMR2k6BzUgvern4n9dNTXQZZEwmqaGSzBdFKUcmRvnjaMAUJYZPLMFEMXsrIiOsMDE2nooNwVt8eZn4jfpV3b07qzWvizTKcATHcAoeXEATbqEFPhAYwTO8wpsjnBfn3fmYt5acYuYQ/sD5/AGEZY1s</latexit>

322 <latexit sha1_base64="CiugEef6SQZd1JVgj1iLtfI55Dc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoN6KXjxWMLbQxrLZbtqlu5uwuxFK6F/w4kHFq7/Im//GTZuDtj4YeLw3w8y8MOFMG9f9dkorq2vrG+XNytb2zu5edf/gQcepItQnMY9VJ8Saciapb5jhtJMoikXIaTsc3+R++4kqzWJ5byYJDQQeShYxgk0unTUeG/1qza27M6Bl4hWkBgVa/epXbxCTVFBpCMdadz03MUGGlWGE02mll2qaYDLGQ9q1VGJBdZDNbp2iE6sMUBQrW9Kgmfp7IsNC64kIbafAZqQXvVz8z+umJroMMiaT1FBJ5ouilCMTo/xxNGCKEsMnlmCimL0VkRFWmBgbT8WG4C2+vEz8Rv2q7t6d15rXRRplOIJjOAUPLqAJt9ACHwiM4Ble4c0Rzovz7nzMW0tOMXMIf+B8/gCBXY1q</latexit>

322 <latexit sha1_base64="CiugEef6SQZd1JVgj1iLtfI55Dc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoN6KXjxWMLbQxrLZbtqlu5uwuxFK6F/w4kHFq7/Im//GTZuDtj4YeLw3w8y8MOFMG9f9dkorq2vrG+XNytb2zu5edf/gQcepItQnMY9VJ8Saciapb5jhtJMoikXIaTsc3+R++4kqzWJ5byYJDQQeShYxgk0unTUeG/1qza27M6Bl4hWkBgVa/epXbxCTVFBpCMdadz03MUGGlWGE02mll2qaYDLGQ9q1VGJBdZDNbp2iE6sMUBQrW9Kgmfp7IsNC64kIbafAZqQXvVz8z+umJroMMiaT1FBJ5ouilCMTo/xxNGCKEsMnlmCimL0VkRFWmBgbT8WG4C2+vEz8Rv2q7t6d15rXRRplOIJjOAUPLqAJt9ACHwiM4Ble4c0Rzovz7nzMW0tOMXMIf+B8/gCBXY1q</latexit>

642 <latexit sha1_base64="a1vfEnc2bFdl/Cy/RoBByqNz31Q=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkpftyKXjxWMLbQxrLZbtqlu5uwuxFK6F/w4kHFq7/Im//GTZuDtj4YeLw3w8y8MOFMG9f9dlZW19Y3Nktb5e2d3b39ysHhg45TRahPYh6rTog15UxS3zDDaSdRFIuQ03Y4vsn99hNVmsXy3kwSGgg8lCxiBJtcOm881vuVqltzZ0DLxCtIFQq0+pWv3iAmqaDSEI617npuYoIMK8MIp9NyL9U0wWSMh7RrqcSC6iCb3TpFp1YZoChWtqRBM/X3RIaF1hMR2k6BzUgvern4n9dNTXQZZEwmqaGSzBdFKUcmRvnjaMAUJYZPLMFEMXsrIiOsMDE2nrINwVt8eZn49dpVzb1rVJvXRRolOIYTOAMPLqAJt9ACHwiM4Ble4c0Rzovz7nzMW1ecYuYI/sD5/AGI+Y1v</latexit>

642 <latexit sha1_base64="a1vfEnc2bFdl/Cy/RoBByqNz31Q=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkpftyKXjxWMLbQxrLZbtqlu5uwuxFK6F/w4kHFq7/Im//GTZuDtj4YeLw3w8y8MOFMG9f9dlZW19Y3Nktb5e2d3b39ysHhg45TRahPYh6rTog15UxS3zDDaSdRFIuQ03Y4vsn99hNVmsXy3kwSGgg8lCxiBJtcOm881vuVqltzZ0DLxCtIFQq0+pWv3iAmqaDSEI617npuYoIMK8MIp9NyL9U0wWSMh7RrqcSC6iCb3TpFp1YZoChWtqRBM/X3RIaF1hMR2k6BzUgvern4n9dNTXQZZEwmqaGSzBdFKUcmRvnjaMAUJYZPLMFEMXsrIiOsMDE2nrINwVt8eZn49dpVzb1rVJvXRRolOIYTOAMPLqAJt9ACHwiM4Ble4c0Rzovz7nzMW1ecYuYI/sD5/AGI+Y1v</latexit>

642 <latexit sha1_base64="a1vfEnc2bFdl/Cy/RoBByqNz31Q=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkpftyKXjxWMLbQxrLZbtqlu5uwuxFK6F/w4kHFq7/Im//GTZuDtj4YeLw3w8y8MOFMG9f9dlZW19Y3Nktb5e2d3b39ysHhg45TRahPYh6rTog15UxS3zDDaSdRFIuQ03Y4vsn99hNVmsXy3kwSGgg8lCxiBJtcOm881vuVqltzZ0DLxCtIFQq0+pWv3iAmqaDSEI617npuYoIMK8MIp9NyL9U0wWSMh7RrqcSC6iCb3TpFp1YZoChWtqRBM/X3RIaF1hMR2k6BzUgvern4n9dNTXQZZEwmqaGSzBdFKUcmRvnjaMAUJYZPLMFEMXsrIiOsMDE2nrINwVt8eZn49dpVzb1rVJvXRRolOIYTOAMPLqAJt9ACHwiM4Ble4c0Rzovz7nzMW1ecYuYI/sD5/AGI+Y1v</latexit>

M <latexit sha1_base64="4aAPXWxyoZO2L1REZ5uCd/EHIaI=">AAAB8nicbVDLSgNBEOyNrxhfUY9eBoPgKWxC8HELevEiRHBNILuG2clsMmT2wUyvEJb8hhcPKl79Gm/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPHnScKsYdFstYdXyquRQRd1Cg5J1EcRr6krf90fXUbz9xpUUc3eM44V5IB5EIBKNoJPfWRRFyTc4aj/VeuWJX7RnIMqnlpAI5Wr3yl9uPWRryCJmkWndrdoJeRhUKJvmk5KaaJ5SN6IB3DY2o2eRls5sn5MQofRLEylSEZKb+nshoqPU49E1nSHGoF72p+J/XTTG48DIRJSnyiM0XBakkGJNpAKQvFGcox4ZQpoS5lbAhVZShialkQqgtvrxMnHr1smrfNSrNqzyNIhzBMZxCDc6hCTfQAgcYJPAMr/BmpdaL9W59zFsLVj5zCH9gff4AMG+Qqg==</latexit>

 642

- summation - concatenation - conv, stride = 1

- conv, stride = 2

- de-conv, stride = 2

Figure 1: A typical u-net module with outer residual connection. M is the number of input features. Across the u-net module, each layer has the same number of output feature maps (except for the final 1 × 1 filter), which we denote N . For better understanding the figure also includes the field of view (FoV) of each convolutional kernel (top) and the feature map size at the output of each filter (bottom), assuming a 64 × 64 input I. Best viewed in color.
fixed number of feature maps, N . To mitigate high frequency noise from the sampling operation, each strided conv/de-conv layer is followed by a convolution. Unlike traditional u-nets, the design of convolutional layers in our u-net module helps in retaining the original size of the feature maps at its output Consequently, multiple u-net modules can be stacked without loosing resolution.
In the following, we briefly highlight some of the design choices of the architecture. In comparison to traditional u-nets, the max-pooling operation is replaced with strided convolution for SUNets. The use of strided convolutions enables different filters in each u-net module to operate at different resolutions (see the discussion in Section 5). Moreover, the repeated use of max-pooling operations can cause gridding artifacts in dilated networks (Yu et al., 2017).
Unlike the u-nets of Ronneberger et al. (2015); Newell et al. (2016), our u-net module is comprised of only two levels of depth. We considered two major factors in choosing the depth: field of view (FoV) of the innermost conv filter and the total number of parameters in a single u-net module. The number of parameters influences the total number of stacks in SUNets. While keeping the total parameters of SUNet approximately constant, we experimented with a higher depth of three and four. We found that the increase in depth indeed led to a decline in performance for the image classification task. This may not be surprising, given that a SUNet with depth of two is able to stack more u-net modules. Moreover, deeper u-net modules make it harder to train the inner-most convolutional layers due to the vanishing gradients problem (Bengio et al., 1994). For instance, in our current design, the maximum length of the gradient path is six. The popular classification networks of He et al. (2016a); Huang et al. (2017) are known to operate primarily on features with 282 and 142 resolution. At this scale, the effective FoV of 19 is more than sufficient to capture long-distance contextual information. Moreover, the stacking of multiple u-net modules will also serve to increase the effective FoV of higher layers.
SUNets train best when there is sufficient gradient flow to the bottom-most u-net layers. To avoid vanishing gradients, we include a skip connection (He et al., 2016a; Huang et al., 2017) around each u-net module. Also, inspired by the design of bottleneck blocks (He et al., 2016a), we also include 1×1 convolutional layers. Bottleneck layers restricts the number of input features to a small number (N ), avoiding parameter inflation.
When stacking multiple u-nets it makes sense for each u-net module to reuse the raw feature maps from all the preceding u-net modules. Thus we also explored replacing the identity connection with dense connectivity (Huang et al., 2017). This new network is memory intensive which in turn prevented proper learning of the batch-norm parameters. Instead, we chose to utilize dense connectivity only within each u-net, i.e., while reusing feature maps from E1 at D1. Thus the proposed u-net module leverages skip connectivity without getting burdened.
4

Under review as a conference paper at ICLR 2019

4 SUNETS: STACKED U-NETS FOR CLASSIFICATION

Layers Output Size

SUNet-64

SUNet-128

SUNet-7-128

Convolution 112 × 112

7 × 7 conv, 64, stride 2

Residual Block

56 × 56

3 × 3 conv, 128, stride 2 3 × 3 conv, 128, stride 1

×1

UNets Block (1)

56 × 56

 1 × 1 conv, 64 

1 × 1 conv, 128

1 × 1 conv, 128

 U-Net, N=64  × 2  U-Net, N=128  × 2  U-Net, N=128  × 2

1 × 1 conv, 256

1 × 1 conv, 512

1 × 1 conv, 512

Transition Layer 28 × 28

2 × 2 average pool, stride 2

UNets Block (2)

28 × 28

 1 × 1 conv, 64 

 1 × 1 conv, 128   1 × 1 conv, 128 

 U-Net, N=64  × 4  U-Net, N=128  × 4  U-Net, N=128  × 7

1 × 1 conv, 512

1 × 1 conv, 1024

1 × 1 conv, 1280

Transition Layer 14 × 14

2 × 2 average pool, stride 2

UNets Block (3)

14 × 14

 1 × 1 conv, 64 

 1 × 1 conv, 128   1 × 1 conv, 128 

 U-Net, N=64  × 4  U-Net, N=128  × 4  U-Net, N=128  × 7

1 × 1 conv, 768

1 × 1 conv, 1536

1 × 1 conv, 2048

Transition Layer 7 × 7

2 × 2 average pool, stride 2

UNets Block (4)

7×7

 1 × 1 conv, 64   1 × 1 conv, 128   1 × 1 conv, 128 

 U-Net+, N=64  × 1  U-Net+, N=128  × 1  U-Net+, N=128  × 1

1 × 1 conv, 1024

1 × 1 conv, 2048

1 × 1 conv, 2304

Classification Layer

1×1

7 × 7 global average pool 1000D fully-connected, softmax

Total Layers

110

110

170

Params

6.9M

24.6M

37.7M

Table 1: SUNet architectures for ImageNet. N denotes the number of filters per convolutional layer. Note that the building block in bracket refers to the integrated u-net module shown in Figure 1.
Before addressing segmentation, we describe a stacked u-net (SUNet) architecture that is appropriate for image classification. Because the amount of labeled data available for classification is much larger than for segmentation, classification tasks are often used to pre-train feature extraction networks, which are then adapted to perform segmentation.
The network design of SUNets for ImageNet classification is summarized in Table 1. Note that each "conv" layer shown in the table corresponds to a sequence of "BN-ReLU-Conv" layers. The three listed configurations mainly differ in the number of output feature maps N of each convolutional layer and the total number of stacks in blocks 2 and 3. Input images are processed using a 7 × 7 conv filter followed by a residual block. Inspired by the work on dilated resnet (Yu et al., 2017), the conventional max-pooling layer at this stage is replaced by a strided convolutional layer inside the residual block. Subsequently, the feature maps are processed bottom-up as well as top-down by multiple stacks of u-nets at different scales and with regularly decreasing resolutions. The feature map input size to block 4 is 7 × 7 and is further reduced to 2 × 2 at the input to the encoder E2 of the u-net module. At this resolution, it is not possible to have E2 and D2 layers, and hence a trimmed version of u-nets (u-net+) are employed in block 4. The u-net+ includes a single level of encoder and decoder (E1, D1) processing. Following block 4, a global average pooling is performed on features and transferred to a softmax classifier.
The residual connection in all but the first u-net in each block is implemented as an identity mapping. In the first u-nets the skip connection is implemented using an expansion layer i.e., a 1 × 1 conv filter. The number of feature map outputs from each block approximately equates to the total number of feature maps generated by all the preceding u-net modules. This arrangement allows flexibility for the network to retain all the raw feature maps of the preceding modules. Moreover among all other possibilities, the above architectures were picked because their performance on the image classification task is roughly equivalent to the ResNet-18, 50 and 101 network architectures (discussed in Section 6), albeit with fewer parameters.

5

Under review as a conference paper at ICLR 2019

Dilation

1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit>

2 <latexit sha1_base64="yGavS8T9crd9pKlDZQjhp0UQdt4ZPC6fxfKBM6liMO+IFWTV8Pwug=">AAAB453icbVBNT8JAEJ3i6iF9fYivF9eOvpSylIkZzHh4xRIiFo0uX69Uob3o0x4hSEMSmKVykiTQgkIdOt0lyChiZXvtbbtbtnO7dNmSpCENGvX+8DCLFBg/xXqvqf//iVLmvv/h3GsBXH6hERHB8yl0SQzvy878t15MkZlu6aUFCqea6DNau53O6037ls7Ya3+Nrsbe2m1d6Xdq6o7p7t3+97wteHFxRxzWjox941fmdiJuIGpAhUj5tFLRqrKIo6R1ISdiUo4uxMETBwfIc7CCObwkKyaRqJkJcLSiATwHTWY75nvZfu3c77ZlCZeaXmpfiDbDwT3DkxMOSEDmjiA4SlPjOazPiGjxSvkqfNtQera39SuNdbWwvuFHyDGrSxVeSA1KWHpEQuoF1Bmv7a/sLV/TGyFmQseoiD1REMaUJKq1jW7vXpce9ZNsTZKBDKTcZTCZgwT5OCv3Zz3jMoR0llpEZWzrM6CnxqKW6SlJksqajoDYgn3Hx+oj6JxSbcWZU2jViAVokNmTSZkhiobzMU13xdM8TFTObYS2e1JnpsHSthT7YKgypZG6e1VlvlbLvy7bn+59X3UITzXE41U0FOl1ZldpuUZlLCLyloxjagIoXxEK8QRk/ZjPYUZ1cGoXXCFMizKIiklJJlCZlQupbym8VslTbBEVQVlxZmcbZmjU2h7DYh81eMZfsXvSrdxBK/sXrDeu8ueuuau1m6ztXKIFTKp3FGAGCFU+zDiDJC/bTDgEDthopQwABA0O3wEFg3QHiDCdM7+fzCJmeX/PUo+lvoD0jvVzp5swe4igdTc90pwPZnk78AggD5PGzLPHYA+=n=Mj</Icla=t</exlait>exit>

4 <latexit sha1_base64="46SWzbUbqiG4Kt0cvm0m1bEHeeI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUvOiVK27VnYOsEi8nFcjR6JW/uv2EZTFKwwTVuuO5qQkmVBnOBE5L3UxjStmIDrBjqaQx6mAyP3RKzqzSJ1GibElD5urviQmNtR7Hoe2MqRnqZW8m/ud1MhNdBRMu08ygZItFUSaIScjsa9LnCpkRY0soU9zeStiQKsqMzaZkQ/CWX14lfq16XXWbtUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/s0oyJ</latexit>

4 <latexit sha1_base64="46SWzbUbqiG4Kt0cvm0m1bEHeeI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUvOiVK27VnYOsEi8nFcjR6JW/uv2EZTFKwwTVuuO5qQkmVBnOBE5L3UxjStmIDrBjqaQx6mAyP3RKzqzSJ1GibElD5urviQmNtR7Hoe2MqRnqZW8m/ud1MhNdBRMu08ygZItFUSaIScjsa9LnCpkRY0soU9zeStiQKsqMzaZkQ/CWX14lfq16XXWbtUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/s0oyJ</latexit>

8 <latexit sha1_base64="3ZYpuMaHocy2UImapsblr2mF/zQ=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIgeN2BFtLCHxhAQuZG+Zg5W9vcvungm58AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLEsG1cd1vp7CxubW9U9wt7e0fHB6Vj08edJwqhh6LRay6AdUouETPcCOwmyikUSCwE0xu537nCZXmsbw30wT9iI4kDzmjxkrtxqBccavuAmSd1HJSgRytQfmrP4xZGqE0TFCtezU3MX5GleFM4KzUTzUmlE3oCHuWShqh9rPFoTNyYZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTdjwMy6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N5K2JgqyozNpmRDqK2+vE68evW66rbrleZNnkYRzuAcLqEGV9CEO2iBBwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDy3oyN</latexit>

8 <latexit sha1_base64="3ZYpuMaHocy2UImapsblr2mF/zQ=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIgeN2BFtLCHxhAQuZG+Zg5W9vcvungm58AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLEsG1cd1vp7CxubW9U9wt7e0fHB6Vj08edJwqhh6LRay6AdUouETPcCOwmyikUSCwE0xu537nCZXmsbw30wT9iI4kDzmjxkrtxqBccavuAmSd1HJSgRytQfmrP4xZGqE0TFCtezU3MX5GleFM4KzUTzUmlE3oCHuWShqh9rPFoTNyYZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTdjwMy6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N5K2JgqyozNpmRDqK2+vE68evW66rbrleZNnkYRzuAcLqEGV9CEO2iBBwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDy3oyN</latexit>

4 <latexit sha1_base64="46SWzbUbqiG4Kt0cvm0m1bEHeeI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUvOiVK27VnYOsEi8nFcjR6JW/uv2EZTFKwwTVuuO5qQkmVBnOBE5L3UxjStmIDrBjqaQx6mAyP3RKzqzSJ1GibElD5urviQmNtR7Hoe2MqRnqZW8m/ud1MhNdBRMu08ygZItFUSaIScjsa9LnCpkRY0soU9zeStiQKsqMzaZkQ/CWX14lfq16XXWbtUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/s0oyJ</latexit>

4 <latexit sha1_base64="46SWzbUbqiG4Kt0cvm0m1bEHeeI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUvOiVK27VnYOsEi8nFcjR6JW/uv2EZTFKwwTVuuO5qQkmVBnOBE5L3UxjStmIDrBjqaQx6mAyP3RKzqzSJ1GibElD5urviQmNtR7Hoe2MqRnqZW8m/ud1MhNdBRMu08ygZItFUSaIScjsa9LnCpkRY0soU9zeStiQKsqMzaZkQ/CWX14lfq16XXWbtUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/s0oyJ</latexit>

2 <latexit sha1_base64="yGavS89Trc9dKplDQZhjp0QUdt4ZPC6fxfBK6MliOM+IWFTV8Pwug=">AAAB453icbVBNT8JAEJ3i6iF9fYivF9eOvpSylkIzZhHx4RIFi0ouX96oUb3o04xShMESmVKkyiTQgIkOd0tlyChZiXvbttbbtnOd7NmSpECGNvX+8DCFLBg/xqXqvf//iLVmvv/3hsGBX6HhERH8BylS0Qzvy788t15MkZlu6aUFCqeaD6Na5u3O0637lsY7a3+Nrseb2md1X6qd6op7t7+397wtHexFRxzWjxo94f1mdiJuIpGAhjUt5FLRqKroIR6I1SdiU4oxuMEBTwfcI7CCObwKkayRqJkJcLSiATwHTWY57nvfZ3uc7Z7lCeZaXmpfibDDwT3DkMxOSEDjmAiS4PljOazPijGxSvkqfNtQear93SudNbWwvFuyHGDSrVxSe1AKWHpEQuo1FBm7va/sLV/GTFymQesoiD1REMaJUKq1j7WvXcpe9ZNTsKZDBTKcZTCgZwT5OCv3Zz3MjoR0llpEZWzrM6CnxqKW6SlkJqsjaoDYgn3xH+oj6JxSbcZWU2VjiAVoNkmTZShkiozbUM31dx8MTFTOYb2Se1JnpsHSthT7KYgypZ6G1elVlvbLyvb7+n95X3IUzTXE410UFlO1ZldupUZLlLClyxoajIgXoEx8KQRkZ/jPUY1ZcGoXXCFMizKIiklJlJCZlQupbym8VlsbTEBVQVlxZmcZbjm2U7hDY8h1eZMfsXvSrdxBKs/XrDue8ueuuau1m6ztXKFITKp3FAGGCFU+zDiDCJ/bTDgEDhtpoQwABA03OEwFg3QiHDCMd7+fzJCemX/UP+olvoDj0vVzp5sew4igdTc90wpPZnk78AggDP5zGPLHY+A=nM=j</Ilc=at</elxait>exit>

1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit>

Conv <latexit sha1_base64="uFnu0W2rH6XT285//BFnvu7y1QQ=">AAACAXicbVA9TwJBEN3DL8Qv1MrYbAQTC0PuaNSOSGOJUYQELmRv2YMNe7uX3TkiIcTGv2JjocbWf2Hnv3GBKxR9ySQv781kZl4QC27Adb+czNLyyupadj23sbm1vZPf3bszKtGU1akSSjcDYpjgktWBg2DNWDMSBYI1gkF16jeGTBuu5C2MYuZHpCd5yCkBK3XyB1Ulh7jotYFHzGCveIpvVAgRue/kC27JnQH/JV5KCihFrZP/bHcVTSImgQpiTMtzY/DHRAOngk1y7cSwmNAB6bGWpZLYhf549sIEH1uli0OlbUnAM/XnxJhExoyiwHZGBPpm0ZuK/3mtBMJzf8xlnACTdL4oTAQGhad54C7XjIIYWUKo5vZWTPtEEwo2tZwNwVt8+S+pl0sXJfe6XKhcpmlk0SE6QifIQ2eogq5QDdURRQ/oCb2gV+fReXbenPd5a8ZJZ/bRLzgf30kOlaI=</latexit>

Conv 7x7 stride 2

Residual Block

U-Net+ <latexit sha1_base64="Qo5uCPMOGzc+qv4VaddwBSHVD1E=">AAAB73icbVBNTwIxEJ3FL8Qv1KOXRjAxMZJdLuqN6MWTwcQVDKykWwo0tN1N2zUhG36FFw9qvPp3vPlvLLAHBV8yyct7M5mZF8acaeO6305uaXlldS2/XtjY3NreKe7u3esoUYT6JOKRaoZYU84k9Q0znDZjRbEIOW2Ew6uJ33iiSrNI3plRTAOB+5L1GMHGSg/+6Q015ceTcqdYcivuFGiReBkpQYZ6p/jV7kYkEVQawrHWLc+NTZBiZRjhdFxoJ5rGmAxxn7YslVhQHaTTg8foyCpd1IuULWnQVP09kWKh9UiEtlNgM9Dz3kT8z2slpncepEzGiaGSzBb1Eo5MhCbfoy5TlBg+sgQTxeytiAywwsTYjAo2BG/+5UXiVysXFfe2WqpdZmnk4QAO4Rg8OIMaXEMdfCAg4Ble4c1Rzovz7nzMWnNONrMPf+B8/gCPEo8f</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>
Pooling <latexit sha1_base64="oHq5k8nnxCAY8DMjW45agAzHYps=">AAAB7XicbVBNTwIxFHzrJ+IX6tFLIzHxRBYu6o3oxSMmrpDAhnTLW2jotpu2a0I2/AgvHtR49f94899YYA8KTtJkMvMmfW+iVHBjff/bW1vf2NzaLu2Ud/f2Dw4rR8ePRmWaYcCUULoTUYOCSwwstwI7qUaaRALb0fh25refUBuu5IOdpBgmdCh5zBm1Tmq3lHLJYb9S9Wv+HGSV1AtShQKtfuWrN1AsS1BaJqgx3bqf2jCn2nImcFruZQZTysZ0iF1HJU3QhPl83Sk5d8qAxEq7Jy2Zq78TOU2MmSSRm0yoHZllbyb+53UzG1+FOZdpZlGyxUdxJohVZHY7GXCNzIqJI5Rp7nYlbEQ1ZdY1VHYl1JdPXiVBo3Zd8+8b1eZN0UYJTuEMLqAOl9CEO2hBAAzG8Ayv8Oal3ov37n0sRte8InMCf+B9/gDQxI9p</latexit> U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit> U-Net <latexit sha1_base64="pmw+8NBAYOWi/NC1jWKit6hf8JQ=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4sSS9qLeiF09SwdhCG8pmO2mXbjZhdyOU0N/gxYOKV/+QN/+N2zYHbX0w8Hhvhpl5YSq4Nq777aysrq1vbJa2yts7u3v7lYPDR51kiqHPEpGodkg1Ci7RN9wIbKcKaRwKbIWjm6nfekKleSIfzDjFIKYDySPOqLGS75/foelVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNfe+Xm1cF2mU4BhO4Aw8uIAG3EITfGDA4Rle4c2Rzovz7nzMW1ecYuYI/sD5/AG3Uo4m</latexit>

Conv <latexit sha1_base64="mFVNfS4yvKeKZqyUMz9ydbQHUGI=">AAAB+HicbVA9TwJBEJ3DL8SvU0ubjWBiRe6gUDsijSUmoiRwIXvLHmzY273s7pGQC//ExkKNrT/Fzn/jAlco+JJJXt6bycy8MOFMG8/7dgobm1vbO8Xd0t7+weGRe3zyqGWqCG0TyaXqhFhTzgRtG2Y47SSK4jjk9CkcN+f+04QqzaR4MNOEBjEeChYxgo2V+q7blGKCKvWeYTHVqF7pu2Wv6i2A1omfkzLkaPXdr95AkjSmwhCOte76XmKCDCvDCKezUi/VNMFkjIe0a6nAdk+QLS6foQurDFAklS1h0EL9PZHhWOtpHNrOGJuRXvXm4n9eNzXRdZAxkaSGCrJcFKUcGYnmMaABU5QYPrUEE8XsrYiMsMLE2LBKNgR/9eV10q5Vb6refa3cuM3TKMIZnMMl+HAFDbiDFrSBwASe4RXenMx5cd6dj2VrwclnTuEPnM8f+lmSHg==</latexit>

De-gridding Filters

Conv <latexit sha1_base64="mFVNfS4yvKeKZqyUMz9ydbQHUGI=">AAAB+HicbVA9TwJBEJ3DL8SvU0ubjWBiRe6gUDsijSUmoiRwIXvLHmzY273s7pGQC//ExkKNrT/Fzn/jAlco+JJJXt6bycy8MOFMG8/7dgobm1vbO8Xd0t7+weGRe3zyqGWqCG0TyaXqhFhTzgRtG2Y47SSK4jjk9CkcN+f+04QqzaR4MNOEBjEeChYxgo2V+q7blGKCKvWeYTHVqF7pu2Wv6i2A1omfkzLkaPXdr95AkjSmwhCOte76XmKCDCvDCKezUi/VNMFkjIe0a6nAdk+QLS6foQurDFAklS1h0EL9PZHhWOtpHNrOGJuRXvXm4n9eNzXRdZAxkaSGCrJcFKUcGYnmMaABU5QYPrUEE8XsrYiMsMLE2LBKNgR/9eV10q5Vb6refa3cuM3TKMIZnMMl+HAFDbiDFrSBwASe4RXenMx5cd6dj2VrwclnTuEPnM8f+lmSHg==</latexit>

1  1,

Softmax

33

33

Output stride Dilation factor
Level

2 <latexit sha1_base64="GyavS8T9cr9dKplDQZhjp0UQdt4ZPCf6fxBK6MliMO+IFWTV8Pwug=">AAAB453icbVBNT8JAEJ36iiFf9YivFe9OvpSylIkzZHhx4RIFio0uX69Uo3bo04xShMEmSKVkyTigQIkOd0tlyChZiXvtbtbbtOnd7NmSpECGNvX+8DCLFBg/xqXqvf//iLVmvv/h3GsBXH6hEHRB8yl0SQzvy788t15kMZlu6aUFCqeaD6aN5u3O0673slY73a+Nsreb2m1d6Xdq6op7t73+79wteHxFRxWzjxo94f1dmJiuIpGAhjU5tFLqRrKoIR61ISdiU4ouxMETBwfIcC7COwbKkyaqRJkJcSLiAwTHTWY57nvfZu3c7Z7lCeZaXmpfiDbDw3TDkxMSODEjmiAS4PlOjazPiGjxSvkfqNtQear39SuNdbWwvuFHyDGrSxVSe1AKWpHEQuo1FBmv7a/sLV/TGyFQmesio1DREaMUJKqj1W7Xvcp9eZNsTKZDBKTcZCTZgwT5OCvZ3z3jMRol0plEZzWrMC6nxqKW6SlJksqjaoDgYn3xH+o6jJxScbZWU2jViAVoNkTmSZhkiozbUM13xd8MTFTObY2Se1nJpsSHhtT7KYygZp6G1elVlvLbyv7b+n953XIUTzXE41U0FOlZ1dlupUZlLCLyloxjagIXoExK8RQkZ/jPYUZ1cGoXXCFMizKIkilJlJZClQpubym8VslTbBEVQlVxZcmZbjmU2h7YD8he1MZfsXvSrdxBKs/XrDeu8ueuauu16mztKXIFTKp3FGAGCFU+zDiDJCb/DTgEDhtopQwBAA0O3EwgF3QiHCDMd+7fzCJme/XPU+ovlDo0jVvzp5swei4dgTc09wpZPnk87AgDgP5GzLPHYA+=n=Mj</Icla=<te/xlaite>xit>
1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit>

1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit>

4 <latexit sha1_base64="46SWzbUbqiG4Kt0cvm0m1bEHeeI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUvOiVK27VnYOsEi8nFcjR6JW/uv2EZTFKwwTVuuO5qQkmVBnOBE5L3UxjStmIDrBjqaQx6mAyP3RKzqzSJ1GibElD5urviQmNtR7Hoe2MqRnqZW8m/ud1MhNdBRMu08ygZItFUSaIScjsa9LnCpkRY0soU9zeStiQKsqMzaZkQ/CWX14lfq16XXWbtUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/s0oyJ</latexit>
1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit>

8 <latexit sha1_base64="3ZYpuMaHocy2UImapsblr2mF/zQ=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIgeN2BFtLCHxhAQuZG+Zg5W9vcvungm58AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLEsG1cd1vp7CxubW9U9wt7e0fHB6Vj08edJwqhh6LRay6AdUouETPcCOwmyikUSCwE0xu537nCZXmsbw30wT9iI4kDzmjxkrtxqBccavuAmSd1HJSgRytQfmrP4xZGqE0TFCtezU3MX5GleFM4KzUTzUmlE3oCHuWShqh9rPFoTNyYZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTdjwMy6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N5K2JgqyozNpmRDqK2+vE68evW66rbrleZNnkYRzuAcLqEGV9CEO2iBBwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDy3oyN</latexit>
1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit>
2 <latexit sha1_base64="yGavS89Tcrd9KplDZQjhp0UQdt4ZCPf6fxBK6MliMO+IWFTV8Pwug=">AAAB453icbVBNT8JAEJ36iFif9iYFve9OvpSlykIZzHhx4RIFio0Xu69Uo3b0ox4ShEMmSVKkyiTgQIkOd0tlyChZiXvbttbbtnOd7NmSpECGNXv8+CDLFgB/xXqqvf/i/VLvmv/3hsGXBH6hERH8BylS0zQvy878t15MkZlu6aUFCqae6DNau53O0637slY73aN+sreb2m1d6Xqd6op7t7+397tweHFxRxzWjxo941fmdiJIuGphAjUt5FLRqrKoIR6I1SdiUo4uxEMTBfwcI7COCbwkKayRqJkcJLSAiTwTHWY75nvfZ3uc77ZlCeZaXpmifDbDwT3kDMxSOEDmjAiS4PlOjaziPGjxSvkfqtNQear39uSdNbWvwuFyHGDrSxVSe1AWKpHEQuo1FBm7va/sLV/TGFyQmesio1DERMaUJKq1j7WvXcpe9NZsTZKDBKTZcCTZgwT5OCv3Z3zMjRol0plEZzWrMC6nxqKW6SlJksqjaDoYgn3Hxo+j6JxSbcZW2UjVAiVoNkTmZShkiozbMU13xdM8FTOTbYS21eJnpsSHthT7KYgyZp6G1elVvlLbvy7bn+593XUIzTEX14U0FlOZ1ldupUZLlLClyxojagIXoExK8RQkZ/jPYUZ1cGXoXCMFizKIiklJJlCZlQpubymV8lsbTEBQVVlxZmcbZjm2Uh7DY8he1MZfsXvrSdxBKs/XrDeu8ueuua1um6tzKXFIKTp3FAGGCFUz+DiDJCb/TDgDEthopwQABA0O3wEgF3QiHCDdM+7fzJCme/XPU+ovloD0jvVpzs5we4idgTc90wpPZnk78gADg5PGzPLHYA+n==Mj</Icla=<t/elxiat>exit>

8 <latexit sha1_base64="3ZYpuMaHocy2UImapsblr2mF/zQ=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIgeN2BFtLCHxhAQuZG+Zg5W9vcvungm58AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLEsG1cd1vp7CxubW9U9wt7e0fHB6Vj08edJwqhh6LRay6AdUouETPcCOwmyikUSCwE0xu537nCZXmsbw30wT9iI4kDzmjxkrtxqBccavuAmSd1HJSgRytQfmrP4xZGqE0TFCtezU3MX5GleFM4KzUTzUmlE3oCHuWShqh9rPFoTNyYZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTdjwMy6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N5K2JgqyozNpmRDqK2+vE68evW66rbrleZNnkYRzuAcLqEGV9CEO2iBBwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDy3oyN</latexit>
1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit>
3 <latexit sha1_base64="ZgOBONPC9+QqObn0T7gaBB49z3Q=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lVUG9FLx5bMLbQhrLZTtq1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZekAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRYLGLVDqhGwSV6hhuB7UQhjQKBrWB0O/VbT6g0j+W9GSfoR3QgecgZNVZqnvfKFbfqzkCWSS0nFcjR6JW/uv2YpRFKwwTVulNzE+NnVBnOBE5K3VRjQtmIDrBjqaQRaj+bHTohJ1bpkzBWtqQhM/X3REYjrcdRYDsjaoZ60ZuK/3md1IRXfsZlkhqUbL4oTAUxMZl+TfpcITNibAllittbCRtSRZmx2ZRsCLXFl5eJd1a9rrrNi0r9Jk+jCEdwDKdQg0uowx00wAMGCM/wCm/Oo/PivDsf89aCk88cwh84nz/r74yK</latexit>

8 <latexit sha1_base64="3ZYpuMaHocy2UImapsblr2mF/zQ=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIgeN2BFtLCHxhAQuZG+Zg5W9vcvungm58AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLEsG1cd1vp7CxubW9U9wt7e0fHB6Vj08edJwqhh6LRay6AdUouETPcCOwmyikUSCwE0xu537nCZXmsbw30wT9iI4kDzmjxkrtxqBccavuAmSd1HJSgRytQfmrP4xZGqE0TFCtezU3MX5GleFM4KzUTzUmlE3oCHuWShqh9rPFoTNyYZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTdjwMy6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N5K2JgqyozNpmRDqK2+vE68evW66rbrleZNnkYRzuAcLqEGV9CEO2iBBwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDy3oyN</latexit>
2 <latexit sha1_base64="GyvaS8T9rcd9pKlDZQhjp0QUdt4ZCP6fxfKB6MilOMI+FWTV8Puwg=">AAAB453icbVBNT8JAEJ3i6Fif9YivF9evOSpylkIZzHhx4IRiFo0uX69Uob3o0x4hSEMSmKVkyiTgQkIdOt0lyChiZvXbtbtbtOnd7NmSpECNGXv8+CDLFBgx/qXqvf/i/LVvmv/h3GsXBH6EhHR8BylS0zQyv878t15kMZul6aFUCqea6DNa5u3O0673lsY73aN+srbem2d16Xdq6op7t73+79wteHxFRxWzjxo94f1dmiJIuGpAhUj5tFLqRrKoIR61IdSiUo4xuMEBTfwIc7COCwbKkayqRkJJcSLAiTwTHWY75nvZfu3c77ZlCeZXapmfiDbDwT3DkxMOSEDjmiAS4lPOjazPiGjSxvkfqNtQera93uSNdbWwvFuyHDGrSxVSeA1KWHpEQuo1FmB7v/asL/VGTFyQmesioD1REMaUJKq1jW7Xvpce9ZNsTKZDBTKcZCTZgwT5OvCZ33zjMRol0plZEWzMr6CxnqKW6SlkJqsajoDYg3nHx+oj6JxSbcZW2UjViAVokNTmZShkiozbUM31xdM8FTTObY2S1eJnpsSHht7TYKygZp6Ge1VllvLbyvb7+n59X3UITzXE14U0FOl1ZldupUZlLLClyxojagIXoxEK8QRk/ZjPYUZ1cGoXCXMFizKIkilJlJCZlQupbymV8slbTEBVQlVxZcmbZmj2Uh7DYh81eZMfsvXSrdxBK/sXrDeuu8ueua1u6mtzXKIFKTp3FGAGCUF+zDiDJCb/DTgDEhtpoQwAB0A3OEwFg3QHiDCdM+7zfCJemX/PUo+vloDj0Vvpzs5wei4gdcT90wpZPnk78gADg5PGzLPYHA+=n=Mj<I/lca=t<e/xliat>exit> 4 <latexit sha1_base64="46SWzbUbqiG4Kt0cvm0m1bEHeeI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUvOiVK27VnYOsEi8nFcjR6JW/uv2EZTFKwwTVuuO5qQkmVBnOBE5L3UxjStmIDrBjqaQx6mAyP3RKzqzSJ1GibElD5urviQmNtR7Hoe2MqRnqZW8m/ud1MhNdBRMu08ygZItFUSaIScjsa9LnCpkRY0soU9zeStiQKsqMzaZkQ/CWX14lfq16XXWbtUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/s0oyJ</latexit>

8 <latexit sha1_base64="3ZYpuMaHocy2UImapsblr2mF/zQ=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIgeN2BFtLCHxhAQuZG+Zg5W9vcvungm58AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLEsG1cd1vp7CxubW9U9wt7e0fHB6Vj08edJwqhh6LRay6AdUouETPcCOwmyikUSCwE0xu537nCZXmsbw30wT9iI4kDzmjxkrtxqBccavuAmSd1HJSgRytQfmrP4xZGqE0TFCtezU3MX5GleFM4KzUTzUmlE3oCHuWShqh9rPFoTNyYZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTdjwMy6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N5K2JgqyozNpmRDqK2+vE68evW66rbrleZNnkYRzuAcLqEGV9CEO2iBBwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDy3oyN</latexit>
4 <latexit sha1_base64="46SWzbUbqiG4Kt0cvm0m1bEHeeI=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUvOiVK27VnYOsEi8nFcjR6JW/uv2EZTFKwwTVuuO5qQkmVBnOBE5L3UxjStmIDrBjqaQx6mAyP3RKzqzSJ1GibElD5urviQmNtR7Hoe2MqRnqZW8m/ud1MhNdBRMu08ygZItFUSaIScjsa9LnCpkRY0soU9zeStiQKsqMzaZkQ/CWX14lfq16XXWbtUr9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/s0oyJ</latexit>
5 <latexit sha1_base64="Cq7a9GxUBIxAJxnxbQ8E5/eAYCM=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lFUW9FLx5bMLbQhrLZTtq1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZekAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRYLGLVDqhGwSV6hhuB7UQhjQKBrWB0O/VbT6g0j+W9GSfoR3QgecgZNVZqXvTKFbfqzkCWSS0nFcjR6JW/uv2YpRFKwwTVulNzE+NnVBnOBE5K3VRjQtmIDrBjqaQRaj+bHTohJ1bpkzBWtqQhM/X3REYjrcdRYDsjaoZ60ZuK/3md1IRXfsZlkhqUbL4oTAUxMZl+TfpcITNibAllittbCRtSRZmx2ZRsCLXFl5eJd1a9rrrN80r9Jk+jCEdwDKdQg0uowx00wAMGCM/wCm/Oo/PivDsf89aCk88cwh84nz/u9YyM</latexit>

8 <latexit sha1_base64="3ZYpuMaHocy2UImapsblr2mF/zQ=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIgeN2BFtLCHxhAQuZG+Zg5W9vcvungm58AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLEsG1cd1vp7CxubW9U9wt7e0fHB6Vj08edJwqhh6LRay6AdUouETPcCOwmyikUSCwE0xu537nCZXmsbw30wT9iI4kDzmjxkrtxqBccavuAmSd1HJSgRytQfmrP4xZGqE0TFCtezU3MX5GleFM4KzUTzUmlE3oCHuWShqh9rPFoTNyYZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTdjwMy6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N5K2JgqyozNpmRDqK2+vE68evW66rbrleZNnkYRzuAcLqEGV9CEO2iBBwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDy3oyN</latexit>
2 <latexit sha1_base64="GyvaS8T9cr9dpKlDZQjhp0QUtd4ZCP6ffxBK6MliOMI+WFTVP8uwg=">AAAB453icbVBNT8JAEJ3i6Fi9fiYvFe9OvSplykIzZhHx4RIiF0ouX69Uo3bo04xShMESmVKykTigQIkOdt0lyChZivXbttbbtOnd7mNSpECGNvX8+CDFLBg/xXqvqf/i/LVmvv/3hGsBXH6EhRH8BlyS0zQyv87t815MkZlu6aUFCqae6DNa5u3O6037lsY73aN+rseb2m1d6Xqdo67p7t3+97wtHeFxxRWzjox491fmdiJuIpGAhjUt5FLRqrKIoR61IdSiUo4xuMEBTwfIc7CCOwbKkyaRqJkcJLSiAwTTHYW75vnfZu3c77ZlCZeXapmfiDbDw3TDkMxOSEDjmiA4SlPOjaziPGjSxkvqfNtQear39uSNdWbvwFuyHDGrSxVSe1AKWHpEQou1FmB7v/asL/VTGFyQmesoiD1REMaUJKq1j7WvXpce9ZNTsKZDBKTcZCTgZwT5OCvZ33zjMoR0llpEZzWrMC6nxqKW6SlkJqsajDogYn3Hxo+6jJxSbcWZ2UVjiAoVkNmTSZkhiobzMU31xdM8TFTObY2S1enJpsSHthT7YKgyZp6Ge1lVvlLbyv7bn+59X3IUTzEX410UFOl1ZdlupUZLlLClyoxajgIXoxEK8RQk/ZjPYU1ZcGoXXCMFziKIkilJlJCZQlupbym8VlsTbEBQVVlZxcmbZjmU2h7YD8h1eMZsfXvSrdxKB/sXDreu8ueuau1um6ztKXIFTKpF3AGCGFU+zDiDCJ/bTDgDEthpoQwBAA0O3wEFg3QHiDCdM+7fzJCme/XUPo+lvDo0jVvpzs5ew4idgTc09wpZPnk78AgDg5PzGPLHYA+=nM=<j/Icla=<te/lxiate>xit>

8 <latexit sha1_base64="3ZYpuMaHocy2UImapsblr2mF/zQ=">AAAB53icbVA9TwJBEJ3DL8Qv1NJmIzGxIgeN2BFtLCHxhAQuZG+Zg5W9vcvungm58AtsLNTY+pfs/DcucIWCL5nk5b2ZzMwLEsG1cd1vp7CxubW9U9wt7e0fHB6Vj08edJwqhh6LRay6AdUouETPcCOwmyikUSCwE0xu537nCZXmsbw30wT9iI4kDzmjxkrtxqBccavuAmSd1HJSgRytQfmrP4xZGqE0TFCtezU3MX5GleFM4KzUTzUmlE3oCHuWShqh9rPFoTNyYZUhCWNlSxqyUH9PZDTSehoFtjOiZqxXvbn4n9dLTdjwMy6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N5K2JgqyozNpmRDqK2+vE68evW66rbrleZNnkYRzuAcLqEGV9CEO2iBBwwQnuEV3pxH58V5dz6WrQUnnzmFP3A+fwDy3oyN</latexit>
1 <latexit sha1_base64="xtYtqk/NAQjj/df88nu05b7+fg0=">AAAB53icbVBNT8JAEJ36ifiFevSykZh4Ii0X9Ub04hESKyTQkO0yhZXtttndmpCGX+DFgxqv/iVv/hsX6EHBl0zy8t5MZuaFqeDauO63s7a+sbm1Xdop7+7tHxxWjo4fdJIphj5LRKI6IdUouETfcCOwkyqkcSiwHY5vZ377CZXmibw3kxSDmA4ljzijxkotr1+pujV3DrJKvIJUoUCzX/nqDRKWxSgNE1TrruemJsipMpwJnJZ7mcaUsjEdYtdSSWPUQT4/dErOrTIgUaJsSUPm6u+JnMZaT+LQdsbUjPSyNxP/87qZia6CnMs0MyjZYlGUCWISMvuaDLhCZsTEEsoUt7cSNqKKMmOzKdsQvOWXV4lfr13X3Fa92rgp0ijBKZzBBXhwCQ24gyb4wADhGV7hzXl0Xpx352PRuuYUMyfwB87nD+hJjIY=</latexit> 6 <latexit sha1_base64="Ey1zm3hx+2YzcVUO977joFYJtEM=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lF/LgVvXhswdhCG8pmO2nXbjZhdyOU0F/gxYOKV/+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RRWVtfWN4qbpa3tnd298v7Bg45TxdBjsYhVO6AaBZfoGW4EthOFNAoEtoLR7dRvPaHSPJb3ZpygH9GB5CFn1FipedErV9yqOwNZJrWcVCBHo1f+6vZjlkYoDRNU607NTYyfUWU4EzgpdVONCWUjOsCOpZJGqP1sduiEnFilT8JY2ZKGzNTfExmNtB5Hge2MqBnqRW8q/ud1UhNe+RmXSWpQsvmiMBXExGT6NelzhcyIsSWUKW5vJWxIFWXGZlOyIdQWX14m3ln1uuo2zyv1mzyNIhzBMZxCDS6hDnfQAA8YIDzDK7w5j86L8+58zFsLTj5zCH/gfP4A8HiMjQ==</latexit>

Figure 2: Dilated SUNet-7-128 network for segmentation at output stride = 8. For dilation > 1, the feature maps are processed with a varying range of dilation factors inside each u-net module (see inset). The de-gridding filters smooth out aliasing artifacts that occur during deconvolution.

As in ResNet and DenseNet, most of the processing in SUNet is performed at the feature scale of 14×14 (46 conv layers) and 7×7 (44 conv layers). However, the order at which the local information is processed can lead to a substantial gap in performance between ResNet and SUNet when extended to object localization, detection, and image segmentation tasks. All these task demands pixel-level localization and hence require a deep architecture that can efficiently integrate local and global cues. The development of SUNet is a first step towards achieving this objective. Intuitively, multiple stacks of u-nets can be seen as multiple iterations of the message passing operation in a CRF.

5 DILATED SUNETS FOR SEGMENTATION
One can directly extend SUNet to segmentation by simply removing a global average pooling layer and operating the network in fully convolutional mode. Akin to other works on semantic segmentation (Chen et al., 2017), the output feature maps are rescaled using bilinear interpolation to the input image size before passing into the softmax layer with multi-class cross-entropy loss.
5.1 DILATION
For an input image of 512 × 512, the output map size at the softmax is 16 × 16 i.e., subsampled by a factor of 32. This is insufficient to preserve precise pixel-level localization information at its output. The precision can be improved by increasing the output map size of the network. This is realized by dropping the pooling stride at the transition layer. Merely eliminating stride leads to the reduction in the receptive field of the subsequent layers by a factor of two. Consequently, this reduces the influence of long-distance context information on the output prediction. Nevertheless, the receptive field is restored to that of the original network by operating each convolutional filter in the subsequent layers at a dilation factor of 2 (Chen et al., 2016a; Yu & Koltun, 2015).
5.2 MULTIGRID
Figure 2 shows a sample dilated SUNet architecture used for the semantic segmentation task. Similar to (Chen et al., 2017), we define output stride to be the ratio of resolution of an input image to that of its output feature map.
To sample at an output stride of 8 the pooling layers preceding blocks (3) and (4) are discarded. Following this, the dilation factor for each u-net module in blocks 3 and 4 is fixed at 2 and 4, respectively. In each subsequent u-net module the 3 × 3 conv layers are operated with stride = 1. To keep the receptive field of the low-resolution layers in these modules constant, a dilation is applied. This arrangement facilitates the network to preserve spatial information learned from the prior modules (because there is no downsampling in the final u-net block) while preserving the distance scale of features within each block. As an example, the inset in Figure 2 displays the effective dilation rate for each layer in the u-net module at block 3. Similarly, the dilation rate of each layer (except for bottleneck layers) in the u-net+ module will be twice that of the corresponding layers in block 3. The steady increase and decrease of dilation factors inside each u-net module is analogous to multigrid solvers for linear systems (Brandt, 1977; Briggs et al., 2000), which use grids

6

Under review as a conference paper at ICLR 2019

at different scales to move information globally. Many recent works (Dai et al., 2017; Wang et al., 2017; Chen et al., 2017) on deep networks advocate the use of special structures for information globalization. In SUNet, the multigrid structure is baked into the model, and no further "frills" are needed to globalize information.
5.3 DE-GRIDDING FILTERS
By adopting dilated SUNets, we observe a vast improvement in segmentation performance. However, for output stride = 8 the segmentation map displays gridding artifacts (Wang et al., 2017; Yu et al., 2017). This aliasing artifact is introduced when the sampling rate of the dilated layer is lower than the high-frequency content of input feature maps. The final conv filter of u-net+ operates at the dilation factor of 4. Directly transferring u-net+'s feature map output to a classification layer can cause gridding artifacts. Following Yu et al. (2017), the u-net+ module is followed by two layers of de-gridding filters with progressively decreasing dilation factor. Each filter is a 3 × 3 conv layer and outputs 512 feature maps.
In all, SUNet does not require any additional post-hoc structural changes popularized by recent works such as decoding layers (Lin et al., 2017a), appending context aggregation blocks (Chen et al., 2017; Zhao et al., 2017) and learning conditional random fields (Chandra et al., 2017). Hence we regard SUNet as a "no-frills" network.

6 EXPERIMENTS

6.1 IMAGENET CLASSIFICATION

In this section, we evaluate three SUNet architectures on the ILSVRC-2012 classification dataset, which contains 1.28M training images and 50, 000 images for validation, with labels distributed over 1000 classes. Training utilized the same data augmentation scheme used for ResNet and DenseNet. Following common practice (He et al., 2016a;b), we apply a 224 × 224 center crop on test images and report Top-1 and Top-5 error on the validation set. More details in Appendix A.
Table 2 compares the performance of SUNet against other classification networks. The comparison is restricted to only ResNet and DenseNet models as most recent work on segmentation builds on top of them. The notable point about the result is that the repeated top-down and the bottom-up processing of features performs equivalently to state-of-the-art classification networks.
We emphasize here that our objective is not to surpass classification accuracy but instead to build a better architecture for segmentation by pre-training on a classification task. Indeed, each SUNet model was selected such that it is the counterpart for the corresponding ResNet model.

Model Top-1 Top-5 Depth Params
ResNet-18 30.24 10.92 18 11.7M ResNet-50 23.85 7.13 50 25.6M ResNet-101 22.63 6.44 101 44.5M
DenseNet-201 22.80 6.43 201 20M DenseNet-161 22.35 6.20 161 28.5M

Model

mIoU

ResNet-101 (Chen et al., 2017) 68.39

SUNet-64 SUNet-128 SUNet-7-128

72.85 77.16 78.95

SUNet-64 29.28 10.21 111 6.9M

Table 3: The semantic segmentation

SUNet-128 23.64 7.42 111 24.6M

performance of dilated SUNet and

SUNet-7-128 22.47 6.85 171 37.7M

ResNet-101 networks on PASCAL

VOC 2012 validation set trained with

Table 2: Error rates for classification networks on the Ima- output stride = 16. Relative to

geNet 2012 validation set.  denotes error rates from the the ResNet-101 network, all SUNets

official PyTorch implementation.

perform very well.

6.2 SEMANTIC SEGMENTATION

Semantic segmentation networks were built using the dilated version of the ImageNet pre-trained SUNet models (Section 5). We evaluate on the PASCAL VOC 2012 semantic segmentation benchmark (Everingham et al., 2015) and urban scene understanding Cityscape (Cordts et al., 2016)

7

Under review as a conference paper at ICLR 2019
datasets. The performance on each of these datasets is reported using intersection-over-union (IoU) averaged over all classes. The experimental details are shared in Appendix B.
Unless mentioned, for all our experiments we set output stride = 16. This means only the unet modules in the final block (4) operate at dilation factor of two; all other modules use the same stride as in the original classification model. The training and inference are 2× faster with output stride = 16 rather than 8.
6.2.1 ABLATION STUDY
We experiment with different SUNet variants on the PASCAL VOC 2012 dataset.
SUNets vs ResNet-101: We compare the performance of the dilated SUNet architecture on semantic segmentation against the popular dilated ResNet-101 model. Models were fine-tuned on the "trainaug" set without the degridding layers and evaluated on the validation set.
The performance of the plain dilated SUNets surpasses that of ResNet-101 by a wide margin (Table 3). In fact, the smallest SUNet model, SUNet-64 with 6.7M parameters, beats ResNet-101 (with 44.5M) by an absolute margin of 4.5% IoU while SUNet-7-128, the counterpart network to ResNet101, improves by over 10.5% IoU. This is substantial, given that the gap between the ResNet-101 and VGG-16 models is  3% (Chen et al., 2016a) (at output stride = 8). This contrasts with the negligible performance differences observed on classification, and suggests that specialized segmentation network architectures can surpass architectures adapted from classification.
Finally, we note that, although SUNets were designed for pixel-level localization tasks, the selected models were chosen only based on their classification performance. By linking the model selection process to the primary task (segmentation) there is a possibility of improving performance.
Multigrid vs Downsampling: We compare the performance of multigrid dilation (as shown in Figure 2) inside each u-net against the usual downsampling (Figure 1). We consider a dilated SUNet-7128 network and report performance at different training output stride. The result is summarized in Table 4. For a dilated network, replacing strided convolutional layers with the corresponding dilated layers is more logical as well as beneficial. This is because, when operating dilated convolutional layers with stride > 1, alternate feature points are dropped without being processed by any of the filters, leading to high frequency noise at the decoder output. Furthermore, due to a skip connection, the features from the lower layers are also corrupted. Due to error propagation, this effect is more prominent in a network with many dilated modules (for eg., output stride = 8).
Output Stride and Inference Strategy: Finally, we experiment with three different training output strides (8,16,32) and multi-scale inference at test time. For OS = 32, none of the layers are dilated and hence de-gridding layers were not used. Training with an OS = 32 is equivalent to fine-tuning a classification network with a global pooling layer removed. For multi-scale inference, each input image is scaled and tested using multiple scales {0.5, 0.75, 1.0, 1.25} and its left-right flipped image. The average over all output maps is used in the final prediction. See results in Table 5. We note that:
1. The network trained with OS = 32 performs 0.7 IoU better (with single scale) than the Resnet101 and Resnet-152 models (Wu et al., 2016) each trained at OS = 8. This is significant as SUNet model outputs 16× fewer pixels at 4× faster training/inference with no performance drop.
2. The degridding layers do not improve performance at OS = 16. Since there is only small change in dilation factor between the final SUNet layer & classification layer, aliasing is not problematic.
3. The margin of performance improvement decreases with decrease in training OS. Given this and the above fact, subsequently we only report performance for models trained at OS = 16 without any degridding layers.
Pretraining with MS-COCO: Following common practice (Chen et al., 2017), we pretrain SUNet7-128 with the MS-COCO dataset (Lin et al., 2014). The dataset contains pixel-level annotation for 80 object classes. Except for the PASCAL VOC classes, the pixel annotation for all other classes is set to the background class. Following this, the images having < 1000 annotated pixel labels were dropped yielding 90, 000 training images. After pretraining, the model is fine-tuned on "trainaug" for 5K iterations with 10× smaller initial learning rate. In the end, our model achieves 83.27% mIoU on the validation set. This performance is slightly better than the ResNet+ASPP model (Chen et al., 2017) (82.70%) and equivalent to Xception+ASPP+Decoder model of Chen et al. (2018) (83.34%).
8

Under review as a conference paper at ICLR 2019

train OS eval OS DL MS Flip mIoU

OS Strided conv Multigrid

8 65.99 16 78.25

78.64 78.95

32 32 32 32 32 32

76.03 77.58 77.57

16 16

78.95

Table 4: Performance com-

16 16

78.10

parison of multigrid dilation

16 16

80.22

against strided convolution

16 16

80.40

inside each u-net module,

88

78.64

using the SUNet-7-128

88

78.88

model and evaluated us-

88

80.37

ing mean IoU. OS denotes

88

80.50

output stride during train-

ing. Table 5: Performance comparison at various output stride and infer-

ence strategies. MS: Multi-scale, DL: with Degridding Layers

Methods

mIoU

Piecewise (VGG16) (Lin et al., 2016) LRR+CRF (Ghiasi & Fowlkes, 2016) DeepLabv2+CRF (Chen et al., 2016a) Large-Kernel+CRF (Peng et al., 2017) Deep Layer Cascade (Li et al., 2017) Understanding Conv (Wang et al., 2017)
RefineNet (Lin et al., 2017a) RefineNet-ResNet152 (Lin et al., 2017a)

78.0 77.3 79.7 82.2 82.7 83.1 82.4 83.4

PSPNet Zhao et al. (2017) SUNet-7-128 (OS = 16)

85.4 84.3

Methods

mIoU

LRR (VGG16) (Ghiasi & Fowlkes, 2016)
DeepLabv2+CRF (Chen et al., 2016a) Deep Layer Cascade (Li et al., 2017)
Piecewise (VGG16) (Lin et al., 2016)
RefineNet (Lin et al., 2017a)

69.7 70.4 71.1 71.6 73.6

Understanding Conv Wang et al. (2017)
PSPNet Zhao et al. (2017) SUNet-7-128 (OS = 16) SUNet-7-128 (OS = 8)

77.6 78.4 75.3 76.8

Table 7: Performance comparison on Cityscapes

Table 6: Performance comparison on PASCAL test set. All methods were trained only using the

VOC 2012 test set. For fair comparison, only "fine" set. All nets utilize ResNet-101 as a base

the methods pre-trained using MS-COCO are displayed. PSPNet uses 82% more parameters.

network, except if specified or marked with .

6.2.2 RESULTS ON TEST SET
PASCAL VOC 2012: Before submitting test set output to an evaluation server, the above model was further fine-tuned on the "trainval" set with batch-norm parameters frozen and at 10× smaller initial learning rate. Table 6 compares the test set results against other state-of-the-art methods. PSPNet performs slightly better than SUNet, but at the cost of 29.1M more parameters while training at an output stride = 8 with 2× runtime. Specifically, the segmentation model of SUNet uses 35.4M parameters while PSPNet requires 64.5M - an increase of 82.2%. The qualitative results can be found in Appendix C.
Cityscapes: A similar training strategy as in PASCAL is adopted except that the multi-scale inference is performed on additional scales {1.5, 1.75, 2.0, 2.25, 2.5}. Only the finer annotation set was used for training. The comparison on the Cityscapes test set results are displayed in Table 7.

7 DISCUSSION AND CONCLUSION
The fundamental structure of conventional bottom-up classification networks limits their efficacy on secondary tasks involving pixel-level localization or classification. To overcome this drawback, a new network architecture, stacked u-nets (SUNets), is discussed in this paper. SUNets leverage the information globalization power of u-nets in a deeper network architecture that is capable of handling the complexity of natural images. SUNets performs exceptionally well on semantic segmentation tasks while achieving fair performance on ImageNet classification. It generalizes well across tasks without any bells and whistles (difference between networks for two task is dilation).
9

Under review as a conference paper at ICLR 2019
REFERENCES
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157­166, 1994.
Achi Brandt. Multi-level adaptive solutions to boundary-value problems. Mathematics of computation, 31(138):333­390, 1977.
William L Briggs, Steve F McCormick, et al. A multigrid tutorial, volume 72. Siam, 2000.
Siddhartha Chandra and Iasonas Kokkinos. Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs. In European Conference on Computer Vision, pp. 402­418. Springer, 2016.
Siddhartha Chandra, Nicolas Usunier, and Iasonas Kokkinos. Dense and low-rank gaussian crfs using deep embeddings. In ICCV 2017-International Conference on Computer Vision, 2017.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv preprint arXiv:1606.00915, 2016a.
Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L Yuille. Attention to scale: Scaleaware semantic image segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3640­3649, 2016b.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoderdecoder with atrous separable convolution for semantic image segmentation. arXiv preprint arXiv:1802.02611, 2018.
Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251­1258, 2017.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3213­3223, 2016.
Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 764­773, 2017.
Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International journal of computer vision, 111(1):98­136, 2015.
Jun Fu, Jing Liu, Yuhang Wang, and Hanqing Lu. Stacked deconvolutional network for semantic segmentation. arXiv preprint arXiv:1708.04943, 2017.
Golnaz Ghiasi and Charless C Fowlkes. Laplacian pyramid reconstruction and refinement for semantic segmentation. In European Conference on Computer Vision, pp. 519­534. Springer, 2016.
Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In International Conference on Computer Vision (ICCV), 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016a.
10

Under review as a conference paper at ICLR 2019
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630­645. Springer, 2016b.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, volume 1, pp. 3, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. CVPR, 2017.
Simon Je´gou, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua Bengio. The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on, pp. 1175­1183. IEEE, 2017.
Philipp Kra¨henbu¨hl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In Advances in neural information processing systems, pp. 109­117, 2011.
Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, and Xiaoou Tang. Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3193­3202, 2017.
Chen Liang-Chieh, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In International Conference on Learning Representations, 2015.
Guosheng Lin, Chunhua Shen, Anton Van Den Hengel, and Ian Reid. Efficient piecewise training of deep structured models for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3194­3203, 2016.
Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017a.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740­755. Springer, 2014.
Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, volume 1, pp. 4, 2017b.
Wei Liu, Andrew Rabinovich, and Alexander C Berg. Parsenet: Looking wider to see better. arXiv preprint arXiv:1506.04579, 2015.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431­3440, 2015.
Ilya Loshchilov and Frank Hutter. Sgdr: stochastic gradient descent with restarts. arXiv preprint arXiv:1608.03983, 2016.
Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3D Vision (3DV), 2016 Fourth International Conference on, pp. 565­571. IEEE, 2016.
Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. In European Conference on Computer Vision, pp. 483­499. Springer, 2016.
Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters­improve semantic segmentation by global convolutional network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4353­4361, 2017.
Gernot Riegler, Ali Osman Ulusoy, Horst Bischof, and Andreas Geiger. Octnetfusion: Learning depth fusion from data. In Proceedings of the International Conference on 3D Vision, 2017.
11

Under review as a conference paper at ICLR 2019
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234­241. Springer, 2015.
Alexander G Schwing and Raquel Urtasun. Fully connected deep structured networks. arXiv preprint arXiv:1503.02351, 2015.
Abhinav Shrivastava, Rahul Sukthankar, Jitendra Malik, and Abhinav Gupta. Beyond skip connections: Top-down modulation for object detection. arXiv preprint arXiv:1612.06851, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Bharat Singh and Larry S Davis. An analysis of scale invariance in object detection-snip. arXiv preprint arXiv:1711.08189, 2017.
Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua Huang, Xiaodi Hou, and Garrison Cottrell. Understanding convolution for semantic segmentation. arXiv preprint arXiv:1702.08502, 2017.
Zifeng Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet model for visual recognition. arXiv preprint arXiv:1611.10080, 2016.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015.
Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated residual networks. In Computer Vision and Pattern Recognition, volume 1, 2017.
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 2881­2890, 2017.
Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random fields as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529­1537, 2015.
12

Under review as a conference paper at ICLR 2019

APPENDIX
A IMAGENET: IMPLEMENTATION DETAILS
All the models were implemented using the PyTorch deep learning framework and trained using four P6000 GPUs on a single node. We use SGD with a batch size of 256. For our largest model, 7-128, we were limited to a batch size of 212, due to the GPUs memory constraints. The initial learning rate was set to 0.01 and decreased by a factor of 10 every 30 epochs. We use a weight decay of 5e-4 and Nesterov momentum of 0.9 without dampening. The weights were initialized as in He et al. (2015) and all the models were trained from scratch for a total of 100 epochs.

B SEMANTIC SEGMENTATION

A DATASETS
PASCAL VOC 2012: This dataset contains 1,464 train, 1,449 validation and 1,456 test images. The pixel-level annotation for 20 objects and one background class is made available for the train and validation set. Following common practice, the train set is augmented with additional annotated data from Hariharan et al. (2011) which finally provides a total of 10,582 (trainaug) training images.
Cityscape: This dataset consists of finely annotated images of urban scenes covering multiple instances of cars, roads, pedestrians, buildings, etc. In total, it contains 19 classes on 2, 975 finely annotated training and 500 validation images.

B IMPLEMENTATION DETAILS

We use the SGD optimizer with a momentum of 0.95 and weight decay of 10-4. Each model is

fine-tuned starting with an initial learning rate of 0.0002 which is decreased every iteration by a

factor of 0.5 learned with

× 1+ a decay

cos rate

 of

iter max iters
0.99 and

Loshchilov & the input crop

Hutter (2016). The batch-norm parameters are size for each training image is set to 512 × 512.

We train each model using two P6000 GPUs and the batch size 22. On PASCAL VOC, each model

is trained for 45K iterations while for Cityscapes we use 90K iterations.

Data Augmentation: To prevent overfitting during the training process, each training image is resized with a random scale from 0.5 to 2 following which the input image is randomly cropped. Additionally, the input is randomly flipped horizontally and also randomly rotated between -10 to 10.

C QUALITATIVE RESULTS
The segmentation results for PASCAL VOC 2012 val and test images are displayed in figure 3 and 4 respectively.

D ACTIVATION MAPS
Figure 5 shows the activation map recorded at the end of each level (as indicated in figure 2) for an example input image of an "Aeroplane." As noted earlier, the inclusion of strided convolutions instead of multigrid dilations leads to noisy feature maps (see col 3; rows 4-6). The addition of de-gridding layers serves to produce a coherent prediction map at the output (see col 2; row 6).

13

Under review as a conference paper at ICLR 2019

input

target

output

input

target

output

Figure 3: Visualization of the segmentation output on PASCAL VOC 2012 val set when trained at an output stride = 16 using SUNet-7-128 network + MS-COCO. Final row shows couple of failure case which happens due to, ambiguous annotation and inability in detecting low resolution
objects.

Figure 4: Visualization of the segmentation output on PASCAL VOC 2012 test set. 14

Under review as a conference paper at ICLR 2019

OS = 8

OS = 8 + DL

OS = 8+Strided

OS = 16

SUNet-64

Figure 5: Activation map recorded at the end of each level of the dilated SUNet for an example input image of an `Aeroplane'. The activation map with total highest magnitude were selected from among all feature map outputs at the corresponding layer. Top to Bottom: output at end of level 1 - 6 followed by classification output. The level 6 output is simply a prediction map before bilinear interpolation.
15

