Under review as a conference paper at ICLR 2019
SUBGRADIENT DESCENT LEARNS ORTHOGONAL DIC-
TIONARIES
Anonymous authors Paper under double-blind review
ABSTRACT
This paper concerns dictionary learning, viz., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can recover orthogonal dictionaries on a natural nonsmooth, nonconvex 1 minimization formulation of the problem, under mild statistical assumption on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing landscapes of nonsmooth functions, which might be of independent interest for provable training of deep networks with nonsmooth activations (e.g., ReLU), among other applications. Preliminary experiments corroborate our analysis and show that our algorithm works well empirically in recovering orthogonal dictionaries.
1 INTRODUCTION
Dictionary learning (DL), viz., sparse coding, concerns the problem of learning compact representations, i.e., given data Y , one tries to find a representation basis A and coefficients X, so that Y  AX where X is most sparse. DL has numerous applications especially in image processing and computer vision (Mairal et al., 2014). When posed in analytical form, DL seeks a transformation Q such that QY is sparse; in this sense DL can be considered as an (extremely!) primitive "deep" network (Ravishankar & Bresler, 2013).
Many heuristic algorithms have been proposed to solve DL since the seminal work of Olshausen & Field (1996), most of them surprisingly effective in practice (Mairal et al., 2014; Sun et al., 2015). However, understandings on when and how DL is solvable have only recently started to emerge. Under appropriate generating models on A and X, Spielman et al. (2012) showed that complete (i.e., square, invertible) A can be recovered from Y , provided that X is ultra-sparse. Subsequent works (Agarwal et al., 2017; Arora et al., 2014; 2015; Chatterji & Bartlett, 2017; Awasthi & Vijayaraghavan, 2018) provided similar guarantees for overcomplete (i.e. fat) A, again in the ultra-sparse regime. The latter methods are invariably based on nonconvex optimization with model-dependent initialization, rendering their practicality on real data questionable.
The ensuing developments have focused on breaking the sparsity barrier and addressing the practicality issue. Convex relaxations based on the sum-of-squares (SOS) SDP hierarchy can recover overcomplete A when X has linear sparsity (Barak et al., 2015; Ma et al., 2016; Schramm & Steurer, 2017), while incurring expensive computation (solving large-scale SDP's or large-scale tensor decomposition). By contrast, Sun et al. (2015) showed that complete A can be recovered in the linear sparsity regime by solving a certain nonconvex problem with arbitrary initialization. However, the second-order optimization method proposed there is still expensive. This problem is partially addressed by (Gilboa et al., 2018) which proved that the first-order gradient descent with random initialization enjoys a similar performance guarantee.
A standing barrier toward practicality is dealing with nonsmooth functions. To promote sparsity in the coefficients, the 1 norm is the function of choice in practical DL, as is common in modern signal processing and machine learning (Cande`s, 2014): despite its nonsmoothness, this choice often admits highly scalable numerical methods, such as proximal gradient method and alternating direction method (Mairal et al., 2014). The analyses in Sun et al. (2015); Gilboa et al. (2018), however, focused on characterizing the algorithm-independent function landscape of a certain nonconvex formulation of DL, which takes a smooth surrogate to 1 to get around the nonsmoothness. The tactic smoothing
1

Under review as a conference paper at ICLR 2019

there introduced substantial analysis difficulty, and broke the practical advantage of computing with the simple 1 function.

In this paper, we show that working directly with a natural 1 norm formulation results in neat analysis and a practical algorithm. We focus on the problem of learning orthogonal dictionaries: given data {yi}i[m] generated as yi = Axi, where A  Rn×n is a fixed unknown orthogonal matrix and each xi  Rn is an iid Bernoulli-Gaussian random vector with parameter   (0, 1), recover A. This statistical model is the same as in previous works (Spielman et al., 2012; Sun et al., 2015).
Write Y =. [y1, . . . , ym] and similarly X =. [x1, . . . , xm]. We propose to recover A by solving the following nonconvex (due to the constraint), nonsmooth (due to the objective) optimization problem:

minimizeqRn

f (q) =.

1 m

q

Y

1 1= m

m
|q

yi|

subject to

q 2 = 1.

i=1

(1.1)

Based on the statistical model, q Y = q AX has the highest sparsity when q is a column of A (up to sign) so that q A is 1-sparse. Spielman et al. (2012) formalized this intuition and optimized the same objective as Eq. (1.1) with a q  = 1 constraint, which only works when   O(1/ n). Sun et al. (2015) worked with the sphere constraint but replaced the 1 objective with a smooth surrogate, introducing substantial analytical and computational deficiencies as alluded above.

In constrast, we show that with sufficiently many samples, the optimization landscape of formulation (1.1) is benign with high probability (over the randomness of X), and a simple Riemannian subgradient descent algorithm can provably recover A in polynomial time.
Theorem 1.1 (Main result, informal version of Theorem 3.1). Assume   [1/n, 1/2]. For m  (-2n4 log4 n), the following holds with high probability: there exists a poly(m, -1)time algorithm, which runs Riemannian subgradient descent on formulation (1.1) from at most O(n log n) independent, uniformly random initial points, and outputs a set of vectors {a1, . . . , an} such that up to permutation and sign change, ai - ai 2  for all i  [n].

In words, our algorithm works also in the linear sparsity regime, the same as established in Sun et al. (2015); Gilboa et al. (2018), at a lower sample complexity O(n4) in contrast to the existing O(n5.5) in Sun et al. (2015). 1 As for the landscape, we show that (Theorems 3.4 and 3.6) each of the desired solutions {±ai}i[n] is a local minimizer of formulation (1.1) with a sufficiently large basin of attraction so that a random initialization will land into one of the basins with at least constant
probability. To obtain the result, we integrate and develop elements from nonsmooth analysis (on
Riemannian manifolds), set-valued analysis, and random set theory, which might be valuable to
studying other nonconvex, nonsmooth optimization problems.

1.1 RELATED WORK
Dictionary learning Besides the many results sampled above, we highlight similarities of our result to Gilboa et al. (2018). Both propose first-order optimization methods with random initialization, and several quantities we work with in the proofs are the same. A defining difference is we work with the nonsmooth 1 objective directly, while Gilboa et al. (2018) built on the smoothed objective from Sun et al. (2015). We put considerable emphasis on practicality: the subgradient of the nonsmooth objective is considerably cheaper to evaluate than that of the smooth objective in Sun et al. (2015), and in the algorithm we use Euclidean projection rather than exponential mapping to remain feasible--again, the former is much lighter for computation.

General nonsmooth analysis While nonsmooth analytic tools such as subdifferential for convex functions are now well received in machine learning and relevant communities, that for general functions are much less so. The Clarke subdifferential and relevant calculus developed for the family of locally Lipschitz functions seem to be particularly relevant, and cover several families of functions of interest, such as convex functions, differentiable functions, and many forms of composition (Clarke, 1990; Aubin, 1998; Bagirov et al., 2014). Remarkably, majority of the tools and results can be generalized to locally Lipschitz functions on Riemannnian manifolds (Ledyaev & Zhu, 2007; Hosseini & Pouryayevali, 2011). Our formulation (1.1) is exactly optimization of a locally Lipschitz
1The sample complexity in Gilboa et al. (2018) is not explicitly stated.

2

Under review as a conference paper at ICLR 2019

function (as it is convex) on a Riemannian manifold (the sphere). For simplicity, we try to avoid the full manifold language, nonetheless.
Nonsmooth optimization on Riemannian manifolds or with constraints Equally remarkable is many of the smooth optimization techniques and convergence results can be naturally adapted to optimization of locally Lipschitz functions on Riemannian manifolds (Grohs & Hosseini, 2015; Hosseini, 2015; Hosseini & Uschmajew, 2017; Grohs & Hosseini, 2016). New optimization methods such as gradient sampling and variants have been invented to solve general nonsmooth problems (Burke et al., 2005; 2018; Bagirov et al., 2014; Curtis & Que, 2015; Curtis et al., 2017a). Almost all available convergence results pertain to only global convergence, which is too weak for our purpose. Our specific convergence analysis gives us a local convergence result (Theorem 3.8).
Nonsmooth landscape characterization Nonsmoothness is not a big optimization barrier if the problem is convex; here we review some recent work on analyzing nonconvex nonsmooth problems. Loh & Wainwright (2015) study the regularized empirical risk minimization problem with nonsmooth regularizers and show results of the type "all stationary points are within statistical error of ground truth" under certain restricted strong convexity of the smooth risk. Duchi & Ruan (2017); Davis et al. (2017) study the phase retrieval problem with L1 loss, characterizing its nonconvex nonsmooth landscape and providing efficient algorithms.
There is a recent surge of work on analyzing one-hidden-layer ReLU networks, which are nonconvex and nonsmooth. Algorithm-independent characterizations of the landscape are mostly local and require strong initialization procedures (Zhong et al., 2017), whereas stronger global results can be established via designing new loss functions (Ge et al., 2017), relating to PDEs (Mei et al., 2018), or problem-dependent analysis of the SGD (Li & Yuan, 2017; Li & Liang, 2018). Our result provides an algorithm-independent chacaterization of the landscape of non-smooth dictionary learning, and is "almost global" in the sense that the initialization condition is satisifed by random initialization with high probability.
Other nonsmooth problems in application Prevalence of nonsmooth problems in control and economy is evident from all monographs on nonsmooth analysis (Clarke, 1990; Aubin, 1998; Bagirov et al., 2014). In modern machine learning and data analysis, nonsmooth functions are often taken to encode structural information (e.g., sparsity, low-rankness, quantization), or whenever robust estimation is desired. In deep learning, the optimization problem is nonsmooth when nonsmooth activations are in use, e.g., the popular ReLU. The technical ideas around nonsmooth analysis, set-valued analysis, and random set theory that we gather and develop here are particularly relevant to these applications.

2 PRELIMINARIES

Problem setup Given an unknown orthogonal dictionary A = [a1, . . . , an]  Rn×n, we wish to recover A through m observations of the form

yi = Axi, or Y = AX in matrix form, where X = [x1, . . . , xm] and Y = [y1, . . . , ym].

(2.1)

The coefficient vectors xi are sampled from the Bernoulli-Gaussian distribution with parameter   (0, 1), denoted as BG(): each entry xij is independently drawn from a standard Gaussian with probability  and zero otherwise. The Bernoulli-Gaussian is a good prototype distribution for sparse vectors, as xi will be on average -sparse. For any z iid Ber(), we let  denote the set of non-zero indices, which is a random set itself.

We assume that n  3 and   [1/n, 1/2]. In particular,   1/n is to require that each xi has at least one non-zero entry on average.

First-order geometry We will focus on the first-order geometry of the non-smooth objec-

tive Eq. (1.1):

f (q)

=

1 m

m i=1

|q

yi|.

In the whole Euclidean space Rn, f is convex with

sub-differential set

1m

f (q) = m

sign(q yi)yi,

(2.2)

i=1

3

Under review as a conference paper at ICLR 2019

where sign(·) is the set-valued sign function (i.e. sign(0) = [-1, 1]). As we minimize f subject

to the constraint q 2 = 1, our problem is no longer convex, and we use Riemannian first-order methods to deal with the constraint. The Riemannian sub-differential of f on Sn-1 is defined as the projection of f (q) onto the tangent space of Sn-1 at q (Hosseini & Uschmajew, 2017):

Rf (q) =. (I - qq )f (q).

(2.3)

A point q is stationary for problem Eq. (1.1) if 0  Rf (q). We will not distinguish between local maxima and saddle points--we call a stationary point q a saddle point if there is a descent direction
(i.e. direction along which the function is locally maximized at q).

Set-valued analysis As the subdifferential is a set-valued mapping, analyzing it requires some setvalued analysis, which we briefly present here. The addition of two sets is defined as the Minkowski summation: X + Y = {x + y : x  X, y  Y }. The expectation of random sets is a straightforward extension of the Minkowski sum allowing any measurable "selection" procedure; for the concrete definition see (Molchanov, 2013). The Hausdorff distance between two sets is defined as

dH (X1, X2) =. sup sup d (x1, X2) , sup d (x2, X1) .

x1 X1

x2 X2

Basic properties about the Hausdorff distance are provided in Appendix A.1.

(2.4)

eNqoutaaltiitoyn=s. Bisolfdorsmdeafillnliettitoenrs.

(e.g., x) For any

paroesivteivcetoirnstaengderbko,ld[kc]ap=.ita{l1s,a.r.e.

matrices , k}. By

(e.g., X default,

).

The · is

dotted the 2

norm if applied to a vector, and the operator norm if applied to a matrix. C and c or any indexed

versions are reserved for universal constants that may change from place to place.

3 MAIN RESULT

We now state our main result, the recovery guarantee for learning orthogonal dictionary by solving formulation (1.1).

Theorem 3.1 (Recovering orthogonal dictionary via subgradient descent). Suppose we observe

m  (n4-2 log4 n)

(3.1)

samples in the dictionary learning problem and we desire an accuracy  (0, 1) for recovering the dictionary. With probability at least 1 - exp -(m3n-3 log-3 m) - exp (-(R/n)), an

algorithm which runs Riemannian subgradient descent R = (n log n) times with independent random initializations on Sn-1 outputs a set of vectors {a1, . . . , an} such that up to permutation
and sign change, ai - ai 2  for all i  [n]. The total number of subgradient descent iterations is bounded by

O R-3/2 n3 log3 n + n3/2 -3/2 + n-3/2 -3 .

(3.2)

At a high level, the proof of Theorem 3.1 consists of the following steps, which we elaborate throughout the rest of this section.
1. Partition the sphere into 2n symmetric "good sets" and show certain directional gradient is strong on population objective E[f ] inside the good sets (Section 3.1).
2. Show that the same geometric properties carry over to the empirical objective f with high probability. This involves proving the uniform convergence of the subdifferential set f to E [f ] (Section 3.2).
3. Under the benign geometry, establish the convergence of Riemannian subgradient descent to one of {±ai : i  [n]} when initialized in the corresponding "good set" (Section 3.3).
4. Calling the randomly initialized optimization procedure O(n log n) times will recover all of {a1, . . . , an} with high probability, by a coupon collector's argument (Section 3.4).
Scaling and rotating to identity Throughout the rest of this paper, we are going to assume WLOG that the dictionary is the identity matrix, i.e. A = In, so that Y = X, f (q) = q X 1, and the goal is to find the standard basis vectors {±e1, . . . , ±en}. The case of a general orthogonal A can be reduced to this special case via rotating by A : q Y = q AX = (q ) X where q = A q and applying the result on q . We also scale the objective by /2 for convenience of later analysis.

4

Under review as a conference paper at ICLR 2019

3.1 PROPERTIES OF THE POPULATION OBJECTIVE

We begin by characterizing the geometry of the expected objective E [f ]. Recall that we have rotated A to be identity, so that we have

f (q) =

· 1 2m

q

X

1=

· 1 m 2m

q

xi ,

f (q) =

i=1

· 1 2m

m

sign

q

xi

xi. (3.3)

i=1

Minimizers and saddles of the population objective We begin by computing the function value and subdifferential set of the population objective and giving a complete characterization of its stationary points, i.e. local minimizers and saddles.

Proposition 3.2 (Population objective value and gradient). We have

E [f ] (q) =

 2

·

E

qx

= E q

(3.4)

E [f ] (q) = E [f ] (q) =

 2 · E sign q x x = E

q/ q , {v : v  1} ,

q = 0, (3.5) q = 0.

Proposition 3.3 (Stationary points). The stationary points of E [f ] on the sphere are

S=

1 q : q  {-1, 0, 1}n , k

q

0 = k, k  [n]

.

(3.6)

The case k = 1 corresponds to the 2n global minimizers q = ±ei, and all other values of k correspond to saddle points.

A consequence of Proposition 3.3 is that the population objective has no "spurious local minima": each stationary point is either a global minimizer or a saddle point, though the problem itself is non-convex due to the constraint.

Identifying 2n "good" subsets We now define 2n subsets on the sphere, each containing one of the
global minimizers {±ei} and possessing benign geometry for both the population and empirical objective, following (Gilboa et al., 2018). For any   [0, ) and i  [n] define

S(i+) =.

q : qi > 0,

qi2 q-i

2 

1+

,

S(i-) =.

q : qi < 0,

qi2 q-i

2 

1+

.

(3.7)

For points in S(i+)  S(i-), the i-th index is larger than all other indices (in absolute value) by a multiplicative factor of . In particular, for any point in these subsets, the largest index is unique, so

by Proposition 3.3 all population saddle points are excluded from these 2n subsets.

Intuitively, this partition can serve as a "tiebreaker": points in S(0i+) is closer to ei than all the other 2n - 1 signed basis vectors. Therefore, we hope that optimization algorithms initialized in this region could favor ei over the other standard basis vectors, which we are going to show is indeed the case. For simplicity, we are going to state our geometry results in S(n+); by symmetry the results will automatically carry over to all the other 2n - 1 subsets.
Theorem 3.4 (Lower bound on directional subgradients). For any fixed 0  (0, 1),

(a) For all q  S(0n+) and all indices j = n such that qj = 0,

inf

E

[Rf

]

(q)

,

1 qj

ej

-

1 qn

en

 1  (1 - ) 0 . 2n 1 + 0

(3.8)

(b) For all q  S(0n+), we have that

inf

E [Rf ] (q) , qnq - en



1  (1
8

-

) 0n-3/2

q-n

.

(3.9)

These lower bounds verify our intuition: points inside S(0n+) have subgradients pointing towards en, both in a coordinate-wise sense and a combined sense: the direction qnq - en is exactly the tangent direction of the sphere at q that points towards en.

5

Under review as a conference paper at ICLR 2019

3.2 BENIGN GEOMETRY OF THE EMPIRICAL OBJECTIVE

We now show that the benign geometry in Theorem 3.4 is carried onto the empirical objective f given sufficiently many samples, using a concentration argument. The key result behind is the concentration of the empirical subdifferential set to the population subdifferential, where concentration is measured in the Hausdorff distance between sets.
Proposition 3.5 (Uniform convergence of subdifferential). For any t  (0, 1], when

m  Ct-2n log2(n/t),

(3.10)

with probability at least 1 - exp -cmt2/log m , we have

Here C, c  0.

dH (f (q) , E [f ] (q))  t for all q  Sn-1.

(3.11)

The concentration result guarantees that the sub-differential set is close to its expectation given sufficiently many samples with high probability. Choosing an appropriate concentration level t, the lower bounds on the directional subgradients carry over to the empirical objective f , which we state in the following theorem.
Theorem 3.6 (Directional subgradient lower bound, empirical objective). Fix a 0  (0, 1). There exists C, c  0 such that the following holds. When m  Cn4-20-2 log2 (n/0), with probability at least 1 - exp -cm302n-3 log-1 m , the following properties hold simultaneously for all the 2n subsets S(0i+), S(0i-) : i  [n] : (stated only for S(0n+))

(a) For all q  S(0n+) and all j  [n] with qj = 0 and qn2 /qj2  3,

inf

11 Rf (q) , qj ej - qn en

 1  (1 - ) 0 . 4n 1 + 0

(3.12)

(b) For all q  S(0n+),



inf Rf (q) , q - en



2 16



(1

-

)

n-

3 2

0

q-n



1  (1
16

-

)

n-

3 2

0

q - en

.

(3.13)

The consequence of Theorem 3.6 is two-fold. First, it guarantees that the only possible stationary point of f in S(0n+) is en: for every other point q = en, property (b) guarantees that 0 / Rf (q), therefore q is non-stationary. Second, the directional subgradient lower bounds allow us to estab-
lish convergence of the Riemannian subgradient descent algorithm, in a way similar to showing
convergence of unconstrained gradient descent on star strongly convex functions.

We now present an upper bound on the norm of the subdifferential sets, which is needed for the convergence analysis.

Proposition 3.7. There exist universal constants C, c  0 such that

sup f (q)  2  q  Sn-1

(3.14)

with probability at least 1 - exp -cm log-1 m , provided that m  Cn2 log n. This particularly implies that

sup Rf (q)  2  q  Sn-1.

(3.15)

3.3 FINDING ONE BASIS VIA RIEMANNIAN SUBGRADIENT DESCENT

The benign geometry of the empirical objective allows a simple Riemannian subgradient descent algorithm to find one basis vector a time. The Riemannian subgradient descent algorithm with initialization q(0) and step size (k) k0 is as follows. For an arbitrary v  Rf q(k) ,

q(k+1) =

q(k) - (k)v ,
q(k) - (k)v

for k = 0, 1, 2, . . . .

(3.16)

6

Under review as a conference paper at ICLR 2019

Each iteration moves in an arbitrary Riemannian subgradient direction followed by a projection back
onto the sphere. We show that the algorithm is guaranteed to find one basis as long as the initialization is in the "right" region. To give a concrete result, we set 0 = 1/(5 log n).2

Theorem 3.8 (One run of subgradient descent recovers one basis). Suppose m  C-2n4 log4 n and

the good events in Theorem 3.6 and Proposition 3.7 happen. If the and we run the projected Riemannian subgradient descent with step

isniziteial(ikz)at=ionk-q(0/)(10S01(n/(n+5))lowginth),

  (0, 1/2), and keep track of the best function value so far until after iterate K is performed,

producing qbest. Then, qbest obeys

f qbest - f (en)  , and

qbest - en

80n3/2 log n   (1 - ) ,

(3.17)

provided that

1

K  max

5,

600n (1 - )

1-

1

24n log2 n + -1 ,1-

 (1 - )

8 (1 - ) -1

14000n2 (1 - )

 1-

1



+.

5 (1 - ) (1 - 2) n

 (1 - )



In particular, choosing  = 1/3, it suffices to let K  K1/3 =. C -3/2 n3 log3 n + n3/2 -3/2 + n-3/2 -3 .

(3.18)

Here C, C , c  0 are universal constants.

The above optimzation result (Theorem 3.8) shows that Riemannian subgradient descent is able to find the basis vector en when initialized in the associated region S1(n/(+5)log n). We now show that a simple uniformly random initialization on the sphere is guaranteed to be in one of these 2n regions with at least probability 1/2.
Lemma 3.9 (Random initialization falls in "good set"). Let q(0)  Uniform(Sn-1), then with probability at least 1/2, q(0) belongs to one of the 2n sets S1(i/+(5)log n), S1(i/-(5)log n) : i  [n] .

3.4 RECOVERING ALL BASES FROM MULTIPLE RUNS

As long as the initialization belongs to S1(i/+(5)log n) or S1(i/-(5)log n), our finding-one-basis result in Theorem 3.8 guarantees that Riemannian subgradient descent will converge to ei or -ei respectively. Therefore if we run the algorithm with independent, uniformly random initializations on the sphere
multiple times, by a coupon collector's argument, we will recover all the basis vectors. This is
formalized in the following theorem.

Theorem 3.10 (Recovering the identity dictionary from multiple random initializations). Let m 

Cn4-2 log4 n, with probability at least 1 - exp -cm3n-3 log-3 m the following happens.

Suppose we run with a uniformly

the Riemannian subgradient random initialization on Sn-1

,daenscdecnhtoaolsgeotrhitehsmtepinsdiezepeansde(nkt)ly=fokr-R1/3ti/m(1e0s,0eanch).

Then, provided that R  C n log n, all standard basis vectors will be recovered up to accuracy

with probability at least 1 - exp (-cR/n) in RK1/3 iterations where K1/3 is defined in Eq. (3.18).

Here C, C , c  0 are universal constants.

When the dictionary A is not the identity matrix, we can apply the rotation argument sketched in the beginning of this section to get the same result, which leads to our main result in Theorem 3.1.

2It is possible to set 0 to other values, inducing different combinations of the final sample complexity, iteration complexity, and repetition complexity in Theorem 3.10.

7

Under review as a conference paper at ICLR 2019

4 PROOF HIGHLIGHTS

A key technical challenge is establishing the uniform convergence of subdifferential sets in Proposition 3.5, which we now elaborate. Recall that the population and empirical subdifferentials are

f (q) =

· 1

m
sign(q

2m

xi)xi,

E [f ] (q) =

i=1

 2

· ExBG()

sign(q

x)x

,

(4.1)

and we wish to show that the difference between f (q) and E [f ] (q) is small uniformly over q  Q = Sn-1. Two challenges stand out in showing such a uniform convergence:

1. The subdifferential is set-valued and random, and it is unclear a-priori how one could formulate and analyze the concentration of random sets.
2. The usual covering argument won't work here, as the Lipschitz gradient property does not hold: f (q) and E [f ] (q) are not Lipschitz in q. Therefore, no matter how fine we cover the sphere in Euclidean distance, points not in this covering can have radically different subdifferential sets.

4.1 CONCENTRATION OF RANDOM SETS

We state and analyze concentration of random sets in the Hausdorff distance (defined in Section 2).

We now illustrate how the Hausdorff distance is the "right" distance to consider for concentration of

subdifferentials--the reason is that the Hausdorff distance is closely related to the support function of

sets, which for any set S  Rn is defined as

hS(u) =. sup x, u .
xS

(4.2)

For convex compact sets, the sup difference between their support functions is exactly the Hausdorff distance.
Lemma 4.1 (Section 1.3.2, Molchanov (2013)). For convex compact sets X, Y  Rn, we have

dH (X, Y ) = sup |hX (u) - hY (u)| .
uSn-1

(4.3)

Lemma 4.1 is convenient for us in the following sense. Suppose we wish to upper bound the difference of f (q) and E [f ] (q) along some direction u  Sn-1 (as we need in proving the key empirical geometry result Theorem 3.6). As both subdifferential sets are convex and compact, by Lemma 4.1 we immediately have

inf g, u - inf g, u

gf (q)

gE[f ](q)

= -hf(q)(-u) + hE[f](q)(-u)  dH(f (q), E [f ] (q)).

(4.4)

Therefore, as long as we are able to bound the Hausdorff distance, all directional differences between the subdifferentials are simultaneously bounded, which is exactly what we want to show to carry the benign geometry from the population to the empirical objective.

4.2 COVERING IN THE dE METRIC

We argue that the absence of gradient Lipschitzness is because the Euclidean distance is not the "right" metric in this problem. Think of the toy example f (x) = |x|, whose subdifferential set
f (x) = sign(x) is not Lipschitz across x = 0. However, once we partition R into R>0, R<0 and {0} (i.e. according to the sign pattern), the subdifferential set is Lipschitz on each subset.

The situation with the dictionary learning objective is quie similar: we resolve the gradient non-
Lipschitzness by proposing a stronger metric dE on the sphere which is sign-pattern aware and averages all "subset angles" between two points. Formally, we define dE as

dE(p, q) =. PxBG() sign(p

x) = sign(q

x)

1 =  E

(p, q),

(4.5)

8

Under review as a conference paper at ICLR 2019

(the second equality shown in Lemma C.1.) Our plan is to perform the covering argument in dE, which requires showing gradient Lipschitzness in dE and bounding the covering number.

Lipschitzness of f and E [f ] in dE For the population subdifferential E [f ], note that E [f ] (q) = ExBG()[sign(q x)x] (modulo rescaling). Therefore, to bound dH(E [f ] (p), E [f ] (q)) by Lemma 4.1, we have the bound for all u  Sn-1

hE[f](p)(u) - hE[f](q)(u) = E sup sign(p x) - sign(q x) · x u
 2E 1 sign(p x) = sign(q x) x u .

(4.6) (4.7)

As long as dE(p, q)  , the indicator is non-zero with probability at most , and thus the above expectation should also be small ­ we bound it by O( log(1/ )) in Lemma F.5.

To show the same for the empirical subdifferential f , one only needs to bound the observed
proportion of sign differences for all p, q such that dE(p, q)  , which by a VC dimension argument is uniformly bounded by 2 with high probability (Lemma C.5).

Bounding the covering number in dE Our first step is to reduce dE to the maximum length-2 angle (the d2 metric) over any consistent support pattern. This is achieved through the following vector angle inequality (Lemma C.2): for any p, q  Rd (d  3), we have

(p, q) 

(p, q) provided (p, q)  /2.

[d],||=2

(4.8)

Therefore, as long as sign(p) = sign(q) (coordinate-wise) and max||=2 (p, q)  /n2, we would have for all || 3 that

(p, q)  /2 and

(p, q) 

(p , q ) 

|| 2

· n2  .

 ,| |=2

(4.9)

By Eq. (4.5), the above implies that dE(p, q)  /, the desired result. Hence the task reduces to constructing an  = /n2 covering in d2 over any consistent sign pattern.

Our second step is a tight bound on this covering number: the -covering number in d2 is bounded

by exp(Cn log(n/)) (Lemma C.3). For bounding this, a first thought would be to take the covering

in all size-2 angles (there are

n 2

of them) and take the common refinement of all their partitions,

which gives covering number (C/)O(n2) = exp(Cn2 log(1/)). We improve upon this strategy by

sorting the coordinates in p and restricting attentions in the consecutive size-2 angles after the sorting

(there are n - 1 of them). We show that a proper covering in these consecutive size-2 angles by /n

will yield a covering for all size-2 angles by . The corresponding covering number in this case is

thus (Cn/)O(n) = exp(Cn log(n/)), which modulo the log n factor is the tightest we can get.

5 EXPERIMENTS
Problem setup We generate dictionary learning instances with both the identity and random orthogonal dictionaries with n  {30, 50, 70}, m = 10n{0.5,1,1.5,2,2.5}, and sparsity level  = 0.3. For each (m, n) we generate 10 problem instances. Note that our theoretical guarantee applies for m = (n4), but here for n = 70, we have 10n2.5/n4 < 0.02, so the sample complexity is already lower than what our theory requires.
To recover the dictionary, we run the Riemannian subgradient descent algorithm Eq. (3.16) with decaying step size (k) = 1/ k, corresponding to the boundary case  = 1/2 in Theorem 3.8.
Metric As Theorem 3.1 guarntees recovering the entire dictionary at R  Cn log n, we perform R = 5n log n runs on each instance, and report the fraction of basis vectors found where "finding" a basis means that one of the R runs get into its < 10-3 neighborhood (up to sign). A fraction close to 1 indicates successful recovery of the entire dictionary.
Result Riemannian subgradient descent succeeds in recovering the dictionary as long as m  O(n2) (Fig. 1). This is consistent with our theory and suggests that the actual sample complexity requirement for guaranteed recovery might be even lower than O(n4).

9

Under review as a conference paper at ICLR 2019

average fraction of recovered standard basis average fraction of recovered standard basis

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0.5

Identity Dictionary

1 1.5 2 number of measurements (as power of n)

n=30 n=50 n=70
2.5

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0.5

Orthogonal Dictionary

1 1.5 2 number of measurements (as power of n)

n=30 n=50 n=70
2.5

Figure 1: Fraction of basis vectors found by Riemannian subgradient descent with R = 5n log n runs, averaged over 10 instances. Left: identity dictionary. Right: orthogonal dictionary.

A faster alternative We also test a faster quasi-Newton type method, GRANSO, based on se-
quential quadratic optimization method that employs LBFGS for solving constrained nonsmooth problems (Curtis et al., 2017b). For dictionary of dimension n = 400 and sample complexity m = 10n2, GRANSO successfully identifies a basis after 1500 iterations with cpu time  4 hours
(10x faster than the Riemannian method), showing the potential for scaling up to larger scales with
second-order methods.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Alekh Agarwal, Animashree Anandkumar, and Praneeth Netrapalli. A clustering approach to learning sparsely used overcomplete dictionaries. IEEE Trans. Information Theory, 63(1):575­592, 2017.
Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcomplete dictionaries. In Conference on Learning Theory, pp. 779­806, 2014.
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. 2015.
Jean-Pierre Aubin. Optima and equilibria: an introduction to nonlinear analysis, volume 140. Springer Science & Business Media, 1998.
Pranjal Awasthi and Aravindan Vijayaraghavan. Towards learning sparsely used dictionaries with arbitrary supports. arXiv preprint arXiv:1804.08603, 2018.
Adil Bagirov, Napsu Karmitsa, and Marko M Ma¨kela¨. Introduction to Nonsmooth Optimization: theory, practice and software. Springer, 2014.
Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp. 143­151. ACM, 2015.
Ste´phane Boucheron, Olivier Bousquet, and Ga´bor Lugosi. Theory of classification: A survey of some recent advances. ESAIM: probability and statistics, 9:323­375, 2005.
James V Burke, Adrian S Lewis, and Michael L Overton. A robust gradient sampling algorithm for nonsmooth, nonconvex optimization. SIAM Journal on Optimization, 15(3):751­779, 2005. doi: 10.1137/030601296.
James V. Burke, Frank E. Curtis, Adrian S. Lewis, Michael L. Overton, and Lucas E. A. Simes. Gradient sampling methods for nonsmooth optimization. arXiv:1804.11003, 2018.
Emmanuel Cande`s. Mathematics of sparsity ( and a few other things ). In Proceedings of the International Congress of Mathematicians, volume 123, 2014.
Niladri Chatterji and Peter L Bartlett. Alternating minimization for dictionary learning with random initialization. In Advances in Neural Information Processing Systems, pp. 1997­2006, 2017.
Frank H Clarke. Optimization and nonsmooth analysis. SIAM, 1990. doi: 10.1137/1.9781611971309.
Frank E Curtis and Xiaocun Que. A quasi-newton algorithm for nonconvex, nonsmooth optimization with global convergence guarantees. Mathematical Programming Computation, 7(4):399­428, 2015. doi: 10.1007/s12532-015-0086-2.
Frank E Curtis, Tim Mitchell, and Michael L Overton. A bfgs-sqp method for nonsmooth, nonconvex, constrained optimization and its evaluation using relative minimization profiles. Optimization Methods and Software, 32(1):148­181, 2017a.
Frank E. Curtis, Tim Mitchell, and Michael L. Overton. A bfgs-sqp method for nonsmooth, nonconvex, constrained optimization and its evaluation using relative minimization profiles. Optimization Methods and Software, 32(1):148­181, 2017b. doi: 10.1080/10556788.2016.1208749.
Damek Davis, Dmitriy Drusvyatskiy, and Courtney Paquette. The nonsmooth landscape of phase retrieval. arxiv:1711.03247, 2017. URL http://arxiv.org/abs/1711.03247.
John C. Duchi and Feng Ruan. Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval. arxiv:1705.02356, 2017. URL http://arxiv.org/abs/1705. 02356.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arxiv:1711.00501, 2017. URL http://arxiv.org/abs/1711.00501.
11

Under review as a conference paper at ICLR 2019
Dar Gilboa, Sam Buchanan, and John Wright. Efficient dictionary learning with gradient descent. In ICML 2018 Workshop on Modern Trends in Nonconvex Optimization for Machine Learning, 2018.
P Grohs and S Hosseini. Nonsmooth trust region algorithms for locally Lipschitz functions on Riemannian manifolds. IMA Journal of Numerical Analysis, 36(3):1167­1192, 2015. doi: 10. 1093/imanum/drv043.
Philipp Grohs and Seyedehsomayeh Hosseini. -subgradient algorithms for locally Lipschitz functions on Riemannian manifolds. Advances in Computational Mathematics, 42(2):333­360, 2016. doi: 10.1007/s10444-015-9426-z.
Jean-Baptiste Hiriart-Urruty and Claude Lemare´chal. Fundamentals of convex analysis. Springer Science & Business Media, 2001. doi: 10.1007/978-3-642-56468-0.
S Hosseini and MR Pouryayevali. Generalized gradients and characterization of epi-lipschitz sets in Riemannian manifolds. Nonlinear Analysis: Theory, Methods & Applications, 74(12):3884­3895, 2011. doi: 10.1016/j.na.2011.02.023.
Seyedehsomayeh Hosseini. Optimality conditions for global minima of nonconvex functions on Riemannian manifolds. Pacific Journal of Optimization, 2015. URL http://uschmajew. ins.uni-bonn.de/research/pub/hosseini/3.pdf.
Seyedehsomayeh Hosseini and Andre´ Uschmajew. A Riemannian gradient sampling algorithm for nonsmooth optimization on manifolds. SIAM Journal on Optimization, 27(1):173­189, 2017. doi: 10.1137/16M1069298.
Yu Ledyaev and Qiji Zhu. Nonsmooth analysis on smooth manifolds. Transactions of the American Mathematical Society, 359(8):3687­3732, 2007. doi: 10.1090/S0002-9947-07-04075-5.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. arxiv:1705.09886, 2017. URL http://arxiv.org/abs/1705.09886.
Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima. Journal of Machine Learning Research, 16:559­616, 2015.
Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-ofsquares. 2016.
Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing. Foundations and Trends R in Computer Graphics and Vision, 8(2-3):85­283, 2014.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural networks. arXiv:1804.06561, 2018.
Ilya Molchanov. Foundations of stochastic geometry and theory of random sets. In Stochastic Geometry, Spatial Statistics and Random Fields, pp. 1­20. Springer, 2013.
Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607, 1996.
Barrett O'neill. Semi-Riemannian geometry with applications to relativity, volume 103. Academic press, 1983.
Saiprasad Ravishankar and Yoram Bresler. Learning sparsifying transforms. IEEE Transactions on Signal Processing, 61(5):1072­1086, 2013.
Tselil Schramm and David Steurer. Fast and robust tensor decomposition with applications to dictionary learning. arXiv preprint arXiv:1706.08672, 2017.
Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In Conference on Learning Theory, pp. 37­1, 2012.
12

Under review as a conference paper at ICLR 2019

Shlomo Sternberg. Dynamical Systems. Dover Publications, Inc, 2013.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere. arxiv:1504.06785, 2015. URL http://arxiv.org/abs/1504.06785.
Aad Van Der Vaart and Jon A Wellner. A note on bounds for VC dimensions. Institute of Mathematical Statistics collections, 5:103, 2009.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. 2018.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees for one-hidden-layer neural networks. arxiv:1706.03175, 2017. URL http://arxiv.org/ abs/1706.03175.

A TECHNICAL TOOLS

A.1 HAUSDORFF DISTANCE

We need the Hausdorff metric to measure differences between nonempty sets. For any set X and a

point p in Rn, the point-to-set distance is defined as d (q, X) =. inf x - p .

(A.1)

xX

For any two sets X1, X2  Rn, the Hausdorff distance is defined as

dH (X1, X2) =. sup sup d (x1, X2) , sup d (x2, X1) .

x1 X1

x2 X2

(A.2)

When X1 is a singleton, say X1 = {p}. Then

dH ({p} , X2) = sup x2 - p .
x2 X2
Moreover, for any sets X1, X2, Y1, Y2  Rn,

(A.3)

dH (X1 + Y1, X2 + Y2)  dH (X1, X2) + dH (Y1, Y2) .

(A.4)

On the sets of nonempty, compact subsets of Rn, the Hausdorff metric is a valid metric; particularly,

it obeys the triangular inequality: for nonempty, compact subsets X, Y, Z  Rn,

dH (X, Z)  dH (X, Y ) + dH (Y, Z) .

(A.5)

See, e.g., Sec. 7.1 of Sternberg (2013) for a proof.

Lemma A.1 (Restatement of Lemma A.1). For convex compact sets X, Y  Rn, we have

dH (X, Y ) = sup |hX (u) - hY (u)| ,

where hS(u) =. supxS

x, u

uSn-1
is the support function associated with the set S.

(A.6)

A.2 SUB-GAUSSIAN RANDOM MATRICES AND PROCESSES

Proposition A.2 (Talagrand's comparison inequality, Corollary 8.6.3 and Exercise 8.6.5 of Vershynin (2018)). Let {Xx}xT be a zero-mean random process on a subset T  Rn. Assume that for all
x, y  T we have

Then, for any t > 0

Xx - Xy 2  K x - y .

(A.7)

sup |Xx|  CK [w(T ) + t · rad(T )]

(A.8)

xT

with probability at least 1 - 2 exp -t2 . Here w(T ) =. EgN (0,I) supxT x, g is the Gaussian

width of T and rad(T ) = supxT x is the radius of T .

Proposition A.3 (Deviation inequality for sub-Gaussian matrices, Theorem 9.1.1 and Exercise 9.1.8

of Vershynin (2018)). Let A be an n × m matrix whose rows Ai's are independent, isotropic, and

sub-Gaussian random vectors in Rm. Then for any subset T  Rm, we have

P sup

 Ax - n x

> CK2 [w (T ) + t · rad (T )]  2 exp -t2 .

(A.9)

xT

Here K = maxi Ai 2 .

13

Under review as a conference paper at ICLR 2019

B PROOFS FOR SECTION 3.1

B.1 PROOF OF PROPOSITION 3.2

We have

E [f ] (q) =

 2

·E

qx

=

 2

·

E

q x

= E [ q 2] ,

(B.1)

where the last equality is obtained by conditioning on  and the fact that EZN(0,2)[|Z|] = 2/.

The subdifferential expression comes from

 

q

,

 q 2 =

q 2

 {v : v 2  1} ,

q = 0; q = 0,

(B.2)

and the fact that E [f ] (q) = E [ q 2] = E [ q 2] as the sub-differential and expectation can be exchanged for convex functions (Hiriart-Urruty & Lemare´chal, 2001). By the same exchangability result, we also have E [f (q)] = E [f ] (q).

B.2 PROOF OF PROPOSITION 3.3

We first show that points in the claimed set are indeed stationary points by taking the choice v = 0
in Eq. (3.5), giving the subgradient choice E [f ] (q) = E [q/ q 2 1 {q = 0}]. Let q  S and
such that q 0 = k. For all j  supp(q), we have

ejT E [f ] (q) = qj · E

1 q

1{j  }

k

= qj ·

i-1(1 - )k-i ·

i=1

k i

=

qj

·

k i=1

i(1

-

)k-i

·

1 i

=.

c(, k)qj

(B.3) (B.4) (B.5)

On the other hand, for all j / supp(q), we always have [q]j = 0, so ej E [f ] (q) = 0. Therefore, we have that E [f ] (q) = c(, k)q, and so

(I - qq )E [f ] (q) = c(, k)q - c(, k)q = 0.

(B.6)

Therefore q  S is stationary. To see that {±ei : i  [n]} are the global minima, note that for all q  Sn-1, we have

E [f ] (q) = E [ q 2]  E

q

2 2

= .

(B.7)

Equality holds if and only if q 2  {0, 1} almost surely, which is only satisfied at q  {±ei : i  [n]}.

To see that the other q's are saddles, we only need to show that there exists a tangent direction along

which q is local max. Indeed, for any other q, there exists at least two non-zero entries (with equal

absolute value): WLOG assume that q1 = qn > 0. Using the reparametrization in Appendix B.3

and

applying

Lemma

B.2,

we

get

that

E

[f ]

(q)

is

directionally

differentiable

along

[-q-n;

1-qn2 qn

],

with derivative zero (necessarily, because 0  E [Rf ] (q)) and strictly negative second derivative.

Therefore E [f ] (q) is locally maximized at q along this tangent direction, which shows that q is a

saddle point.

The other direction (all other points are not stationary) is implied by Theorem 3.4, which guarantees
that 0 / E [Rf ] (q) whenever q / S. Indeed, as long as q / S, q has a max absolute value coordinate (say n) and another non-zero coordinate with strictly smaller absolute value (say j). For
this pair of indices, the proof of Theorem 3.4(a) goes through for index j (even if q  S(0n+) does not necessarily hold because the max index might not be unique), which implies that 0 / E [Rf ] (q).

14

Under review as a conference paper at ICLR 2019

B.3 REPARAMETRIZATION
For analysis purposes, we introduce the reparametrization w = q1:(n-1) in the region S0(n+), following (Sun et al., 2015) . With this reparametrization, the problem becomes

minimizewRn-1 g (w) =.

· 1 2m

w; 1 - w 2

X subject to w 
1

 The constraint comes from the fact that qn  1/ n and thus w  (n - 1)/n. Lemma B.1. We have

n-1 .
n (B.8)

ExiidBG()g (w) = (1 - ) E w + E 1 - wc 2.

(B.9)

Proof. Direct calculation gives ExiidBG()g (w)
= EiidBer(),Ber()EziidN (0,1),zN (0,1) w; 1 - w 2

([; ] [z; z])

(B.10) (B.11)

= (1 - ) EiidBer()EziidN (0,1) w z + EiidBer()EziidN (0,1),zN (0,1) w z +

1 - w 2z

(B.12)

= (1 - ) EiidBer() w + EiidBer() 1 - wc 2,

(B.13)

as claimed.

Lemma B.2 (Negative-curvature region). For all unit vector v  Sn-1 and all s  (0, 1), let

hv (s) =. E [g] (sv)

(B.14)

it holds that

2hv (s)  - (1 - ) .

(B.15)

In other words, for all w = 0, ±w/ w is a direction of negative curvature.

Proof. By Lemma B.1,

hv (s) = (1 - ) sE v + E 1 - s2 vc 2.

For s  (0, 1), hv(s) is twice differentiable, and we have

2hv(s) = -E

vc 2 1 - s2 vc 2

3/2

 -E vc 2 = - (1 - ) ,

completing the proof.

Lemma B.3 (Inward gradient). For any w with

w 2+

w

2 



1,

Dwc / w E [g] (w)   (1 - )

1/

1+

w

2 

/

w

2-

w

.

(B.16) (B.17) (B.18)
(B.19)

Proof. For any unit vector v  Rn-1, define hv (t) =. E [g] (tv) for t  (0, 1). We have from Lemma B.1

hv (t) = (1 - ) tE v + E 1 - t2 vc 2.

(B.20)

15

Under review as a conference paper at ICLR 2019

Moreover,

thv (t) = (1 - ) E v - E

t vc 2 1 - t2 vc 2

(B.21)

= (1 - ) E

v v

2

- E

t vc 2 1 - t2 vc 2

(assuming 0 =. 0) 0

(B.22)

n-1
= (1 - ) E
i=1

vi21 {i  } vi21 {i  } + v\i

n-1

-
2

E

i=1

tvi21 {i / } 1 - t2vi21 {i / } - t2 vc\i 2
(B.23)


n-1
=  (1 - ) E 
i=1

vi2 - vi2 + v\i 2

tvi2 1 - t2vi2 - t2


 vc\i 2

(B.24)

n-1



=  (1 - ) t vi2E 

i=1

1- t2vi2 + t2 v\i 2

1 1 - t2vi2 - t2


 vc\i 2

(B.25)

n-1



=  (1 - ) t vi2E 

i=1

1- t2vi2 + t2 v\i 2

 1
 . (B.26) 1 - t2 v 2+t2 v\i 2

We are interested in the regime of t so that

1 - t2

v

2  t2

v

2 

= t  1/

1 + v 2.

So thv (t)  0 holds always for t  1/ 1 + v 2. By Lemma B.2, 2hv (t)  - (1 - ) over t  (0, 1), which implies
thv (t1) - thv (t2) , t1 - t2  - (1 - ) (t1 - t2)2 .

Taking t1 = 1/

1+

v

2 

and

considering

t2



[0,

t1],

we

have

(B.27) (B.28)

thv (t2)  thv (t1) +  (1 - ) (t1 - t2)   (1 - )

1/

1+

v

2 

-

t2

.

(B.29)

For any w, applying the above result to the unit vector w/ w and recognizing that thw/ w (t) = Dw/ w g (w) = Dwc / w g (w), we complete the proof.

B.4 PROOF OF THEOREM 3.4

We first show Eq. (3.9) using the reparametrization in Appendix B.3. We have

Rf (q) , q - en = f (q) , qnq - en = qn g (w) , w ,

(B.30)

where the second equality follows by differentiating g via the chain rule. Now, by Lemma B.3,

 qn E [g] (w) , w  w  (1 - ) · qn 



w - w .

w 2+

w

2 

(B.31)

For each radial direction v =. w/ w , consider points of the form tv with t  1/

1+

v

2 

.

Obviously, the function

 h¯ (t) =. qn (tv) 



tv

- tv  = qn (tv) 

tv

2+

tv

2 



1 - t

1+

v

2 

(B.32)

16

Under review as a conference paper at ICLR 2019

is monotonically decreasing wrt t. Thus, to derive a lower bound, it is enough to consider the largest

t allowed. In S(0n+), the limit amounts to requiring qn2 /

w

2 

=

1

+

0,

1 - t02 = t20

v

2 

(1

+

0)

=

t0

=

1 .

1 + (1 + 0)

v

2 

(B.33)

So for any fixed v and all allowed t for points in S(0n+), a uniform lower bound is

 qn (t0v) 



1 - t0

1+

v

2 





1 n



1-

1+

v

2 



1 1 + (1 + 0)

v

2





1 8n

0



v

2 



1 8

0n-3/2.

(B.34) (B.35)

So we conclude that for all q  S(0n+),

E [f ] (q) , qnq - en



1 
8

(1

-

) 0n-3/2

w

=

1 
8

(1

-

) 0n-3/2

q-n

.

(B.36)

We now turn to showing Eq. (3.8). For ej with qj = 0,

11 qj ej E [f ] (q) = qj ej E

1 = qj E

q, ej q

1 {q = 0}

q q

1 {q = 0} + {v :

v

1 = qj qj E

1 1 {j  }
q

 1} 1 {q = 0}

 = E 

 1
  . qj2 + q\{j} 2
(B.37)

So for all j with qj = 0, we have

E

[Rf

]

(q)

,

1 qj

ej

-

1 qn

en

=

I - qq

E

[f ]

(q)

,

1 qj

ej

-

1 qn

en

11 = E [f ] (q) , qj ej - qn en

 = E 



qj2 +

1 q\{j}

2  - E 


1 
qn2 + q\{n} 2

 = 2E 



1 qj2 + qn2 +

q\{j,n}

2  +  (1 - ) E 

qj2 +


1 
q\{j,n} 2

 - 2E 



1 qj2 + qn2 +

q\{j,n}

2  -  (1 - ) E 

qn2 +

1 q\{j,n}



2

 =  (1 - ) E 

1- qj2 + q\{j,n} 2

qn2 +


1 
q\{j,n} 2

qn2 1

=  (1 - ) E

qj2

2

t+

dt q\{j,n} 2 3/2

(B.38) (B.39) (B.40)
(B.41) (B.42) (B.43)

17

Under review as a conference paper at ICLR 2019

  (1 - ) 1 2

qn2 - qj2



1  (1

-

)

2

= 1  (1 - ) 0 2n 1 + 0

qn2 -

q-n

2 



1 
2

(1

-

)

1

0 + 0

qn2

completing the proof.

(B.44) (B.45)

C PROOFS FOR SECTION 3.2

C.1 COVERING IN THE dE METRIC

For any   (0, 1), define

dE, (p, q) =. E 1 sign p x = sign q x

with x iid BG().

(C.1)

We stress that this notion always depend on , and we will omit the subscript  when no confusion is expected. This indeed defines a metric on subsets of Sn-1. Lemma C.1. Over any subset of Sn-1 with a consistent support pattern, dE is a valid metric. Proof. Recall that (x, y) =. arccos x, y defines a valid metric on Sn-1.3 In particular, the triangular inequality holds. For dE and p, q  Sn-1 with the same support pattern, we have

dE (p, q) = E 1 sign p x = sign q x

(C.2)

= 1EEzN(0,I) sign p z = sign q z

(C.3)

= E Ez1 p zqz < 0 + Ez1 p z = 0 or q z = 0, not both

(C.4)

= E Ez1 p zqz < 0

(C.5)

1 =  E (p, q) ,

(C.6)

where we have adopted the convention that (0, v) =. 0 for any v. It is easy to verify that dE (p, q) = 0  p = q, and dE (p, q) = dE (q, p). To show the triangular inequality, note that for any p, q and r with the same support pattern, p, q, and r are either identically zero, or all nonzero. For
the former case,

(p, q)  (p, r) + (q, r)

(C.7)

holds trivially. For the latter, since (·, ·) obeys the triangular inequality uniformly over the sphere,

p , q p q



p , r p r

+

q , r q r

,

(C.8)

which implies

(p, q)  (p, r) + (q, r) .

(C.9)

So

E (p, q)  E (p, r) + E (q, r) ,

(C.10)

completing the proof.
Lemma C.2 (Vector angle inequality). For n  2, consider u, v  Rn so that (u, v)  /2. It holds that

(u, v) 

(u, v) .

(C.11)

([n2 ])

3This fact can be proved either directly, see, e.g., page 12 of this online notes: http://www.math. mcgill.ca/drury/notes354.pdf, or by realizing that the angle equal to the geodesic length, which is the Riemmannian distance over the sphere; see, e.g., Riemannian Distance of Chapter 5 of the book O'neill (1983).

18

Under review as a conference paper at ICLR 2019

Proof. The inequality holds trivially when either of u, v is zero. Suppose they are both nonzero and wlog assume both are normalized, i.e., u = v = 1. Then,

sin2 (u, v) = 1 - cos2 (u, v)

= u 2 v 2 - u, v 2

= (uivj - ujvi)2 (Lagrange's identity)
i,j:j>i

= ([n2 ])
= ([n2 ])

u 2 u 2

v 2 - u, v 2 v 2 sin2 (u, v)

 sin2 (u, v) . ([n2 ])

(C.12) (C.13) (C.14) (C.15)
(C.16)
(C.17)

If ([n2]) (u, v) > /2, the claimed inequality holds trivially, as (u, v)  /2 by our assumption. Suppose ([n2]) (u, v)  /2. Then,

sin2 (u, v)  sin2

(u, v)

([n2 ])

([n2 ])

(C.18)

by recursive application of the following inequality:  1, 2  [0, /2] with 1 + 2  /2, sin2 (1 + 2) = sin2 1 + sin2 2 + 2 sin 1 sin 2 cos (1 + 2)  sin2 1 + sin2 2. (C.19)
So we have that when ([n2]) (u, v)  /2,

sin2 (u, v)  sin2

(u, v) = (u, v) 

(u, v) ,

([n2 ])

([n2 ])

(C.20)

as claimed.
Lemma C.3 (Covering in maximum length-2 angles). For any   (0, 1/3), there exists a subset Q  Sn-1 of size at most (5n log(1/)/)2n-1 satisfying the following: for any p  Sn-1, there exists some q  Q such that (p, q)   for all   [n] with || 2.

Proof. Define

d2(p, q) = max (p, q) ,
||2

our goal is to give an -covering of Sn-1 in the d2 metric.

(C.21)

Step 1 We partition Sn-1 according to the support, the sign pattern, and the ordering of the non-zero elements. For each configuration, we are going to construct a covering with the same configuration of support, sign pattern, and ordering. There are no more than 3n · n! such configurations. Note that
we only need to construct one such covering for each support size, and for each support size we can
ignore the zero entries ­ the angle (p, q) is always zero when p, q have matching support and  contains at least one zero index.

Therefore, the task reduces to bounding the covering number of

An = p  Sn-1 : p  0, 0 < p1  p2  . . .  pn

(C.22)

in d2 for all n.

Step 2

We bound the covering number of An by induction. Suppose that

N (An ) 

5 log(1/) · n 

n -1
= (Cn )n -1

(C.23)

19

Under review as a conference paper at ICLR 2019

holds for all n  n - 1. (The base case m = 2 clearly holds.) Let Cn  Sn -1 be the correpsonding covering sets.
We now construct a covering for An. Let R =. 1/ = rk for some r  1 and k to be determined. Consider the set

Qr,k =

q  Sn-1 :

qi+1  qi

1, r, r2, . . . , rk-1

for all 1  i  n - 1

.

(C.24)

We claim that Qr,k with properly chosen (r, k) gives a covering of

An,R =

p



An

:

pi+1 pi



R,

i

 An.

(C.25)

Indeed, we can decompose [1, R) into [1, r), [r, r2), . . . , [rk-1, R). Each consecutive ratio pi+1/pi
falls in one of these intervals, and we choose q so that qi+1/qi is the left endpoint of this interval. Such a q satisfies q  Qr,k and

pi+1/pi  [1, r) for all i  [n - 1]. qi+1/qi By multiplying these bounds, we obtain that for all 1  i < j  n,

(C.26)

pj /pi  [1, rn-1). qj /qi

(C.27)

Take r = 1 + /2n, we have rn-1 = (1 + /2n)n-1  exp(/2)  1 + . Therefore, for all i, j,

we have

pj /pi qj /qi



[1, 1 + ), which further implies that

((pi, pj), (qi, qj))   by Lemma F.4. Thus

we have for all || 2 that (p, q)  . (The size-1 angles are all zero as we have sign match.)

For this choice of r, we have k = log R/log r and thus

|Qr,k|= kn-1 =

log R n-1 =
log r

and we have N (An,R)  Nn.

log(1/)

n-1



log(1 + /(2n))

4n log(1/) 

n-1 =. Nn,

(C.28)

Step 3 We now construct the covering of An \ An,R. For any p  An \ An,R, there exists some i such that pi+1/pi  [R, ), which means that the angle of the ray (pi, pi+1) is in between [arctan(R), /2) = [/2 - , /2). As p is sorted, we have that

pi+j  R for all j  1, l  0, pi-l

(C.29)

So if we take q such that qi+1/qi  [R, ), q also has the above property, which gives that

((pi-l, pi+j), (qi-l, qi+j))  /2 - (/2 - ) =  for all j  1, l  0.

(C.30)

Therefore to obtain the cover in d2, we only need to consider the angles for   {1, . . . , i} and   {i + 1, . . . , n}, which can be done by taking the product of the covers in Ai and An-i.

By considering all i  {1, . . . , n - 1}, we obtain the bound

n-1
N (An \ An,R)  N (Ai)N (An-i).
i=1

(C.31)

Step 4 Putting together Step 2 and Step 3 and using the inductive assumption, we get that

n-1

N (An)  N (An,R) + N (An \ An,R)  Nn + N (Ai)N (An-i)

i=1



4n log(1/) 

n-1 n-1
+ (Ci)i-1(C(n - i))n-i-1

i=1

(C.32) (C.33)

20

Under review as a conference paper at ICLR 2019



4 5

n-1
(Cn)n-1 + (n - 1) · Cn-2nn-2



4 n-1 1 +
5 C

(Cn)n-1  (Cn)n-1.

This shows the case for m = n and completes the induction.

Step 5 Considering all configurations of {support, sign pattern, ordering}, we have

N (Sn-1)  3n · n! ·N (An)  (3n)n

5 log(1/) n

n-1




5 log(1/) 2n-1 n.


(C.34) (C.35)
(C.36)

Lemma C.4 (Covering number in the dE metric). Assume n  3. There exists a numerical constant C > 0 such that for any  (0, 1), Sn-1 admits an -net of size exp(Cn log n ) w.r.t. dE defined in Eq. (C.1): for any p  Sn-1, there exists a q in the net with supp (q) = supp (p) and dE (p, q)  . We say such nets are admissible for Sn-1 wrt dE.

Proof. Let  = /n2. By Lemma C.3, there exists a subset Q  Sn-1 of size at most

5n log(1/) 2n-1 =

5n3 log(n2/ )

2n-1
 exp

n Cn log



(C.37)

such that for any p  Sn-1, there exists q  Sn-1 such that supp(p) = supp(q) and (p, q)   for all || 2. In particular, the ||= 1 case says that sign(p) = sign(q), which implies that

(p, q)  /2    {0, 1}n .

(C.38)

Thus, applying the vector angle inequality (Lemma C.2), for any p  Sn-1 and the corresponding q  Q, we have

(p, q) 

(p , q )  2

|| 2

  ||2

  with 3  || n.

| |=2, 

(C.39)

Summing up, we get

(p, q)  max 2, ||2   n2 =  .

(C.40)

Therefore dE(p, q)  .

Below we establish the "Lipschitz" property in terms of dE distance. Lemma C.5. Fix   (0, 1). For any  (0, 1), let N be an admissible -net for Sn-1 wrt dE. Let x1, . . . , xm be iid copies of x iid BG() in Rn. When m  C -2n, the inequality

sup
p  Sn-1,q  N

R (p, q) =. 1 m

m
1

sign

p

xi

= sign

q

xi

i=1

supp(p)=supp(q), dE(p,q) 

2

(C.41)

holds with probability at least 1 - exp -c 2m . Here C and c are universal constants independent of .

Proof. We call any pair of p, q  Sn-1 with q  N , supp (p) = supp (q), and dE (p, q)  an admissible pair. Over any admissible pair (p, q), E [R] = dE (p, q). We next bound the deviation R - E [R] uniformly over all admissible (p, q) pairs. Observe that the process R is the sample
average of m indicator functions. Define the hypothesis class

H = x  1 sign p x = sign q x : (p, q) is an admissible pair .

(C.42)

and let dvc(H) be the VC-dimension of H. From concentration results for VC-classes (see, e.g., Eq (3) and Theorem 3.4 of Boucheron et al. (2005)), we have

P sup {R(p, q) - E [R] (p, q)}  C0
(p,q) admissible

dvc(H) + t  exp(-mt2) m

(C.43)

21

Under review as a conference paper at ICLR 2019

for any t > 0. It remains to bound the VC-dimension dvc(H). First, we have
dvc (H)  dvc x  1 sign p x = sign q x : p, q  Sn-1 .

(C.44)

Observe that each set in the latter hypothesis class can be written as

x  1 sign p x = sign q x : p, q  Sn-1 = x  1 p x > 0, q x  0 : p, q  Sn-1  x  1 p x  0, q x < 0 : p, q  Sn-1
(C.45)
 x  1 p x < 0, q x  0 : p, q  Sn-1  x  1 p x  0, q x > 0 : p, q  Sn-1 .
(C.46)

the union of intersections of two halfspaces. Thus, letting

H0 = x  1 x z  0 : z  Rn

(C.47)

be the class of halfspaces, we have

H  (H0 H0) (H0 H0) (H0 H0) (H0 H0).

(C.48)

Note that H0 has VC-dimension n + 1. Applying bounds on the VC-dimension of unions and intersections (Theorem 1.1, Van Der Vaart & Wellner (2009)), we get that

dvc(H)  C1dvc(H0 H0)  C2dvc(H0)  C3n.

(C.49)

Plugging this bound into Eq. (C.43), we can set t = /2 and make m large enough so that C0 C3 n/m  /2, completing the proof.

C.2 POINTWISE CONVERGENCE OF SUB-DIFFERENTIAL

Proposition C.6 (Pointwise convergence). For any fixed q  Sn-1,

P dH (f (q) , E [f ] (q)) > Ca

 n/m + Cbt/ m

 2 exp

-t2

 t > 0.

(C.50)

Here Ca, Cb  0 are universal constants.

Proof. Recall that

dH (f (q) , E [f (q)]) = sup hf(q) (u) - hEf(q) (u) = sup hf(q) (u) - Ehf(q) (u) .

uSn-1

uSn-1

(C.51)

Write Xu =. hf(q) (u) - Ehf(q) (u) and consider the zero-mean random process {Xu} defined on Sn-1. For any u, v  Sn-1, we have

Xu - Xv 2 = hf(q) (u) - Ehf(q) (u) - hf(q) (v) + Ehf(q) (v) 2

(C.52)

1 = C0 m

(hQi (u) - EhQi (u) - hQi (v) + EhQi (v))

i[m]

2

 1/2



C1

1 m



hQi (u) - EhQi (u) - hQi (v) + EhQi (v)

2
2 

i[m]

(C.53) (C.54)

 1/2

1  C2 m 

hQi (u) - hQi (v)

2
2 

i[m]

(centering),

(C.55)

where we write Qi =. sign q xi xi for all i  [m]. Next we estimate hQi (u) - hQi (v) 2 . By definition,

hQi (u) - hQi (v) = sup z, u - sup z , v .

zQi

z Qi

(C.56)

22

Under review as a conference paper at ICLR 2019

If hQi (u) - hQi (v)  0 and let z =. arg maxzQi z, u , we have hQi (u) - hQi (v)  z, u - z, v = z, u - v ,
and

(C.57)

hQi (u) - hQi (v) 2  z, u - v 2  xi (u - v) 2  C3 u - v ,

(C.58)

where we have used Lemma F.1 to obtain the last upper bound. If hQi (u) - hQi (v)  0, hQi (v) - hQi (u)  0 and we can use similar argument to conclude that

hQi (u) - hQi (v) 2  C3 u - v .

(C.59)

So

Xu - Xv

2



C4 m

u-v

.

(C.60)

 Thus, {Xu} is a centered random process with sub-Gaussian increments with a parameter C4/ m.

We can apply Proposition A.2 to conclude that

P

sup

hf(q) (u) - Ehf(q) (u) > C5

 n/m + C6t/ m

 2 exp

-t2

uSn-1

 t > 0, (C.61)

which implies the claimed result.

C.3 PROOF OF PROPOSITION 3.5 (UNIFORM CONVERGENCE)

Throughout the proof, we let c, C denote universal constants that could change from step to step.

Fix an  (0, 1/2) to be decided later. Let N be an admissible net for Sn-1 wrt dE, with |N |  exp(Cn log(n/ )) (Lemma C.4). By Proposition C.6 and the union bound,

P [ q  N , dH (f (q) , E [f ] (q)) > t/3]  exp -cmt2 + Cn log n

(C.62)

provided that m  Ct-2n.

For any p  Sn-1, let q  N satisfy supp (q) = supp (p) and dE (p, q)  . Then we have dH (f (p) , E [f ] (p))  dH (f (q) , E [f ] (q)) + dH (E [f ] (p) , E [f ] (q)) + dH (f (p) , f (q))

I II III
(C.63)

by the triangular inequality for the Hausdorff metric. By the preceding union bound, term I is bounded by t/3 as long as the bad event does not happen. For term II, we have

dH (E [f ] (p) , E [f ] (q))

= sup hE[f](p) (u) - hE[f](q) (u)
uSn-1

= sup E hf(p) (u) - hf(q) (u)
uSn-1

=

 · sup 2 uSn-1

E

sup sign

p

xi

xi u - sup sign

q

xi

xi u



 · sup
2 uSn-1

E

|xi u|1

sign

p

xi

= sign

q

xi



 ·3

1 log .

2

(C.64) (C.65) (C.66) (C.67) (C.68)

where the last line follows from Lemma F.5. As long as  ct/ log(1/t), the above term is upper bounded by t/3. For term III, we have

dH (f (p) , f (q))

23

Under review as a conference paper at ICLR 2019

= sup hf(p) (u) - hf(q) (u)
uSn-1

(C.69)

1

=

· sup 2 m uSn-1 i[m]:sign(p

xi )=sign(q

sup sign
xi )

p

xi

xi u - sup sign

q

xi

xi u

(C.70)

2

=

· sup 2 m uSn-1

i[m]:sign(p

xi )=sign(q

sixi u
xi )

(si  {+1, -1} dependent on p, q, xi and u)

(C.71)

2

=· 2m

sixi .

i[m]:sign(p xi)=sign(q xi)

(C.72)

By Lemma C.5, with probability at least 1 - exp(-c 2m), the number of different signs is upper

bounded be upper

by 2m for bounded as

all p, q such that follows. Define

adsEe(tpT, q)=. {s

. On this good event, the  Rm : si  {+1, -1, 0}

above , s0

quantity  2m }

can and

consider the quantity supsT Xs , where X = [x1, . . . , xm]. Then,

sixi  sup Xs

i[m]:sign(p xi)=sign(q xi)

sT

uniformly (i.e., indepdent of p, q and u). We have

(C.73)

w (T ) = E sup s g = E sup

|gi|

sT

K[m],|K|2m iK

(C.74)



2m 

E

g

  4m

log m ,

(here g  N (0, Im))

(C.75)

rad (T ) = 2m .

(C.76)



Noting that 1/  · X has independent, isotropic, and sub-Gaussian rows with a parameter C/ ,

we apply Proposition A.3 and obtain that

sup

Xs

  n 2m

+ C

sT 

4m

 log m + t0 2m

(C.77)

with probability at least 1 - 2 exp -t20 . So we have over all admissible (p, q) pairs,

dH (f (p) , f (q)) 

· 2 2m

 n 2m

+ C



4m

log m

 + t0 2m

(C.78)

= · 2

8n log m 8

+C m



+ Ct0

. m

(C.79)

 Setting t0 = ct m and = ct /log m, we have that

dH

(f

(p) , f

(q))



t ,
3

(C.80)

provided that m  C t-2n = Ct-1n /log m, which is subsumed by the earlier requirement m  Ct-2n.

Putting together the three bounds Eq. (C.62), Eq. (C.67), Eq. (C.80), we can choose

= ct   ct · min log(m/t)

1 ,
log m log(1/t)

and get that dH (f (p), E [f ] (p))  t with probability at least 1 - 2 exp -cmt2 - exp(-cm 2) - exp -cmt2 + Cn log n

(C.81) (C.82)

24

Under review as a conference paper at ICLR 2019

 1 - 2 exp(-cmt2) - exp cmt2
 1 - exp - log(m/t)

- cmt2 log(m/t)

- exp -cmt2 + Cn log n log(m/t) t

(C.83) (C.84)

provided that m



C nt-2

log

n

log(m/t) t

.

A sufficient condition is that m



Cnt-2 log2(n/t)

for sufficiently large C. When this is satisfied, the probability is further lower bounded by 1 -

exp(-cmt2/log m).

C.4 PROOF OF THEOREM 3.6

Define

t

=

1 32n3/2 (1

- )0



min



1 8n3/2

(1

-

)

1

0 + 0

,

2- 2 16n3/2

(1

-

)0

.

(C.85)

By Proposition 3.5, with probability at least 1 - exp -cm302n-3 log-1 m we have

dH (E [f ] (q) , f (q))  t,

(C.86)

provided that m  Cn4-20-2 log (n/0). We now show the properties Eq. (3.12) and Eq. (3.13) on this good event, focusing on S(0n+) but obtaining the same results for all other 2n - 1 subsets by
the same arguments.

For Eq. (3.12), we have

Rf (q) , ej/qj - en/qn = f (q) , I - qq (ej/qj - en/qn) = f (q) , ej/qj - en/qn . (C.87)

Now

sup f (q) , en/qn - ej/qj = hf(q) (en/qn - ej /qj ) = Ehf(q) (en/qn - ej /qj ) - Ehf(q) (en/qn - ej /qj ) + hf(q) (en/qn - ej /qj )
 Ehf(q) (en/qn - ej /qj ) + en/qn - ej /qj sup Ehf(q) (u) - hf(q) (u)
uSn-1
= sup E [f ] (q) , en/qn - ej/qj + en/qn - ej/qj dH (E [f ] (q) , f (q)) .
By Theorem 3.4(a),

(C.88) (C.89) (C.90)
(C.91)

sup

E [f ] (q) , en - qnq

 - 1  (1 - ) 0 . 2n 1 + 0

(C.92)

Moreover, en/qn - ej/qj =

1/qn2 + 1/qj2 

 1/qn2 + 3/qn2  2 n. Meanwhile, we have

We conclude that

dH (E [f ] (q) , f

(q))



t



1  (1 - ) 8n3/2

0 . 1 + 0

(C.93)

inf f (q) , ej/qj - en/qn = - sup f (q) , en/qn - ej/qj

(C.94)

 1  (1 - ) 0

 -2 n·

1  (1 - )

0

· 1

(C.95)

2n 1 + 0 4n 1 + 0 2 n

 1  (1 - ) 0 , 4n 1 + 0

(C.96)

as claimed.

For Eq. (3.13), we have by Theorem 3.4(b) that

sup f (q) , en - qnq = hf(q) (en - qnq)

(C.97)

25

Under review as a conference paper at ICLR 2019

= Ehf(q) (en - qnq) - Ehf(q) (en - qnq) + hf(q) (en - qnq) (C.98)

 Ehf(q) (en - qnq) + en - qnq sup Ehf(q) (u) - hf(q) (u)
uSn-1
(C.99)
= sup E [f ] (q) , en - qnq + q-n dH (E [f ] (q) , f (q)) . (C.100)

As we are on the good event



dH

(E [f ] (q) , f

(q))



t



2- 2 16n3/2

·  (1

- ) 0,

we have

(C.101)

inf f (q) , qnq - en = - sup f (q) , en - qnq



1 
8

(1

-

) 0n-3/2

w

-

q-n





2 16



(1

-

)

0n-3/2

q-n

.

(C.102)



2- 16

2 ·  (1 - ) 0n-3/2

(C.103)

(C.104)

Noting that

q-n

 1
2

q - en

for all q with qn  0 completes the proof.

C.5 PROOF OF PROPOSITION 3.7

For any q  Sn-1,

sup f (q) = dH ({0} , f (q))  dH ({0} , E [f ] (q)) + dH (f (q) , E [f ] (q)) (C.105)

by the metric property of the Hausdorff metric. On one hand, we have

sup E [f ] (q) = sup E

q q

1 {q = 0} + {v :

v

 1} 1 {q = 0}

 1. (C.106)

On the other hand, by Proposition 3.5,

dH (f (q) , E [f ] (q))  1  q  Sn-1

(C.107)

with probability at least 1 - exp -c1m log-1 m , provided that m  C2n2 log n (simplified using   1/n). Combining the two results complete the proof.

C.6 ADDITIONAL GEOMETRIES ON THE EMPIRICAL OBJECTIVE

Proposition C.7. On the good event in Theorem 3.6, the following holds: for all q  S(0n+),

f

(q)

-

f

(en)



1  (1
16

-

) n-3/20

q - en

,

f (q) - f (en)  2 n q - en .

(C.108) (C.109)

Proof. We use Lebourg's mean value theorem4 for locally Lipschitz functions, i.e., Theorem 2.3.7 of Clarke (1990). It is convenient to work in the w space here. We have

f (q) - f (en) = g (w) - g (0) = v, w

(C.110)

for a certain t0  (0, 1) and a certain v  g (t0w). Now for any q and the corresponding w,

1

g (w) , w

= qn

Rf (q) , q - en

.

(C.111)

4It is possible to directly apply the manifold version of Lebourg's mean value theorem, i.e., Theorem 3.3 of Hosseini & Pouryayevali (2011). We avoid this technicality by working with the Euclidean version in w space.

26

Under review as a conference paper at ICLR 2019

By the above equality and also the good event in Theorem 3.6(b), we have

v, w

 1 inf t0

g (t0w) , t0w

11 = inf
t0 qn (t0w) 

Rf (q (t0w)) , q (t0w) - en



1 t0

qn

1 (t0w)

2  (1
16

- ) n-3/20

t0w



2 16



(1

-

)

n-3/20

w



1  (1 - 16

) n-3/20

q - en

.

On the other hand,

(C.112) (C.113) (C.114) (C.115)

v, w



1 sup

t0

g (t0w) , t0w

11 = sup
t0 qn (t0w)

Rf (q (t0w)) , q (t0w) - en

 sup Rf (q (t0w)) ·

q (t0w) - en t0qn (t0w)

 2 q (t0w) - en , t0qn (t0w)

and we have

(C.116)

q (t0w) - en t0qn (t0w)

 2
=
=

1/2
1 - 1 - t20 w 2

t0

1 - t02

w 

2

2t0 w

t0 1 - t20 w 2   2 1 - qn2 qn 1 + qn = 2 1 - qn  qn  n q - en ,

1 + 1 - t20 w 2

1/2

completing the proof.

(C.117)

D PROOFS FOR SECTION 3.3

D.1 STAYING IN THE REGION S(0n+)

Lemma D.1 (Progress in S(0n+) \ S1(n+)).

 Set  = t0/(100 n) for t0

 (0, 1). For any 0

 (0, 1),

on the good events stated in Proposition 3.7 and Theorem 3.6, we have for all q  S(0n+) \ S1(n+)

and q+ being the next step of Riemannian subgradient descent that

q+2 ,n q+,-n

2 



qn2

q-n

2 

1

+

t

 (1 - ) 0 400n3/2 (1 + 0

)

2
.

(D.1)

In particular, we have q+  S(0n+).

Proof. We divide the index set [n - 1] into three sets
I0 =. {j  [n - 1] : qj = 0} , I1 =. j  [n - 1] : qn2 /qj2 > 1 + 21 = 3, qj = 0 I2 =. j  [n - 1] : qn2 /qj2  1 + 21 = 3 .

(D.2) (D.3) (D.4)

27

Under review as a conference paper at ICLR 2019

We perform different arguments on different sets. We let g (q)  Rf (q) be the subgradient taken at q and note by Proposition 3.7 that g  2, and so |gi|  2 for all i  [n]. We have

q+2 ,n q+2 ,j

=

(qn (qj

- gn)2 / - gj)2 /

q - g q - g

2 2

=

(qn (qj

- -

gn)2 gj )2

.

(D.5)

For any j  I0,

q+2 ,n q+2 ,j

=

(qn - gn)2 2gj2

=

qn2

(1

-

gn/qn 2gj2

)2



(1 - 2n)2 4n2 .



Provided that   1/(4 n), 1 - 2 n  1/2, and so

(1 - 2n)2 4n2



1 16n2



5 ,
2



where the last inequality holds when   1/ 40n.

(D.6) (D.7)

For any j  I1,

q+2 ,n q+2 ,j



qn2 (1 - gn/qn)2 qj2 + 2gj2



qn2 (1 - gn/qn)2 qn2 /3 + 42

=

3 (1 - gn/qn)2 1 + 122/qn2



3 (1 - 2n)2 1 + 12n2



5 ,
2

(D.8)

 where the very last inequality holds when   1/(26 n).

Since q  S(0n+) \ S1(n+), I2 is nonempty. For any j  I2,

q+2 ,n q+2 ,j

=

qn2 qj2

1

+



gj /qj 1-

- gn/qn gj /qj

2
.



(D.9)

Since gj/qj  2 3n, 1 - gj/qj  1/2 when   1/ 4 3n . Conditioned on this and due to that

gj/qj - gn/qn  0, it follows

1+ gj/qj - gn/qn 1 - gj/qj

2
 [1+2 (gj/qj -gn/qn)]2 

1+2

 2 3n+2 n

2


 1+11 n

2

(D.10)

If qn2 /qj2  2, q+2 ,n/q+2 ,j  5/2 provided that

 1 + 11 n

2



5/2

=

5

=





1 .

24

100 n

(D.11)

As q / S1(n+), we have qn2 /

q-n

2 

 2, so there must be a certain j

 I2 satisfying qn2 /qj2  2.

We conclude that when

  min  1 , 1 , 1 = 1 , 40n 26 n 100 n 100 n

(D.12)

the index of largest entries of q+,-n remains in I2.

 On the other hand, when   1/(100 n), for all j  I2,

1 +  gj/qj - gn/qn 1 - gj/qj

2
 [1 +  (gj/qj - gn/qn)]2 

1 +   (1 - ) 0 4n 1 + 0

2
.

(D.13)



So when  = t/(100 n) for any t  (0, 1),

completing the proof.

q+2 ,n q+,-n

2 



qn2

q-n

2 

1

+

t

 (1 - ) 0 400n3/2 (1 + 0

)

2
,

(D.14)

28

Under review as a conference paper at ICLR 2019

Proposition D.2. For any 0  (0, 1), on the good events stated in Proposition 3.7 and Theorem 3.6, if the step sizes satisfy

(k)  min

1 , 1 -0 for all k,

100 n 9 n

(D.15)

the iteration sequence will stay in S(0n+) provided that our initialization q(0)  S(0n+).

Proof. By Lemma D.1, if the current iterate q 
provided that   1/(100 n). Now if the current

 q

S(0n+) \  S1(n+),

S1(n+), the next iterate i.e., qn2 /qj2  2 for all j

q+  S(0n+),  [n - 1], we

can emulate the analysis of the set I1 in proof of Lemma D.1. Indeed, for any j  [n - 1],

q+2 ,n q+2 ,j



qn2 (1 - gn/qn)2 qj2 + 2gj2



qn2 (1 - 2n)2 qn2 /2 + 42



2 (1 - 2n)2 1 + 8n2



1 + 0,

(D.16)

 where the last inequality holds provided that   (1 - 0) /(9 n). Combining the two cases finishes

the proof.

D.2 PROOF OF THEOREM 3.8

As we have (k)



1 100 n

and q(0)



S(0n+),

the entire sequence

q(k) k0 will stay in S(0n+)

by Proposition D.2.

For any q and any v  Rf (q), we have v, q = 0 and therefore q - v 2 = q 2 + 2 v 2  1.
So q - v is not inside Bn. Since projection onto Bn is a contraction, we have

(D.17)

q+ - en 2 =

q - v q - v

2
- en 

q - v - en 2



q - en 2 +2

v 2 -2 v, q - en



q - en

2

+

42

-

1 8



(1

-

)

n-3/2

0

q - en

,

(D.18)

where we have used the bounds in Proposition 3.7 and Theorem 3.6 to obtain the last inequality. Further applying Proposition C.7, we have

q+ - en 2 

q - en

2

+

42

-

1 
16

(1

-

)

n-20

(f

(q)

-

f

(en))

.

Summing up the inequalities until step K (assumed  5), we have

(D.19)

2K
0  q(K) - en + 4

(j)

2

-

1 
16

(1

-

)

n-20

K

(j)

f

q(j)

- f (en)

j=1

j=1

K
= (j)
j=1

f

q(j)

- f (en)

16 

q(K) - en 2 + 64

K j=1

(j) 2

 (1 - ) n-20

= f

qbest

16 - f (en) 

q(K) - en 2 + 64  (1 - ) n-20

K j=1

(j)

K j=1

(j)

2
.

Substituting the following estimates

(D.20) (D.21) (D.22)

K j=1

(j)

21  104n

K 0

t-2

dt



1 104n

1

1 - 2

(K )1-2

,

K

(j)



1 102 n

j=K

K 1

t-

dt



1 102 n

(K)1- - 1 1-

(D.23) (D.24)

29

Under review as a conference paper at ICLR 2019

and noting 16 q(K) - en 2  32, we have

Noting that

f

qbest

-f

(en)



3200n5/2

(1 

- (1

)

+

16/25

·

n3/2

1- 1-2

- ) 0 (K1- - 1)

K

1-2

.

(D.25)

1

6400n5/2 (1 - )

1-

3200n5/2 (1 - )

K

+1  (1 - ) 0

=

 (1

- ) 0 (K1-

- 1)



, 2

and when K  5 and   (0, 1/2], 1  K1-/2, yielding that

(D.26)

K

64n3/2

1- 1-2

25  (1 - ) 0

1



=

32n3/2

1- 1-2

K

-

25 (1 - ) 0



2

=

16n3/2

1- 1-2

K

1-2

25 (1 - ) 0 (K1- - 1)



. 2

(D.27)

So we conclude that when

1

 6400n5/2 (1 - )

1-

K  max

+1 ,

  (1 - ) 0

64n3/2

1- 1-2

25  (1 - ) 0

1

 ,


(D.28)

f qbest - f (en)  0. When this happens, by Proposition C.7,

qbest - en

 16n3/2 .  (1 - ) 0

(D.29)

Plugging in the choice 0 = 1/(5 log n) in Eq. (D.28) gives the desired bound on the number of iterations.

E PROOFS FOR SECTION 3.4

E.1 PROOF OF LEMMA 3.9

Lemma E.1. For all n  3 and   0, it holds that

vol S(n+)

1 9 log n

vol (Sn-1)

- 2n 8

n

.

(E.1)

We note that a similar result appears in (Gilboa et al., 2018) but our definitions of the region S are slightly different. For completeness we provide a proof in Lemma F.3.

We now prove Lemma 3.9. Taking  = 1/(5 log n) in Lemma E.1, we obtain

vol S1(n/(+5)log n) vol (Sn-1)

 1 - 9 log n ·

1



1 .

2n 8 n 5 log n 4n

(E.2)

By symmetry, all the 2n sets S1(i/+(5)log n), S1(i/-(5)log n) : i  [n] have the same volume which is
at least 1/(4n). As q(0)  Uniform(Sn-1), it falls into their union with probability at least 2n · 1/(4n) = 1/2, on which it belongs to a uniformly random one of these 2n sets.

E.2 PROOF OF THEOREM 3.10
Assume that the good event in Proposition 3.7 happens and that in Theorem 3.6 happens to all the 2n sets S1(i/+(5)log n), S1(i/-(5)log n) : i  [n] , which by setting 0 = 1/(5 log n) has probability at least 1 - exp(-cm302n-3 log-1 m) - exp(-cm log-1 m) = 1 - exp(-c m3n-3 log m-3). (E.3)

30

Under review as a conference paper at ICLR 2019

By Lemma 3.9, random initialization will fall these 2n sets with probability at least 1/2. When it

falls in one basis vector

of these 2n sets, by Theorem 3.8, one up to accuracy. With R independent

rruunnso, fatthleeaasltgSor=i.th41mRwoifllthfienmd

a signed standard are effective with

probability at least 1 - exp -(R/4)2/(R/4 · 2) = 1 - exp (-R/8), due to Bernstein's inequality.

After these effective runs, the probability any standard basis vector is missed (up to sign) is bounded

by

1S

S

S

n 1-

 exp - + log n  exp - ,

nn

2n

(E.4)

where the second inequality holds whenever S  2n log n.

F AUXILIARY CALCULATIONS

Lemma F.1. For x  BG(), x 2  Ca. For any vector u  Rn and x iid BG(), x u 2  Cb u . Here Ca, Cb  0 are universal constants.

Proof. For any   R,

exp (x) =  exp (x)  exp (x) .

(F.1)

So x 2 is bounded by a universal constant. Moreover,

u x 2 =

uixi  C1
i 2

1/2

u2i

xi

2 2

i

 C2 u ,

as claimed.

Lemma F.2. Let a1, . . . , am be iid copies of a iid BG(). Then,



P  sup

q xi - E q x

qSn-1 i[m]



 > Ca mn + Cb mt  2 exp

-t2

.

(F.2) (F.3)

for any t  0. Here Ca, Cb  0 are universal constants.

Proof.

Consider the zero-centered random process defined on Sn-1: Xq =.

i[m] q xi - E q x . Then, for any p, q  Sn-1,

Xp - Xq 2 =

p xi - q xi - E p x + E q x

i[m]

2

 1/2

 C1 

p xi - q xi - E p x + E q x

2
2 

i[m]

 1/2

 C2 

p xi - q xi

2
2 

i[m]

(centering)



 C2 

(p - q)

i[m]
 = C3 m p - q ,

1/2
2
xi 
2

(F.4) (F.5) (F.6) (F.7) (F.8)

where we use the estimate in Lemma F.1 to obtain random process, and we can invoke Proposition A.2

twheithlawst(iSnne-qu1a)l=ity.CN4otne

that and

Xq rad

is a mean-zero Sn-1 = 2 to

get the claimed result.

31

Under review as a conference paper at ICLR 2019

Lemma F.3. For all n  3 and   0, it holds that

vol S(n+)

1 9 log n

vol (Sn-1)

- 2n 8

n

.

(F.9)

Proof. We have

vol S(n+) vol (Sn-1)

= Pquniform(Sn-1)

qn2  (1 + )

q-n

2 

, qn



0

(F.10)

= PxN (0,In) xn  0, xn2  (1 + ) xi2  i = n

= (2)n/2


 n-1
e-x2n/2 
0 j=1



xn/ 1+ 

e-x2j /2

dxj 

dxn

-xn/ 1+

(F.11) (F.12)

= (2)1/2 
= 1 +  2


e-x2n/2n-1 xn/ 1 +  dxn

0


e-(1+)x2/2n-1 (x)

dx =. h¯ () > 0,

0

(F.13) (F.14)

where we write (t) =. 1
2

t -t

exp

-s2/2

ds. Now we derive a lower bound of the volume ratio

by considering a first-order Taylor expansion of the last equation around  = 0 (as we are mostly

interested in small ). By symmetry, ¯h (0) = 1/(2n). Moreover, we have

 h¯ ( )

= 1 1


e-x2/2n-1 (x)

dx -

1


e-x2/2x2n-1 (x) dx

 =0 2 2 0

2 0

= 1 - 1


e-x2/2x2n-1 (x) dx.

4n 2 2 0

(F.15) (F.16)

Now we provide an upper bound for the second term of the last equation. Note that

1 2


e-x2/2x2n-1 (x) dx = ExN (0,In) xn2 1 x2n 
0

1 = 2n ExN (0,In)

x

2 

.

x-n

2 

1 {xn  0}

(F.17) (F.18)

Now for any   (0, 1/2),

exp

E

x

2 

 E exp



x

2 

n
 E exp x2j
j=1

= nExN (0,1) exp x2

 n . 1 - 2 (F.19)

Taking logarithm on both sides, rearranging the terms, and setting  = 1/4, we obtain

E

x

2 



inf
(0,1/2)

log n +

1 2

log (1 - 2)-1 



4 log n + 2 log 2.

(F.20)

So

¯h ()  1 - 1 (4 log n + 2 log 2)  - 9 log n ,

 =0 4n 4n

8n

(F.21)

provided that n  3. Now we show that ¯h ()  ¯h (0) + h¯ (0) by showing that h¯ ()  0. We have

2¯h () 2

=

 1 + 
4 2

 0

x4

-

2x2 1+

-

(1

1 + )2

e-

1+ 2

x2

n-1

(x)

dx.

(F.22)

Using integration by part, we have

 x4 - 3x2

e-

1+ 2

x2

n-1

(x)

dx

0 1+

(F.23)

32

Under review as a conference paper at ICLR 2019

=-

1



e-

1+ 2

x2 x3

·

n-1

(x)

+



1

e-

1+ 2

x2

x3

(n

-

1)

n-2

(x)

1+

0 0 1+

2

e-

x2 2

dx



(F.24)

=



1

e-

1+ 2

x2

x3

(n

-

1)

n-2

(x)

0 1+

2

e-

x2 2

dx  0,



(F.25)

and similarly


x2 -

1

e-

1+ 2

x2

n-1

(x)

dx

0 1+

=-

1



e-

1+ 2

x2

x

·

n-1

(x)

+



1

e-

1+ 2

x2

x

(n

-

1)

n-2

(x)

1+

0 0 1+

(F.26)

2

e-

x2 2

dx



(F.27)

=



1

e-

1+ 2

x2

x

(n

-

1)

n-2

(x)

0 1+

2

e-

x2 2

dx  0.



(F.28)

Noting that

x4

-

2x2 1+

-

1 (1 + )2

=

x4 -

3x2 1+

+

1 1+

x2 - 1 1+

(F.29)

and combining the above integral results, we conclude that ¯h ()  0 and complete the proof.

Lemma F.4. Let (x1, y1), (x2, y2)  R>2 0 be two points in the first quadrant satisfying y1  x1 and

y2



x2,

and

y2 /x2 y1 /x1



[1, 1 + ]

for

some





1,

then

we

have

((x1, y1), (x2, y2))  .

Proof. For i = 1, 2, let i be the angle between the ray (xi, yi) and the x-axis. Our assumption implies that i  [/4, /2) and 2  1, thus ((x1, y1), (x2, y2)) = 2 - 1, so we have

tan ((x1, y1), (x2, y2))

= tan 2 - tan 1 1 + tan 2 tan 1

= y2/x2 - y1/x1 1 + y2y1/(x2x1)

=

y2 /x2 y1 /x1

-1

y2/x2 + x1/y1

 y2/x2 - 1 y1/x1
 .

(F.30)

Therefore ((x1, y1), (x2, y2))  arctan()  .

Lemma F.5. For any p, q  Sn-1 with the same support pattern such that dE(p, q) 



1 2

,

we

have for all u  Sn-1 that

ExBG() |u x|1 sign(p x) = sign(q x)

3

1 log .

(F.31)

Proof. Fix some threshold t > 0 to be determined. We have
E |u x|1 sign(p x) = sign(q x)  E |u x|1 |u x|> t + E |u x|1 |u x| t, sign(p x) = sign(q x)  E (u x)2 · P |u x|> t 1/2 + tE 1 sign(p x) = sign(q x)
  · 2 exp(-t2/2) 1/2 + t.

(F.32) (F.33) (F.34) (F.35)

33

Under review as a conference paper at ICLR 2019

The second to last inequality uses Cauchy-Schwarz, and the last inequality uses the fact that u x =

u x is u 22-sub-Gaussian conditioned on  and thus 1-sub-Gaussian marginally. Taking t =

2 log

1
2

,

the

above

bound

simplifies

to

2 exp

- log

1
2

+

1 2 log 2 = 2

1 1 + log 2 =

where we have used   1/2 and  1/2.

 2 +

1 2 log 2

3

1 log

(F.36)

34

