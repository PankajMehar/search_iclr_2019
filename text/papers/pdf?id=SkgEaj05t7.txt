Under review as a conference paper at ICLR 2019
On the Relation between the sharpest directions of DNN Loss and the SGD step length
Anonymous authors Paper under double-blind review
Abstract
Training of deep neural networks with Stochastic Gradient Descent (SGD) typically ends in regions of the weight space, where both the generalization properties and the flatness of the local loss curvature depend on the learning rate and the batch size. We discover that a related phenomena happens in the early phase of training and study its consequences. Initially, SGD visits increasingly sharp regions of the loss surface, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. At this early peak value, an SGD step is on average too large to minimize the loss along the directions corresponding to the largest eigenvalues of the Hessian (i.e. the sharpest directions). To query the importance of this phenomena for training, we study a variant of SGD using a reduced learning rate along the sharpest directions and show that it can improve training speed while finding both sharper and better­generalizing solution, compared to vanilla SGD. Overall, our results show that the SGD dynamics along the sharpest directions influence the regions of the weight space visited, the overall training speed, and the generalization ability of the final model.
1 Introduction
Deep Neural Networks (DNNs) are often massively over-parameterized (Zhang et al., 2016), yet show state-of-the-art generalization performance on a wide variety of tasks when trained with Stochastic Gradient Descent (SGD). While understanding the generalization capability of DNNs remains an open challenge, it has been hypothesized that SGD acts as an implicit regularizer, limiting the complexity of the found solution (Poggio et al., 2017; Advani and Saxe, 2017; Wilson et al., 2017; Jastrzbski et al., 2017).
Various links between the curvature of the final minima reached by SGD and generalization have been studied (Murata et al., 1994; Neyshabur et al., 2017). In particular, it is a popular view that models corresponding to the so-called wide minima of the loss in the parameter space generalize better than sharp ones (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016; Jastrzbski et al., 2017; Wang et al., 2018). The existence of the empirical correlation between curvature of the final minima and generalization motivates our study.
Our work aims at understanding the interaction between SGD and the sharpest directions of the loss surface, i.e. those corresponding to the largest eigenvalues of the Hessian. In contrast to studies such as those by Keskar et al. (2016) and Jastrzbski et al. (2017) we focus mostly on the early phase of training and SGD dynamics rather than on the endpoint. We will show in Sec. 3.1 that the evolution of the largest eigenvalues of the Hessian follows a consistent pattern for the different networks and datasets that we explore. Initially, SGD is in a region of broad curvature, and as the loss decreases, SGD visits regions in which the top eigenvalues of the Hessian are increasingly large, reaching a peak value, with a magnitude influenced by both learning rate and batch size. After that point in training, we typically observe a decrease or stabilization of the largest eigenvalues.
To understand the role of this early bias of SGD, in Sec. 3.2 and Sec. 3.3 we study the dynamics of SGD in relation to the sharpest directions. Projecting to the sharpest directions, we see that the region found at the end of the early phase resembles a bowl with curvature such that an SGD step is typically too large, in the sense that an SGD step cannot get near
1

Under review as a conference paper at ICLR 2019

A BC B AC

200
150
100
50
0 0

50 100 Iteration

150

125 100
75 50 25
0 0

100 200 Epoch

300

Figure 1: Left: Outline of the phenomena discussed in the paper. Curvature along the

sharpest direction initially grows (A). In most iterations, we find that SGD crosses the

minimum, often taking too large a step (B and C); the sharpest direction(s) matches the

SGD step. Finally, curvature stabilizes or decays with a peak value determined by learning

rate and batch size (C, see also right). Right two: Representative example of the evolution

of the top 30 (decreasing, red to blue) eigenvalues of the Hessian for a SimpleCNN model

during

training

(with



=

0.005,

note

that



is

close

to

1 max

=

1 160

).

the minimum of this bowl-like subspace; rather it steps from one side of the bowl to the other. We refer to this relation as the matching of the DNN's sharpest directions and the SGD step, see Fig. 1 for an illustration. Finally in Sec. 4 we study further practical consequences of our observations and investigate an SGD variant which uses a reduced and fixed learning rate along the sharpest directions. In most cases we find this variant optimizes faster and leads to a sharper region, which generalizes better compared to vanilla SGD withe the same (small) learning rate.
Overall this paper shows that in the beginning SGD steers to a region whose curvature matches the SGD step. We further argue that this influences the magnitude of the largest eigenvalues of the Hessian, the training speed, and the final generalization capability.

2 Experimental setup and notation
We perform experiments mainly on Resnet-321 and a simple convolutional neural network CNN, which we refer to as SimpleCNN (details in the Appendix D), and the CIFAR-10 dataset (Krizhevsky et al.). SimpleCNN is a 4 layer CNN, achieving roughly 86% test accuracy on the CIFAR-10 dataset. For training both of the models we use standard data augmentation on CIFAR-10 and for Resnet-32 L2 regularization with coefficient 0.005. We additionally investigate the training of VGG-11 (Simonyan and Zisserman, 2014) on the CIFAR-10 dataset (we adapted the final classification layers for 10 classes) and of a bidirectional Long Short Term Memory (LSTM) model (following the "small" architecture employed by Zaremba et al. (2014), with added dropout regularization of 0.1) on the Penn Tree Bank (PTB) dataset. All models are trained using SGD.
The notation and terminology we use in this paper are now described. We will use t (time) to refer to epoch or iteration, depending on the context. By  and S we denote the SGD learning rate and batch size, respectively. H is the Hessian of the empirical loss at the current D-dimensional parameter value evaluated on the training set, and its eigenvalues are denoted as i, i = 1 . . . D (ordered by their absolute magnitudes). max is the maximum eigenvalue, which is equivalent to the spectral norm of H. The top K eigenvectors of H are denoted by ei, for i  {1, . . . , K}, and referred to in short as the sharpest directions. We will refer to the mini-batch gradient calculated based on a batch of size S as g(S)(t) and to g(S)(t) as the SGD step. We will often consider the projection of this gradient onto one of the top eigenvectors, given by g~i(t) = g~i(t)ei(t), where g~i(t)  g(S)(t), ei(t) .
Computing the full spectrum of the Hessian H for reasonably large models is computationally infeasible. Therefore, we approximate the top K (up to 50) eigenvalues using the
1In Resnet-32 we omit Batch-Normalization layers due to their interaction with the loss surface curvature (Bjorck et al., 2018) and use initialization scaled by the depth of the network (Taki, 2017). Additional results on Batch-Normalization are presented in the Appendix

2

Under review as a conference paper at ICLR 2019

SimpleCNN 20 10
0 50 Epo1c0h0 150

Accuracy

100 % 80 % 60 % 40 %
0

SimpleCNN
Accuracy Validation accuracy 50 Epo1c0h0 150

(a) SimpleCNN

SimpleCNN (zoom)

40 % SimpleCNN (zoom)

Accuracy

50 30 %

25
0 0.0 0.1 0E.2poc0h.3 0.4 0.5

20 % Accuracy

10 %0.0

Validation accuracy 0.1 0E.2poc0h.3 0.4 0.5

(c) SimpleCNN (zoom)

Resnet-32 30 20 10
0 50 Epo1c0h0 150

Accuracy

100 % 75 % 50 % 25 % 0

Resnet-32
Accuracy Validation accuracy 50 Epo1c0h0 150

(b) Resnet-32

Resnet-32 (zoom)

Resnet-32 (zoom)

20 0 0.0 0.1 0E.2poc0h.3 0.4 0.5

Accuracy

20 %

15 % Accuracy

10 %0.0

Validation accuracy 0.1 0E.2poc0h.3 0.4 0.5

(d) Resnet-32 (zoom)

Figure 2: Top: Evolution of the top 10 eigenvalues of the Hessian for SimpleCNN and Resnet-32 trained on the CIFAR-10 dataset with  = 0.1 and S = 128. Bottom: Zoom on the evolution of the top 10 eigenvalues in the beginning of training. A sharp initial growth of the largest eigenvalues followed by an oscillatory-like evolution is visible. Training and test accuracy of the corresponding models are provided for reference.

Lanczos algorithm (Lanczos, 1950; Dauphin et al., 2014), an extension of the power method, on approximately 5% of the training data (using more data was not beneficial). Where applicable, the Hessian is estimated with regularization applied (such as dropout, L2 or data augmentation). The code for the project is made available at https://goo.gl/gXmtyG.
3 A study of the Hessian along the SGD path
We study here the geometric and dynamical properties of the region SGD is steered to in the early phase of training, and show their importance for training. We highlight that SGD is steered in the early phase to a region where the SGD step length is too large compared to the curvature. Moreover, SGD finds a flatter region along the sharpest directions for a larger learning rate or smaller batch-size.
3.1 Largest eigenvalues of the Hessian along the SGD path
We first investigate the training loss curvature in the sharpest directions, along the training trajectory for both the SimpleCNN and Resnet-32 models.
Largest eigenvalues of the Hessian grow initially. In the first experiment we train SimpleCNN and Resnet-32 using SGD with  = 0.1 and S = 128 and estimate the 10 largest eigenvalues of the Hessian, throughout training. As shown in Fig. 2 (top) the spectral norm (which corresponds to the largest eigenvalue), as well as the other tracked eigenvalues, grows in the first epochs up to a maximum value. After reaching this maximum value, we observe a relatively steady decrease of the largest eigenvalues in the following epochs. To investigate the evolution of the curvature in the first epochs more closely, we track the eigenvalues at each iteration during the beginning of training, see Fig. 2 (bottom). We observe that initially the magnitudes of the largest eigenvalues grow rapidly. After reaching some maximum value, the largest eigenvalues behave in a stochastic and somewhat oscillatory way; this behaviour is also reflected in the evolution of the accuracy. This suggests SGD is initially driven to regions that are difficult to optimize due to large curvature. We relegate a further study of this phenomena, in the case of full-batch training, to the Appendix A.1. Finally, we report some additional results on the late phase of training, e.g. the impact of learning rate schedules, in Fig. 10 in the Appendix.
Learning rate and batch-size limit the maximum spectral norm. Next, we investigate how the choice of learning rate and batch size impacts the SGD path in terms of its
3

Under review as a conference paper at ICLR 2019

103 ResNet-32, : 10 3/10 2/10 1 SimpleCNN, : 10 3/10 2/10 1

LSTM, : 5 10 1/10 1/2 10 1

102 102

101

101 100

101

100 10 1

0.0 1.0 Ep2o.0ch 100 200 0.0 1.0 Ep2o.0ch 100 200 0.0 0.25 Ep0o.5ch 20 30

(a) Effect of learning rate on curvature along SGD trajectory

102 ResNet-32, S:1024/128/32 102 SimpleCNN, S:1024/128/32

LSTM, S:128/32/8

101

101 101

100

100 10 1

0.0 1.0 Ep2o.0ch 100 200 0.0 1.0 Ep2o.0ch 100 200 0.0 0.25 Ep0o.5ch 20

30

(b) Effect of batch size on curvature along SGD trajectory

Figure 3: Evolution of the two largest eigenvalues (solid and dashed line) of the Hessian for Resnet-32, SimpleCNN, and LSTM (trained on the PTB dataset) models on a log-scale for different learning rates (top) and batch-sizes (bottom). Red/green/blue correspond to increasing  and decreasing S in each figure. Left side of the vertical blue bar in each plot corresponds to the early phase of training. Larger learning rate or a smaller batch-size correlates with a smaller and earlier peak of the spectral norm and the next largest eigenvalue.

curvatures. Fig. 3 shows the evolution of the two largest eigenvalues of the Hessian during training of the SimpleCNN and Resnet-32 on CIFAR-10, and an LSTM on PTB, for different values of  and S. We observe in this figure that a larger learning rate or a smaller batch-size correlates with a smaller and earlier peak of the spectral norm and the subsequent largest eigenvalue. Note that the phase in which curvature grows for low learning rates or large batch sizes can take many epochs. Additionally, momentum has an analogous effect ­ using a larger momentum leads to a smaller peak of spectral norm, see Fig. 11 Appendix. Similar observations hold for VGG-11 and Resnet-32 using Batch-Normalization, see Appendix A.2.
Summary. These results show that the learning rate and batch size not only influence the SGD endpoint maximum curvature, but also impact the whole SGD trajectory. A high learning rate or a small batch size limits the maximum spectral norm along the path found by SGD from the beginning of training.
3.2 Sharpest direction and SGD step
The training dynamics (which later we will see affect the speed and generalization capability of learning) are significantly affected by the evolution of the largest eigenvalues discussed in Section 3.1. To demonstrate this we study the relationship between the SGD step and the loss surface shape in the sharpest direction(s). As we will show, SGD dynamics are largely coupled with the shape of the loss surface in the sharpest direction in the sense that when projected onto the sharpest direction, the typical step taken by SGD is too large to enable to approach the minimum. We study the same SimpleCNN and Resnet-32 models as in the previous experiment in the first 6 epochs of training with SGD with =0.01 and S = 128.
The sharpest direction matches the SGD step. First, we investigate the relation between the SGD step and the sharpest direction by looking at how the loss value changes on average when moving from the current parameters taking a step only along the sharpest direction - see Fig. 4 left. For all training iterations, we compute E[L((t)-g~1(t))]-L((t)), for   {0.25, 0.5, 1, 2, 4}; the expectation is approximated by an average over 10 different mini-batch gradients. We find that E[L((t) - g~1(t))] increases relative to L((t)) for  > 1, and decreases for  < 1. More specifically, for SimpleCNN we find that  = 2 and  = 4 lead to a 2.1% and 11.1% increase in loss, while  = 0.25 and  = 0.5 both lead to a decrease of approximately 2%. For Resnet-32 we observe a 3% and 13.1% increase for  = 2 and  = 4, and approximately a 0.5% decrease for  = 0.25 and  = 0.5, respectively.
4

Under review as a conference paper at ICLR 2019

Loss (%) Loss (%)
Epoch Epoch

SimpleCNN e1 40
20

10 Resnet-32 e1 5

00

-20 -5

-40 0

2 Epoch4

6 -10 0

2 Epoch4

6

SimpleCNN e1

2.6 2.4
22..02L
1.8 1.6 1.4

5 k0 5

3.0 4.0 5.0 6.0

Resnet-32 e1

2.6
22..24L
2.0

5 k0 5

3.0 4.0 5.0 6.0

Figure 4: Early on in training, SGD finds a region such that in the sharpest direction, the SGD step length is often too large compared to curvature. Experiments on SimpleCNN and Resnet32, trained with  = 0.01, S = 128, learning curves are provided in the Appendix. Left two: Average change in loss E[L((t) - g~1(t))] - L((t)), for  = 0.5, 1, 2 corresponding to red, green, and blue, respectively. On average, the SGD step length in the direction of the sharpest direction does not minimize the loss. The red points further show that increasing the step size by a factor of two leads to increasing loss (on average). Right two: Qualitative visualization of the surface along the top eigenvectors for SimpleCNN and Resnet-32 support that SGD step length is large compared to the curvature along the top eigenvector. At iteration t we plot the loss L((t) + ke11(t)), around the current parameters (t), where
1(t) is the expected norm of the SGD step along the top eigenvector e1. The x-axis represents the interpolation parameter k, the y-axis the epoch, and the z-axis the loss value, the color indicated spectral norm in the given epoch (increasing from blue to red).

The observation that SGD step does not minimize the loss along the sharpest direction suggests that optimization is ineffective along this direction. This is also consistent with the observation that learning rate and batch-size limit the maximum spectral norm of the Hessian (as both impact the SGD step length).
Additionally, we compute the average cosine between the mini-batch gradient gS(t) and the top 5 sharpest directions and ei(t). We find the gradient to be highly aligned with the sharpest direction, depending on  and model the maximum average cosine is roughly between 0.2 and 0.4. Full results are relegated to in Fig. 12 in the Appendix.
Qualitatively, SGD step crosses the minimum along the sharpest direction. Next, we qualitatively visualize the loss surface along the sharpest direction in the first few epochs of training, see Fig 4 (right). To better reflect the relation between the sharpest direction and the SGD step we scaled the visualization using the expected norm of the SGD step 1(t) = E(|g~1(t)|) where the expectation is over 10 mini-batch gradients. Specifically, we evaluate L((t) + ke11(t)), where (t) is the current parameter vector, and k is an interpolation parameter (we use k  [-5, 5]). For both SimpleCNN and Resnet-32 models, we observe that the loss on the scale of 1(t) starts to show a bowl-like structure in the largest eigenvalue direction after six epochs. This further corroborates the previous result that SGD step length is large compared to curvature in the sharpest direction.
Training and validation accuracy are reported in Appendix B. Furthermore, in the Appendix B we demonstrate that a similar phenomena happens along the lower eigenvectors, for different , and in the later phase of training.
Summary. We infer that in the early phase of training, SGD reaches a region in which the SGD step matches the curvature of the sharpest direction. In particular, the SGD step would typically overshoot the minimum in this subspace, if restricted to it. These observations suggest that optimization is ineffective along the sharpest direction, which we will further study in Sec. 4.
3.3 How SGD steers to sharp regions in the early phase
Here we discuss the dynamics around the initial growth of the spectral norm of the Hessian. We will look at some variants of SGD which change how the sharpest directions get optimized.
5

Under review as a conference paper at ICLR 2019

Validation accuracy Loss
max max
Test||acH|c|Furacy ||H||F

1.7 750

1.6 Baseline

1.5 Top

1.4

Const. Top No Top

500 250

0 200 4It0e0rati6o0n0 800 1000

0

Ite5ra0t0ion

1000

Figure 5: Evolution of the loss and the top eigenvalue during training with variations of SGD with parameter updates restricted to certain subspaces. All variants are initialized at the last iteration in Fig. 1. An SGD variant (orange) that follows at each iteration only the projection of the gradient on the top eigenvector of H effectively finds a region with lower maxbut increases the loss, while a variant subtracting this projection from the gradient (red), finds a sharper region while achieving a similar loss level as vanilla SGD (blue).

0.8 0.6 0.4
0

K=1 K=5 K = 10 K = 20 SGD(0.01)
2E0p0och 400

8000 6000 4000 2000
00

K=1 K=5 K = 10

10000 7500

K = 20 SGD(0.01)

5000

2500

20E0poch 400 0

88.0 87.5 87.0

1500
1000
500
5 10K 15 20

Figure 6: Nudged-SGD for an increasing number of affected sharpest directions (K) optimizes significantly faster, whilst generally finding increasingly sharp regions. Experiment is run using Resnet-32 and the Cifar-10 dataset. Left and center: Validation accuracy and the max and Frobenius norm (y axis, solid and dashed) using increasing K (blue to red) compared against SGD baseline using the same  (black), during training (x axis). Rightmost: Test accuracy (red) and Frobenius norm (blue) achieved using NSGD with an increasing K (x axis) compared to an SGD baseline using the same  (blue and red horizontal lines).

Experiment. We used the same SimpleCNN model initialized with the parameters reached by SGD in the previous experiment at the end of epoch 4. The parameter updates used by the three SGD variants, which are compared to vanilla SGD (blue), are based on the mini-batch gradient g(S) as follows: variant 1 (SGD top, orange) only updates the parameters based on the projection of the gradient on the top eigenvector direction, i.e. g(t) = g(S)(t), e1(t) e1(t); variant 2 (SGD constant top, green) performs updates along the constant direction of the top eigenvector e1(0) of the Hessian in the first iteration, i.e. g(t) = g(S)(t), e1(0) e1(0); variant 3 (SGD no top, red) removes the gradient information in the direction of the top eigenvector, i.e. g(t) = g(S)(t) - g(S)(t), e1(t) e1(t). We show results in the left two plots of Fig. 5. We observe that if we only follow the top eigenvector, we get to wider regions but don't reach lower loss values, and conversely, if we ignore the top eigenvector we reach lower loss values but sharper regions.
Summary. The take-home message is that SGD updates in the top eigenvector direction strongly influence the overall path taken in the early phase of training. Based on these results in the next section we will study a related variant of SGD throughout the training.
4 Optimizing faster while finding a good sharp region
We are now ready to return to the original motivation of this work and investigate the practical importance of the sharpest directions for training DNNs. To recapitulate, results in Sec. 3 suggest reducing the SGD step along the sharpest directions might improve training speed (because the optimization does not make progress along it), while steering towards sharper regions of the loss surface. We investigate this via a variant of SGD, which we call Nudged-SGD (NSGD). Instead of using the standard SGD update, (t) = -g(S)(t), NSGD uses a different learning rate,  = , along just the top K eigenvectors, while
6

Under review as a conference paper at ICLR 2019

Table 1: Nudged-SGD optimizes faster and finds a sharper final endpoint with a slightly better generalization performance. Experiments on Resnet-32 model on the Cifar-10 dataset. Columns: the Frobenius norm of the Hessian at the best validation point and the final epoch; test accuracy; validation at epoch 50; cross-entropy loss in the final epoch; distance of the parameters from the initialization to the best validation epoch parameters. Experiments were performed with  = 0.01 and S = 128. In the last rows we report SGD using   {0.005, 0.1}.

 = 0.01  = 0.1  = 1.0  = 5.0
SGD(0.005) SGD(0.1)

||H||F
1, 018/1, 019 931/929 272/324 111/83
678/946 24/25

Test acc.
87.3% 87.2% 86.6% 85.5%
86.0% 88.0%

Val. acc. (50)
69.3% 69.0% 63.0% 52.3%
49.9% 84.7%

Loss
0.01324 0.00940 0.01719 1.17170
0.06221 0.00100

Dist.
21.08 21.32 22.90 25.32
18.87 50.73

following the normal SGD gradient along all the others directions2. In particular we will study NSGD with a low base learning rate , which will allow us to capture any implicit regularization effects NSGD might have. We ran experiments with Resnet-32 and SimpleCNN on CIFAR-10. Note that we do not use here state-of-the-art models, which we leave for future work.
We investigated NSGD with a different number of sharpest eigenvectors K, in the range between 1 and 20; and with the rescaling factor   {0.01, 0.1, 1, 5}. The top eigenvectors are recomputed at the beginning of each epoch. We compare the sharpness of the reached endpoint by both computing the Frobenius norm (approximated by the top 50 eigenvectors), and the spectral norm of the Hessian. The learning rate is decayed by a factor of 10 when validation loss has not improved for 100 epochs. Experiments are averaged over two random seeds. When talking about the generalization we will refer to the test accuracy at the best validation accuracy epoch. Results for Resnet-32 are summarized in Fig. 6 and Tab. 1; for full results on SimpleCNN we relegate the reader to Appendix, Tab. 2. In the following we will highlight the two main conclusions we can draw from the experimental data.
NSGD optimizes faster, whilst traversing a sharper region. First, we see that in the early phase of training NSGD optimizes significantly faster than the baseline, whilst traversing a region which is an order of magnitude sharper. We start by looking at the impact of K which controls the amount of eigenvectors with adapted learning rate; we test K in {1, . . . , K} with a fixed  = 0.01. On the whole, increasing K correlates with a significantly improved training speed and visiting much sharper regions (see Fig. 6). We highlight that NSGD with K = 20 reaches maximum max of approximately 8 · 103 compared to baseline SGD reaching approximately 150. Further, NSGD retains an advantage of over 5% (1% for SimpleCNN) validation accuracy, even after 50 epochs of training (see Tab. 1).
NSGD can improve the final generalization performance, while finding a sharper final region. Next, we turn our attention to the results on the final generalization and sharpness. We observe from Tab. 1 that using  < 1 can result in finding a significantly sharper endpoint exhibiting a slightly improved generalization performance compared to baseline SGD using the same  = 0.01. On the contrary, using  > 1 led to a wider endpoint and a worse generalization, perhaps due to the added instability. Finally, using a larger K generally correlates with an improved generalization performance (see Fig. 6, right).
More specifically, baseline SGD using the same learning rate reached 86.4% test accuracy with the Frobenius norm of the Hessian ||H||F = 272 (86.6% with ||H||F = 191 on SimpleCNN). In comparison, NSGD using  = 0.01 found endpoint corresponding to 87.0% test accuracy and ||H||F = 1018 (87.4% and ||H||F = 287 on SimpleCNN). Finally, note that in the case of Resnet-32 model K = 20 leads to 88% test accuracy and ||H||F = 1100 which closes the
2While NSGD is a second order method, it crucially does not use eigenvalues magnitudes. See also Appendix E for a discussion on differences between NSGD and Newton method.
7

Under review as a conference paper at ICLR 2019
generalization gap to SGD using  = 0.1. We note that runs corresponding to  = 0.01 generally converge at final cross-entropy loss around 0.01 and over 99% training accuracy.
As discussed in (Sagun et al., 2017) the structure of the Hessian can be highly dataset dependent, thus the demonstrated behavior of NSGD could be dataset dependent as well. In the Appendix C we include results on the CIFAR-100 and Fashion MNIST (Xiao et al., 2017) datasets, but studies on more diverse datasets are left for future work. Finally, we relegate to the Appendix C additional studies using a high base learning and momentum.
Summary. We have investigated what happens if SGD uses a reduced learning rate along the sharpest directions. We show that his variant of SDG, i.e. NSGD, is capable of optimizing faster and finding good generalizing sharp minima, i.e. regions of the loss surface at the convergence which are sharper compared to those found by vanilla SGD using the same low learning rate, while exhibiting better generalization performance. Note that in contrast to Dinh et al. (2017) the sharp regions that we investigate here are the endpoints of an optimization procedure, rather than a result of a mathematical reparametrization.
5 Related work
Tracking the Hessian: The largest eigenvalues of the Hessian of the loss of DNNs were investigated previously but mostly in the late phase of training. Some notable exceptions are: LeCun et al. (1998) who first track the Hessian spectral norm, and the initial growth is reported (though not commented on). Sagun et al. (2016) report that the spectral norm of the Hessian reduces towards the end of training. Keskar et al. (2016) observe that a sharpness metric grows initially for large batch-size, but only decays for small batch-size. Our observations concern the eigenvalues and eigenvectors of the Hessian, which follow the consistent pattern, as discussed in Sec. 3.1.
Wider minima generalize better: Hochreiter and Schmidhuber (1997) argued that wide minima should generalize well. Keskar et al. (2016) provided empirical evidence that the width of the endpoint minima found by SGD relates to generalization and the used batch-size. Jastrzbski et al. (2017) extended this by finding a correlation of the width and the learning rate to batch-size ratio. Dinh et al. (2017) demonstrated the existence of reparametrizations of networks which keep the loss value and generalization performance constant while increasing sharpness of the associated minimum, implying it is not just the width of a minimum which determines the generalization. Recent work further explored importance of curvature for generalization (Wen et al., 2018; Wang et al., 2018).
Stochastic gradient descent dynamics. Our work is related to studies on SGD dynamics such as Goodfellow et al. (2014); Chaudhari and Soatto (2017); Xing et al. (2018); Zhu et al. (2018). In particular, recently Zhu et al. (2018) investigated the importance of noise along the top eigenvector for escaping sharp minima by comparing at the final minima SGD with other optimizer variants. In contrast we show that from the beginning of training SGD visits regions in which SGD step is too large compared to curvature. Concurrent with this work Xing et al. (2018) by interpolating the loss between parameter values at consecutive iterations show it is roughly-convex, whereas we show a related phenomena by investigating the loss in the subspace of the sharpest directions of the Hessian.
6 Conclusions
The somewhat puzzling empirical correlation between the endpoint curvature and its generalization properties reached in the training of DNNs motivated our study. We showed that SGD steers from the beginning to a region of the loss surface in which the SGD step length is too large compared to the curvature scale. Furthermore, reducing the alignment between the SGD step and the sharpest directions can improve training speed and generalization, while steering SGD towards sharper regions. On the whole, we propose that understanding the behavior of optimization along the sharpest directions (not only at the final minima) is a promising avenue for understanding generalization properties of neural networks, and designing novel optimizers tailor-fit to neural networks.
8

Under review as a conference paper at ICLR 2019
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv preprint arXiv:1710.03667, 2017.
J. Bjorck, C. Gomes, and B. Selman. Understanding Batch Normalization. ArXiv e-prints, May 2018.
P. Chaudhari and S. Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. ArXiv e-prints, October 2017.
Yann Dauphin, Razvan Pascanu, Çaglar Gülçehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. CoRR, abs/1406.2572, 2014. URL http://arxiv.org/abs/ 1406.2572.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933, 2017.
I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization problems. ArXiv e-prints, December 2014.
Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017.
Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1­42, 1997.
Stanislaw Jastrzbski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017.
N.S.= Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ArXiv e-prints, September 2016.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/~kriz/cifar.html.
Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. J. Res. Natl. Bur. Stand. B, 45:255­282, 1950. doi: 10.6028/jres.045.026.
Yann LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. Efficient backprop. In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 9­50, London, UK, UK, 1998. Springer-Verlag. ISBN 3-540-65311-2. URL http://dl.acm.org/citation.cfm?id=645754.668382.
Noboru Murata, Shuji Yoshizawa, and Shun ichi Amari. Network information criteriondetermining the number of hidden units for an artificial neural network model. IEEE transactions on neural networks, 5 6:865­72, 1994.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. CoRR, abs/1706.08947, 2017. URL http://arxiv.org/ abs/1706.08947.
Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-overfitting puzzle. arXiv preprint arXiv:1801.00173, 2017.
Levent Sagun, Léon Bottou, and Yann LeCun. Singularity of the hessian in deep learning. arXiv preprint arXiv:1611.07476, 2016.
9

Under review as a conference paper at ICLR 2019
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.
Masato Taki. Deep residual networks and weight initialization. CoRR, abs/1709.02956, 2017. URL http://arxiv.org/abs/1709.02956.
H. Wang, N. Keskar, C. Xiong, and R. Socher. Identifying Generalization Properties in Neural Networks. ArXiv e-prints, September 2018.
W. Wen, Y. Wang, F. Yan, C. Xu, C. Wu, Y. Chen, and H. Li. SmoothOut: Smoothing Out Sharp Minima to Improve Generalization in Deep Learning. ArXiv e-prints, May 2018.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. 2017.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. ArXiv e-prints, August 2017.
C. Xing, D. Arpit, C. Tsirigotis, and Y. Bengio. A Walk with SGD. ArXiv e-prints, February 2018.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization, 2014. URL https://arxiv.org/abs/1409.2329.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Z. Zhu, J. Wu, B. Yu, L. Wu, and J. Ma. The Regularization Effects of Anisotropic Noise in Stochastic Gradient Descent. ArXiv e-prints, February 2018.
10

Under review as a conference paper at ICLR 2019

A Additional results for Sec. 3.1

A.1 Full-batch size training instability and curvature
In Sec. 3.1 we rather briefly discussed that the instability in the eigenvalues of the Hessian seem to co-incidence with the sharp drops in training accuracy. To study this further we look at a full-batch Gradient Descent training of Resnet-323, motivated by the instability of large-batch size training reported in the literature, see for instance (Goyal et al., 2017). Experiments are reported in Fig. 7. In the case of Resnet-32 without Batch-Normalization we can clearly see that the magnitude of the largest eigenvalues of the Hessian grows initially (in agreement with results in the main text), which is followed by a sharp drop in accuracy suggesting instability of the optimization. We also can see that the instability is partially solved through use of Batch-Normalization layers, consistent with (Bjorck et al., 2018).

Resnet-32 =0.01

Resnet-32 =0.01

25 %

Accuracy

50 20 % 15 %

0 0 20 4E0poc6h0 80 100 10 % 0 20 4E0poc6h0 80 100

(a) Resnet-32,  = 0.01, BN

Resnet-BN-32 =0.01 1000

60 % Resnet-BN-32 =0.01

Accuracy

40 % 500

20 %

0 0 20 4E0poc6h0 80 100

0 20 4E0poc6h0 80 100

(c) Resnet-32,  = 0.01, without BN

150 100 50
00

Resnet-32 =0.05 20 4E0poc6h0 80 100

Accuracy

20 % 15 % 10 %
0

Resnet-32 =0.05 20 4E0poc6h0 80 100

(b) Resnet-32,  = 0.05, BN

Resnet-BN-32 =0.05

Resnet-BN-32 =0.05

Accuracy

1000 40 %

500 20 %

0 0 20 4E0poc6h0 80 100

0 20 4E0poc6h0 80 100

(d) Resnet-32,  = 0.05, without BN

Figure 7: Evolution of the top 10 eigenvalues of the Hessian for Resnet-32 trained on the CIFAR-10 dataset with  = 0.01 and  = 0.05 (columns) and with (top) or without (bottom) Batch-Normalization.

A.2 Additional results on the evolution of the spectral norm
We extend results of Sec. 3.1 to VGG-11 and Batch-Normalized Resnet-32 models, see Fig. 9 and Fig. 8. Importantly, we evaluated Hessian in the inference mode, which seems to result in large absolute magnitudes of the eigenvalues on Resnet-32.

VGG11, : 10 3/10 2/10 1 103
102

103 102

VGG11, S:512/128/32

101 101

100 100 0.0 1.0 Ep2o.0ch 100 200 0.0 1.0 Ep2o.0ch 100 200

Figure 8: Same as Fig. 3, but for the VGG-11 architecture.

3To avoid memory limitations we preselected the first 2560 examples of CIFAR-10 to simulate full-batch gradient training for this experiment.
11

Under review as a conference paper at ICLR 2019

max
Accuracy

150

L=40 L=20 L=80 L=160

100

50

0 0 50 Ep1o0c0h 150

200 100 2000

1.0 0.9 0.8 0.7
0

50 Ep1o0c0h

L=40 L=20 L=80 L=160
150

0.10 0.08 0.06 0.04 0.02
0

L=40 L=20 L=80 L=160
50 Ep10o0ch 150 200

Figure 10: Depending on the length of the first stage of a learning rate schedule, spectral norm of the Hessian and generalization performance evolve differently. Left: Spectral norm of the Hessian (solid) and Frobenius norm (dashed), during training (x axis). Center: Accuracy and validation accuracy, during training (x axis). Right: Learning rate, during training (x axis).

ResNet-BN-32, : 10 3/10 2/10 1 103 102 101
0.0 1.0 Ep2o.0ch 100 200

103 ResNet-BN-32, S:512/128/8

102

101 0.0

1.0 Ep2o.0ch 100 200

Figure 9: Same as Fig. 3, but for Resnet-32 architecture using Batch-Normalization layers.

A.3 Impact of learning rate schedule
In the paper we focused mostly on SGD using a constant learning rate and batch-size. We report here the evolution of the spectral and Frobenius norm of the Hessian when using a simple learning rate schedule in which we vary the length of the first stage L; we use  = 0.1 for L epochs and drop it afterwards to  = 0.01. We test L in {10, 20, 40, 80}. Results are reported in Fig. 10. The main conclusion is that depending on the learning rate schedule in the next stages of training curvature along the sharpest directions (measured either by the spectral norm, or by the Frobenius norm) can either decay or grow. Training for a shorter time (a lower L) led to a growth of curvature (in term of Frobenius norm and spectral norm) after the learning drop, and a lower final validation accuracy.

A.4 Impact of using momentum
In the paper we focused mostly on experiments using plain SGD, without momentum. In this section we report that large momentum similarly to large  leads to a reduction of spectral norm of the Hessian, see Fig. 11 for results on the VGG11 network on CIFAR10 and CIFAR100.

max max

800 700 600 500 400

= 0.0 = 0.3 = 0.6 = 0.8

300

200

100

600 500 400 300 200 100

= 0.0 = 0.3 = 0.6 = 0.8

0 0 20 4E0poc6h0 80 100

0 0 20 4E0poc6h0 80 100

Figure 11: Momentum limits the maximum spectral norm of the Hessian throughout training. Training at  = 0.1 and S = 128 on VGG11 and CIFAR10 (left) and CIFAR100 (right).

12

Under review as a conference paper at ICLR 2019

Cosine Cosine

Resnet-32 0.3

0.2 Random vector

0.1

= 0.001 = 0.01

0.0

= 0.1
20 30Acc4u0ra5c0y (6%0) 70 80

0.3 SimpleCNN 0.2 0.1 0.0 30 4A0ccu5r0acy60(%7)0 80

Figure 12: Average cosine between mini-batch gradient (y axis) and top sharpest directions (averaged over top 5) for different  (color) evaluated at different level of accuracies, during training (x axis). For comparison, the horizontal purple line is alignment with a random vector in the parameter space. Curves were smoothed for clarity.

B Additional results for Sec. 3.2
First, in Fig. 12 we report full results for the cosine alignment computation. We observe that mini-batch gradients are relatively (in comparison to a random vector) highly aligned with the sharpest directions.
In Fig. 13 we report the corresponding training and validation curves for the experiments depicted in Fig. 4. Next, we plot an analogous visualization as in Fig. 4, but for the 3rd and 5th eigenvector, see Fig. 14 and Fig. 15, respectively. To ensure that the results do not depend on the learning rate, we rerun the Resnet-32 and SimpleCNN experiments with  = 0.05, see Fig. 16.
Finally, we run the same experiment as in Sec. 3.2, but instead of focusing on the early phase we replot Fig. 4 for the first 200 epochs, see Fig. 17

Accuracy Accuracy

0.6 0.4
0.4

0.2

Val. accuracy 0.2 Train accuracy

Val. accuracy Train accuracy

0 25I0terat5io0n0 750

0 25I0terat5io0n0 750

Figure 13: Training and validation accuracy for experiments in Fig 4. Left corresponds to SimpleCNN. Right corresponds to Resnet-32.

Loss (%) Loss (%)
Epoch Epoch

SimpleCNN e3 40
20

10 Resnet-32 e3 5

00

-20 -5

-40 0

2 Epoch4

6 -10 0

2 Epoch4

6

SimpleCNN e3

2.6 2.4
22..02L
1.8 1.6 1.4

5 k0 5

3.0 4.0 5.0 6.0

Resnet-32 e3

2.6
22..24L
2.0

5 k0 5

3.0 4.0 5.0 6.0

Figure 14: Similar to Fig 4 but computed for the third eigenvector of the Hessian. We see that the loss surface is much flatter in this direction. To facilitate a direct comparison, scale of each axis is kept the same as in Fig. 4.

13

Under review as a conference paper at ICLR 2019

Loss (%) Loss (%)

SimpleCNN e4 40
20

10 Resnet-32 e4 5

00

-20 -5

-40 0

2 Epoch4

6 -10 0

2 Epoch4

6

SimpleCNN e5

2.6 2.4
22..02L
1.8 1.6 1.4

5 k0 5

3.0 4.0 5.0 6.0

Resnet-32 e5

2.6
22..24L
2.0

5 k0 5

3.0 4.0 5.0 6.0

Figure 15: Similar to Fig 4 but for the fourth eigenvector. We see that the loss surface is much flatter in this direction. To facilitate a direct comparison, scale of each axis is kept the same as in Fig. 4.

Epoch Epoch Epoch Epoch Epoch Epoch

Loss (%) Loss (%)

SimpleCNN e1 40
20

10 Resnet-32 e1 5

00

-20 -5 -40
0 2 Epoch4 6 -10 0 2 Epoch4 6

SimpleCNN e1

2.50 2.25
12..7050L
1.50 1.25 1.00

3.0

4

2 k0

2

4.0 5.0 4 6.0

Resnet-32 e1

2.6 2.4
22..02L
1.8 1.6

3.0

4

2 k0

2

4.0 5.0 4 6.0

Figure 16: Similar to Fig 4 but for Resnet-32 (top) and SimpleCNN (bottom) with  = 0.05.

Loss (%) Loss (%)

SimpleCNN e1 40

Resnet-32 e1 40

20 20

00

-20 -20

-40 -40

0 100Epoch200 300

0 100Epoch200 300

SimpleCNN e1

2.0
1.5L
1.0 0.5

150

5 k0

5

200 250 300

Resnet-32 e1

2.5
2.0
1.5L
1.0 0.5

150

5 k0

5

200 250 300

Figure 17: Similar to Fig 4 but for Resnet-32 (top) and SimpleCNN (bottom) run for 200 epochs.

C Additional results for Sec. 4
Here we report additional results for Sec. 4. Most importantly, in Tab. 2 we report full results for SimpleCNN model. Next, we rerun the same experiment, using the Resnet-32 model, on the CIFAR-100 dataset, see Tab. 5, and on the Fashion-MNIST dataset, see Tab. 6. In the case of CIFAR-100 we observe that conclusion carry-over fully. In the case of Fashion-MNIST we observe that the final generalization for the case of  < 1 and  = 1.0 is similar. Therefore, as discussed in the main text, the behavior seems to be indeed dataset dependent.
In Sec. 4 we purposedly explored NSGD in the context of suboptimally picked learning rate, which allowed us to test if NSGD has any implicit regularization effect. In the next two experiments we explored how NSGD performs when using either a large base learning , or when using momentum. Results are reported in Tab. 3 (learning rate  = 0.1) and Tab. 4 (momentum µ = 0.9). In both cases we observe that NSGD improves training speed and reaches a significantly sharper region initially, see Fig. 18. However, the final region curvature in both cases, and the final generalization when using momentum, is not significantly affected. Further study of NSGD using a high learning rate or momentum is left for future work.
14

Under review as a conference paper at ICLR 2019

max max

100 50 00

100 Epo2c0h0

=0.01 200 150= 0.1
= 1.0 =5
100 50 300 0

200 150 100 50
00

=0.01 300
= 0.1 = 1.0
=5 200 100
20E0poch 400 0

Figure 18: Evolution of the spectral norm and Frobenius norm of the Hessian (y axis, solid and dashed line, respectively) for different  (color) for the experiment using a larger learning (left) and momentum (right), see text for details.

Table 2: Same as Tab. 1, but for Simple-CNN model. NSGD with  = 5.0 diverged and was excluded from the Table.

 = 0.01  = 0.1  = 1.0  = 5.0
SGD(0.005) SGD(0.05)

||H||F
423/429 294/268 127/127 130/129
476/516 34/26

Test acc.
87.1% 87.4% 86.6% 85.2%
85.8% 87.4%

Val. acc. (50)
83.5% 83.3% 82.3% 79.8%
78.0% 85.1%

Loss
0.00143 0.00126 0.00063 0.00160
0.00862 0.00008

Dist.
17.63 17.98 19.76 20.26
14.94 35.38

 = 0.01  = 0.1  = 1.0

||H||F
32/34 20/26 25/19

Test acc.
89.3% 89.2% 87.9%

Val. acc. (50)
85.8% 85.9% 83.2%

Loss
0.00322 0.00105 0.00246

Dist.
49.04 50.73 50.70

Table 3: Same as Tab. 1, but for Resnet-32 and NSGD using base learning rate  = 0.1. NSGD with  = 5.0 diverged and was excluded from the Table.

 = 0.01  = 0.1  = 1.0  = 5.0

||H||F
20/20 36/32 27/34 35/44

Test acc.
89.4% 89.3% 89.2% 88.2%

Val. acc. (50)
86.9% 87.1% 85.8% 83.9%

Loss
0.00225 0.00082 0.00239 0.00438

Dist.
47.57 47.14 49.57 51.07

Table 4: Same as Tab. 1, but for Resnet-32 and NSGD using momentum coefficient 0.9.

name
 = 0.01  = 0.1  = 1.0  = 5.0

||H||F
294/431 257/341 159/162 75/101

Test acc.
92.6% 92.4% 92.5% 91.7%

Val. acc. (50)
90.3% 90.1% 89.8% 89.2%

Loss
0.05300 0.04340 0.04672 0.07820

Dist.
13.73 13.62 14.85 14.65

Table 5: Same as Tab. 1, but for Simple-CNN and NSGD on the Fashion-MNIST dataset.

15

Under review as a conference paper at ICLR 2019

name
 = 0.01  = 0.1  = 1.0  = 5.0

||H||F
1, 315/1, 544 797/1, 192 470/720 240/378

Test acc.
56.9% 57.4% 54.1% 53.2%

Val. acc. (50)
27.8% 27.0% 25.6% 20.6%

Loss
0.18048 0.29081 0.28299 0.51348

Dist.
31.37 31.18 32.67 34.20

Table 6: Same as Tab. 1, but for Resnet-32 and NSGD on the CIFAR-100 dataset.

D SimpleCNN Model
The SimpleCNN used in this paper has four convolutional layers. The first two have 32 filters, while the third and fourth have 64 filters. In all convolutional layers, the convolutional kernel window size used is (3,3) and `same' padding is used. Each convolutional layer is followed by a ReLU activation function. Max-pooling is used after the second and fourth convolutional layer, with a pool-size of (2,2). After the convolutional layers there are two linear layers with output size 128 and 10 respectively. After the first linear layer ReLU activation is used. After the final linear layer a softmax is applied. Please also see the provided code.

E Comparing NSGD to Newton Method
Nudged-SGD is a second order method in the sense that it leverages curvature of the loss surface. In this section we argue that it is significantly different from the Newton method, a representative second order method.
The key reason for that is that NSGD similarly to SGD is driven to a region in which curvature is too large compared to its typical step. In other words NSGD does not use an optimal learning rate for the curvature, which is the key design principle for second order methods. This is visible in Fig. 19, where we report results of a similar study as in Sec. 3.2, but for NSGD (K = 5,  = 0.01). The loss surface appears sharper in this plot, because reducing gradients along the top K sharpest directions allows optimizing over significantly sharper regions.
As discussed, the key difference stems for the early bias of SGD to reach maximally sharp regions. It is therefore expected that in case of a quadratic loss surface Newton and NSGD optimizers are very similar. In the following we construct such an example. First, recall that update in Newton method is typically computed as:

(t + 1) = (t) - (H + I)-1gS((t)),

(1)

where  is a scalar. Now, if we assume that H is diagonal and put diag(H) = [100, 100, 100, 100, 100, 1, 1], and finally let  = 0, it can be seen that the update of NSGD with  = 0.01 and K = 5 is equivalent to that of Newton method.

Loss (%) Loss (%)
Epoch Epoch

SimpleCNN + NSGD e1 40
20
0
-20
-40 0 2 Epoch4 6

10 Resnet-32 + NSGD e1 5 0 -5 -10 0 2 4 6 E8p1o0ch12 14 16 18 20

SimpleCNN + NSGD e1 2.6 2.4 22..02L 1.8 1.6 1.4

5 k0 5

3.0 4.0 5.0 6.0

Resnet-32 + NSGD e1 2.6 2.4 22..02L 1.8 1.6 1.4 k5 0 5 1150.50-..000.0

Figure 19: Same as Fig. 4, but for NSGD with  = 0.01 and K = 5.

16

