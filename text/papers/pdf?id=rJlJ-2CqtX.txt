Under review as a conference paper at ICLR 2019
SUCCESS AT ANY COST: VALUE CONSTRAINED MODEL-FREE CONTINUOUS CONTROL
Anonymous authors Paper under double-blind review
ABSTRACT
Applying Reinforcement Learning algorithms to continuous control problems ­ such as locomotion and robot control ­ often results in policies which rely on high-amplitude, high-frequency control signals, known colloquially as bang-bang control. While such policies can implement the optimal solution, particularly in simulated systems, they are often not desirable for real world systems since bangbang control can lead to increased wear and tear and energy consumption and tends to excite undesired second-order dynamics. To counteract this issue, multiobjective optimization can be used to simultaneously optimize both the reward and some auxiliary cost that discourages undesired (e.g. high-amplitude) control. In principle, such an approach can yield the sought after, smooth, control policies. It can, however, be hard to find the correct trade-off between cost and return that results in the desired behavior. In this paper we propose a new constraint-based approach which defines a lower bound on the return while minimizing one or more costs (such as control effort). We employ Lagrangian relaxation to learn both (a) the parameters of a control policy that satisfies the desired constraints and (b) the Lagrangian multipliers for the optimization. Moreover, we demonstrate policy optimization which satisfies constraints either in expectation or in a perstep fashion, and we learn a single conditional policy that is able to dynamically change the trade-off between return and cost. We demonstrate the efficiency of our approach using both the cart-pole swing-up task as well as a realistic, energyoptimized quadruped locomotion task.1
1 INTRODUCTION
Deep Reinforcement Learning (RL) has achieved great successes over the last couple of years, enabling learning of effective policies from high-dimensional input, such as pixels, on complicated tasks. However, compared to problems with discrete action spaces, continuous control problems with high-dimensional continuous state-action spaces ­ as often encountered in robotics ­ have proven much more challenging. Beyond the issue of exploration in high-dimensional continuous action spaces, RL algorithms rarely learn policies that produce smooth control signals when just optimizing for success. Instead, policies often exhibit control signals that switch between extreme values at high-frequency, often colloquially referred to as bang-bang control. Smoothness, however, is a desirable property in most real-world control problems. Unnecessary oscillations are not only energy inefficient, they also exert stress on a physical system by exciting second-order dynamics and increasing wear and tear on structural elements and actuators.
To regularize the behavior, one can add penalties to the reward function. As a result, the reward function is composed of positive reward for achieving the goal and negative reward (penalties) for control action discontinuities or high energy use. This effectively casts the problem into a multiobjective optimization setting, where ­ depending on the ratio between the reward and the different penalties ­ different behaviors may be achieved. While every ratio will have its optimal policy, finding the ratio that results in the desired behavior, i.e. smooth control while still achieving an acceptable task success rate, is not always trivial and requires excessive hyperparameter tuning. Often, one must find different hyperparameter settings for different reward-penalty trade-offs or tasks. The process of finding these parameters is tedious and cumbersome, and may prevent robust
1Videos available at https://sites.google.com/view/minitauriclr2019
1

Under review as a conference paper at ICLR 2019
general solutions. In this paper we rephrase the problem: instead of trying to find the right ratios between reward and penalties, we regularize the optimization problem by adding constraints, thereby reducing its effective dimensionality. More specifically, we propose to minimize the penalty with respect to a lower bound on the success rate of the task.
Using a Lagrangian relaxation technique, we introduce cost coefficients for each of the imposed constraints that are tuned automatically during the optimization process. In this way we can find the optimal trade-off between reward and costs (that also satisfies the imposed constraints) automatically. By making the cost multipliers state-dependent, and adapting them alongside the policy, we can not only impose constraints on expected reward or cost, but also on their instantaneous values. Such point-wise constraints allow for much tighter control over the behavior of the policy, since a constraint that is satisfied only in overall expectation could still be violated momentarily. Finally, the entire constrained optimization procedure that we introduce can furthermore be conditioned on the constraint bounds themselves in order to learn a single, bound-conditioned policy that is able dynamically trade-off reward and penalties. This allows us to, for example, learn energy-efficient locomotion at a range of different velocities.
Our approach, as described in more detail in Section 3, is general and flexible in that it can be applied to any value-based RL algorithm and any number of constraints. We evaluate our approach on two continuous control problems in Section 4, the cart-pole swing up task and a (precisely simulated) locomotion task with the Minitaur quadruped.
2 BACKGROUND AND RELATED WORK
We consider the classical Markov Decision Process (MDP) setting (Sutton & Barto, 1998), where an agent sequentially interacts with an environment. More precisely, the agent observes the state of the environment s and decides on which action to take according to a policy a   (s | s). Executing the action in the environment, then, causes a state transition. Each transition has an associated reward defined by some utility function r (s, a). The goal of the agent is to maximize the expected sum of rewards, also known as the return, max Es,a [ t r (st, at)]. While some tasks have a welldefined reward, such as the increase in score when playing a game, for many others the objective is not as easily defined. Designing reward functions that produce a desired behavior policy can thus be extremely difficult, even in the single-objective case (e.g. Popov et al., 2017; Amodei et al., 2016).
Multi-Objective RL (MORL) problems arise in many domains, including robotics, and have been covered by a rich body of literature (see e.g. Roijers et al., 2013, for a recent review), suggesting a variety of solution strategies. For instance, Mossalam et al. (2016) devise a Deep RL algorithm that implements an outer loop method and repeatedly calls a single-objective solver. Mannor & Shimkin (2004) propose an algorithm for learning in a stochastic game setting with vector valued rewards (their approach is based on approachability of a target set in the reward space). However, most of these approaches explicitly recast the multi-objective problem into a single-objective problem (that is amenable to existing methods), where one aims to find the trade-off between the different objectives that yields the desired result. In contrast, we aim for a method that automatically trades off different components in the objective to achieve a particular goal. To achieve this, we cast the problem in the framework of Constrained Markov Decision Processes (CMDPs) (Altman, 1999). CMDPs have been considered in a variety of works, including in the robotics and control literature. For instance, Achiam et al. (2017) and Dalal et al. (2018) focus on constraints motivated by safety concerns and propose algorithms that ensure that constraints remain satisfied at all times. These works, however, assume that the initial policy already satisfies the constraint, which is not the case when the constraint involves the task success rate; as in this work. The motivation for the work by Tessler et al. (2018) is similar to ours. In contrast to our work, their approach maximizes reward subject to a constraint on the cost and enforces constraints only in expectation. Additionally, as an advance over the existing literature, we explicitly learn separate values for the reward and cost, as well as state-dependent coefficients that enable us to trade off the two in the policy optimization.
Constraint-based formulations are also used frequently in single-objective policy search algorithms where bounds on the policy divergence are employed to control the rate of change in the policy from one iteration to the next (e.g. Peters & Mu¨lling, 2010; Levine & Koltun, 2013; Schulman et al., 2015; Abdolmaleki et al., 2018). Our use of constraints, while similar in the employed methods, can be seen as orthogonal to the idea of using constraints to bound the rate of change in a policy.
2

Under review as a conference paper at ICLR 2019

While we note that our approach can be applied to any value-based off-policy method, we make use of the method described in Maximum a Posteriori Policy Optimisation (MPO) (Abdolmaleki et al., 2018) as the underlying policy optimization algorithm ­ without loss of any generality of our method. MPO is an actor-critic algorithm that is known to yield robust policy improvement. In each policy improvement step, for each state sampled from replay buffer, MPO creates a population of actions. Subsequently, these actions are re-weighted based on their estimated values such that better actions will have higher weights. Finally, MPO uses a supervised learning step to fit a new policy in continuous state and action space. See Abdolmaleki et al. (2018) and Appendix A for more details.

3 CONSTRAINED OPTIMIZATION FOR CONTROL

We consider MDPs where we have both a reward and cost, r (s, a) and c (s, a), which are functions of state s and action a. The goal is to automatically find a probabilistic policy (a|s) (with parameter ) that trades-off between maximizing the (expected) reward and minimizing the cost ­ in order to achieve the desired behavior. In the case of continuous control, desirable behavior would be solving the task (e.g. stable swing up in cart-pole) while minimizing other quantities, such as control effort or energy. In effect we want to optimize the total return subject to a penalty proportional to the total cost

max


Es,a

r (st, at) -  · c (st, at) ,

t

(1)

where we take max to mean maximizing the objective with respect to the policy parameters . The expectation over states s is with respect to the state visitation probability under the policy p (s).

The problem of finding the right trade-off then becomes a matter of finding a good value for . Finding this trade-off is often non-trivial. An alternative way of looking at this dilemma is to take a multi-objective optimization perspective. Instead of fixing , we can optimize for it simultaneously and can obtain different Pareto-optimal solutions for different values of . In addition, to ease the definition of a desirable regime for , one can consider imposing hard constraints on the cost to reduce dimensionality (Deb, 2014), instead of linearly combining the different objectives. Defining such hard constraints is often more intuitive than trying to manually tune coefficients. For example, in locomotion, it is easier to define desired behavior in terms of a lower bound on speed or an upper bound on an energy cost.

3.1 CONSTRAINED MDPS
The constrained perspective outlined above can be formalized as CMDPs (Altman, 1999). While a constraint can be placed on either the reward or the cost, in this work we consider a lower bound on the total return (although the theory derived below equivalently applies to constraints on cost), resulting in the following constrained optimization problem:

min


Es,a

c (st, at) , s.t. Es,a

r (st, at)  R,

tt

(2)

where R is the minimum desired return. In the case of an infinite horizon with a given
stationary state distribution, the constraint can instead be formulated for the per-step reward, i.e. Es,a [r (s, a)]  r. In practice one often optimizes the -discounted return in both cases. To apply model-free RL methods to this problem we first define an estimate of
the expected discounted return for a given policy as the action-value function Qr (s, a) = Es,a [ t t · r (st, at) |s0 = s, a0 = a]. Further, let Qc (s, a) denote the similarly constructed expected discounted cost action-value function. Equipped with these value functions, we can then recast the CMDP in value-space, where Vr = r/ (1 - ) (i.e. scaling the desired reward r with the limit of the converging sum over discounts):

min


Es,a

[Qc

(s,

a)]

,

s.t.

Es,a

[Qr

(s,

a)]



Vr.

3

(3)

Under review as a conference paper at ICLR 2019

3.2 LAGRANGIAN RELAXATION

We formulate task success via a constraint on the reward. Fulfilling this constraint indicates task success. Generally the constraint is not satisfied at the start of learning, as the agent first needs to learn how to solve the task. This limits the choice of existing methods that can be used to solve the CMDP, as many of these methods assume that the constraint is satisfied at the start and limit the policy update to remain within the constraint-satisfying regime (e.g. Achiam et al., 2017).

Lagrangian relaxation is a general method for solving general constrained optimization problems; and CMDPs by extension (Altman, 1999). In this setting, the hard constraint is relaxed into a soft constraint, where any constraint violation acts as a penalty for the optimization. Applying Lagrangian relaxation to Equation 3 results in the unconstrained dual problem

max


min
0

Es,a

[Q

(s,

a)]

,

with

Q

(s,

a)

=



(Qr

(s,

a)

-

Vr)

-

Qc

(s,

a)

,

(4)

with an additional minimization objective over the Lagrangian multiplier .

A larger  results in a higher penalty for violating the constraint. Hence, we can iteratively update  by gradient descent on Q (s, a) until the constraint is satisfied. Under assumptions described in Tessler et al. (2018), this approach converges to a saddle point. At convergence, when E [Q (s, a)] = 0,  is exactly the desired trade-off between reward and cost we aimed to find. To perform the outer policy optimization for  any off-the-shelf off-policy optimization algorithm
can be used (since we assume that we have a learned, approximate Q-function at our disposal). In
practice, we perform policy optimization using the MPO algorithm (Abdolmaleki et al., 2018) and
refer to Appendix A for additional details.

Learning Decomposed Values To learn an approximate Q (s, a), we can make use of the fact that the Bellman operator decomposes for a linear combination of rewards and a fixed discount. Given a total reward r (s, a) = i iri (s, a) and individual action-value functions for each of the terms Qi (s, a) = Es ,a  [ri (s, a) + Qi (s , a )], the total action-value can be found as:

iQi (s, a) = iEs ,a  [ri (s, a) + Qi (s , a )]
ii

= Es ,a 

iri (s, a) + 

i

= Q (s, a) .

iQi (s , a )
i

(5)

Each individual Qi (s, a) can then be learned separately using Temporal Difference (TD) learning. If we parameterize the different value functions such that there is some overlap in parameters, i.e. Qi (s, a; i, ), where i,  denote the parameters of the function approximator, we can benefit from more learning signal to optimize the overlapping parameters . In effect we train a single critic
model to output values for the different reward terms.

Scale invariance At the start of learning, as the constraint is not yet satisfied,  will grow in order
to suppress the cost Qc (s, a) and focus the optimization on maximizing Qr (s, a). Depending on how quickly the constraint can be satisfied,  can grow very large, resulting in a overall large mag-
nitude of Q (s, a). This can result in unstable learning as most actor-critic methods that have an explicit parameterization of  are especially sensitive to large (swings in) values. To improve stability, we re-parameterize Q (s, a) to be a projection into a convex combination of (Qr (s, a) - Vr) and Qc (s, a). Instead of scaling only the reward term, we can then adaptively reweigh the relative importance of reward and cost. To enforce   0, we can perform a change of variable  = log ()
to obtain the following dual optimization problem

max


min
 R

Es,a

[Q

(s, a)] , with Q

(s, a) =

exp ( ) (Qr (s, a) - Vr) - Qc (s, a) . exp ( ) + 1

(6)

In practice, we limit  to [min, max], with (exp (max) + 1)-1 = for some small , and initialize to max.

4

Under review as a conference paper at ICLR 2019

3.3 POINT-WISE CONSTRAINTS

One downside of the CMDP formulation given in Equation 3 is that the constraint is placed on

the expected total episode return, or expected reward. This implies that the constraint will not

necessarily be satisfied at every single timestep, or visited state, during the episode. For some

tasks this difference, however, turns out to be of importance. For example, in locomotion, a constant

speed is more desirable than a fluctuating one, even though the latter might also satisfy a minimum

velocity in expectation. Fortunately, we can extend the single constraint introduced in Section 3.1 to

a set, possibly infinite, of point-wise constraints; one for each state induced by the policy. This can

be formulated as the following optimization problem:

min


Es,a

[Qc

(s,

a)]

,

s.t.

s





:

Ea

[Qr

(s,

a)]



Vr.

(7)

Analogous to Section 3.2, this problem can be optimized with Lagrangian relaxation by introducing state-dependent Lagrangian multipliers. Formally, we can write,

max


Es

min Ea [Q (s, a)]
(s)0

, with Q (s, a) =  (s) (Qr (s, a) - Vr) - Qc (s, a) . (8)

Analogously to how one often assumes that nearby states have a similar value, here we have made the assumption that nearby states have similar  multipliers. This allows learning a parametric function  (s) alongside the action-value, which can generalize to unseen states s. In practice, we train a single critic model that outputs  (s) as well as Qc (s, a) and Qr (s, a).
Note that, in this case, the lower bound is still a fixed value and does not depend on the state. In general such a constraint might be impossible to satisfy for some states in a given task if the state distribution is not stationary (e.g. we cannot satisfy a reward constraint in the swing-up phase of the simple pendulum). However, the lower bound can also be made state-dependent and our approach will still be applicable.

3.4 CONDITIONAL CONSTRAINTS

Up to this point, we have made the assumption that we are only interested in a single, fixed value

for the lower bound. However, in some tasks one would want to solve Equation 7 for different lower

bounds Vr, i.e. minimizing cost for various success rates. For example, in a locomotion task, one

could be interested in optimizing energy for multiple different target speeds or gaits. Assuming

locomotion is a [0, vm ax]. In the

stationary behavior, one could limit this is would achieve the

set Vr = v/ same result as

(1 - ) for a range of velocities v  multi-objective optimization­it would

identify the set of solutions wherein it is impossible to increase one objective without worsening

another­also known as a Pareto front. To avoid the need to solve a large number of optimization problems, i.e., solving for every Vr separately, we can condition the policy, value function and Lagrangian multipliers on the desired target value and, effectively, learn a bound-conditioned policy

Ezp(z)

max Es(z)
(z)

min Ea(z) [Q (s, a, z)]
(s,z)0

,

with Q (s, a, z) =  (s, x) (Qr (s, a, z) - Vr (z)) - Qc (s, a, z) . (9)

Here z is a goal variable, the desired lower bound for the reward, that is observed by the policy and critic and maps to a lower bound for the value Vr (z). Such a conditional constraint allows a single policy to dynamically trade off cost and return.

4 EXPERIMENTS
We apply our constraint-based approach to the two continuous control domains shown in Figure 1, the cart-pole swing up task and a more challenging robot locomotion task.
4.1 CART-POLE SWING UP
We use the cart-pole swing up task as defined in the DeepMind Control Suite (Tassa et al., 2018), but modify the reward to exclude any cost objective. The reward is defined as the cosine of the pole's

5

Under review as a conference paper at ICLR 2019

(a) DM Control Suite cart-pole swing up task (Tassa et al., 2018).

(b) The Minitaur robot simulated in MuJoCo. The red dot denotes the IMU.

Figure 1: The continuous control environments used in the experiments.

angle re-scaled to [0, 1] multiplied by 1 + exp - · d2 /2, where d is the distance between the cart and the center of the rail and exp - · d2 = 0.1 for d = 2. The total reward lies in [0, 1] and can only be maximized by both swinging up the pole and centering the cart on the rail.
We train a neural network controller using the MPO algorithm (Abdolmaleki et al., 2018). More specifically, we train a two-layer MLP policy with 100 ReLU units in each layer to output the mean and variance of a Gaussian policy. As MPO is an off-policy algorithm, 32 concurrent actors are used to fill a shared replay buffer while the training loops asynchronously. We use a fixed lower bound on the expected per-step reward of 0.9, which corresponds to a lower bound on thedse value of 90 when setting  = 0.99. We use the absolute force output by the policy as the cost to minimize. The critic has the same architecture as the policy, but outputs the value estimates and Lagrangian multiplier instead. More details about the training setup can be found in Appendix A.
Figure 2a shows a typical execution of the noisy policy when optimizing for the reward alone. Note that actions are clamped in [-1, 1]. We can observe that the average absolute control signal is large and the agent keeps switches rapidly between a large negative and large positive force even after the swing up phase. While the agent is able to solve the task (and the behaviour can be somewhat smoothed by executing only the mean of the learned Gaussian policy for this simple system), this kind of bang-bang control is not desirable for any real-world control system. Figure 2b shows a typical execution of a policy learned with the constrained approach compared to the unconstrained case. The average applied mechanical power of the constrained policy is approximately one third that of the baseline policy (Figure 2c). It is clearly visible that the policy is much smoother; in particular it never reaches maximum or minimum actuation levels after the swing up (where a switch between maximum and minimum actuation is indeed the optimal solution).
4.2 MINITAUR LOCOMOTION
Our second experiment is based on the the Minitaur robot developed by Ghost Robotics (Kenneally et al., 2016). It is a quadruped with two Degrees of Freedom (DoFs) in each of the four legs. Highpower direct-drive actuators are used for each joint, allowing the robot to express a multitude of dynamic gaits such as trotting, pronking and galloping. These gaits, however, require a large engineering effort when implemented using state-of-the-art control techniques, and, when model-based approaches are used, performance becomes sensitive to modeling errors. Learning-based approaches have shown promise as an alternative for devising locomotion controllers for the Minitaur (Tan et al., 2018). Learning approaches are less dependent on gait and other task dependent heuristics and can lead to more versatile and very dynamic behaviors. We do however want learned gaits to be sufficiently well-behaved, avoiding high-frequency changes or large steps in the control signal that cause vibrations which ultimately can lead to control instability or mechanical stress. One way to achieve smooth control and locomotion is to optimize for energy efficiency, as fast, opposing actions typically require more power. We hence adopt an energy penalty in the following.
6

Under review as a conference paper at ICLR 2019

actions [-]

(a) Unconstrained
25 20 15 10
5 0
0246
time [s]
(c) Mechanical output power

(b) Constrained
8 10

Figure 2: Representative results of the executed policies in the cartpole swing up task. Plots (a) and (b) show the mean and standard deviation as output by the policy, following a trajectory generated using actions sampled from this distribution. (c) Mechanical power used by both trajectories is plotted, showing that the constrained policy has roughly one third the power usage of the unconstrained baseline policy.

4.3 EXPERIMENTAL SETUP
Although the Minitaur experiments are conducted in simulation, we have made a significant effort to capture many of the challenges of real robots: physical robot complexity, realistic and partial observations, control latency, plus additional perturbations, variations, and noise. We model the Minitaur in MuJoCo (Todorov et al., 2012) as seen in Figure 1b, using model parameters obtained from data sheets as well as system identification to improve the fidelity. The Minitaur is placed on a varying, rough terrain that is procedurally generated for every rollout. To model the drive train we use a non-linear actuator model based on a general DC motor model and the torque-current characteristic described in De & Koditschek (2015). The observations of the RL agent include noisy motor positions, yaw, pitch, roll, and angular velocities and accelerometer readings, but no direct perception of the surroundings or terrain. The policy outputs position setpoints at 100Hz that are fed to a low-level proportional position controller running at 1KHz, with a forced delay of 20ms added between sensor readings and the corresponding control signal, to match delays observed on the real hardware. To improve control robustness and with the aim to transfer the controllers from simulation to real hardware, we perform domain randomization (Tobin et al., 2017) on a number of model parameters, as well as apply random external forces to the body (see Appendix B for details).
As we are only considering forward locomotion, we set the reward r (s, a) to be the forward velocity of the robot's base expressed in the world frame. The cost c (s, a) is set to be the total power usage of the motors according to the actuator model. As the legs can collide with the main body when giving the agent access to the full control range, a constant penalty is added to the penalty computed from the power consumption during any self-collision. We use a largely similar training setup as in Section 4.1; however, since the episodes are 30sec in length and only partial and noisy observations are available, the agent requires memory for effective state estimation. For both the policy and the critic we use a two-layer MLP with 300 and 200 ReLU units, followed by an LSTM of 100 cells. In addition to learning separate values for Qr (s, a) and Qc (s, a), we split up Qc (s, a) into separate value functions for the power usage and collision penalty. We also increase the number of actors to 100 to sample a larger number of domain variations more quickly. More details can be found in Appendix A.
4.4 RESULTS
We first look at the effect of applying the lower bound to each individual state instead of on the global average velocity. Figure 3 shows a comparison between learning dynamics between a model
7

Under review as a conference paper at ICLR 2019

Average velocity [m/s] Average penalty [W]
'

1.0

0.8

0.6

0.4

0.2

0.00.0

0.2

0.4 0.6 Wall time [s]

0.8 1e15.0

104 103 1020.0 0.2 0.4 0.6 0.8 1.0
Average velocity [m/s]

8

6 4

Single  State-dependent 

2

0

2

4

6

80.0

0.2

0.4 0.6 Wall time [s]

0.8 1e15.0

(a) Reward over time

(b) Reward vs. penalty

(c) Multiplier over time

Figure 3: Comparison of a single versus a state-dependent  multiplier for models trained to achieve a minimum velocity of 0.5m/s. A single multiplier results in large swings in reward and on average higher values of . In b, policies start off at 0m/s and first learn to satisfy the constraint before optimizing the penalty. In c, for the state-dependent case, we show the mean and standard deviation of  across the training batch.

using a single  multiplier and a model with a state-dependent one, i.e. constraint in expectation or per-step. Both agents try to achieve a lower bound on the value that is equivalent to a minimum velocity of 0.5m/s. At first, both agents "focus" on satisfying the constraint, increasing the penalty significantly in order to do so. Once the target velocity is exceeded, the agents start to optimize the penalty, which drives them back to the imposed bound. A single multiplier that is applied to all states leads to larger changes in behavior space, where the agent oscillates between moving too slow at a lower penalty or too fast at a higher penalty. The agent with the state-dependent multiplier tracks the target velocity more closely, and achieves slightly lower penalties. Looking at the  values over time in Figure 3c, we see that they are generally lower in the latter case as well.
In Table 1, we compare the reward-penalty trade-off for settings trained to achieve a fixed lower bound on the velocity. We compare our approach to baselines where we clip the reward as r (st, at) = min (r (st, at) , r) and use a fixed coefficient  for the penalty. As there is less incentive for the agent to increase the reward over r, there is more opportunity to optimize the penalty. We still learn separate value functions to achieve an improved learning signal. Results shown are the per-step error with respect to the desired target velocity and the penalty, averaged across 4 seeds and 100 episodes each (the first 100ms is clipped to disregard transient behavior when starting from a stand-still). We also compare to a baseline where the reward is unbounded, marked as  in Table 1. In the unbounded reward case, we observe that it is difficult to achieve a positive but moderately slow speed. Either  is too high and the agent is biased towards standing still, or it is too low and the agent reaches the end of the course before the time limit (corresponding to an average velocity of approx. 1.25m/s). For the clipped reward, we observe a similar issue when  is set too high. In nearly

Table 1: Results for models trained to achieve a fixed lower bound on the velocity. Reported numbers are average per-step (velocity error [m/s], penalty [W]), except for the unbounded case where we report actual velocity. Each entry is an average over 4 seeds. We highlight the best constant , in terms of error, for each target bound. As can be seen the constraint version achieves error comparable or better than the fixed alpha in each condition while achieving significantly lower penalty (coloring: green (good); red (bad)).

Target
0.1 0.2 0.3 0.4 0.5 

 = 3e-3 error penalty -0.1, 35.74 -0.2, 46.48 -0.3, 50.3 -0.4, 54.05 -0.5, 60.71 0.0, 54.63

 = 1e-3 error penalty -0.01, 104.2 -0.01, 210.04 0.06, 154.91 0.06, 195.98 0.13, 250.69 1.25, 775.08

 = 3e-4 error penalty 0.07, 112.35 0.15, 207.19 0.16, 213.1 0.11, 306.1 0.13, 332.53 1.24, 1556.97

 = 1e-4 error penalty 0.1, 245.49 0.23, 399.83 0.24, 429.6 0.32, 627.66 0.26, 808.38 1.24, 1656.42

Constraint error penalty 0.01, 127.14 0.03, 106.88 0.04, 89.97 0.05, 132.97 0.05, 142.93 -, -

8

Under review as a conference paper at ICLR 2019

Table 2: Results of models that are conditioned on the target velocity, evaluated for for different values. Reported numbers are average per-step (velocity error [m/s], penalty [W]). Each row is an average over 4 seeds. The highlighted numbers mark the best individual alpha for each target velocity (in terms of velocity error). As can be observed no single  performs well across target velocities. In contrast the constraint version achieves low error in all conditions; and also achieves lower penalty than the best  in all but one case (as indicated by the coloring: green (good) and red (bad)).

Target
0.0 0.1 0.2 0.3 0.4 0.5 0.6

 = 3e-3 error penalty 0.0, 53.68 -0.1, 54.49 -0.2, 53.54 -0.3, 53.6 -0.4, 54.82 -0.5, 52.37 -0.6, 52.36

 = 1e-3 error penalty 0.01, 116.59 0.0, 158.68 0.02, 256.68 -0.02, 314.71 -0.07, 384.94 -0.1, 366.48 -0.2, 686.36

 = 3e-4 error penalty 0.17, 272.45 0.21, 324.16 0.21, 373.13 0.16, 336.48 0.15, 467.21 0.01, 594.36 -0.07, 770.67

 = 1e-4 error penalty 0.37, 757.53 0.37, 619.3 0.36, 627.19 0.42, 747.24 0.32, 870.34 0.27, 1026.3 0.02, 1632.96

Constraint error penalty 0.0, 84.07 0.0, 141.86 0.04, 174.79 0.02, 188.18 0.05, 252.54 0.05, 361.16 -0.04, 773.79

all other cases, the targeted speed is exceeded by some margin that increases with decreasing . While there is less incentive to exceed r, a larger margin decreases the chances of the actual speed momentarily dropping below the target speed. Using the constraint-based approach, we generally achieve average actual speeds closer to the target speed and at a lower average penalty, showing the merits of adaptively trading of reward and cost.
Table 2 shows a comparison between agents are trained across varying target speeds sampled uniformly in [0, 0.5] m/s. These agents are given the target speed as observations. The evaluation procedure is the same as before, except we evaluate the same conditional policy for multiple target values. We make similar observations: a fixed penalty coefficient generally leads to higher speeds then the set target, and higher penalties. Interestingly, for higher target velocities, the actual velocity exceeds the target less, indicating that different values for  are required for different targets. As we learn multipliers that are conditioned on the target, we can track the target more closely, even for higher speeds. We also evaluate these models for a target speed outside out the training range. Performance degrades quite rapidly, with the constraint no longer satisfied, and at significantly higher cost. This can be explained by the way the policies change behavior to match the target speed. Generally the speed is changed by modulating the stride length. Increasing the stride length much further than observed during training, however, results in collisions occurring that were not present at lower speeds, and hence higher penalties. The same observation also explains why the penalties in the conditional case are higher than in the fixed case (final column in Table 2 vs. Table 1), as more distinct behaviors are needed to be optimal for each target velocity. This is likely a limitation of the relatively simple policy architecture, and improving diversity across goal velocities will be studied in future work.
Figure 4 extends the comparisons by plotting penalty over absolute velocity errors for the different approaches. The plots show that finding a suitable weighting that works for all tasks and setpoints is difficult. While it is clear to identify values for  that are clearly too high or low, even for well-tuned values, performance over tasks can vary. Our approach as shown in Figure 4e is able to achieve a very consistent performance of low velocity tracking errors and low penalty across all tests. These results suggest, that our approach requires less problem specific tuning and is less sensitive to changes in the task. Therefore, a constraint-based approach can greatly reduce computationally expensive hyperparameter tuning.
Videos showing some of the learned behaviors, both in the fixed and conditional constraint case, can be found at https://sites.google.com/view/minitauriclr2019.
5 CONCLUSION
In order to regularize behavior in continuous control RL tasks in a controllable way, we introduced a constraint-based approach that is able to automatically trade off rewards and penalties, and can be used in conjunction with any model-free, value-based RL algorithm. Specifically, we minimize the penalties with respect to a lower bound on the reward value. The constraints are applied in a

9

Under review as a conference paper at ICLR 2019

(a)  = 3e-3

(b)  = 1e-3

(c)  = 3e-4

(d)  = 1e-4

(e) Constraint

Figure 4: Comparison of the constrained optimization approach with baselines using a fixed penalty. Each data point shows the average absolute velocity error and penalty for an agent optimized for a specific target velocity. The different ellipse shades show one to three standard deviations, both for the fixed (red) and the varying (blue) velocity setpoints. For each setting we train four agents. In the fixed target case, these are different models. In the conditional target case, these are evaluations of a single model conditioned on desired velocities. In both cases we observe: for the highest , agents do not move and get high velocity errors effectively not solving the task. For low  we have both high error and penalty, as the latter is ignored. Higher target velocities generally result in higher errors. Our approach achieves both lower errors across target speeds and lower penalty then the best baseline. Overall it exhibits the most consistent performance across the different tests.

point-wise fashion, for each state that the learned policy encounters, to allow for tighter control over the learned behavior. The resulting constrained optimization problem is solved using Lagrangian relaxation by iteratively adapting a set of Lagrangian multipliers, one per state, during training. By learning these state-dependent Lagrangian multipliers in the critic model alongside the value estimates of the policy, we can generalize multipliers to neighbouring states and efficiently and closely track the imposed bounds. The policy and critic can furthermore generalize across lower bounds by making the constraint value observable, resulting in a single bound-conditional RL agent that is able to dynamically trade off reward and costs in a controllable way. We applied our approach to two continuous control problems. In cart-pole we observed we are able to mitigate the detrimental bang-bang-style of control after the pole has been successfully swung up. In a simulated locomotion task with the Minitaur quadruped, we are able to minimize electrical power usage with respect to a lower bound on the forward velocity. We show that our method can achieve both lower velocity errors as well as lower power usage for different lower bounds compared to a baseline that uses a fixed coefficient for the penalty. We also learn a single, goal-conditioned policy that is able to move efficiently across a range of target velocities.
REFERENCES
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a Posteriori Policy Optimisation. In International Conference on Learn-
10

Under review as a conference paper at ICLR 2019
ing Representations, 2018. URL https://openreview.net/forum?id=S1ANxQW0b.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained Policy Optimization. In Proceedings of the 34th International Conference on Machine Learning, pp. 22­31, 2017.
E. Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane´. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. CoRR, abs/1801.08757, 2018.
Avik De and Daniel E. Koditschek. The Penn Jerboa: A platform for exploring parallel composition of templates. CoRR, abs/1502.05347, 2015. URL https://arxiv.org/abs/1502. 05347.
Kalyanmoy Deb. Multi-objective optimization. In Search methodologies, pp. 403­449. Springer, 2014.
G. Kenneally, A. De, and D. E. Koditschek. Design principles for a family of direct-drive legged robots. IEEE Robotics and Automation Letters, 1(2):900­907, July 2016.
Sergey Levine and Vladlen Koltun. Guided policy search. In Proceedings of the 30th International Conference on Machine Learning, 2013.
Shie Mannor and Nahum Shimkin. A geometric approach to multi-criterion reinforcement learning. Journal of Machine Learning Research, 5:325­360, December 2004. ISSN 1532-4435.
Hossam Mossalam, Yannis M. Assael, Diederik M. Roijers, and Shimon Whiteson. Multi-objective deep reinforcement learning. CoRR, abs/1610.02707, 2016. URL https://arxiv.org/ abs/1610.02707.
Re´mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and efficient offpolicy reinforcement learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS), 2016.
Jan Peters and Katharina Mu¨lling. Relative entropy policy search. 2010.
Ivaylo Popov, Nicolas Heess, Timothy P. Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin A. Riedmiller. Data-efficient deep reinforcement learning for dexterous manipulation. CoRR, abs/1704.03073, 2017. URL http: //arxiv.org/abs/1704.03073.
Diederik M. Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multiobjective sequential decision-making. Journal of Artificial Intelligence Research, 48(1):67­113, October 2013. ISSN 1076-9757.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1889­1897, Lille, France, 07­09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/schulman15.html.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. Robotics: Science and Systems (RSS), 2018.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller. DeepMind Control Suite. CoRR, abs/1801.00690, 2018. URL https: //arxiv.org/abs/1801.00690.
11

Under review as a conference paper at ICLR 2019 Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization.
CoRR, abs/1805.11074, 2018. URL https://arxiv.org/abs/1805.11074. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. CoRR, abs/1703.06907, 2017. URL https://arxiv.org/abs/1703.06907. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026­5033, Oct 2012. doi: 10.1109/IROS.2012.6386109.
12

Under review as a conference paper at ICLR 2019

APPENDIX A: TRAINING SETUP DETAILS
Policy Evaluation Our method needs to have access to a Q-function for optimization. While any method for policy evaluation can be used, we rely on the Retrace algorithm (Munos et al., 2016). More concretely, we learn the Q-function for each cost term Qi (s, a; i, ), where i,  denote the parameters of the function approximator, by minimizing the mean squared loss:

min
i ,

L(i,

)

=

min
i ,

Eµb (s),b(a|s)

Qi (st, at; i, ) - Qrtet 2 , with


Qrtet = Qi (st, at; i,  ) + j-t

j
ck ri(sj , aj )+

j=t k=t+1

(10)

E(a|sj+1)[Qi (sj+1, a; i, )] - Qi (sj , aj ; i,  ) ,

ck = min

1, (ak|sk) b(ak |sk )

,

where Qi (s, a; i,  ) denotes the output of a target Q-network, with parameters i,  , that we copy from the current parameters after a fixed number of updates. Note that while the above descrip-
tion uses the definition of reward ri we learn the value for the costs analogously. We truncate the infinite sum after N steps by bootstrapping with Q . Additionally, b(a|s) denotes the probabilities
of an arbitrary behaviour policy, in our case given through data stored in a replay buffer.

We use the same critic model to predict all values as well as the Lagrangian multipliers  (s, , ). Following Equation 8, we hence also minimize the following loss:

min
 ,

L

(,

)

=

Eµb(s)

min Ea [Q (s, a)]
(s, ,)0

(11)

Our total critic loss to minimize is i L (i, ) +  · L (, ), where  is used to balance the constraint and value prediction losses.

Maximum a Posteriori Policy Optimization Given the Q-function, in each policy optimization step, MPO use expectation-maximization(EM) to optimize the policy. In the E-step MPO finds the solution to a following KL regularized RL objective; the KL regularization here helps avoiding premature convergence, we note, however, that our method would work with any other policy gradient algorithm for updating . MPO performs policy optimization via an EM-style procedure. In the E-step a sample based optimal policy is found by minimizing:

max
q

Eµ(s)

Eq(a|s)

Qi (st, at; i, )

s.t.Eµ(s) KL(q(a|s), old(a|s)) < .

(12)

Afterwards the parametric policy is fitted via weighted maximum likelihood learning (subject to staying close to the old policy) given via the objective:

max


Eµ(s)

Eq(a|s)

log (a|s)

s.t. Eµ(s) KL(old(a|s), (a|s)) < .

(13)

assuming a Gaussian policy (as in this paper) this objective can further be decoupled into mean and covariance parts for the policy (which in-turn allows for more fine-grained control over the policy change) yielding:

13

Under review as a conference paper at ICLR 2019

max


Eµ(s)

Eq(a|s)

log (a|s)

s.t. Cµ < µ

C < 

(14)

where

µ(s)KL(old(a|s), (a|s)) = Cµ + C,

Cµ =

µ(s)

1 2

(tr(-1old

)

-

n

+

ln(

 old

))ds,

C =

µ(s)

1 2

(µ

-

µold

)T

-1(µ

-

µold)ds.

(15)

This decoupling of updating mean and covariance allows for setting different learning rate for mean and covariance matrix and controlling the contribution of the mean and co-variance to KL seperatly. For additional details regarding the rationale of this procedure we refer to the original paper Abdolmaleki et al. (2018).

Hyperparameters The hyperparameters for the Q-learning and policy optimization procedure are listed in Table 3. We perform optimization of the above given objectives via gradient descent; using different learning rates for critic and policy learning. We use Adam for optimization.
Table 3: Overview of the hyperparameters used for the experiments.

Parameter Policy learning rate Critic learning rate Constraint loss scale () Number of actors E-step constraint( ) M-step constraint on µ ( µ) M-step constraint on  ( )

Cart-pole 1e-5 1e-4 1e0 32 1e-1 1e-2 1e-5

Minitaur 1e-5 3e-4 1e-3 100 1e-2 1e-4 1e-6

14

Under review as a conference paper at ICLR 2019

APPENDIX B: MINITAUR SIMULATION DETAILS

Table 4: Overview of the different model variations and noise models in the Minitaur domain. N (µ, ) is the normal distribution, Lognormal (µ, ) the corresponding log-normal. U (a, b) is the uniform distribution and B (p) the Bernouilli distribution.

Parameter Body mass
Joint damping
Battery voltage
IMU position Motor calibration
Gyro bias Accelerometer bias
Terrain friction Gravity
Motor position noise Angular position noise
Gyro noise Accelerometer noise
Perturbations

Sample frequency episode
episode
episode
episode episode episode episode episode episode time step time step time step time step time step

Description
global scale  Lognormal (0, 0.1), with scale for each separate body  Lognormal (0, 0.02)
global scale  Lognormal (0, 0.1), with scale for each separate joint  Lognormal (0, 0.02)
global scale  Lognormal (0, 0.1), with scale for each separate motor  Lognormal (0, 0.02)
offset  N (0, 0.01), both cartesian and angular
offset  N (0, 0.02)
N (0, 0.001)
N (0, 0.01)
U (0.2, 0.8)
scale  Lognormal (0, 0.033)
N (0, 0.04), additional dropout  B (0.001)
N (0, 0.001)
N (0, 0.01)
N (0, 0.02)
Per-step decay of 5%, with a chance  B (0.001) of adding a force  N (0, 10) in any planar direction

15

