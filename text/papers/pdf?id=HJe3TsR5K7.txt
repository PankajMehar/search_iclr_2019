Under review as a conference paper at ICLR 2019
LEARNING JOINT WASSERSTEIN AUTO-ENCODERS FOR JOINT DISTRIBUTION MATCHING
Anonymous authors Paper under double-blind review
ABSTRACT
We study the joint distribution matching problem which aims at learning bidirectional mappings to match the joint distribution of two domains. This problem occurs in unsupervised image-to-image translation and video-to-video synthesis tasks, which, however, has two critical challenges: (i) it is difficult to exploit sufficient information from the joint distribution; (ii) how to theoretically and experimentally evaluate the generalization performance remains an open question. To address the above challenges, we propose a new optimization problem and design a novel Joint Wasserstein Auto-Encoders (JWAE) to minimize the Wasserstein distance of the joint distributions in two domains. We theoretically prove that the generalization ability of the proposed method can be guaranteed by minimizing the Wasserstein distance of joint distributions. To verify the generalization ability, we apply our method to unsupervised video-to-video synthesis by performing video frame interpolation and producing visually smooth videos in two domains, simultaneously. Both qualitative and quantitative comparisons demonstrate the superiority of our method over several state-of-the-arts.
1 INTRODUCTION
The joint distribution matching problem has attracted extensive attention in computer vision, such as unsupervised unsupervised image-to-image translation (I2IT) (Zhu et al., 2017; Liu et al., 2017) and video-to-video synthesis (V2VS) (Bashkirova et al., 2018). The goal of this problem is to learn the bidirectional mappings between unpaired data in two different domains. Unlike the marginal distribution in each domain, learning a joint distribution is often ignored and has the following two critical challenges.
The first key challenge, from a probabilistic modeling perspective, is how to exploit the joint distribution of unpaired data by learning the bidirectional mappings between two different domains. In the unsupervised learning setting, there are two sets of samples drawn separately from two marginal distributions in two domains. Based on the coupling theory (Lindvall, 2002), there exist an infinite set of joint distributions given two marginal distributions, and hence infinite bidirectional mappings between two different domains. Therefore, directly learning the joint distribution without additional information between the marginal distributions is a highly ill-posed problem. Recently, many studies (Zhu et al., 2017; Yi et al., 2017; Kim et al., 2017) have been proposed to learn the mappings in two domains separately, which may incur the joint distribution mismatching issue. Therefore, how to exploit sufficient information from the joint distribution still remains an open question.
Another important challenge is that the generalization ability w.r.t. the learned joint distribution of two different domains is still unknown. Existing theoretical results (Pan et al., 2018; Galanti et al., 2018) ignore the joint distribution of different data and cannot guarantee the generalization ability of such joint distribution. Moreover, it is also very hard to evaluate the generalization ability practically. Regarding this issue, according to (Bojanowski et al., 2018), the generalization ability can be evaluated by the interpolation performance in the target domain. In this sense, we can extend image-to-image translation to video space by performing video interpolation in one domain and investigating the performance of the translated video in another domain. To achieve this, one may directly apply existing unsupervised image-to-image translation methods (Zhu et al., 2017; Kim et al., 2017; Yi et al., 2017). However, these methods may result in significantly incoherent videos
1

Under review as a conference paper at ICLR 2019
with low visual quality. Therefore, it is important to design an effective joint distribution learning method and provide necessary theoretical analysis.
Regarding the above two challenges, in this paper, we propose a Joint Wasserstein Auto-Encoders (JWAE) to learn the bidirectional mappings between two domains by minimizing Wasserstein distance of joint distributions. Relying on the optimal transport theory, we are able to exploit sufficient information by matching latent distributions of images in two domains.
The contributions of this paper are summarized as follows:
· We propose a novel JWAE to solve the joint distribution matching problem. Based on Theorem 1, an intractable primal problem of optimal transport can be reduced to a simple optimization problem. Moreover, our method is a generalization of CycleGAN (Liu et al., 2017) and UNIT (Liu et al., 2017).
· We provide a generalization bound of JWAE (see Theorem 4). In particular, we theoretically prove that the generalization ability of our method w.r.t. the learned joint distribution can be guaranteed by minimizing Wasserstein distance of joint distributions.
· To practically verify the generalization ability, we apply our method to unsupervised video-tovideo synthesis and obtain two visually smooth videos in two different domains. Experiments on real-world datasets show the superiority of the proposed method over several state-of-the-arts.
2 RELATED WORK
In this paper, we consider the joint distribution matching problem. Recently, this problem has attracted extensive attention in image-to-image translation and video-to-video synthesis.
Image-to-image translation. Recently, Generative adversarial networks (GAN) (Goodfellow et al., 2014; Cao et al., 2018; Salimans et al., 2018), Variational Auto-Encoders (VAE) (Kingma & Welling, 2014) and Wasserstein Auto-Encoders (WAE) (Tolstikhin et al., 2017) have emerged as popular techniques for the image-to-image translation (I2IT) problem. For the unsupervised I2IT problem, CycleGAN (Zhu et al., 2017), DiscoGAN (Kim et al., 2017) and DualGAN (Yi et al., 2017) aim at minimizing the adversarial loss and the cycle-consistent loss in different domains, which may induce a joint distribution mismatching issue. To address this, CoGAN (Liu & Tuzel, 2016) learns a joint distribution by enforcing a weight-sharing constraint. Moreover, UNIT (Liu et al., 2017) builds upon CoGAN by using a shared-latent space assumption and the same weight-sharing constraint. However, these methods are not well-supported by any theoretical justifications.
Video-to-video synthesis. In this paper, we consider unsupervised video-to-video synthesis (V2VS) problem. Existing image-to-image methods (Zhu et al., 2017; Kim et al., 2017; Yi et al., 2017) cannot be directly used in the video-to-video synthesis problem, we further combine some video frame interpolation methods (Zhou et al., 2016; Ji et al., 2017; Niklaus et al., 2017; Liu et al., 2018) to synthesize video. Although UNIT (Liu et al., 2017) can be applied to video synthesis by interpolating in the latent space, it often results in temporally incoherent videos of low visual quality. Recently, a video-to-video translation method (Bashkirova et al., 2018) is proposed to translate a video in one domain to a video in another domain, but this method can not conduct video frame interpolation. Moreover, Wang et al. (2018) propose a video-to-video synthesis method and synthesize video results, but it cannot work for the unsupervised learning setting.
3 NOTATIONS
We use calligraphic letters (e.g., X ) for sets, capital letters (e.g., X) for random variables, and bold lower case letter (e.g., x) for their corresponding values. We denote probability distributions with capital letters (i.e., P (X)) and corresponding densities with bold lower case letters (i.e., p(x)). Let P(X ) be the set of all the probability measures over X , and PX be the marginal distribution over X . Sx={xi}iN=1 and Sy={yi}Mi=1 are two sets of unpaired training data. For convenience, we assume N =M . We denote by XX and Y Y two true images from two different domains, X and Y generated images sampled from the models, and ZZ a latent code. For a set S and two functions F : SR and G: SR, we denote F (s) G(s), sS if and only if  C1, C2>0 (independent of s) such that F (s)C1·G(s)+C2.
2

Under review as a conference paper at ICLR 2019

Cross-domain mapping

Mapping

X E1 Z1
XX^  G1

G2
Z2 E2

(a) Scheme of JWAE

Reconstruction
YY^ begin

Y end

Distribution divergence
E1 begin G2
G1  G2 E1 end G2

Distance
begin  end

(b) Interpolation based video-to-video synthesis

Figure 1: Demonstrations of (a) the JWAE scheme and (b) the interpolation based V2VS method.

4 PROPOSED METHOD

In this section, we propose a novel Joint Wasserstein Auto-Encoders (JWAE) method to solve the joint distribution matching problem. The overall scheme of JWAE is shown in Figure 1. Lastly, we propose a interpolation based V2VS method in Algorithm 1 and Algorithm 2.

4.1 JOINT DISTRIBUTION MATCHING PROBLEM

To learn a joint distribution, one can learn a shared latent space for two different domains (Liu
et al., 2017). In this sense, any pair of images in different domains can be mapped to the same
latent representation. We define latent variable models PG1 and PG2 by a two-step procedure: first a code ZZ is sampled from some prior distribution PZ and then Z can be mapped to X and Y , respectively. Then, we have the following densities:

pG1 (x):= pG1 (x|z)pz(z)dz,  xX , pG2 (y):= pG2 (y|z)pz(z)dz,  yY, (1)
ZZ
where all involved densities are properly defined. In this paper, we focus on non-random decoders G1: ZX and G2: ZY, i.e., generative models PG1 (X |Z) and PG2 (Y |Z) deterministically mapping Z to X =G1(Z) and Y =G2(Z), respectively. Based on these two models, we then construct joint distributions in two domains and define the following Wasserstein distance.

Wasserstein distance between joint distributions. Let PA(X, Y ) and PB(X , Y ) be joint distributions between real and generated images. Based on the optimal transport theory (Villani, 2008), we minimize Wasserstein distance W(PA, PB) between joint distributions PA and PB, i.e.,

Wc(PA, PB) = min E(X,Y ;X ,Y )P [c(X, Y ; X , Y )],
P P(PA,PB)

(2)

where P(PA, PB) is the set of couplings which is composed of joint probability distributions with the probability distributions (PA, PB), and c(X, Y ; X , Y ) is any measurable cost function between two joint probability distributions PA and PB.

In practice, there are two important challenges on Wasserstein distance and the cost function. First, directly optimizing Problem (2) raises intractable computational and statistical difficulties (Genevay et al., 2018). Second, how to choose a cost function is very challenging. In this paper, we set c(X, Y ; X , Y )=c1(X, X )+c2(Y , Y ) (Bhushan Damodaran et al., 2018), where c1 and c2 can be any metric to measure the distance in two different feature spaces (X ×X ) and (Y×Y), respectively. This cost function helps to derive the following theorem so that the intractable Problem (2) can be reduced to a simple optimization problem.

Theorem 1 Given two deterministic models PG1 (X |Z) and PG2 (Y |Z) as Dirac measures, i.e., PG1 (X |Z = z) = G1(z) and PG2 (Y |Z = z) = G2(z) for all z  Z, we have

Wc(PA,

PB )

=

inf
QQ1

EPX

EQ(Z1|X)[c1(X,

G1(Z1))]

+

inf
QQ2

EPY

EQ(Z2|Y

)[c2(G2(Z2),

Y

)],

(3)

where Q1={Q(Z1|X)|QZ1 =PZ =QZ2 , PY =PG2 } and Q2={Q(Z2|Y )|QZ1 =PZ =QZ2 , PX =PG1 } are the set of all probabilistic encoders, where QZ1 and QZ2 are the marginal distributions of Z1Q(Z1|X) and Z2Q(Z2|Y ), where XPX and Y PY , respectively. 1

1See supplementary materials for the proof.

3

Under review as a conference paper at ICLR 2019

As previously mentioned, finding an optimal couplings between joint distributions PA and PB is very challenging. Fortunately, according to Theorem 1, we can instead optimize problem (3) for
joint distribution matching. The details of objective functions and optimizations are given below.

4.2 JOINT WASSERSTEIN AUTO-ENCODERS

As shown in Figure 1, given real data X and Y , we learn the cross-domain mappings (i.e., E1G2 and E2G1) to generate samples Y and X such that the generated distributions are close to the real
distribution, i.e., PX =PG1 , PY =PG2 . Moreover, the latent distributions generated by two AutoEncoders (i.e., E1G1 and E2G2) should be close to each other, i.e., QZ1 =QZ2 . To optimize Problem (3), we relax these constraints PX =PG1 , PY =PG2 and QZ1 =PZ =QZ2 by introducing penalties into (3). Then we minimize the regularized optimization problem:

Wc(PA,

PB

)=

inf
QQ1

EPX

EQ(Z1|X)[c1(X,

G1(Z1))]+

inf
QQ2

EPY

EQ(Z2|Y

)[c2(Y,

G2(Z2))]

+ DX (PX , PG1 ) + DY (PY , PG2 ) + DZ (QZ1 , QZ2 ),

(4)

where , ,  are positive hyper-parameters, DX (PX , PG1 ), DY (PY , PG2 ) and DZ (QZ1 , QZ2 ) can be arbitrary distribution divergence between two distributions. The above problems involves two
kinds of functions, namely the reconstruction loss and distribution divergence.

(i) Reconstruction loss. In practice, we minimize the empirical reconstruction losses of
EPX EQ(Z1|X)[c1(X, G1(Z1))] and EPY EQ(Z2|Y )[c2(Y, G2(Z2))], denoted by Rx(E1, E2, G1, G2) and Ry(E1, E2, G1, G2), respectively. Taking the case for domain Q1 as an example, the empirical reconstruction loss Rx(E1, E2, G1, G2) can be rewritten as follows:

Rx

(E1

,

E2,

G1,

G2)=

1 N

N
c1(xi, G1(E1(xi))) + c1(xi, G1(E2(G2(E1(xi))))),
i=1

Auto-Encoder loss

Cycle consistency loss

(5)

Here, the first term represents the loss on the Auto-Encoders reconstruction and the second term
inherently enforce the cycle consistency that widely studied in image-to-image translation tasks
(Zhu et al., 2017; Liu et al., 2017; Kim et al., 2017; Yi et al., 2017). However, unlike exist-
ing methods, in our paper, the cycle consistency loss is directly derived from the joint distribution matching problem. The loss Ry(E1, E2, G1, G2) can be constructed similarly. Last, let R(E1, E2, G1, G2)=Rx(E1, E2, G1, G2)+Ry(E1, E2, G1, G2) be the final reconstruction loss.

(ii) Distribution divergence. The distribution divergences DX (PX , PG1 ), DY (PY , PG2 ) and DZ (QZ1 , QZ2 ) in (4) can be measured by GAN divergence (Goodfellow et al., 2014; LI et al., 2017), Maximum Mean Discrepancy (MMD) and Kullback-Leibler (KL) divergence, etc. Here,
we use triple GAN (LI et al., 2017) to measure the GAN divergence, denoted by GAN(PX , PG1 ), GAN(PY , PG2 ) and GAN(QZ1 , QZ2 ), respectively. Taking GAN(PX , PG1 ) as example, the loss function Lx(E1,E2,G1,G2,Dx) can be formulated as

Lx

(E1

,E2

,G1

,G2,Dx)=

1 M

M
i=1 2 log(Dx(xi)) + log(1-Dx(G1(E2(G2(E1(xi)))))

1 +
N

N
i=1 log(1-Dx(G1(E2(yi)))).

(6)

The loss functions Ly(E1,E2,G1,G2,Dx) and Lz(E1,E2,G1,G2,Dz) w.r.t. GAN(PY , PG2 ) and GAN(QZ1 , QZ2 ) can similarly constructed. Please find the details in supplementary materials.

4.3 A CONCRETE EXAMPLE: INTERPOLATION BASED VIDEO-TO-VIDEO SYNTHESIS

Since the generalization ability can be evaluated by the performance of interpolation (Bojanowski et al., 2018), we apply our method on the interpolation based video-to-video synthesis (V2VS) problem. The training and inference methods are shown in Algorithm 1 and Algorithm 2, respectively.
In the training, given the images {xi}iM=1 and {yj}Nj=1 in two different domains, we seek to learn a joint distribution mapping between these two distributions (see Algorithm 1). In the inference, given two input frames xbegin and xend, we apply our method to perform interpolation based videoto-video synthesis to produce two videos in two different domains. Specifically, we perform linear

4

Under review as a conference paper at ICLR 2019

Algorithm 1 Training details for JWAE.

Algorithm 2 Inference for unsupervised V2VS.

Input: Training data in two different domains:

Input: Testing data pair in the first domain:

{xi}iM=1 and {yj }Nj=1.
Initialization: Models: E1, E2, G1, G2; Discriminators: Dw, w{x, y, z}. repeat
Update Dx, Dy, Dz by ascending: w Lw(E1, E2, G1, G2, Dw), w{x, y, z}

{xbegin, xend}. Step 1: Video frame interpolation
zbegin = E1(xbegin), zend = E1(xend) z^ = zbegin+(1-)zend,   (0, 1) x^ = G1(z^) Synthesized video: {xbegin, x^, xend} Step 2: Video translation

Update E1, E2, G1, G2 by descending:

ybegin = G2(zbegin), y^ = G2(z^),

w Lw(E1, E2, G1, G2, Dw)+R(E1, E2, G1, G2) until models converged

yend = G2(zend) Synthesized video: {ybegin, y^, yend}

interpolation based on the latent space Z extracted from the first domain and then decode it to produce a corresponding video in the second domain (see Figure 1 (b) and Algorithm 2). In this sense, we can directly measure the quality of the synthesized video in the second domain to evaluate the generalization ability of the learned joint distribution mapping.

5 GENERALIZATION ANALYSIS

In this section, we analyze generalization performance of JWAE. To begin with, we provide the definitions of the generalization error and probabilistic cross-domain Lipschitzness.

Definition 1 (Generalization Error) Define cross-domain functions as f =G2E1 and f =G1E2
when QZ1 =QZ2 , and cost functions c1 and c2 which are bounded, symmetric, Lc-Lipschitz and satisfies the triangle inequality, and given two joint distributions PA(X, Y ) and PB(X, Y ), where
Y in PA(X, Y ) and X in PB(X, Y ) are unknown, then the generalization error E(f, g) becomes:

E(f, g) = E(X,Y )PA(X,Y ) [c2(Y, f (X))] + E(X,Y )PB(X,Y ) [c1(X, g(Y ))] .

(7)

Note that in I2IT problem, it is common to assume that two close samples will have close outputs with high probability, i.e., it satisfies a probabilistic Lipschitzness assumption (Courty et al., 2017; Urner et al., 2011). For convenience, we extend the definition as follows.

Definition 2 (-Probabilistic Cross-domain Lipschitzness) Given real and the generated marginal distribution PX and PX , and let  : R+  [0, 1], we say that a function f : X  Y w.r.t.
a joint distribution set P(PX , PX ) over PX and PX is -Lipschitz if for all  > 0,

P(X,X )P(PX,PX ) [ f (X) - f (X ) < c1(X, X )]  1 - ().

(8)

Intuitively, given a joint distribution set P(PX , PX ), a function f satisfying the -Lipschitz property holds with some probability. Then, we have the following results on the generalization error.

Theorem functions

2 c1

Let P = arg and c2. Let

fmuninctPionPs(PfAf,PBFg )

E(X,Y f ;Xg,Y )P [c(X, Y f ; Xg, Y )] with Lipschitz cost and gG be probabilistic cross-domain Lipschitzness

w.r.t. P  that minimizes the joint error E(f , g). Given M and N instances drawn form PX and

PY , respectively, with Lc1 =Lc2 =1, the following relation holds with probability at least 1-:

E(f, g)

W(PAf , PBg) +

1 log


1 + 1 + E(f , g) + (, ), MN

(9)

where E(f , g)=EA(f )+EBg (f )+EB(g)+EAf (g), and (, )=Lc1 M1()+Lc2 M2(), where f (X1) - f (X2) M1, X1, X2  X and g(Y1) - g(Y2) M2, Y1, Y2Y. 2

Remark 1 Theorem 4 provides an upper bound on the generalization error. The first term in right hand of (9) corresponds to the empirical version of (2); While the second term means that we should minimize it with sufficient unpaired data from two different domains. The third term E(f , g)
correspond to the joint error. When the last term (, ) is sufficiently small (i.e., f and g satisfy Lipschitz property with high probability), the cross-domain mappings would be well learned.
2See supplementary materials for the detailed proof.

5

Under review as a conference paper at ICLR 2019
6 EXPERIMENTS
We apply our method to interpolation based video-to-video synthesis (V2VS) in the unsupervised setting. To be specific, we firstly conduct video interpolation between two input frames in one domain and then translate it to produce a corresponding video in another domain.
Datasets. We conduct experiments on two widely used benchmark datasets, namely Cityscapes (Cordts et al., 2016) and SYNTHIA (Ros et al., 2016). (i) Cityscapes contains 2048×1024 street scene video of several German cities and a portion of ground truth semantic segmentation in the video. To obtain more semantic segmentation masks, following (Wang et al., 2018), we use a pre-trained DeepLab V3 network (Chen et al., 2017) to extract extra segmentation videos. (ii) We also study the generality of our algorithm on the unpaired dataset SYNTHIA (Ros et al., 2016), which contains a large collection of synthetic videos in different scenes and seasons. We perform unsupervised V2VS on four splits of SYNTHIA with different seasons, i.e., spring, summer, fall and winter. In this paper, we adopt the winter split as the common domain and train models to translate videos from winter to the other three seasons.
Evaluation metrics. For quantitative comparisons, we adopt Fre´chet Inception Distance (FID) (Heusel et al., 2017) to evaluate the quality of the frames in the synthesized videos. FID captures the similarity of the generated samples to real ones and correlates well with human judgement. Moreover, we also use a variant of FID (Wang et al., 2018) (FID4Video) to evaluate the quality of video. FID4Video measures the distribution similarity based on the extracted feature of videos. In general, for both FID and FID4Video, a lower score means the better performance.
6.1 IMPLEMENTATION DETAILS
We implement our method based on PyTorch34. We follow the experimental settings in CycleGAN (Zhu et al., 2017). For the optimization, we use Adam solver (Kingma & Ba, 2015) with a mini-batch size of 1 to train the models, and use a learning rate of 0.0002 for the first 100 epochs and gradually decrease it to zero for the next 100 epochs. Following (Zhu et al., 2017), we set ==0.1 in Eqn. (4). By default, we set =0.1 in our experiments.
6.2 BASELINE METHODS
We adopt several state-of-the-art baselines, including UNIT (Liu et al., 2017) with the latent space interpolation and several constructed variants of CycleGAN (Zhu et al., 2017) using different view synthesis algorithms. For those constructed baselines, we first conduct video interpolation and then perform image-to-image translation. Moreover, we also construct a variant of our method to conduct ablation study on the Triple GAN loss. All considered baselines are summarized as follows.
· UNIT (Liu et al., 2017). UNIT is a state-of-the-art unsupervised I2IT method which can perform supervised video-to-video synthesis by interpolating in the latent space.
· DVF-Cycle. This method combines the view synthesis method DVF (Liu et al., 2018) with CycleGAN (Zhu et al., 2017). To be specific, DVF produces videos by video interpolation in one domain. Then, we use CycleGAN to translate the generated video to another domain.
· DVM-Cycle. We use a geometrical view synthesis DVM (Ji et al., 2017) for video synthesis, and we replace DVF in DVF-Cycle with DVM and construct a new baseline called DVM-Cycle.
· AdaConv-Cycle. We also compare against a state-of-the-art video interpolation method AdaConv (Niklaus et al., 2017). For cross-domain video synthesis, we combine this method with a pre-trained CycleGAN model and term it AdaConv-CycleGAN in the following experiments.
· W/O-Triple. To investigate the effect of the Triple GAN loss, we construct a baseline method by removing it from our method. We refer to the method without Triple GAN loss as W/O-Triple.
3PyTorch is from http://pytorch.org/. 4Due to the page limit, more visual results are shown in the supplementary materials.
6

Under review as a conference paper at ICLR 2019

Table 1: Performance comparisons with state-ot-the-art baselines on Cityscapes and SYNTHIA.

Cityscapes

SYNTHIA

Method

photo2segmentation segmentation2photo

winter2spring

winter2summer

winter2fall

FID FID4Video FID FID4Video FID FID4Video FID FID4Video FID FID4Video

DVF-Cycle 110.59 23.95 151.27 40.61 152.44 42.22 160.69 42.43 163.13 41.04

DVM-Cycle 50.51 17.33 116.62 40.83 129.80 38.19 140.86 36.66 129.02 36.64

AdaConv-Cycle 33.50 14.96 99.67 30.24 117.40 23.83 126.01 20.62 110.52 16.77

UNIT

31.27 10.12 76.72 29.21 96.40 23.12 108.01 24.70 97.73 20.39

W/O-Triple 24.32 8.34 47.41 27.37 92.77 21.83 84.83 19.54 91.37 15.87

Ours 22.74 6.80 43.48 25.87 88.24 21.37 77.12 17.99 87.50 14.14

Methods

First frame1 & Last frame1 & translated image translated image

DVF-Cycle

DVM-Cycle AdaConv-Cycle

UNIT

Ours

Street scene (Interpolated)

Segmentation

Segmentation (Interpolated)
Street scene

Figure 2: Comparisons of different methods for photosegmentation translation on Cityscapes dataset. We first synthesize a video of street scene and then translate it to the segmentation domain (Top), and vice versa for the mapping from segmentation to street scene (Bottom).
6.3 QUANTITATIVE COMPARISONS
We compare the performance on Cityscapes and SYNTHIA and show the results in Table 1. We can draw the following observations. First, our method consistently outperforms the baselines in terms of both FID and FID4Video scores. It means that our method produces frames and videos of promising quality and exhibits strong generalization ability. Second, with the help of Triple GAN loss, our method achieves better results than W/O-Triple for both FID and FID4Video. This indicates that the reconstructed images after the cycle translation helps to learn a better joint distribution. The above observations demonstrate the superiority of our method over the competitive methods.
6.4 VISUAL COMPARISONS
Visual results on Cityscapes. We first interpolate videos in the cityscape domain and then translate them to the segmentation domain. We compare the visual quality of both the interpolated and the translated images in Figure 2. From Figure 2 (top), our method produces sharper cityscape images and yields more accurate results in the semantic segmentation domain, which significantly outperforms the baseline methods, and vice versa in Figure 2 (bottom).
Visual results on SYNTHIA. We further evaluate the performance of our method on SYNTHIA. We synthesize videos among the domains of four seasons shown in Figure 3. First, our method is able to produce sharper images when interpolating the missing in-between frames (see top row of Figure 3). Second, the translated frames produced by our method in the other three seasons look
7

Under review as a conference paper at ICLR 2019

Methods

First frame & Last frame & translated image translated image

DVF-Cycle

DVM-Cycle AdaConv-Cycle

UNIT

Winter (Interpolated)

Ours

Spring

Summer Fall

Figure 3: Comparison of different methods for season translation on SYNTHIA dataset. Top row: The synthesized video in the winter domain. Rows 2-4: The corresponding translated video in the domains of the other three seasons, i.e., spring, summer and fall.

Table 2: Influence of  for the adversarial loss on Z with different values. We compare the results

of wintersummer on SYNTHIA dataset in terms of FID and FID4Video scores.



winter2summer

summer2winter

FID FID4Video FID FID4Video

0.01 94.91 20.29 107.65 18.90

0.1 77.12 17.99 89.03 17.36

1 89.07 21.04 102.18 18.63

10 101.07 23.66 108.47 20.50

more photo-realistic than those produced by the other baseline methods (see the shape of cars in rows 2-4 of Figure 3). These results demonstrate that our method is able to produce more promising videos and consistently outperforms other methods in different domains.
6.5 INFLUENCE OF  FOR THE ADVERSARIAL LOSS ON Z
We study the effect of the trade-off parameter  over the adversarial loss on Z in Eqn. (4). The results are shown in Table 2. Given a very small weight =0.01, the model obtains larger FID and FID4Video scores compared to that with =0.1. When we increase it to  = 1 and  = 10, we also observe large performance degrades. Therefore, we suggest setting  = 0.1 in our method.
7 CONCLUSION
In this paper, we have proposed a novel joint Wasserstein Auto-Encoders method for the joint distribution matching problem. Instead of directly optimizing the primal problem of Wasserstein distance, we turn to propose a simple but effective optimization problem. In this way, we are able to conduct analysis on the generalization ability of JWAE and theoretically prove that minimizing the Wasserstein distance can guarantee the generalization ability. Extensive experiments on unsupervised V2VS task over several benchmark datasets demonstrate the superiority of the proposed method over the state-of-the-art methods.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Dina Bashkirova, Ben Usman, and Kate Saenko. Unsupervised video-to-video translation. arXiv preprint arXiv:1806.03698, 2018.
Bharath Bhushan Damodaran, Benjamin Kellenberger, Remi Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In European Conference on Computer Vision, 2018.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space of generative networks. In International Conference on Machine Learning, 2018.
Franc¸ois Bolley, Arnaud Guillin, and Ce´dric Villani. Quantitative concentration inequalities for empirical measures on non-compact spaces. Probability Theory and Related Fields, 2007.
Jiezhang Cao, Yong Guo, Qingyao Wu, Chunhua Shen, Junzhou Huang, and Mingkui Tan. Adversarial learning with local coordinate coding. In International Conference on Machine Learning, 2018.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Computer Vision and Pattern Recognition, 2016.
Nicolas Courty, Re´mi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In Advances in Neural Information Processing Systems, 2017.
Tomer Galanti, Sagie Benaim, and Lior Wolf. Generalization bounds for unsupervised cross-domain mapping with wgans. arXiv preprint arXiv:1807.08501, 2018.
Aude Genevay, Gabriel Peyre´, and Marco Cuturi. Learning generative models with sinkhorn divergences. In Artificial Intelligence and Statistics, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, 2017.
Dinghuang Ji, Junghyun Kwon, Max McFarland, and Silvio Savarese. Deep view morphing. In Computer Vision and Pattern Recognition, 2017.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. In International Conference on Machine Learning, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference for Learning Representations, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.
Chongxuan LI, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In Advances in Neural Information Processing Systems, 2017.
Torgny Lindvall. Lectures on the Coupling Method. Courier Corporation, 2002.
Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In Advances in Neural Information Processing Systems, 2016.
9

Under review as a conference paper at ICLR 2019
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems, 2017.
Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame synthesis using deep voxel flow. In International Conference on Computer Vision, 2018.
Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In International Conference on Computer Vision, 2017.
Xudong Pan, Mi Zhang, and Daizong Ding. Theoretical analysis of image-to-image translation with adversarial learning. In International Conference on Machine Learning, 2018.
Svetlozar Todorov Rachev et al. Duality theorems for kantorovich-rubinstein and wasserstein functionals. Instytut Matematyczny Polskiej Akademi Nauk (Warszawa), 1990.
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Computer Vision and Pattern Recognition, 2016.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal transport. In International Conference on Learning Representations, 2018.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein autoencoders. In International Conference on Learning Representations, 2017.
Ruth Urner, Shai Shalev-Shwartz, and Shai Ben-David. Access to unlabeled data can speed up prediction time. In International Conference on Machine Learning, 2011.
Ce´dric Villani. Optimal Transport: Old and New. Springer Science & Business Media, 2008. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catan-
zaro. Video-to-video synthesis. In Advances in Neural Information Processing Systems, 2018. Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning
for image-to-image translation. In International Conference on Computer Vision, 2017. Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A Efros. View synthesis
by appearance flow. In European conference on computer vision, 2016. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In International Conference on Computer Vision, 2017.
10

Under review as a conference paper at ICLR 2019

APPENDIX: "JOINT WASSERSTEIN AUTO-ENCODERS"

A PROOF OF THEOREM 1

Proof We denote by P(XPX , X PG1 ) and P(Y PY , Y PG2 ) the set of all joint distri-
butions of (X, X ) and (Y, Y ) with marginals PX , PG1 and PY , PG2 , respectively, and denote by P(PA, PB) the set of all joint distribution of PA and PB. Recall the definition of Wasserstein dis-
tance Wc(PA, PB), we have

Wc(PA, PB) = inf E(X,Y ;X ,Y )[c(X, Y ; X , Y )]
P (PA ,PB )

(10)

= inf E(X,Y ;X ,Y )[c1(X, X )] + inf E(X,Y ;X ,Y )[c2(Y , Y )]

P (PA ,PB )

P (PA ,PB )

= inf
P PX,X

E(X,X

)P [c1(X, X

)]

+

inf
P PY,Y

E(Y ,Y )P [c2(Y

, Y )]

(11)

= inf E(X,X )P [c1(X, X )] + inf E(Y ,Y )P [c2(Y , Y )] (12)

P P(PX ,PG1 )

P P(PY ,PG2 )

= Wc1 (PX , PG1 ) + Wc2 (PG2 , PY ).

Line (10) holds by the definition of Wc(PA, PB). Line (11) uses the fact that the variable pair
(X, X ) is independent of the variable pair (Y, Y ), and PX,X and PY,Y are the marginals on
(X, X ) and Y, Y induced by joint distributions in PX,Y ,X ,Y . In Line (12), if PG1 (X |Z) and PG2 (Y |Z) are Dirac measures (i.e., X =G1(Z) and Y =G2(Z)), we have

PX,X =P(PX , PG1 ),

PY,Y =P(PY , PG2 ).

We consider certain sets of joint probability distributions PX,X ,Z1 and PY,Y ,Z2 of three random variables (X, X , Z1)  X ×X ×Z and (Y , Y, Z2)  Y×Y×Z, respectively. We de-
note by P(XPX , Z1PZ1 ) and P(Y PY , Z2PZ2 ) the set of all joint distributions of (X, Z1)
and (Y, Z2) with marginals PX , PZ1 and PY , PZ2 , respectively. The set of all joint distributions PX,X ,Z1 such that XPX , (X , Z1)PG1,Z1 and (X X)|Z1, and likewise for PY,Y ,Z2 . We denote by PX,X and PX,Z1 the sets of marginals on (X, X ) and (X, Z1) induced by distributions in PX,X ,Z1 , respectively, and likewise for PY,Y and PY,Z2 . For the further analyses, we have

Wc1 (PX , PG1 ) + Wc2 (PY , PG2 )

=

inf
P PX,X

,Z1

E(X,X

,Z1)P [c1(X,

X

)]+

inf
P PY ,Y,Z2

E(Y

,Y,Z2)P [c2(Y

, Y )]

(13)

= inf
P PX,X

,Z1

EPZ1 EXP (X|Z1)EX

P (X

|Z1)[c1(X, X

)]

(14)

+

inf
P PY ,Y,Z2

EPZ2

EY

P (Y

|Z2)EY

P (Y

|Z2)[c2(Y

, Y )]

=

inf
P PX,X

,Z1

EPZ1

EXP (X|Z1)[c1(X,

G1(Z1))]+

inf
P PY ,Y,Z2

EPZ2

EY

P (Y

|Z2)[c2(G2(Z2),

Y

)]

=

P

inf
PX,Z1

E(X,Z1 )P

[c1(X,

G1(Z1))]+

P

inf
PY,Z2

E(Y ,Z2 )P

[c2(G2(Z2),

Y

)]

(15)

=

P

inf
P (X,Z1 )

E(X,Z1 )P

[c1(X,

G1(Z1))]+

P

inf
P (Y ,Z2 )

E(Y ,Z2 )P

[c2(G2(Z2),

Y

)]

(16)

=

inf
QQ1

EPX

EQ(Z1|X

)[c1(X,

G1(Z1))]+

inf
QQ2

EPY

EQ(Z2|Y

)[c2(G2(Z2),

Y

)],

(17)

where Q1={Q(Z1|X)|QZ1 =PZ =QZ2 , PY =PG2 } and Q2={Q(Z2|Y )|QZ1 =PZ =QZ2 , PX =PG1 }
are the set of all probabilistic encoders, where QZ1 and QZ2 are the marginal distributions of Z1Q(Z1|X) and Z2Q(Z2|Y ), where XPX and Y PY , respectively.

Line (13) uses the tower rule of expectation and Line (14) holds by the conditional indepen-
dence property of PX,X ,Z . In line (15), we take the expectation w.r.t. X and Y , respectively,
and use the total probability. Line (16) follows the fact that PX,Z1 =P(XPX , Z1PZ1 ) and PY,Z2 =P(Y PY , Z2PZ2 ) since P(PX , PG1 ), PX,X ,Z1 and PX,Y depend on the choice of conditional distributions PG1 (X |Z1), while PX,Z1 does not, and likewise for distributions w.r.t. Y and G2. In line (17), the generative model Q(Z1|X) can be derived from two cases where Z1 can be
sampled from E1(X) and E2(G2(E1(X))) when QZ1 =QZ2 and PY =PG2 , and likewise for the generative model Q(Z2|Y ).

11

Under review as a conference paper at ICLR 2019

B PROOF OF THEOREM 4

Lemma 1 Given an Lc2 -Lipschitz cost function c2, a probabilistic cross-domain Lipschitz function f , and two joint distributions PAf and PBg w.r.t. f and g, respectively, then for any coupling P (X, Y f ; Xg, Y )  P(PAf , PBg), we have

c2(Y, f (X))d(PAf -PBg) 

c2(Y f , f (X))-c2(Y, f (Xg)) dP (X, Y f ; Xg, Y ),

X ×Y

(X ×Y)2

where PAf =(X, Y f )XPX and PBg=(Xg, Y )Y PY , and Y f =f (X) and Xg=g(Y ).

Proof Given joint distributions PAf =(X, f (X))XPX and PBg=(g(Y ), Y )Y PY , and an Lc2 Lipschitz cost function c2 on Y × Y, and based on the duality form of the Kantorovitch-Rubinstein theorem (Rachev et al., 1990; Villani, 2008), we have

sup c2(Y, f (X))d(PAf - PBg) = inf

df (X, Y f ; Xg, Y )dP (X, Y f ; Xg, Y )

X ×Y

(X ×Y)2

where df (X, Y f ; Xg, Y ) is a cost function of f  w.r.t. (X, Y f ; Xg, Y ), and the cost function c2 satisfies the condition, i.e., c2(Y f , f (X)) - c2(Y, f (Xg))  df (X, Y f ; Xg, Y ). Specifically,
we choose its equality , then we have

c2(Y, f (X))d(PAf - PBg)
X ×Y

 c2(Y f , f (X)) - c2(Y, f (Xg))dP (X, Y f ; Xg, Y )
(X ×Y)2

 c2(Y f , f (X)) - c2(Y, f (Xg)) dP (X, Y f ; Xg, Y ).
(X ×Y)2

pLreombambaili2stiGc icvreonssa-ndoLmca1 -inLiLpispcshcihtzitczofsutnfcutniocntiofn

c1, two joint distributions PAf , we assume the input space X

and PBg, and a ()is bounded such that

f (X1) - f (X2)  M1, then we have

EAf (f ) - EBg (f )  W(PAf , PBg) + Lc1 M1().

Proof Based on the definition of EAf (f ) and EBg (f ), we discuss the absolute value of the difference between them as follows:
EAf (f ) - EBg (f )

= E(X,Y )PAf (X,Y ) [c2(Y, f (X))] - E(X,Y )PBg(X,Y ) [c2(Y, f (X))]

= c2(Y, f (X))d PAf - PBg
X ×Y

(18)

 c2(Y f , f (X)) - c2(Y, f (Xg)) dP (X, Y f ; Xg, Y )
(X ×Y)2

(19)

= c2(Y f , f (X)) - c2(Y f , f (Xg))+c2(Y f , f (Xg)) - c2(Y, f (Xg)) dP (X, Y f ; Xg, Y )
(X ×Y)2

 c2(Y f , f (X))-c2(Y f , f (Xg)) + c2(Y f , f (Xg))-c2(Y, f (Xg)) dP (X, Y f ; Xg, Y )
(X ×Y)2

 Lc2 f (X)-f (Xg) +c2(Y f , Y ) dP (X, Y f ; Xg, Y ) (X ×Y)2

(20)

 L1c1(X, Xg)+c2(Y f , Y ) dP (X, Y f ; Xg, Y )+L1M1()
(X ×Y)2

(21)

 c1(Xg, X)+c2(Y, Y f ) dP (X, Y f ; Xg, Y ) + L1M1()
(X ×Y)2
=W(PAf , PBg)+L1M1().

(22) (23)

12

Under review as a conference paper at ICLR 2019

Line (18) follows by the definition of EAf (f ) and EBg (f ) and the definition of the expectation w.r.t. PAf and PBg. Line (19) holds by Lemma 1 and we choose the optimal coupling P , i.e.,
P  = arg minP P(PAf ,PBg) E(X,Y ;X ,Y )P [c(X, Y ; X , Y )].
Line (20) holds by the Lc2 -Lipschitz cost function c2 w.r.t. the second argument, i.e.,
c2(Y f , f (X)) - c2(Y f , f (Xg))  Lc2 |f (X) - f (Xg)| ,
and the triangle inequality of the cost c2, i.e., |c2(Y f , f (Xg)) - c2(Y, f (Xg))|  c2(Y f , Y ). Based on Definition 2, line (21) contains two cases: (1) f  and P satisfies the -probabilistic cross-domain Lipschitzness with probability 1 - (); (2) the difference of f  between any two instances is bounded by M1, and we have the additional term Lc1 M1() that covers the regions where the -probabilistic cross-task Lipschitzness does not hold, i.e.

Lc1 |f (X) - f (Xg)| dP (X, Y f ; Xg, Y )
(X ×Y)2
 Lc1 c2(X, Xg)dP (X, Y f ; Xg, Y ) + Lc1 M1().
(X ×Y)2
Line (22) holds by the Lipschitz cost function c1, and line (23) uses the definition of W(PAf , PBg).
Lemma 3 Given Lipschitz cost function c1 and c2, and the input instances are bounded for f  and g, i.e., f (x1)-f (x2)  M1 and g(y1)-g(y2)  M2, if Lc1 =Lc2 =1, we have
EA(f )  Wc(PAf , PBg) + Lc1 M1() + EA(f ) + EBg (f ), EB(g)  Wc(PAf , PBg) + Lc2 M2() + EB(g) + EAf (g),
with probability at least 1 - .

Proof Given EA(f ) = E(X,Y )PA [c2(Y, f (X))] and EB(g) = E(X,Y )PB [c1(X, g(Y ))], without loss of generality, we analyze the former as follows, and the latter is similar for the results.

EA(f ) =E(X,Y )PA [c2(Y, f (X))] =E(X,Y )PA [c2(Y, f (X)) + c2(f (X), f (X))] =E(X,Y )PA [c2(f (X), f (X))] + EA(f ) =E(X,Y )PAf [c2(f (X), f (X)] + EA(f ) =EAf (f ) + EA(f ) =EAf (f ) - EBg (f ) + EBg (f ) + EA(f )
 EAf (f ) - EBg (f ) + EBg (f ) + EA(f )

(24) (25) (26)
(27)

Wc(PAf , PBg) + L1M1() + EBg (f ) + EA(f ).

(28)

Line (24) holds by an assumption that the loss function c1 satisfies triangle inequality. Line (25) holds by a symmetric loss function c1 and the definition of EA(f ), i.e., EA(f ) = E(X,Y )PA [c2(Y, f (X)]. Line (26) uses a fact that

E(X,Y )PA [c2(f (X), f (X))] = E(X,f(X))PAf [c2(f (X), f (X))] = EAf (f ).

In order to prove Theorem 4, we introduce an important theorem that shows the concentration inequality for Wasserstein distance of the empirical measure and its true measure.

Theorem 3 (Empirical Concentration (Bolley et al., 2007), Theorem 1.1) Let µ be a probability

measure

on



and

µ^

=

1 N

N i=1

wi

be

the

associated

empirical

measure

defined

on

a

sample

of

independent variables {wi}iN=1 drawn from µ. Then, for any d > dim() and C < C, there exists

13

Under review as a conference paper at ICLR 2019

some constant N0 depending on d and some square exponential monments of µ such that for any > 0 and N  N0 max{ -(d +2), 1},

P [W(µ, µ) >

]



e-

C 2

N

2
,

where C can be calculated explicitly, and C is such that µ verifies for any measure  the Talagrand

(transportation)

inequality

T1 (C )

:

W

(µ,



)(

2 C

H

(

|µ))

1 2

with

the

relative

entropy

H,

and

T1 (C )

holds when for some  > 0 and ed(w,w )2 dµ(w)<+,  w .

This theorem allows us to propose generalization bounds based on the Wasserstein distance. Now we can use this theorem and the previous Lemma to prove the following theorem.

Theorem functions

4 c1

Let P = arg and c2, and

lmetinfuPncPti(oPnAfs,PfBg)EF(Xa,Ynfd;Xgg,YG)bPe[cp(rXob, aYbfil;iXstigc,

Y )] with Lipschitz cost cross-domain Lipschitz-

ness w.r.t. P  that minimizes the joint error E(f , g), given M instances and N instances drawn

form PX and PY , respectively, with Lc1 =Lc2 =1, we have with probability at least 1-:

E(f, g)  2W1(PAf , PBg) +

22 log
C

2 + 2 + E(f , g) + ~(, ), MN

where where

Ef(f(X, g1))-=Ef A((Xf2))+EBg M(f1,)+XE1B,(Xg2)+EXAf (agnd),

agnd(Y~1()-, g)=(YL2c)1

M1()+Lc2  M2, Y1,

M2 Y2

(),  Y.

Here, C is some constant satisfying Theorem 3.

Proof Based on the definition of E(f, g), we have

E(f, g) =E(X,Y )PA [c2(Y, f (X))] + E(X,Y )PB [c1(X, g(Y ))] 2W(PAf , PBg) + E(f , g) + ~(, ) 2W(PAf , PBg) + W(PAf , PAf )+W(PBg, PBg)+E(f , g)+~(, )

2W(PAf , PBg) +

22 log
t

2 + 2 + E(f , g) + ~(, ). MN

(29) (30)
(31)

Line (29) holds by the conclusion in Lemma 3, and line (30) uses the triangle inequality for 1-

Wasserstein distance. In line (31), we apply the union bound of W(PAf , PAf ) and W(PBg, PBg) (with

probability

 2

for

each)

in

Theorem

3.

C DETAILS OF OPTIMIZATION FOR Y AND Z .

Here, we discuss some details of optimization for Y and Z. By using triple GAN (LI et al., 2017) to measure the GAN divergence GAN(PY , PG2 ) and GAN(QZ1 , QZ2 ), denoted by GAN(PY , PG2 ) and GAN(QZ1 , QZ2 ), respectively.
For GAN(PX , PG1 ), the loss function Ly(E1,E2,G1,G2,Dy) can be formulated as the following minimax problem:

Ly

(E1,E2,G1,G2

,Dy

)=

1 N

N

i=1 2 log(Dy(yi)) + log(1-Dy(G2(E1(G1(E2(yi)))))

1 +
M

M
i=1 log(1-Dy(G2(E1(xi)))).

(32)

For GAN(QZ1 , QZ2 ), we optimize GANZ (PZ , QZ1 ) and GANZ (PZ , QZ2 ) simultaneously. Note that the model Q(Z1|X) contains E1(x) and E2(G2(E1(x))), we can also employ Triple GAN to the following minimax problem:
1M Lz1 (E1,E2,G2,Dz)= M 2 log(Dz(zi))+ log(1-Dz(E1(xi)))+ log(1-Dz(E2(G2(E1(xi)))),
i=1

14

Under review as a conference paper at ICLR 2019

where z is drawn from the prior distribution PZ . Similarly, the another term DZ (PZ , QZ2 ) can be optimized by Lz2 (E1, E2, G1, Dz), i.e.,

Lz2

(E1

,E2

,G1

,Dz

)=

1 N

N

2 log(Dz(zi))+ log(1-Dz(E2(yi)))+ log(1-Dz(E1(G1(E2(yi)))),

i=1

Let Lz(E1,E2,G1,G1,Dz)=Lz1 (E1,E2,G2,Dz)+Lz2 (E1,E2,G1,Dz) be the final loss.

D VISUAL RESULTS ON SYNTHIA AND CITYSCAPES DATASETS

Ours

UNIT

AdaConv-Cycle

DVM-Cycle

DVF-Cycle
Figure 4: Comparisons of different methods for season winterspring translation on SYNTHIA dataset. The figure shows all frames of a video synthesized and translated by these mehtods.

15

Under review as a conference paper at ICLR 2019
Ours UNIT AdaConv-Cycle DVM-Cycle DVF-Cycle Figure 5: Comparisons of different methods for season wintersummer translation on SYNTHIA dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
16

Under review as a conference paper at ICLR 2019
Ours UNIT AdaConv-Cycle DVM-Cycle DVF-Cycle Figure 6: Comparisons of different methods for season winterfall translation on SYNTHIA dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
17

Under review as a conference paper at ICLR 2019
Ours UNIT AdaConv-Cycle DVM-Cycle DVF-Cycle Figure 7: Comparisons of different methods for photosegmentation translation on Cityscapes dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
18

Under review as a conference paper at ICLR 2019
Ours UNIT AdaConv-Cycle DVM-Cycle DVF-Cycle Figure 8: Comparisons of different methods for segmentationphoto translation on Cityscapes dataset. The figure shows all frames of a video synthesized and translated by these mehtods.
19

Under review as a conference paper at ICLR 2019
Ours UNIT AdaConv-Cycle DVM-Cycle DVF-Cycle Figure 9: Comparisons of different methods for photosegmentation translation on Cityscapes dataset. The figure shows all frames of another video synthesized and translated by these mehtods.
20

Under review as a conference paper at ICLR 2019
Ours UNIT AdaConv-Cycle DVM-Cycle DVF-Cycle Figure 10: Comparisons of different methods for segmentationphoto translation on Cityscapes dataset. The figure shows all frames of another video synthesized and translated by these mehtods.
21

