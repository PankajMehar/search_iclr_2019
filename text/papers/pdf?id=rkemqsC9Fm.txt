Under review as a conference paper at ICLR 2019

INFORMATION THEORETIC LOWER BOUNDS ON NEGATIVE LOG LIKELIHOOD
Anonymous authors Paper under double-blind review

ABSTRACT
In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model? One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model. The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing ratedistortion functions, and then to use this to derive a lower bound on negative log likelihood. Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions. We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.

1 INTRODUCTION

A statistician plans to use a latent variable model

p(x) = p(z) (x|z)dz,

(1)

where p(z) is known as the prior over the latent variables, and (x|z) is the likelihood of the data given the latent variables; we will often use (p(z), (x|z)) as a shorthand for the model. Frequently, both the prior and the likelihood are parametrized and the statistician's job is to find reasonable parametric families for both - an optimization algorithm then chooses the parameter within those families. The task of designing these parametric families can sometimes be time consuming - this is one of the key modeling tasks when one adopts the representation learning viewpoint in machine learning.

In this article we ask the question of how much p(z) can be improved if one fixes (x|z) and vicev-
ersa, with the goal of equipping the statistician with tools to make decisions on where to invest her time. One way to answer whether p(z) can be improved for fixed (x|z) is to drop the assumption
that p(z) must belong to a particular family and ask how a model could improve in an unrestricted setting. Mathematically, given data {x1, · · · , xN } the first problem we study is the following optimization problem: for a fixed (x|z),

min - 1

N
log

p(z) N

i=1

p(z) (xi|z)dz

(2)

which as we will show, is also connected to the problem of determining if (x|z) can be improved for a given fixed p(z). The quantity being optimized in (2) is called the average negative log likelihood of the data, and is used whenever one assumes that the data {x1, · · · , xN } have been drawn statistically independently at random. A negative log likelihood has units of bits (if the logarithm is base 2) or nats (if the logarithm is base e), because (2) can also be interpreted as the average number of bits that a compressor could use to compress multiple samples drawn from the distribution p(x).

1

Under review as a conference paper at ICLR 2019

Obviously, for any given (x|z), p(z), from the definition (1) we have the trivial upper bound

min - 1

N
log

p(z) N

i=1

p(z)

(xi|z)dz



-1 N

N

log p(xi)

i=1

(3)

The question we ask here is, can we give a good lower bound? A lower bound could tell us how far

we can improve the model by changing the prior. The answer turns out to be in the affirmative. By

connecting (2) to the computation of rate-distortion functions, which is how information theorists

study the problem of lossy compression, we will show that

min - 1

N
log

p(z) N

i=1

p(z)

(xi|z)dz



-1 N

N
log p(xi) - sup log
i=1 z

1 N (xi|z)

N
i=1

p(xi)

.

(4)

This result is very general - it holds for both discrete and continuous latent variable spaces, scalar

or vector. It is also sharp - if you plug in the right prior, the upper and lower bounds match. It also

has the advantage that the lower bound is written as a function of the trivial upper bound (3) - if someone proposes a latent variable model p(x) which uses a likelihood function (x|z), the optimal

negative log likelihood value when we optimize the prior is thus known to be within a gap of

sup log 1 N (xi|z)

z

N
i=1

p(xi)

bits. The individual quantities under the sup

(5)

c(z) = 1 N (xi|z)

N
i=1

p(xi)

(6)

have a specific functional meaning: they can be regarded as multiplicative factors that tell you how

to modify the prior to improve the log likelihood (see the Blahut-Arimoto algorithm (Blahut, 1972)):

p(z)  p(z)c(z).

(7)

Note that we are not necessarily advocating to choose the best possible prior according to the criterion (2), as one has to account for the possibility of overfitting. But we are suggesting that this line of inquiry gives nonetheless useful suggestions to the modeling experts.

More frequently than not, the problem is not to improve a prior, but for a fixed prior, to improve the likelihood function. Interestingly, rate-distortion theory still is relevant to this problem, although the question that we are able to answer with it is smaller in scope. Through a simple change of variable argument, we will argue that if the negative log likelihood can be improved by modifying the prior, exactly the same negative log likelihood can be attained by modifying the likelihood function instead. Thus if rate-distortion theory predicts that there is scope for improvement for a prior, the same holds for the likelihood function but conversely, while rate-distortion theory can precisely determine when it is that a prior can no longer be improved, the same cannot be said for the likelihood function.

We stress that we are not first in noticing that there are relations between rate-distortion theory and the general field of latent variable modeling. The Information Bottleneck method of Tishby et al. (1999) is a preeminent example of a successful idea that exists in this boundary, having created a subfield of research that remains relevant nowadays (Tishby & Zaslavsky, 2015) (Shwartz-Ziv & Tishby, 2017). The autoencoder concept extensively used in the neural network community is arguably directly motivated by the encoder/decoder concepts in lossy/lossless compression. Giraldo & Pr´incipe (2013) proposed using a matrix based expression motivated by rate-distortion ideas in order to train autoencoders while avoiding estimating mutual information directly. Recently, Alemi et al. (2017) exploited the -VAE loss function of Higgins et al. (2017) to explicitly introduce a trade-off between rate and distortion in the latent variable modeling problem, where the notions of rate and distortion have similarities to those used in this article.

In contrast to earlier works, our contribution is more basic - it goes to directly tying formally the most elementary rate-distortion setup, introduced by Shannon (1959), to the problem of latent variable modeling which to our knowledge had not been done to this depth. The field of rate-distortion theory of course has not been idle since Shannon introduced it, and we think it that there are ideas within it, beyond those exposed in this article, that could be adapted to good use in the problem of latent variable modeling.

2

Under review as a conference paper at ICLR 2019

2 TECHNICAL PRELIMINARIES

In our treatment, we will use notation that is commonly used in information theory. If you have two distributions P, Q the KL divergence between these distributions is denoted as DKL(P Q) which is known to be nonnegative. For a discrete random variable A, we denote its entropy H(A). If you have two random variables A, B, their mutual information is
I(A; B)
We assume that the data {x1, · · · , xN } comes from some arbitrary alphabet X which could be continuous or discrete. For example, it could be the set of all sentences of length up to a given number of words, the set of all real valued images of a given dimension, or the set of all real valued time series with a given number of samples.

Let X be a random variable distributed uniformly over the training data {x1, · · · , xN }. The entropy H(X) of this random variable is log N , and the fundamental lossless compression theorem of Shan-
non tells us that any compression technique for independently, identically distributed data samples following the law of X must use at use at H(X) bits per sample. But what if one is willing to allow
losses in the data reconstruction? This is where rate-distortion theory comes into play. Shannon introduced the idea of a reconstruction alphabet Z (which need not be equal to X ), and a distortion function d : X × Z  R which is used to measure the cost of reproducing an element of X with an element of Z. He then introduced an auxiliary random variable Z, which is obtained by passing X through a channel QZ|X , and defined the rate-distortion function as

R(D) = min I(X; Z)
QZ|X :EX,Z d(X,Z)D

(8)

Shannon's fundamental lossy source coding theorem in essence shows that R(D) plays the same role as the entropy plays for lossless compression - it represents the minimum number of bits per sample needed to compress X within a fidelity D, showing both necessity and sufficiency when the number of samples being compressed approaches infinity. This fundamental result though is not needed for this paper; its relevance to the modeling problem will be shown in a subsequent publication. What we will use instead is the theory of how you compute rate-distortion functions.

3 THE MAIN MATHEMATICAL RESULTS

In latent variable modeling, the correct choice for the distortion function turns out to be
d(x, z) = - log (x|z).
The fundamental reason for this will be revealed in the statement of Theorem 1 below. Computing a rate-distortion function starts by noting that the constraint in the optimization (8) defining the rate-distortion function can be eliminated by writing down the Lagrangian:
min {I(X; Z) - EX,Z log (X|Z)}
QZ|X
Our first result connects the prior optimization problem (2) with the optimization of the Lagrangian:

Theorem 1 (prior optimization is an instance of rate-distortion) For any  > 0,

min - 1

N
log

p(z) N

i=1

p(z) (xi|z)dz = min {I(X; Z) - EX,Z log (X|Z)}
QZ|X

(9)

The two central actors in this result p(z), QZ|X both have very significant roles in both latent variable modeling and rate-distortion theory. The prior p(z) is known as the "output marginal" in ratedistortion theory, as it is related to the distribution of the compressed output in a lossy compression system. On the other hand, QZ|X , which is called the "test channel" in rate-distortion theory, is the conditional distribution that one uses when optimizing the famous Evidence Lower Bound (ELBO) which is an upper bound to the negative log likelihood (in contrast, (4) is a lower bound on the same). In the context of variational autoencoders, which is a form of latent variable modeling, QZ|X is also called the "stochastic encoder". An optimal prior according to (2) is also an optimal output distribution for lossy compression, and conversely, from an optimal test channel in lossy compression one can derive an optimal prior for modeling.

3

Under review as a conference paper at ICLR 2019

3.1 PROOF OF THEOREM 1

The proof consists on proving "less than or equal" first in (9) and then "more than or equal". We start with the following lemma which is a simple generalization of the Evidence Lower Bound (ELBO). It can be proved through a straightforward application of Jensen's inequality.

Lemma 1 For any conditional distribution Q(z|x), p(z), (x|z), and for any x, and  > 0, log p(z) (x|z)dz  - Q(z|x) log Q(z|x) dz +  Q(z|x) log (x|z)dz p(z)

(10)

Taking the expectation over the law of X in (10) (assuming X is uniform over {x1, · · · , xN }, and

through an elementary rearrangement of terms, one derives that for any (x|z), any conditional

distribution Q(z|x), any p(z), and defining (X, Z) to be random variables distributed according to

1 N

Q(z

|x),

min -EX log p(z) (X|z)dz  -EX log p(z) (X|z)dz
p(z)

 I(X; Z) + DKL

N

1 N

Q(z|xi

)

p(z)

+ EX

i=1

Q(z|X)(- log (X|z))dz

(11)

Since we are free to choose p(z) whatever way we want, we set p(z) =

N i=1

1 N

Q(z|xi)

thus

eliminating the divergence term in (11). Since this is true for any conditional distribution Q(z|x),

we can take the infimum in the right hand side, proving "less than or equal". To prove the other

direction, let p(z) be any distribution and define

Q(z|x) =

p(z) (x|z) p(z) (x|z)dz

(12)

Let

(X, Z)

be

distributed

according

to

1 N

Q(z|x).

Then

min {I(X; Z) + EX,Z [- log (X|Z)]}  I(X; Z) + EX,Z [- log (X|Z)]
Q(z|x)

N1

= -DKL

N Q(z|xi) p(z) - EX log

i=1

p(z) (X|z)dz  -EX log

p(z) (X|z)dz

Since this is true for any distribution p(z), we can take the minimum in the right hand side, completing the proof.

3.2 LOWER BOUND ON THE NEGATIVE LOG LIKELIHOOD: OPTIMIZING PRIORS

The goal in this section is to derive the lower bound (4). Theorem 1 suggests that the problem of lower bounding negative log likelihood in a latent variable modeling problem may be related to the problem of lower bounding a rate-distortion function. This is true - mathematical tricks in the latter can be adapted to the former. The twist is that in information theory, these lower bounds apply to (8), where the object being optimized is the test channel QZ|X whereas in latent variable modeling we want them to apply directly to (1), where the object being optimized is the prior p(z).

The beginning point is an elementary result, inspired by the arguments in Theorem 2.5.3 of Berger

(1971),

which

can

be

proved

using

the

inequality

log

1 u



1

- u:

Lemma 2 For any p(z), and (x) > 0

- log p(z) (x|z)dz  - log (x) + 1 - Taking the expectation with respect to X in Lemma 2 we obtain

(x|z) p(z) dz.
(x)

-EX log

p(z) (X|z)dz  -EX log (X) + 1 -

p(z)EX

(X |z ) dz
 (X )

4

Under review as a conference paper at ICLR 2019

For any  (x) > 0, if you substitute (x) =  (x) p(z)EX

(X |z )  (X)

dz then we obtain

-EX log p(z) (X|z)dz  -EX [log  (X)] - log

(X |z ) p(z)EX  (X) dz

We now eliminate any dependence on p(z) on the right hand side by taking the sup, and then take the min over p(z) on both sides, obtaining that for any  (x) > 0,

min -EX log p(z) (X|z)dz
p(z)

(X |z )

 -EX [log  (X)] - sup log EX
z

 (X)

dz

The p(z) we eliminated refers to the "optimal" prior, which we don't know (hence why we want to
eliminate it). We now bring the "suboptimal" prior: since the above is true for any  (x) > 0, set  (x) = p(x), where p(x) is the latent variable model (1) for a given (p(z), (x|z)), and assuming that X is distributed uniformly over the training data {x1, · · · , xN }, then we obtain

Theorem 2 (information theoretic lower bound on negative log likelihood)

min - 1

N
log

p(z) N

i=1

p(z)

(xi|z)dz



-1 N

N
log p(xi) - sup log
i=1 z

1 N (xi|z)

N
i=1

p(xi)

.

(13)

This bound is sharp. If you plug in the p(z) that attains the min, then the sup is exactly zero, and conversely, if p(z) does not attain the min, then there will be slackness in the bound. It's sharpness can be argued in a parallel manner to the original rate-distortion theory setting. In particular, by examining the variational problem defining the latent variable modeling problem, we can deduce that an optimal p(z) must satisfy

1N

(xi|z)

N
i=1

p(z) (xi|z)dz

= 1 if p(z) > 0  1 if p(z) = 0

Thus for an optimal p(z) the sup in Theorem 2 is exactly zero and the lower bound matches exactly

with the trivial upper bound (3).

3.3 OPTIMIZING THE LIKELIHOOD FUNCTION FOR A FIXED PRIOR.

An very common assumption is to fix the prior to a simple distribution, for example, a unit variance, zero mean Gaussian random vector, and to focus all work on designing a good likelihood function (x|z). Can rate-distortion theory, which we have only shown relevant to the problem optimizing a prior, say anything about this setting?

We assume that Z is an Euclidean space in some dimension. Assume that we start with a latent variable model (p(z), (x|z)), and then we notice that there is a better choice p^(z) for the same (x|z), in the sense that

-1

N
log

N

i=1

p^(z)

(xi|z)dz

<

-1 N

N

log

i=1

p(z) (xi|z)dz

(14)

Further assume that there is an invertible, continuously differentiable function g : Z  Z with the

property that if Z is a random variable distributed as p(z), then g(Z) is a random variable distributed

as p^(z). Define y = g(z), so that

p(z) (xi|g(z))dz = p(g-1(y)) (xi|y)| det(Dg-1)(y)|dy = p^(y) (xi|y)dy

The last integral is identical to ith integral in the left hand side of (14). Therefore, one can define a new ^(x|z) = (x|g(z)), and the negative log likelihood of the latent variable model (p(z), ^(x|z)) will be identical to that of (p^(z), (x|z)).

A key consequence is that if the rate-distortion theoretic lower bound on negative log likelihood (Theorem 2) shows signs of slackness, then it automatically implies that the likelihood function admits improvement for a fixed prior. It is important to note that this is only one possible type of modification to a likelihood function. Thus if rate-distortion theory predicts that a prior cannot be improved any more, then this does not imply that the likelihood function cannot be improved - it only means that there is a specific class of improvements that are ruled out.

5

Under review as a conference paper at ICLR 2019

4 EXPERIMENTAL VALIDATION

The purpose of this section is to answer the question: are the theoretical results introduced in this article of practical consequence? The way we intend to answer this question is to pose a hypothetical situation involving a specific image modeling problem where there has been a significant amount of innovation in the design of priors and likelihood functions, and then to see if our lower bound, or quantities motivated by the lower bound, can be used, without the benefit of hindsight to help guide the modeling task.

The main analysis

tool

revolves around

the quantity

c(z) defined

as

c(z)

=

1 N

N i=1

(xi |z ) p(xi )

.

We

know that if supz log c(z) > 0, then it is possible to improve the negative log likelihood of a

model by either changing the prior while keeping the likelihood function fixed, or by changing the

likelihood while keeping the prior fixed, the scope of improvement being identical in both cases, and

upper bounded by supz log c(z) nats. We also know that if supz log c(z) = 0 then the prior cannot be improved any more (for the given likelihood function), but the likelihood function may still be

improved.

Thus supz log c(z) is in principle an attractive quantity to help guide the modeling task. If the latent space Z is finite, as it is done for example with discrete variational autoencoders (Rolfe, 2017), then it is straightforward to compute this quantity provided Z is not too large. However if Z is continuous, in most practical situations computing this quantity won't be possible.
The alternative is to choose samples {z1, · · · , zm}  Z in some reasonable way, and then to compute some statistic of {c(zi)}mi=1. Recall that the individual c(z) elements can be used to improve the prior using the Blahut-Arimoto update rule:

log p(z)  log p(z) + log c(z)

If log c(z) is close to a zero (infinite) vector, then the update rule would modify the prior very little, and because rate-distortion functions are convex optimization problems, this implies that we are closer to optimality. Inspired on this observation, we will compute the following two statistics:

max
i

{log

c(zi)}mi=1

,

Std {log c(zi)}im=1

(15)

which we will call the glossy statistics, in reference to the fact that they are supporting a generative modeling process using lossy compression ideas. The core idea is that the magnitude of these statistics are an indicator of the degree of sub optimality of the prior. As discussed previously, sub-optimality of a prior for a fixed likelihood function immediately implies sub-optimality of the likelihood function for a fixed prior. These statistics of will vary depending on how the sampling has been done, and thus can only be used as a qualitative metric of the optimality of the prior.

In Variational Inference it is a common practice to introduce a "helper" distribution Q(z|x) and to

optimize the ELBO lower bound of log likelihood log p(x) 

Q(z|x)

log

p(z) (x|z) Q(z|x)

dz.

Beyond

its use as an optimization trick, the distribution Q(z|x), also called the "encoder" in the variational

autoencoder setting (Kingma & Welling, 2014), plays an important practical role: it allows us to

map a data sample x to a distribution over the latent space.

Note that one wants to find values of z for which log c(z) is "large". One way to find good such instances is to note that the dependency on z is through (x|z). Additionally note that Q(z|x) in essence is designed to predict for a given x values for z for which (x|z) will be large. Our proposal thus is to set zi to be the mean of Q(z|xi) for i = 1, · · · , N (and thus m = N ). There are many additional variants of this procedure that will in general result in different estimates for the glossy
statistics. We settled on this choice as we believe that the general conclusions that one can draw
from this experiment are well supported by this choice.

4.1 EXPERIMENTS AND INTERPETATION
The data sets that we will use are image modeling data sets are Static MNIST (Larochelle & Murray, 2011), OMNIGLOT (Lake et al., 2015), Caltech 101 Silhouettes (Marlin et al., 2010), Frey Faces (FreyFaces), Histopathology (Tomczak & Welling, 2016) and CIFAR (Krizhevsky, 2009). In order to explore a variety of prior and likelihood function combinations we took advantage of the publicly

6

Under review as a conference paper at ICLR 2019

Table 1: Glossy statistics for our experiments - test NLL in nats. Lower is better. L denotes the number of stochastic layers.

staticMNIST

Omniglot

likelihood function class

prior class

NLL

glossy glossy max stat std stat

NLL

glossy glossy max stat std stat

VAE
HVAE (L = 2) PixelHVAE (L = 2)

standard 88.44 15.7 34.3 107.83 18.4

VampPrior 85.78 12.1 37.6 107.62 19.0

standard 86.21 11.8 38.3 103.40 16.3

VampPrior 84.96 9.8 40.1 103.78 14.6

standard 80.41

7.3

3.0 90.80 10.3

VampPrior 79.86

7.7

4.1 90.99

8.0

27.3 25.6 33.3 31.6 1.0 0.8

Caltech 101

Frey Faces

likelihood function class

prior class

NLL

glossy glossy max stat std stat

NLL

glossy glossy max stat std stat

VAE
HVAE (L = 2) PixelHVAE (L = 2)

standard 128.63 79.4 160.5 1808.19 89.3 331.5

VampPrior 127.64 63.6 143.7 1746.33 82.3 349.1

standard 125.82 59.4 204.8 1798.42 170.2 347.0

VampPrior 121.02 47.2 210.7 1761.82 152.0 325.5

standard 85.79 18.4

7.4 1687.10 45.7 55.1

VampPrior 86.34 20.8

7.5 1676.04 9.4

33.8

Histopathology

cifar10

likelihood function class

prior class

NLL

glossy glossy max stat std stat

NLL

glossy glossy max stat std stat

VAE
HVAE (L = 2) PixelHVAE (L = 2)

standard VampPrior standard VampPrior standard VampPrior

3295.07 3286.61 3149.11 3127.67 2636.78 2625.61

167.8 69.5 127.3 92.2 -0.0 -0.1

324.5 348.9 475.3 484.9 0.0 0.0

13812.19 13817.33 13400.96 13399.95 10915.34 10961.67

198.1 628.1 479.3 195.6 178.4 246.7

1325.3 1303.9 1634.4 1539.0 105.5 36.3

available source code that the authors of the VampPrior article (Tomczak & Welling, 2018) used in their work, and extended their code to create sample from Z using the strategy described above, and implemented the computation of the c(z) quantities, both of which are straightforward given that the ELBO optimization gives us Q(z|x) and an easy means to evaluate c(z) for any desired value of z.
For the choice of priors, we use the standard zero mean, unit variance Gaussian prior as well as the variational mixture of posteriors prior parametric family from (Tomczak & Welling, 2018) with 500 pseudo inputs for all experiments. Our choices for the autoencoder architectures are a single stochastic layer Variational Autoencoder (VAE) with two hidden layers (300 units each), a two stochastic layer hierarchical Variational Autoencoder (HVAE) described in (Tomczak & Welling, 2018) as well as a PixelHVAE, a variant of PixelCNN (van den Oord et al., 2016) which uses the HVAE idea described earlier. Each of these (VAE, HVAE, PixelHVAE) represent distinct likelihood function classes. We are aware that there are many more choices of priors and models than considered in these experiments; we believe that the main message of the paper is sufficiently conveyed with the restricted choices we have made. In all cases the dimension of the latent vector is 40 for both the first and second stochastic layers. We use Adam (Kingma & Ba, 2015) as the optimization algorithm. The log test likelihood reported is not an ELBO evaluation - it is obtained through importance sampling (Burda et al., 2016). We refer the reader to this Tomczak & Welling (2018) for a precise description of their setup.
We now refer the reader to Table 1. We want the reader to focus on this prototypical setting:
A modeling expert has chosen a Gaussian unit variance, zero mean prior p(z) (denoted by "standard"), and has decided on a simple likelihood function class (denoted by "VAE"). Can these choices be improved?
7

Under review as a conference paper at ICLR 2019
The goal is to answer without actually doing the task of improving the parametric families. This setting is the first row in all of the data sets in Table 1. We first discuss the MNIST experiment (top left in the table). The modeling expert computes the max and Std statistics, and notices that they are relatively large compared to the negative log likelihood that was estimated. Given the discussion in this paper, the expert can conclude that the negative log likelihood can be further improved by either updating the prior or the likelihood function.
A modeling expert may then decide to improve the prior (second row of the table), or improve the likelihood function (third and fifth rows of the table). We see that in all these cases, there are improvements in the negative log likelihood. It would be incorrect to say that this was predicted by the glossy statistics in the first row; instead what we can say is that this was allowed - it can still be the case that the statistics allow an improvement on the negative log likelihood, but a particular enhancement proposal does not result on any improvement. Next notice that the std glossy statistic for the fifth row is much smaller than the in the previous configurations. The suggestion is that improving the negative log likelihood by improving the prior is becoming harder. Indeed, in the sixth row we see that a more complex prior class did not result in an improvement in the negative log likelihood. As discussed previously, this does not mean that the likelihood function class can no longer be improved. It does mean though that a particular class of enhancements to the likelihood function - in particular, transforming the latent variable with an invertible transformation - will likely not be fruitful directions for improvement.
The general pattern described above repeats in other data sets. For example, for Caltech 101 the reduction of the max and Std glossy statistics when PixelHVAE is introduced is even more pronounced than with MNIST, suggesting a similar conclusion more strongly. A result that jumps out though is the PixelHVAE result for Histopathology, which prompted us to take a close look - here the statistics are actually zero. It turns out that this is an instance where the likelihood function learned to ignore the latent variable, a phenomenon initially reported by Bowman et al. (2016). It also serves as a cautionary tale: the glossy statistics only tell us whether for a fixed likelihood function one can improve the prior, or whether for a fixed prior, a certain type of changes to the likelihood function will actually be an improvement to the negative log likelihood. If the likelihood function is ignoring the latent variable, any prior would be optimal, and no transformation of that prior would result in a better likelihood function, which is what the statistics report. We stress that the exact numbers being reported matter little - a simple change in the sampling process for producing the {z1, · · · , zm} will immediately result in different numbers. However our experimentation with changing the sampling process resulted in essentially the same general conclusion we are deriving above.
Based on these experiments, we claim that the information theoretically motivated quantity log c(z) and its associated glossy statistics (15) do provide useful guidance in the modeling task, and given how easy they are to estimate, could be regularly checked by a modeling expert to gauge whether their model can be further improved by either improving the prior or the likelihood function.
5 CONCLUSIONS AND FUTURE WORK
In this article, we have established a fundamental relation between two fields of research - ratedistortion theory and latent variable modeling - and exploited this connection in order to derive a lower bound on the negative log likelihood that is applicable to the setting where the prior is optimized with a fixed likelihood function. We also showed that if a prior can be improved, then we can also keep the prior fixed and instead improve the likelihood function arriving to an identical negative log likelihood, thereby bringing additional relevance of rate-distortion theory to the general problem of latent variable modeling. We then argued through experimentation that statistics derived from this lower bound can give useful information on the optimality of a latent variable modeling.
Beyond the specific modeling problem identified in this paper, we believe we have given further ammunition to the assertion that rate-distortion theory could become a fundamental pillar in the theory of representation learning, adding evidence to previous successes such as the information bottleneck method. To further this assertion, we have turned our attention to using algorithmic ideas from lossy compression to model latent variables in a more principled way; this will be the subject of a future publication.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy. An information-theoretic analysis of deep latent-variable models. CoRR, abs/1711.00464, 2017. URL http://arxiv.org/abs/1711.00464.
T. Berger. Rate Distortion Theory: A Mathematical Basis for Data Compression. Prentice-Hall electrical engineering series. Prentice-Hall, 1971.
R. Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on Information Theory, 18(4):460­473, Jul 1972.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jo´zefowicz, and Samy Bengio. Generating sentences from a continuous space. In The SIGNLL Conference on Computational Natural Language Learning. 2016.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. In The International Conference on Learning Representations (ICLR). 2016.
FreyFaces. Frey Faces Data Set. URL https://cs.nyu.edu/~roweis/data/frey_ rawface.mat.
Luis Gonzalo Sa´nchez Giraldo and Jose´ C. Pr´incipe. Rate-distortion auto-encoders. CoRR, abs/1312.7381, 2013. URL http://arxiv.org/abs/1312.7381.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In The International Conference on Learning Representations (ICLR), Toulon. 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR). 2015.
D.P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In The International Conference on Learning Representations (ICLR), Banff. 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. In Science, volume 350, pp. 1332?1338. 2015.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS). 2011.
Benjamin Marlin, Kevin Swersky, Bo Chen, and Nando Freitas. Inductive principles for restricted boltzmann machine learning. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9, pp. 509­516, 2010.
Jason Tyler Rolfe. Discrete variational autoencoders. In The International Conference on Learning Representations (ICLR), Toulon. 2017.
C. E. Shannon. Coding theorems for a discrete source with a fidelity criterion. In IRE Nat. Conv. Rec., Pt. 4, pp. 142­163. 1959.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. CoRR, abs/1703.00810, 2017. URL http://arxiv.org/abs/1703.00810.
N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing, pp. 368­377. 1999.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop. 2015.
Jakub M. Tomczak and Max Welling. Improving variational auto-encoders using householder flow. Neural Information Processing Systems Workshop on Bayesian Deep Learning, 2016.
9

Under review as a conference paper at ICLR 2019 Jakub M. Tomczak and Max Welling. VAE with a VampPrior. In The 21nd International Conference
on Artificial Intelligence and Statistics. 2018. Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks.
In Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA. 2016.
10

