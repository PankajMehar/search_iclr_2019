Under review as a conference paper at ICLR 2019
DYNCNN: AN EFFECTIVE DYNAMIC ARCHITECTURE ON CONVOLUTIONAL NEURAL NETWORK FOR SURVEILLANCE VIDEOS
Anonymous authors Paper under double-blind review
ABSTRACT
The large-scale surveillance video analysis becomes important as the development of intelligent city. The heavy computation resources neccessary for state-of-theart deep learning model makes the real-time processing hard to be implemented. This paper exploits the characteristic of high scene similarity generally existing in surveillance videos and proposes dynamic convolution reusing the previous feature map to reduce the computation amount. We tested the proposed method on 45 surveillance videos with various scenes. The experimental results show that dynamic convolution can reduce up to 75.7% of FLOPs while preserving the precision within 0.7% mAP. Furthermore, the dynamic convolution can enhance the processing time up to 2.2 times.
1 INTRODUCTION
Nowadays, with the development of deep learning technologies, large-scale surveillance video analysis for intelligent city draws more and more attention in the real world applications, for instance, in person re-identification (Ahmed et al. (2015)), vehicle re-identification (Liu et al. (2016))(Shen et al. (2017)), pedestrian detection (Tian et al. (2014)) and crowd segmentation (Kang & Wang (2014)), etc. However, these deep learning methods are extremely computationally expensive--state-of-theart methods for object detection performed on a state-of-the-art NVIDIA P100 GPU run at 10-80 frames per second. Although processing on one video can be done by utilizing one to several topperforming hardware equipments, it still a challenge for large-scale actual deployment--it would cost over 5 billion USD in hardware to analyze over 4 million CCTVs in real time in the UK alone when considering these computational overheads in context (News (2015)).
Those real world applications for intelligent city employed the state-of-the-art network architectures from the ILSVRC competition, e.g. VGG (Simonyan & Zisserman (2014)), GoogLeNet (Szegedy et al. (2014)), and ResNet (He et al. (2015)) as their feature extraction architecture. However, these powerful networks tend to be resource-hungry models with high computational demands at inference time to win the competition--processing an one-minute surveillance video through VGG16 network costs 23 TFLOPs. To make the feature extraction architectures more effective in terms of surveillance video, this paper aims to explore ways to reduce the calculation on feature extraction.
Figure 1 shows two frame clips of a surveillance video with its corresponding feature map through 3×3 kernel. It is observed that most of the scene between two adjacent frames in the surveillance video are almost the same with merely about 2% to 15% difference. Due to the linear characteristic of convolution, the derived feature maps preserve the similarity of input video frames--it can reduce significant amount of calculation if the previous feature map can be reused for the current feature map. Suppose that the frame size is 512×512 while the amount of the changing pixels in feature map is 52k, the calculation can be reduced from 7 MFLOPs to 1.4 MFLOPs, that is 5 times saving ratio.
In this paper, we exploit the key characteristic of surveillance video--high scene similarity--and propose a new architecture dynamic convolution which can be directly applied to off-the-shelf model without retraining the weights. Dynamic convolution can inspect the inter-frame variation and thus predict the changing pixels in each corresponding feature map. Accordingly, convolution performs only on those predicted parts to achieve less computation while not affecting the accuracy. Finally,
1

Under review as a conference paper at ICLR 2019
Figure 1: The subtle difference in the feature map.
we apply dynamic convolution to one of the state-of-the-art object detector Single Shot MultiBox Detection (Liu et al. (2015)) and test on several surveillance video datasets including overall 45 videos. In addition, we conduct a further analysis in the FLOP computation and execution time of each convolutional layer to improve the understanding of dynamic convolution in practice.
2 PRIOR WORKS
For the last several years, tuning deep learning neural architectures to strike an optimal balance between precision and processing time has been an area of active research. Many methods have been proposed to reduce storage space and computing resources and also made great progress from theoretical research to platform implementation. Basically, they can be divided into three major techniques, effective model design, pruning, and quantization. Effective model design MobileNets (Howard et al. (2017)) is a lightweight network architecture proposed by Google for mobile deployment. The main idea is to disassemble the original convolution calculation into two parts: depthwise convolutions and pointwise convolutions. Although the calculation amount on convolution can greatly be reduced, the precision would be decreased. In addition, the feature map accordingly increases. The increasing data transfer followed by feature map makes MobileNets hard to perform on GPU. To improve the precision, ShuffleNet (Zhang et al. (2017)) proposed two new strategies channel shuffle and pointwise group convolutions based on the concept that helps the information flowing across feature channels. On the other hand, Inception (Szegedy et al. (2016)) introduces bottleneck structure to approximate sparse structures into several dense sub-matrices to achieve more efficient use of computing resources. In the ultra-lightweight neural network SqueezeNet (Iandola et al. (2016)), the 1x1 and 3x3 convolution kernels are extensively used compared with Inception to achieve more compression on calculation amount. Pruning Besides of the model design, pruning is another efficient way to reduce storage space and computing resources by removing redundant parts of model. Li et al. (2016) proposed a method for cropping filters of convolutional layer. This method determines the importance of filter by the magnitude of the absolute sum of weights on the filter. After that, the model needs to be trained again. Without lossing greately precision, such method can effectively reduce the complexity of the model. However, heavy analysis on each convolution layer costs lots of time and may occur the concern of one-case model. Liu et al. (2017) used another factor, the scaling coefficient  in batch normalization, to determine the importance of the filter. Quantization Network weight sharing quantization is also an important type of network compression method which focuses on the weight representation. The nature of this technique is to find the center value which can represent the original weight distribution by clustering method. Deep Compression proposed by Han et al. (2015) clusters the weights and replaces them by the center value of each cluster. The transformation between the weights and center values is stored in a codebook. Such transformation greatly compresses the weight representation from a 32-bit floating point number to a short bit number.
2

Under review as a conference paper at ICLR 2019
3 DYNAMIC CONVOLUTION
Dynamic convolution is an architecture which applies to each layer of convolutional neural network. Figure 2 illustrates a convolution neural network with our dynamic convolution, called dynamic convolutional neural network (DynCNN). The work flow of dynamic convolution consists of three main parts: 1) frame differencing; 2) prediction; 3) dyn-convolution. Let IDMt denote the input difference map of t-th frame (F ramet) derived by frame differencing and iDMt denote the inner difference map derived from the previous inner difference map or input difference map by prediction. Dyn-convolution preserves the feature map (FM) from the previous layer and selectively do convolution according to the position which the inner difference map indicates in order to update the feature map.

Figure 2: DynCNN Architecture

3.1 FRAME DIFFERENCING

The frame differencing method (Liu & Hou (2012)) is often used in the moving object detection and

segmentation methods. In this work, we employ it to inspect the inter-frame variation between two

adjacent frames. The basic concept of frame differencing is to subtract two frames to calculate the

difference of each pixel. However, in most surveillance video sequences there exist speckle noise

which severely affects the difference value. Accordingly, the difference map, as referred to input

difference map (IDM), is derived by thresholding the result of frame differencing with the following

statement:

D(i, j) =

1, |I(i, j)| < IDM 0, otherwise

(1)

where IDM denotes the threshold of the statement. The pixel at (i,j) is denoted as a changed pixel when D is determined as 1 and we called it changing point. I(i, j) in (1) represents the result of
frame differencing as the following expression:

I(i, j) = ICurr(i, j) - IP rev(i, j)

(2)

where ICurr(i, j) and IP rev(i, j) in (2) denote the pixel intensity at position (i, j) of the current and previous frame respectively.

The value of the threshold IDM is an important factor which determines the amount of convolution calculation. Therefore, the effect of the threshold on the speed and accuracy during the inference
phase will be well analyzed in Section 4.

3.2 PREDICTION
In the convolution process, there exists a diffusion effect in the result if the input is changed. Suppose that the kernel size of convolution is 3×3, each pixel of the result is determined by the corresponding

3

Under review as a conference paper at ICLR 2019
9 pixels from the input--the pixel at the same position and its 8 peripheral pixels. On the contrary, when a certain pixel in the input is changed, 9 pixels in the output which the pixel involves will be affected simultaneously and are denoted as impacted pixels. Based on the characteristic of diffusion effect in convolution process, the position of impacted pixels in each feature map can be predicted by using dilation operation and recorded on the inner difference map.
3.3 DYN-CONVOLUTION The iDM records the position of the impacted pixel of the FM and also implicates which pixel value of the FM needs to be updated. To update the value, the pixels of the previous FM which contributes to it will be re-convoluted and are denoted as needed pixels while this process is called dyn-convolution. In this work, we employ the lastest cuDNN library developed by Nvidia to optimize the performance on calculation speed. However, the convolution calculation provided by the library is only allowed for contiguous blocks instead of discrete blocks with given position. Therefore, we create another memory space, denoted as new array, in GPU to store those needed pixels for the requirement of library.
3.4 PRACTICAL CONCERN AND IMPLEMENTATION In real surveillance video application, although the calculation reduction on convolution is the main concern of speeding up the overall processing time, the data transfer is another important factor which contributes to the time. Dyn-convolution is intuitive but rude and also not efficient in GPU. Accordingly, we apply two strategies to improve the overall processing time, cell-based partition and continuous convolution. Cell-based Partition Compared with pixel-based map iDM, each FM is divided into several block cells. If any impacted pixel exists in the cell, the cell will be noted as "impacted" and recorded on cell difference map (CDM). The dyn-convolution thus performs according to the cell difference map rather than the pixel difference map--the new array stores the needed pixels of impacted cells. Figure 3 illustrates the difference between the naive method and the improved method.
Figure 3: The diagram of data transfer. In the pixel-based method, the amount of needed pixels for one cell (e.g. 2 × 2 pixels) is 4 × 9 while the cell-based method requires 4 × 4 needed pixels, which greatly reduces the amount of data transfer.
In this strategy, the data in new array may contains "non-needed" pixels when the cell involves nonimpacted pixels. In addition, new array performs unnecessary convolution on the boundary of two needed pixel blocks, where generates don't-care feature value x. Nevertheless, the needed pixels are reused to reduce the amount of data transfer and the characteristic of cuDNN library--convolution costs less processing time when stride is 1--is contemplated to speed up the convolution process, which indicates that less sacrification on computation while greatly improving the processing time. Continuous Convolution Another part which costs processing time is the layer number. It multiplies the data transfer on a new array. We partition whole convolution layer by pooling process and
4

Under review as a conference paper at ICLR 2019

group the convolution layers up, called convolution group. For each convolution group, the convolution structure is modified from "once padding before once convolution" to "continuous padding first then continuous convolution". Continuous convolution aims at directly updating the last layer of the convolution group rather than updating each layer to further reduce the amount of data transfer on new array. Although the updated value may be blemished by the don't-care value x in continuous convolution, it causes insignificant effect on the overall precision while significantly saving the processing time.

4 EXPERIMENTS

This section will evaluate the performance of the dynamic convolution by employing the Single Shot MultiBox Detector 512 × 512 model 1 (SSD512) on GTX1080 GPU with cuDNN v5.1.
Ground Truth. The existing benchmarks with ground truth for multi-object detection are designed to evaluate the performance of CNN model on still image evaluation, such as Microsoft COCO (Lin et al. (2014)) and PASCAL VOC datasets (Everingham et al. (2010)). However, such benchmarks tested on the surveillance video are not given the ground truth. To well evaluate our proposed dynamic convolution, the ground truth is defined as the detection result of SSD512 model on surveillance video. We choose VOC712Plus SSD 512x512 iter 240000--trained on VOC2007, VOC2012 and COCO, 240000 step iterations, 83.2% mAP on VOC2007 test--as the weight parameters.
Baseline. Dynamic convolution is a mechanism aimed at reducing the computation of original CNN model on surveillance video without affecting the performance. In the experiments, we choose the weight parameters VOC0712 SSD 512x512 iter 120000--trained on VOC2007 and VOC2012, 120000 step iterations, 79.8% mAP on VOC2007 test--as our baseline to evaluate our proposed method. The performance critiria is how close the precision is compared to the baseline.
Dataset. The dynamic convolution is tested on four surveillance video datasets: PETS2009 (Ferryman & Ellis (2014)), AVSS2007 (AVS (2007)), VIRAT (Oh et al. (2011)) and some videos from Youtube. In the surveillance video, person and car are the main objects which appear in the scene and are the most significant targets for surveillance application. Therefore, the performance in the experiments focuses on the objects of person and car. Table 1 lists the charascteristics of four surveillance video datasets, where the scene similarity ratio is defined as the ratio of the number of non-changing points to frame size.

Table 1: Comparison of characteristics of datasets

Resolution Avg. Person Height in Pixels Avg. Person to video height ratio
Avg. Car Height in Pixels Avg. Car to video height ratio Avg. Scene Similarity Ratio (IDM = 20)

PEST2009 768x576
75 13% 49 20% 96.6%

AVSS2007 720x576
193 34% 66 6% 97.4%

VIRAT 1920x1080
139 13% 104 10% 99.9%

Youtube 1280x720
149 21% 98 14% 97.4%

4.1 ON PETS 2009
The PETS 2009 dataset is a benchmark which focuses on several challenges for crowd analysis in public area including the estimation of crowd person count and density, tracking of individual(s) within a crowd, and detection of flow and crowd events. In this dataset, we choose 20 difference scene videos totally involving 4,884 photos as test data. Table 2 presents the result of the performance on objects person and car with the threshold of frame differencing setting at 20. It is clear to see that the proposed dynamic convolution can preserve the precision with respect to baseline, where the difference is only 0.06% and 0.21% on person and car respectively.
1available at https://github.com/weiliu89/caffe/tree/ssd
5

Under review as a conference paper at ICLR 2019

Table 2: Average precision results on PETS2009

Class-Person Baseline DynCNN
Class-Car Baseline DynCNN

TotalGT 40720 40720
TotalGT 4845 4845

TotalPred 39357 39227
TotalPred 5592 5540

TruePositives 35202 35047
TruePositives 4661 4642

FalsePositives 4155 4180
FalsePositives 931 898

AvgPrecision 80.54% 80.48%
AvgPrecision 90.22% 90.01%

4.2 ON AVSS 2007
AVSS2007 is a data set for evaluating the algorithm on event detection and tracking. In this dataset, we choose six videos, which contains public surveillance scene for train station and road located in the UK, as our test data including 35,000 images. Table 3 presents the result of the performance on objects person and car with the threshold of frame differencing setting at 20. It is clear to see that the proposed dynamic convolution can preserve the precision with respect to baseline where difference is only 0.16% and 0.42% on person and car respectively.

Table 3: Average precision results on AVSS2007

Class-Person Baseline DynCNN
Class-Car Baseline DynCNN

TotalGT 227232 227232
TotalGT 44275 44275

TotalPred 226448 222102
TotalPred 44682 44116

TruePositives 198876 191999
TruePositives 40485 39984

FalsePositives 27538 30067
FalsePositives 4195 4130

AvgPrecision 80.57% 79.89%
AvgPrecision 89.39% 89.23%

4.3 ON VIRAT
VIRAT dataset collects broad surveillance videos in terms of various realism scenes including ground camera videos and aerial videos. In this dataset, 10 different scene videos on parking lot, lane and plaza totally involving 63,947 photos are selected as test data. Table 4 presents the result of the performance on objects person and car with the threshold of frame differencing setting at 20. It is clear to see that the proposed dynamic convolution can preserve the precision with respect to baseline, where the difference is only 0.54% and 0.1% on person and car respectively.

Table 4: Average precision results on VIRAT

Class-Person Baseline DynCNN
Class-Car Baseline DynCNN

TotalGT 48514 48514
TotalGT 192489 192489

TotalPred 42154 39716
TotalPred 189550 186340

TruePositives 36895 34733
TruePositives 173042 169027

FalsePositives 5110 4843
FalsePositives 16491 17313

AvgPrecision 70.50% 69.96%
AvgPrecision 80.94% 80.84%

4.4 ON YOUTUBE
The collected surveillance videos on Youtube contain 9 scene videos with different viewing angles on the zoo, MRT station, campus and street involving 115,652 images. Table 5 presents the result of the performance on objects person and car with the threshold of frame differencing setting at 20. It is clear to see that the proposed dynamic convolution can preserve the precision with respect to baseline, where the difference is only 0.7% and 0.7% on person and car respectively.
6

Under review as a conference paper at ICLR 2019

Table 5: Average precision results on Youtube

Class-Person Baseline DynCNN
Class-Car
Baseline DynCNN

TotalGT 465470 465470
TotalGT
59867 59867

TotalPred 458898 449537
TotalPred
61832 61304

TruePositives 383821 372898
TruePositives
47447 46402

FalsePositives 75077 76639
FalsePositives
14385 14905

AvgPrecision 78.90% 78.20%
AvgPrecision
70.92% 70.22%

4.5 CONTINUOUS CONVOLUTION
Table 6 shows the average calculation process in each layer over all test videos of PETS 2009 dataset including the DynCNN with and without continuous convolution. The fifth and eighth column represents the new array size of dyn-convolution and the horizontal lines represent the pooling process which is used to delimit the convolution groups. As the layer more deeper, the pruned ratio generally decreases due to the increasing size of needed pixels. Note that the increasing behavior of pruned ratio in each convolution group under DynCNN (w/). In continuous convolution, the inner difference map of the first layer of convolution group directly predicts the impacted pixels of the last layer in advance, which results in that the needed pixels on the new array is more than DynCNN (w/o) and decreases gradually as layer deeper. This phenomenon can also be found in other datasets as shown in Appendix 7.4. Table 7 summarizes the average calculation amount and processing time per frame on each dataset. From this table, although DynCNN (w/o) can prune more calculation amount than DynCNN (w/), DynCNN (w/o) needs to cost more processing time on data transfer. The improvement of the overall processing time is not significant and even invalid in dataset AVSS2007 with respect to baseline. Therefore, the strategy of continuous convolution is demonstrated that can make DynCNN more valuable at the practical application as mentioned in Subsection 3.4.

Table 6: Average FLOPs result on PETS2009

Layer-type
Conv1-1 Conv1-2 Conv2-1 Conv2-2 Conv3-1 Conv3-2 Conv3-3 Conv4-1 Conv4-2 Conv4-3 Conv5-1 Conv5-2 Conv5-3 FC6 FC7
Other Total

Maps
64 64 128 128 256 256 256 512 512 512 512 512 512 1024

Baseline wxh FLOPs 512 x 512 4.5E+08 512 x 512 9.6E+09 256 x 256 4.8E+09 256 x 256 9.6E+09 128 x 128 4.8E+09 128 x 128 9.6E+09 128 x 128 9.6E+09 64 x 64 4.8E+09 64 x 64 9.6E+09 64 x 64 9.6E+09 32 x 32 2.4E+09 32 x 32 2.4E+09 32 x 32 2.4E+09
1 1.3E+08 1.07E+10 9.05e+10

DynCNN (w/o) wxh FLOPs %Pruned 1456x16 4.0E+07 91.1% 1572x16 9.2E+08 90.4% 978x8 5.9E+08 87.7% 1044x8 1.23E+09 87.2% 389x8 9.1E+08 81.0% 421x8 1.97E+09 79.5% 453x8 2.13E+09 77.8% 181x8 1.69E+09 64.8% 196x8 3.66E+09 61.9% 212x8 3.96E+09 58.8% 84x8 1.54E+09 35.8% 89x8 1.64E+09 31.7% 95x8 1.75E+09 27.1% 1024 1.3E+08 0%
1.07E+10 0% 3.29e+10 63.7%

DynCNN (w/) wxh FLOPs %Pruned 1704x18 5.3E+07 88.2% 1702x16 1.0E+09 89.4% 1085x10 8.0E+08 83.2% 1083x8 1.3E+09 86.5% 536x12 1.9E+09 59.8% 534x10 3.2E+09 66.5% 532x8 2.5E+09 73.3% 264x12 3.7E+09 22.3% 262x10 6.1E+09 35.6% 260x8 4.9E+09 48.9% 118x12 3.3E+09 -39.1% 116x10 2.7E+09 -13.7% 114x8 2.1E+09 10.4% 1024 1.3E+08 0%
1.07E+10 0% 4.47e+10 50.5%

Table 7: Overview results on all datasets

Dataset
PETS2009 AVSS2007
VIRAT Youtube

Baseline FLOPs Time(ms) 9.05E+10 30.7 9.05E+10 30.7 9.05E+10 30.7 9.05E+10 30.7

DynCNN (w/o)

FLOPs %Pruned Time(ms)

3.29E+10 63.7%

28.3

3.91E+10 56.8%

32.1

1.5E+10 83.3%

20.3

2.46E+10 72.8%

27.5

DynCNN (w/)

FLOPs %Pruned Time(ms)

4.47E+10 50.5%

19.2

5.36E+10 40.8%

22.9

2.2E+10 75.7%

13.9

4.47E+10 50.6%

20

7

Under review as a conference paper at ICLR 2019
5 DISCUSSION
5.1 THRESHOLD OF FRAME DIFFERENCING The threshold of IDM is used to filter the speckle noise in video. In addition, it can greatly speed up the processing time due to the high scene similarity of video--most difference values derived from frame differencing are introduced by noise. For various surveillance videos, the threshold of IDM should be different, resulting in a single threshold is difficult to be applicable to all surveillance images. To find the best threshold, we conduct a research on the relationship between the precision and calculation amount over different thresholds. Figure 4 shows the test result on 4 datasets.
(a) (b) Figure 4: The average precision and calculation amount on 4 dataset over different threshold of input difference map It is observed from Fig. 4a that the calculation amount exists a significant drop in the threshold interval of 5 to 15 over 4 datasets, which indicates the threshold greater or equal to 15 can filter the speckle noise efficiently in four datasets. However, the precision will be affected as the threshold is greater or equal to 25 as shown in Fig. 4b. Therefore, by trading the calculation amount off against precision, 20 is the best choice of threshold applicable to various surveillance videos and is selected as the default in experiments. 5.2 THE EFFECT ON SCENE SIMILARITY
(a) (b) Figure 5: The distribution of each test surveillance videos In Fig. 5, we reveal the distribution of each test surveillance videos on calculation amount and FPS with respect to the average scene similarity. The triangle mark denotes the average of videos in the same dataset while the purple line represents the value of baseline. It is obvious that the higher the
8

Under review as a conference paper at ICLR 2019
average scene similarity the more the calulation amount decreases. In addition, the FPS also tends to increase as the average scene similarity is higher, especially in the dataset VIRAT. In dataset VIRAT, the most scene of surveillance videos is parking lot where most of time are the still scene, resulting in 99% average scene similarity. The high scene similarity prunes the calculation amount of baseline model up to 75.7% (90.5 GFLOPs to 22.0 GFLOPs) and improves 2.2 times (32.5 to 71.7) of FPS. From the distribution map shown in Fig. 5, it is implied that the surveillance videos generally exist high scene similarity. Furthermore, the proposed dynamic convolution is applicable to all the videos. Therefore, it is believed that dynamic convolution can be applied to the practical applications on surveillance videos.
6 CONCLUSION
This paper exploits the high scene similarity generally existing in surveillance videos and proposes a dynamic convolution to reduce the calculation amount efficiently for heavy-computational surveillance analyses. The dynamic convolution only updates the feature value for the position which is predicted as changed to achieve the reuse of previous feature map. Besides the calculation amount, dynamic convolution also considers the importance of the processing time on pratical applications and employs two strategies to optimize the processing time by sacrificing a few calculation amounts. Compared with the existing technology, the it can directly be applied to the existing convolution neural network architecture without retraining and analyzing weights. Experimental results on four famous and authoritative datasets of surveillance videos provides a powerful evident that the proposed method is applicable to most of surveillance videos. The reduction on the calculation amount can be up to 75.7% while the precision preserves within 0.7% mAP. Furthermore, the processing time can be enhanced up to 2.2 times with respect to baseline. Finally, the aspect of design in the proposed method is completely different from the existing relevant methods. In the future work, we will complement our work with the existing relevent methods to verify the possibility of further acceleration.
REFERENCES
Fourth IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2007, 5-7 September, 2007, Queen Mary, University of London, London, United Kingdom, 2007. IEEE Computer Society.
Ejaz Ahmed, Michael Jones, and Tim K. Marks. An improved deep learning architecture for person re-identification. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3908­3916, June 2015.
Mark Everingham, Luc Gool, Christopher K. Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. Int. J. Comput. Vision, 88(2):303­338, June 2010. ISSN 0920-5691. doi: 10.1007/s11263-009-0275-4. URL http://dx.doi.org/10. 1007/s11263-009-0275-4.
James Ferryman and Anna-Louise Ellis. Performance evaluation of crowd image analysis using the pets2009 dataset. Pattern Recogn. Lett., 44(C):3­15, July 2014. ISSN 0167-8655. doi: 10.1016/j. patrec.2014.01.005. URL http://dx.doi.org/10.1016/j.patrec.2014.01.005.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015. URL http://arxiv.org/abs/1510.00149.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385. Published in Proc. CVPR, 2016.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. CoRR, abs/1704.04861, 2017. URL http://arxiv.org/abs/ 1704.04861.
9

Under review as a conference paper at ICLR 2019
Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. CoRR, abs/1602.07360, 2016. URL http://arxiv.org/abs/1602.07360.
Kai Kang and Xiaogang Wang. Fully convolutional neural networks for crowd segmentation. CoRR, abs/1411.4464, 2014. URL http://arxiv.org/abs/1411.4464.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. CoRR, abs/1608.08710, 2016. URL http://arxiv.org/abs/1608. 08710.
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/ 1405.0312.
Honghai Liu and Xianghua Hou. Moving detection research of background frame difference based on gaussian model. In 2012 International Conference on Computer Science and Service System, pp. 258­261, Aug 2012.
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. SSD: single shot multibox detector. CoRR, abs/1512.02325, 2015. URL http://arxiv.org/abs/1512.02325.
Xinchen Liu, Wu Liu, Tao Mei, and Huadong Ma. A deep learning-based approach to progressive vehicle re-identification for urban surveillance. In Computer Vision ­ ECCV 2016, pp. 869­884, Cham, 2016. Springer International Publishing.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. CoRR, abs/1708.06519, 2017. URL http://arxiv.org/abs/1708.06519.
BBC News. CCTV: Too many cameras useless, warns surveillance watchdog Tony Porter. https: //www.bbc.com/news/uk-30978995, January 2015. Accessed: 2018-08-21.
Sangmin Oh, A. Hoogs, A. Perera, N. Cuntoor, Chia-Chih Chen, Jong Taek Lee, S. Mukherjee, J. K. Aggarwal, Hyungtae Lee, L. Davis, E. Swears, Xioyang Wang, Qiang Ji, K. Reddy, M. Shah, C. Vondrick, H. Pirsiavash, D. Ramanan, J. Yuen, A. Torralba, Bi Song, A. Fong, A. RoyChowdhury, and M. Desai. A large-scale benchmark dataset for event recognition in surveillance video. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '11, pp. 3153­3160, Washington, DC, USA, 2011. IEEE Computer Society.
Yantao Shen, Tong Xiao, Hongsheng Li, Shuai Yi, and Xiaogang Wang. Learning deep neural networks for vehicle re-id with visual-spatio-temporal path proposals. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 1918­1927, Oct 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556. Published in Proc. ICLR, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842. Published in Proc. CVPR, 2015.
Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR, abs/1602.07261, 2016. URL http: //arxiv.org/abs/1602.07261.
Yonglong Tian, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Pedestrian detection aided by deep learning semantic tasks. CoRR, abs/1412.0069, 2014. URL http://arxiv.org/abs/ 1412.0069. Published in Proc. CVPR, 2015.
10

Under review as a conference paper at ICLR 2019
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. CoRR, abs/1707.01083, 2017. URL http: //arxiv.org/abs/1707.01083.
7 APPENDIX
7.1 DYNAMIC CONVOLUTION MODEL The proposed dynamic convolution is implemented on SSD512 model by caffe framework. Figure 6 and 7 illustrate the first convolution group (gp1) of SSD512 model and that with dynamic convolution (DynSDD512) respectively. In the implementation of dynamic convolution, the original model is combined with three additional layers: 1) Comparison Layer; 2) Position Layer; 3) Recovery Layer. Comparison layer is used to store the current frame and output input difference map (IDM) by comparing with the previous frame. After recieving the IDM, position layer will output the cell index table (CIT), for the position indication, the inner difference table (iDM) and the new array (NA) to next stage. Position layer generates the iDM by directly predicting the impacted pixels of the last layer in gp1 for the continuous convoltion as mentioned in subsection 3.4. Accordingly, the NA is generated according to the iDM through cell-based partition. Finally, recovery layer recieves the feature value from convolution and transfer those value where they locates on feature map through CIT.
Figure 6: SSD512 model
Figure 7: DynSSD512 model
11

Under review as a conference paper at ICLR 2019

7.2 GPU EXECUTION TIME
For a better understanding of the proposed method, we list the average execution time of each layer in detail. Take the PETS2009 dataset for example.

Table 8: Average execution time per layer

Layer-type Compare Position1 Conv1-1 Relu1-1 Conv1-2 Relu1-2 Recovery1 Group1 Position2 Conv2-1 Relu2-1 Conv2-2 Relu2-2 Recovery2 Group2 Position3 Conv3-1 Relu3-1 Conv3-2 Relu3-2 Conv3-3 Relu3-3 Recovery3 Group3 Position4 Conv4-1 Relu4-1 Conv4-2 Relu4-2 Conv4-3 Relu4-3 Recovery4 Group4 Position5 Conv5-1 Relu5-1 Conv5-2 Relu5-2 Conv5-3 Relu5-3 Recovery5 Group5 FC6 FC7
Other

Baseline (ms) -
1.15 0.58 2.53 0.58
4.84
1.2 0.58 1.73 0.29 3.52 0.88 0.15 1.49 0.15 1.49 0.15 4.3 0.85 0.076 2.86 0.078 2.81 0.078 6.75 0.78 0.014 0.77 0.014 0.77 0.014 2.36 -

DynCNN (ms) 0.043 0.18 0.13 0.075 0.39 0.062 0.036 0.92 0.27 0.32 0.062 0.46 0.042 0.047 1.19 0.39 0.60 0.059 0.76 0.048 0.42 0.04 0.078 2.4 0.63 0.86 0.057 1.87 0.046 1.51 0.036 0.16 5.17 1.08 1.28 0.026 0.99 0.019 0.78 0.013 0.13 4.31 -

Saved Time%
88.2% 87.0% 84.5% 89.4%
80.9%
73.2% 89.4% 73.5% 85.7%
65.9%
31.5% 60.4% 49.1% 67.0% 71.8% 72.7%
44.1%
-1.6% 25.2% 34.6% 41.0% 46.2% 53.5%
23.3%
-64.5% -80.9% -28.4% -37.3% -0.9% 5.6%
-82.6% 0% 0%

12

Under review as a conference paper at ICLR 2019 7.3 EXAMPLE DETECTIONS

(a) Baseline on PETS2009

(b) DynCNN on PETS2009

(c) Baseline on AVSS2007

(d) DynCNN on AVSS2007

(e) Baseline on VIRAT

(f) DynCNN on VIRAT

(g) Baseline on YouTube

(h) DynCNN on YouTube

Figure 8: Example detections on 4 test datasets

13

Under review as a conference paper at ICLR 2019

7.4 CALCULATION RESULTS ON OTHER DATASETS

Table 9: Average FLOPs result on AVSS2007

Layer-type
Conv1-1 Conv1-2 Conv2-1 Conv2-2 Conv3-1 Conv3-2 Conv3-3 Conv4-1 Conv4-2 Conv4-3 Conv5-1 Conv5-2 Conv5-3 FC6 FC7
Other Total

Maps
512 x 512 512 x 512 256 x 256 256 x 256 128 x 128 128 x 128 128 x 128 64 x 64 64 x 64 64 x 64 32 x 32 32 x 32 32 x 32
1

Baseline wxh FLOPs 64 4.5E+08 64 9.6E+09 128 4.8E+09 128 9.6E+09 256 4.8E+09 256 9.6E+09 256 9.6E+09 512 4.8E+09 512 9.6E+09 512 9.6E+09 512 2.4E+09 512 2.4E+09 512 2.4E+09 1024 1.3E+08
1.07E+10 9.05e+10

DynCNN (w/o) wxh FLOPs %Pruned 1704 x 16 4.7E+07 89.6% 1916 x 16 1.1E+09 88.2% 1276 x 8 7.5E+08 84.4% 1385 x 8 1.6E+09 83.0% 532 x 8 1.2E+09 74.0% 578 x 8 2.7E+09 71.7% 619 x 8 2.9E+09 69.7% 236 x 8 2.2E+09 54.2% 254 x 8 4.7E+09 50.5% 273 x 8 5.1E+09 46.8% 99 x 8 1.8E+09 23.8% 104 x 8 1.8E+09 20.0% 108 x 8 2.0E+09 16.7% 1024 1.3E+08 0%
1.07E+10 0% 3.91e+10 56.8%

DynCNN (w/)

wxh FLOPs %Pruned

1931 x 18 6.3E+07 85.9%

1929 x 16 1.2E+09 87.5%

1507 x 10 1.1E+09 77.1%

1505 x 8 2.2E+09 77.0%

753 x 12 2.6E+09 44.6%

751 x 10 4.4E+09 53.9%

749 x 8 3.5E+09 63.2%

377 x 12 4.7E+09 1.7%

375 x 10 7.8E+09 18.5%

373 x 8 6.2E+09 35.3%

129 x 12 3.5E+09 -48.7%

127 x 10 2.9E+09 -22.1%

125 x 8 2.3E+09 4.2%

1024 1.3E+08

0%

1.07E+10 0%

5.36e+10 40.8%

Table 10: Average FLOPs result on VIRAT

Layer-type
Conv1-1 Conv1-2 Conv2-1 Conv2-2 Conv3-1 Conv3-2 Conv3-3 Conv4-1 Conv4-2 Conv4-3 Conv5-1 Conv5-2 Conv5-3 FC6 FC7
Other Total

Maps
512 x 512 512 x 512 256 x 256 256 x 256 128 x 128 128 x 128 128 x 128
64 x 64 64 x 64 64 x 64 32 x 32 32 x 32 32 x 32
1

Baseline wxh FLOPs 64 4.5E+08 64 9.6E+09 128 4.8E+09 128 9.6E+09 256 4.8E+09 256 9.6E+09 256 9.6E+09 512 4.8E+09 512 9.6E+09 512 9.6E+09 512 2.4E+09 512 2.4E+09 512 2.4E+09 1024 1.3E+08
1.07E+10 9.05e+10

DynCNN (w/o))

wxh FLOPs %Pruned

120x16 3.2E+06 99.3%

138x16 8.0E+07 99.2%

99x8 5.7E+07 98.8%

110x8 1.3E+08 98.7%

54x8 1.2E+08 97.5%

60x8 2.7E+08 97.2%

67x8 3.1E+08 96.8%

36x8 3.2E+08 93.3%

40x8 7.2E+08 92.5%

45x8 8.1E+08 91.6%

26x8 4.5E+08 81.1%

28x8 4.9E+08 79.6%

31x8 5.4E+08 77.5%

1024 1.3E+08

0%

1.07E+10 0%

1.5e+10 83.3%

DynCNN (w/) wxh FLOPs %Pruned 310x18 9.6E+06 97.8% 308x16 1.8E+08 98.1% 188x10 1.4E+08 97.1% 186x8 2.2E+08 97.7% 129x12 4.5E+08 90.6% 127x10 7.4E+08 92.3% 125x8 5.8E+08 93.9% 90x12 1.2E+09 74.1% 88x10 2.1E+09 78.1% 86x8 1.5E+09 83.7% 60x12 1.6E+09 32.1% 58x10 1.3E+09 45.8% 56x8 1.1E+09 58.3% 1024 1.3E+08 0%
1.07E+10 0% 2.2e+10 75.7%

Table 11: Average FLOPs result on YouTube

Layer-type
Conv1-1 Conv1-2 Conv2-1 Conv2-2 Conv3-1 Conv3-2 Conv3-3 Conv4-1 Conv4-2 Conv4-3 Conv5-1 Conv5-2 Conv5-3 FC6 FC7
Other Total

Maps
512 x 512 512 x 512 256 x 256 256 x 256 128 x 128 128 x 128 128 x 128
64 x 64 64 x 64 64 x 64 32 x 32 32 x 32 32 x 32
1

Baseline wxh FLOPs 64 4.5E+08 64 9.6E+09 128 4.8E+09 128 9.6E+09 256 4.8E+09 256 9.6E+09 256 9.6E+09 512 4.8E+09 512 9.6E+09 512 9.6E+09 512 2.4E+09 512 2.4E+09 512 2.4E+09 1024 1.3E+08
1.07E+10 9.05e+10

DynCNN (w/o) wxh FLOPs %Pruned 742x16 2.0E+07 95.6% 845x16 5.0E+08 94.8% 582x8 3.4E+08 92.9% 637x8 7.5E+08 92.2% 251x8 5.8E+08 87.9% 272x8 1.3E+09 86.8% 293x8 1.4E+09 85.7% 113x8 1.0E+09 78.3% 122x8 2.3E+09 76.5% 131x8 2.4E+09 74.7% 55x8 1.0E+09 58.3% 59x8 1.1E+09 55.4% 62x8 1.1E+09 52.9% 1024 1.3E+08 0%
1.07E+10 0% 2.46e+10 72.8%

DynCNN (w/) wxh FLOPs %Pruned 1702x18 5.3E+07 88.2% 1700x16 1.0E+09 89.6% 1205x10 8.8E+08 81.7% 1203x8 1.6E+09 82.9% 571x12 2.0E+09 58.1% 569x10 3.3E+09 65.2% 567x8 2.6E+09 72.3% 265x12 3.7E+09 22.5% 263x10 6.1E+09 35.9% 261x8 4.9E+09 49.1% 110x12 3.0E+09 -27.9% 108x10 2.5E+09 -4.6% 106x8 1.9E+09 17.9% 1024 1.3E+08 0%
1.07E+10 0% 4.47e+10 50.6%

14

